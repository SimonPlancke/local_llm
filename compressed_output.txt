<?xml version='1.0' encoding='utf-8'?>
<source type="github_repository" url="https://github.com/SimonPlancke/local_llm"><file name="README.md"># local_llm</file><file name="answer.txt">text='python sportcompetition program following potential shortcomings: 1. lack input validation: program perform input validation checks, could lead unexpected behavior errors input data expected format contains invalid values. 2. magic numbers: program contains several hard-coded "magic numbers" used calculations conditional statements. numbers defined constants variables descriptive names improve readability maintainability. 3. code duplication: program contains several instances duplicate code, could refactored reusable functions methods improve maintainability reduce risk bugs. 4. error handling: program handle errors gracefully could crash produce unexpected results error occurs. implementing proper error handling logging mechanisms could help identify address errors efficiently. 5. scalability: program designed handle large datasets, could impact performance efficiency. implementing optimizations best practices could improve scalability performance. 6. testing: program appear testing implemented, could lead undetected bugs errors. implementing testing validation checks could improve reliability robustness program. 7. code documentation: program lacks documentation, making difficult understand purpose, functionality, implementation. adding comments documentation could improve readability maintainability. 8. code organization: program could benefit better organization structure, breaking smaller functions modules, make easier understand, maintain, extend.'</file><file name="debug.py">content = '/tree/thisworks?' content.startswith(('/downloads', '/tree', '/uptodate')): pdf_url = 'https://sci-hub.se' + content else: pdf_url = 'https:/' + content print(pdf_url)</file><file name="requirements.txt">langchain==0.3.13 langchain_community weaviate-client&gt;=3.26.7,&lt;4.0.0 protobuf==3.20 huggingface-hub faiss-cpu sentence-transformers langchain_huggingface==0.1.2</file><file name="src/python/create_text_file/create_text_file.py">import sys urllib.parse import urlparse import tiktoken import import pyperclip # import functions library rich (https://github.com/textualize/rich) rich import print rich.console import console rich.panel import panel rich.text import text rich.prompt import prompt rich.style import style rich.syntax import syntax rich.traceback import install rich.progress import progress, textcolumn, barcolumn, timeremainingcolumn # repository classes scripts git_methods import gitmethods import generic_functions generic_functions file_processing import textfilemethods, pdffilemethods, foldermethods, transcriptionmethods def safe_file_read(filepath, fallback_encoding:str='latin1') -&gt; str: """ safely reads content file, attempting handle encoding issues. function first tries read file using utf-8 encoding. unicodedecodeerror occurs, attempts read file using specified fallback encoding. parameters: ---------- fallback_encoding : str, optional encoding use file cannot read utf-8. default 'latin1'. returns: ------- str content file string. raises: ------ filenotfounderror specified file exist. ioerror error reading file (e.g., permission issues). """ try: open(filepath, "r", encoding='utf-8') file: return file.read() except unicodedecodeerror: open(filepath, "r", encoding=fallback_encoding) file: return file.read() def get_token_count(text, disallowed_special=[], chunk_size=1000): enc = tiktoken.get_encoding("cl100k_base") # remove xml tags text_without_tags = re.sub(r'&lt;[^&gt;]+&gt;', '', text) # split text smaller chunks chunks = [text_without_tags[i:i+chunk_size] range(0, len(text_without_tags), chunk_size)] total_tokens = 0 chunk chunks: tokens = enc.encode(chunk, disallowed_special=disallowed_special) total_tokens += len(tokens) return total_tokens def main(): console = console() intro_text = text("\ninput paths urls processed:\n", style="dodger_blue1") input_types = [ (" local folder path (flattens files text)", "bright_white"), (" github repository url (flattens files text)", "bright_white"), (" github pull request url (pr + repo)", "bright_white"), (" github issue url (issue + repo)", "bright_white"), (" documentation url (base url)", "bright_white"), (" youtube video url (to fetch transcript)", "bright_white"), (" arxiv paper url", "bright_white"), (" doi pmid search sci-hub", "bright_white"), ] input_type, color input_types: intro_text.append(f"\n{input_type}", style=color) intro_panel = panel( intro_text, expand=false, border_style="bold", title="[bright_white]copy file clipboard[/bright_white]", title_align="center", padding=(1, 1), ) console.print(intro_panel) # argument passed, use it. otherwise, prompt user pass argument len(sys.argv) &gt; 1: input_path = sys.argv[1] else: input_path = prompt.ask("\n[bold dodger_blue1]enter path url[/bold dodger_blue1]", console=console) console.print(f"\n[bold bright_green]you entered:[/bold bright_green] [bold bright_yellow]{input_path}[/bold bright_yellow]\n") output_file = "./uncompressed_output.txt" processed_file = "./compressed_output.txt" # urls_list_file = "processed_urls.txt" progress( textcolumn("[bold bright_blue]{task.description}"), barcolumn(bar_width=none), timeremainingcolumn(), console=console, ) progress: task = progress.add_task("[bright_blue]processing...", total=100) # parse input path call correct function try: # git functions "github.com" input_path: gitmethods_object = gitmethods(url=input_path) final_output = gitmethods_object.handle_git_url() # url functions elif urlparse(input_path).scheme ["http", "https"]: "youtube.com" input_path "youtu.be" input_path: final_output = transcriptionmethods.fetch_youtube_transcript(input_path) elif "arxiv.org" input_path: final_output = pdffilemethods.process_arxiv_pdf(input_path) # else: # crawl_result = crawl_and_extract_text(input_path, max_depth=2, include_pdfs=true, ignore_epubs=true) # final_output = crawl_result['content'] # open(urls_list_file, 'w', encoding='utf-8') urls_file: # urls_file.write('\n'.join(crawl_result['processed_urls'])) # scientific papers https://sci-hub.se/ elif input_path.startswith("10.") "/" input_path input_path.isdigit(): final_output = pdffilemethods.process_doi_or_pmid(input_path) # local folder else: final_output = foldermethods.process_local_folder(filepath=input_path) progress.update(task, advance=50) # write uncompressed output open(output_file, "w", encoding="utf-8") file: file.write(final_output) # process compressed output textfilemethods.parse_text_as_xml(output_file, processed_file) progress.update(task, advance=50) compressed_text = safe_file_read(processed_file) compressed_token_count = get_token_count(compressed_text) console.print(f"\n[bright_green]compressed token count:[/bright_green] [bold bright_cyan]{compressed_token_count}[/bold bright_cyan]") uncompressed_text = safe_file_read(output_file) uncompressed_token_count = get_token_count(uncompressed_text) console.print(f"[bright_green]uncompressed token count:[/bright_green] [bold bright_cyan]{uncompressed_token_count}[/bold bright_cyan]") console.print(f"\n[bold bright_yellow]{processed_file}[/bold bright_yellow] [bold bright_blue]{output_file}[/bold bright_blue] created working directory.") pyperclip.copy(uncompressed_text) console.print(f"\n[bright_white]the contents [bold bright_blue]{output_file}[/bold bright_blue] copied clipboard.[/bright_white]") except exception e: console.print(f"\n[bold red]an error occurred:[/bold red] {str(e)}") console.print("\nplease check input try again.") raise # re-raise exception debugging purposes __name__ == "__main__": main()</file><file name="src/python/create_text_file/file_processing.py">import import os import requests import wget generic_functions import get_stopword_list, escape_xml, is_allowed_filetype, process_ipynb_file, download_file git_methods import gitmethods import xml.etree.elementtree et bs4 import beautifulsoup pypdf2 import pdfreader youtube_transcript_api import youtubetranscriptapi youtube_transcript_api.formatters import textformatter class textfilemethods(): def __init__(self, filepath:str): self.filepath = filepath @staticmethod def clean_text(text): # remove new-line characters text = re.sub(r"[\n\r]+", "\n", text) # remove unwanted characters text = re.sub(r"[^a-za-z0-9\s_.,!?:;@#$%^&amp;*()+\-=[\]{}|\\&lt;&gt;`~'\"/]+", "", text) # normalize whitespace text = re.sub(r"\s+", " ", text) # convert lowercase text = text.lower() # split words words = text.split() # drop stop words (the, is, in...) stop_words = get_stopword_list() words = [word word words word stop_words] return " ".join(words) def parse_text_as_xml(self, input_file, output_file): open(input_file, "r", encoding="utf-8") input_file: input_text = input_file.read() try: # try parse input xml root = et.fromstring(input_text) # process text content preserving xml structure elem root.iter(): elem.text: elem.text = self.clean_text(elem.text) elem.tail: elem.tail = self.clean_text(elem.tail) # write processed xml output file tree = et.elementtree(root) tree.write(output_file, encoding="utf-8", xml_declaration=true) print("text preprocessing completed xml structure preserved.") except et.parseerror: # xml parsing fails, process text without preserving xml structure processed_text = self.clean_text(input_text) open(output_file, "w", encoding="utf-8") out_file: out_file.write(processed_text) warning("xml parsing failed. text preprocessing completed without xml structure.") class pdffilemethods(): def add_xml_tags_for_paper(self, text): paper_xml_formatted_text = '&lt;paper&gt;\n' paper_xml_formatted_text += escape_xml(' '.join(text)) paper_xml_formatted_text += '\n&lt;/paper&gt;\n' return paper_xml_formatted_text def process_pdf(self, url): # download pdf content response = requests.get(url) response.raise_for_status() content = response.content open('temp.pdf', 'wb') pdf_file: pdf_file.write(content) text = [] open('temp.pdf', 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) page range(len(pdf_reader.pages)): text.append(pdf_reader.pages[page].extract_text()) os.remove('temp.pdf') return ' '.join(text) def process_arxiv_pdf(self, arxiv_abs_url): pdf_url = arxiv_abs_url.replace("/abs/", "/pdf/") + ".pdf" text = self.process_pdf(pdf_url) formatted_text = f'&lt;source type="arxiv_paper" url="{arxiv_abs_url}"&gt;\n' formatted_text += self.add_xml_tags_for_paper(text) formatted_text += '&lt;/source&gt;' os.remove('temp.pdf') print("arxiv paper processed successfully.") return formatted_text def process_doi_or_pmid(self, identifier): headers = { 'user-agent': 'mozilla/5.0 (windows nt 6.3) applewebkit/537.36 (khtml, like gecko) chrome/98.0.4758.102 safari/537.36', 'connection': 'keep-alive' } try: payload = { 'sci-hub-plugin-check': '', 'request': identifier } base_url = 'https://sci-hub.se/' response = requests.post(base_url, headers=headers, data=payload, timeout=60) soup = beautifulsoup(response.content, 'html.parser') pdf_element = soup.find(id='pdf') pdf_element none: raise valueerror(f"no pdf found identifier {identifier}. sci-hub might inaccessible document available.") content = pdf_element.get('src').replace('#navpanes=0&amp;view=fith', '').replace('//', '/') content.startswith(('/downloads', '/tree', '/uptodate')): pdf_url = 'https://sci-hub.se' + content else: pdf_url = 'https:/' + content pdf_filename = f"{identifier.replace('/', '-')}.pdf" wget.download(pdf_url, pdf_filename) open(pdf_filename, 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) text = "" page range(len(pdf_reader.pages)): text += pdf_reader.pages[page].extract_text() formatted_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n' formatted_text += self.add_xml_tags_for_paper(text) formatted_text += '&lt;/source&gt;' os.remove(pdf_filename) print(f"identifier {identifier} processed successfully.") return formatted_text except (requests.requestexception, valueerror) e: error_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n' error_text += f'&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n' error_text += '&lt;/source&gt;' print(f"error processing identifier {identifier}: {str(e)}") print("sci-hub appears inaccessible document found. please try later.") return error_text class foldermethods(): def __init__(self, filepath): self.filepath = filepath def process_local_directory(self): content = [f'&lt;source type="local_directory" path="{escape_xml(self.local_path)}"&gt;'] root, files os.walk(self.local_path): file files: is_allowed_filetype(file): print(f"processing {os.path.join(root, file)}...") file_path = os.path.join(root, file) relative_path = os.path.relpath(file_path, self.local_path) content.append(f'&lt;file name="{escape_xml(relative_path)}"&gt;') file.endswith(".ipynb"): content.append(escape_xml(process_ipynb_file(file_path))) else: open(file_path, "r", encoding='utf-8', errors='ignore') f: content.append(escape_xml(f.read())) content.append('&lt;/file&gt;') content.append('&lt;/source&gt;') return '\n'.join(content) def process_local_folder(self, local_path): formatted_content = self.process_local_directory(local_path) print("all files processed.") return formatted_content def process_directory(self, url, output): headers = gitmethods.get_github_token() response = requests.get(url, headers=headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file) output.write(f"# {'-' * 3}\n") output.write(f"# filename: {file['path']}\n") output.write(f"# {'-' * 3}\n\n") file["name"].endswith(".ipynb"): output.write(process_ipynb_file(temp_file)) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: output.write(f.read()) output.write("\n\n") os.remove(temp_file) elif file["type"] == "dir": self.process_directory(file["url"], output) class transcriptionmethods(): def __init__(self, url): self.url = url def extract_video_id(self, url): pattern = r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\s+\/|(?:v|e(?:mbed)?)\/|\s*?[?&amp;]v=)|youtu\.be\/)([a-za-z0-9_-]{11})' match = re.search(pattern, url) match: return match.group(1) return none def fetch_youtube_transcript(self): video_id = self.extract_video_id(self.url) video_id: return f'&lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&gt;\n&lt;error&gt;could extract video id url.&lt;/error&gt;\n&lt;/source&gt;' try: transcript_list = youtubetranscriptapi.get_transcript(video_id) formatter = textformatter() transcript = formatter.format_transcript(transcript_list) formatted_text = f'&lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&gt;\n' formatted_text += '&lt;transcript&gt;\n' formatted_text += escape_xml(transcript) formatted_text += '\n&lt;/transcript&gt;\n' formatted_text += '&lt;/source&gt;' return formatted_text except exception e: return f'&lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&gt;\n&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n&lt;/source&gt;'</file><file name="src/python/create_text_file/generic_functions.py">import requests import os import nbconvert import pythonexporter import nbformat nltk.corpus import stopwords import nltk urllib.parse import urlparse def is_allowed_filetype(filename): allowed_extensions = ['.py', '.txt', '.js', '.tsx', '.ts', '.md', '.cjs', '.html', '.json', '.ipynb', '.h', '.localhost', '.sh', '.yaml', '.example', '.ps1', '.sql'] return any(filename.endswith(ext) ext allowed_extensions) def download_file(url, target_path, headers): response = requests.get(url, headers=headers) response.raise_for_status() open(target_path, "wb") f: f.write(response.content) def escape_xml(text): return ( str(text) .replace("&amp;", "&amp;amp;") .replace("&lt;", "&amp;lt;") .replace("&gt;", "&amp;gt;") # remove following lines stop converting apostrophes quotes # .replace("\"", "&amp;quot;") # .replace("'", "&amp;apos;") ) def process_ipynb_file(temp_file): open(temp_file, "r", encoding='utf-8', errors='ignore') f: notebook_content = f.read() exporter = pythonexporter() python_code, _ = exporter.from_notebook_node(nbformat.reads(notebook_content, as_version=4)) return python_code ## new: check def is_same_domain(base_url, new_url): return urlparse(base_url).netloc == urlparse(new_url).netloc def is_within_depth(base_url, current_url, max_depth): base_parts = urlparse(base_url).path.rstrip('/').split('/') current_parts = urlparse(current_url).path.rstrip('/').split('/') current_parts[:len(base_parts)] != base_parts: return false return len(current_parts) - len(base_parts) &lt;= max_depth def extract_links(input_file, output_file): url_pattern = re.compile(r'http[s]?://(?:[a-za-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fa-f][0-9a-fa-f]))+') open(input_file, 'r', encoding='utf-8') file: content = file.read() urls = re.findall(url_pattern, content) open(output_file, 'w', encoding='utf-8') output: url urls: output.write(url + '\n') def get_stopword_list(): nltk.download("stopwords", quiet=true) stop_words = set(stopwords.words("english")) return stop_words</file><file name="src/python/create_text_file/git_methods.py">import requests import os import dotenv import load_dotenv generic_functions import is_allowed_filetype, escape_xml, process_ipynb_file, download_file class gitmethods(): def __init__(self, url): self.url = url self.headers = self.get_github_token() @staticmethod def get_github_token(): load_dotenv() github_token = os.getenv('github_token') headers = {"authorization": f"token {github_token}"} return headers def process_github_main_branch(self, repo_owner, repo_name): repo_url = f"https://github.com/{repo_owner}/{repo_name}" repo_content = self.process_github_repo(repo_url) xml_formatted_repository = '&lt;repository&gt;\n' xml_formatted_repository += repo_content xml_formatted_repository += '&lt;/repository&gt;\n' xml_formatted_repository += '&lt;/source&gt;' return xml_formatted_repository def handle_git_url(self): "/pull/" self.url: final_output = self.process_github_pull_request() elif "/issues/" self.url: final_output = self.process_github_issue() else: final_output = self.process_github_repo() return final_output def process_git_directory(self, url, repo_content): response = requests.get(url, headers=self.headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file, self.headers) repo_content.append(f'&lt;file name="{escape_xml(file["path"])}"&gt;') file["name"].endswith(".ipynb"): repo_content.append(escape_xml(process_ipynb_file(temp_file))) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: repo_content.append(escape_xml(f.read())) repo_content.append('&lt;/file&gt;') os.remove(temp_file) elif file["type"] == "dir": self.process_git_directory(file["url"], repo_content) else: print(f"skipped {file}") def process_github_repo(self, base_url=none): # possible public repositories # pull repository locally use local path private base_url: base_url = self.url print(base_url) api_base_url = "https://api.github.com/repos" repo_url_parts = base_url.split("https://github.com/")[-1].split("/") repo_name = "/".join(repo_url_parts[:2]) # detect branch tag reference branch_or_tag = "" subdirectory = "" len(repo_url_parts) &gt; 2 repo_url_parts[2] == "tree": # branch tag name index 3 len(repo_url_parts) &gt; 3: branch_or_tag = repo_url_parts[3] # remaining parts branch/tag name form subdirectory len(repo_url_parts) &gt; 4: subdirectory = "/".join(repo_url_parts[4:]) contents_url = f"{api_base_url}/{repo_name}/contents" subdirectory: contents_url = f"{contents_url}/{subdirectory}" branch_or_tag: contents_url = f"{contents_url}?ref={branch_or_tag}" # configure variable xml-like structure # variable exported .txt file repo_content = [f'&lt;source type="github_repository" url="{base_url}"&gt;'] self.process_git_directory(contents_url, repo_content) repo_content.append('&lt;/source&gt;') print("all files processed.") return "\n".join(repo_content) def process_github_pull_request(self): url_parts = self.url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] pull_request_number = url_parts[-1] api_base_url = "https://api.github.com/repos" pull_request_url = f"{api_base_url}/{repo_owner}/{repo_name}/pulls/{pull_request_number}" #region pr: get changes comments # 1. get metadata related pr response = requests.get(pull_request_url, headers=self.headers) pull_request_data = response.json() # 2. get changes made repository diff_url = pull_request_data["diff_url"] diff_response = requests.get(diff_url, headers=self.headers) pull_request_diff = diff_response.text # 3. get pr-level comments (general comments left submitted pr) comments_url = pull_request_data["comments_url"] comments_response = requests.get(comments_url, headers=self.headers) comments_data = comments_response.json() # 4. get code-level comments (comments changed files) review_comments_url = pull_request_data["review_comments_url"] review_comments_response = requests.get(review_comments_url, headers=self.headers) review_comments_data = review_comments_response.json() # 5. sort comments position all_comments = comments_data + review_comments_data all_comments.sort(key=lambda comment: comment.get("position") float("inf")) #endregion #region pr: add xml elements # add opening element: pull_request_info formatted_text = f'&lt;source type="github_pull_request" url="{self.url}"&gt;\n' formatted_text += '&lt;pull_request_info&gt;\n' # add generic elements: title, description, merge_details formatted_text += f'&lt;title&gt;{escape_xml(pull_request_data["title"])}&lt;/title&gt;\n' formatted_text += f'&lt;description&gt;{escape_xml(pull_request_data["body"])}&lt;/description&gt;\n' formatted_text += '&lt;merge_details&gt;\n' formatted_text += f'{escape_xml(pull_request_data["user"]["login"])} wants merge {pull_request_data["commits"]} commit {repo_owner}:{pull_request_data["base"]["ref"]} {pull_request_data["head"]["label"]}\n' formatted_text += '&lt;/merge_details&gt;\n' # iteratively add pr changes comments left pr text fields # parent element: diff_and_comments formatted_text += '&lt;diff_and_comments&gt;\n' diff_lines = pull_request_diff.split("\n") comment_index = 0 line diff_lines: formatted_text += f'{escape_xml(line)}\n' comment_index &lt; len(all_comments) all_comments[comment_index].get("position") == diff_lines.index(line): comment = all_comments[comment_index] # comment element """ &lt;review_comment&gt; &lt;author&gt;xxx&lt;/author&gt; &lt;content&gt;xxx&lt;/content&gt; &lt;path&gt;xxx&lt;/path&gt; &lt;line&gt;xxx&lt;/line&gt; &lt;/review_comment&gt; """ formatted_text += f'&lt;review_comment&gt;\n' formatted_text += f'&lt;author&gt;{escape_xml(comment["user"]["login"])}&lt;/author&gt;\n' formatted_text += f'&lt;content&gt;{escape_xml(comment["body"])}&lt;/content&gt;\n' formatted_text += f'&lt;path&gt;{escape_xml(comment["path"])}&lt;/path&gt;\n' formatted_text += f'&lt;line&gt;{comment["original_line"]}&lt;/line&gt;\n' formatted_text += '&lt;/review_comment&gt;\n' comment_index += 1 # close open elements formatted_text += '&lt;/diff_and_comments&gt;\n' formatted_text += '&lt;/pull_request_info&gt;\n' # regular processing repo formatted_text += self.process_github_main_branch(repo_owner, repo_name) #endregion print(f"pull request {pull_request_number} repository content processed successfully.") return formatted_text def process_github_issue(self): url_parts = self.issue_url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] issue_number = url_parts[-1] api_base_url = "https://api.github.com/repos" issue_url = f"{api_base_url}/{repo_owner}/{repo_name}/issues/{issue_number}" #region issue: get metadata response = requests.get(issue_url, headers=self.headers) issue_data = response.json() comments_url = issue_data["comments_url"] comments_response = requests.get(comments_url, headers=self.headers) comments_data = comments_response.json() #endregion #region issue: add xml elements # add opening element: issue_info formatted_text = f'&lt;source type="github_issue" url="{self.issue_url}"&gt;\n' formatted_text += '&lt;issue_info&gt;\n' # add generic elements: title, description formatted_text += f'&lt;title&gt;{escape_xml(issue_data["title"])}&lt;/title&gt;\n' formatted_text += f'&lt;description&gt;{escape_xml(issue_data["body"])}&lt;/description&gt;\n' # iteratively add comments left pr text fields # parent element: comments formatted_text += '&lt;comments&gt;\n' comment comments_data: # comment element """ &lt;comment&gt; &lt;author&gt;xxx&lt;/author&gt; &lt;content&gt; &lt;code_snippet&gt;xxx&lt;/code_snippet&gt; &lt;/content&gt; &lt;/comment&gt; """ formatted_text += '&lt;comment&gt;\n' formatted_text += f'&lt;author&gt;{escape_xml(comment["user"]["login"])}&lt;/author&gt;\n' formatted_text += f'&lt;content&gt;{escape_xml(comment["body"])}&lt;/content&gt;\n' # extract code snippets comments github issue # regex: find occurrences urls comment body match pattern github file links line ranges code_snippets = re.findall(r'https://github.com/.*#l\d+-l\d+', comment['body']) snippet_url code_snippets: # split url two parts: base url line range. url_parts = snippet_url.split("#") # use file url get raw contents file try: file_url = url_parts[0].replace("/blob/", "/raw/") file_response = requests.get(file_url, headers=self.headers) file_content = file_response.text # using file content, get relevant code snippets back using start end indices line_range = url_parts[1] start_line, end_line = map(int, line_range.split("-")[0][1:]), map(int, line_range.split("-")[1][1:]) code_lines = file_content.split("\n")[start_line-1:end_line] code_snippet = "\n".join(code_lines) # format final code snippet using xml formatted_text += '&lt;code_snippet&gt;\n' formatted_text += f'&lt;![cdata[{code_snippet}]]&gt;\n' formatted_text += '&lt;/code_snippet&gt;\n' except requests.exceptions.requestexception e: warning(f"error fetching file: {e}") continue # skip next snippet there's error formatted_text += '&lt;/comment&gt;\n' # close open elements formatted_text += '&lt;/comments&gt;\n' formatted_text += '&lt;/issue_info&gt;\n' # regular processing repo formatted_text += self.process_github_main_branch(repo_owner, repo_name) #endregion print(f"issue {issue_number} repository content processed successfully.") return formatted_text</file><file name="src/python/create_text_file/onefilellm.py">import requests bs4 import beautifulsoup, comment urllib.parse import urljoin, urlparse pypdf2 import pdfreader import os import sys import tiktoken import nltk nltk.corpus import stopwords import pathlib import path import nbformat nbconvert import pythonexporter youtube_transcript_api import youtubetranscriptapi youtube_transcript_api.formatters import textformatter import pyperclip import wget tqdm import tqdm time import sleep rich import print rich.console import console rich.panel import panel rich.text import text rich.prompt import prompt rich.style import style rich.syntax import syntax rich.traceback import install rich.progress import progress, textcolumn, barcolumn, timeremainingcolumn import xml.etree.elementtree et dotenv import load_dotenv load_dotenv() github_token = os.getenv('github_token') def safe_file_read(filepath, fallback_encoding='latin1'): try: open(filepath, "r", encoding='utf-8') file: return file.read() except unicodedecodeerror: open(filepath, "r", encoding=fallback_encoding) file: return file.read() nltk.download("stopwords", quiet=true) stop_words = set(stopwords.words("english")) token = os.getenv('github_token', github_token) token == 'default_token_here': raise environmenterror("github_token environment variable set.") headers = {"authorization": f"token {token}"} def download_file(url, target_path): response = requests.get(url, headers=headers) response.raise_for_status() open(target_path, "wb") f: f.write(response.content) def is_allowed_filetype(filename): allowed_extensions = ['.py', '.txt', '.js', '.tsx', '.ts', '.md', '.cjs', '.html', '.json', '.ipynb', '.h', '.localhost', '.sh', '.yaml', '.example'] # allowed_extensions = ['.md'] return any(filename.endswith(ext) ext allowed_extensions) def process_ipynb_file(temp_file): open(temp_file, "r", encoding='utf-8', errors='ignore') f: notebook_content = f.read() exporter = pythonexporter() python_code, _ = exporter.from_notebook_node(nbformat.reads(notebook_content, as_version=4)) return python_code def process_directory(url, output): response = requests.get(url, headers=headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file) output.write(f"# {'-' * 3}\n") output.write(f"# filename: {file['path']}\n") output.write(f"# {'-' * 3}\n\n") file["name"].endswith(".ipynb"): output.write(process_ipynb_file(temp_file)) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: output.write(f.read()) output.write("\n\n") os.remove(temp_file) elif file["type"] == "dir": process_directory(file["url"], output) def process_local_directory(local_path, output): root, dirs, files os.walk(local_path): file files: is_allowed_filetype(file): print(f"processing {os.path.join(root, file)}...") output.write(f"# {'-' * 3}\n") output.write(f"# filename: {os.path.join(root, file)}\n") output.write(f"# {'-' * 3}\n\n") file_path = os.path.join(root, file) file.endswith(".ipynb"): output.write(process_ipynb_file(file_path)) else: open(file_path, "r", encoding='utf-8', errors='ignore') f: output.write(f.read()) output.write("\n\n") def process_github_repo(repo_url): api_base_url = "https://api.github.com/repos/" repo_url_parts = repo_url.split("https://github.com/")[-1].split("/") repo_name = "/".join(repo_url_parts[:2]) # detect branch tag reference branch_or_tag = "" subdirectory = "" len(repo_url_parts) &gt; 2 repo_url_parts[2] == "tree": # branch tag name index 3 len(repo_url_parts) &gt; 3: branch_or_tag = repo_url_parts[3] # remaining parts branch/tag name form subdirectory len(repo_url_parts) &gt; 4: subdirectory = "/".join(repo_url_parts[4:]) contents_url = f"{api_base_url}{repo_name}/contents" subdirectory: contents_url = f"{contents_url}/{subdirectory}" branch_or_tag: contents_url = f"{contents_url}?ref={branch_or_tag}" repo_content = [f'&lt;source type="github_repository" url="{repo_url}"&gt;'] def process_directory(url, repo_content): response = requests.get(url, headers=headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file) repo_content.append(f'&lt;file name="{escape_xml(file["path"])}"&gt;') file["name"].endswith(".ipynb"): repo_content.append(escape_xml(process_ipynb_file(temp_file))) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: repo_content.append(escape_xml(f.read())) repo_content.append('&lt;/file&gt;') os.remove(temp_file) elif file["type"] == "dir": process_directory(file["url"], repo_content) process_directory(contents_url, repo_content) repo_content.append('&lt;/source&gt;') print("all files processed.") return "\n".join(repo_content) def process_local_folder(local_path): def process_local_directory(local_path): content = [f'&lt;source type="local_directory" path="{escape_xml(local_path)}"&gt;'] root, dirs, files os.walk(local_path): file files: is_allowed_filetype(file): print(f"processing {os.path.join(root, file)}...") file_path = os.path.join(root, file) relative_path = os.path.relpath(file_path, local_path) content.append(f'&lt;file name="{escape_xml(relative_path)}"&gt;') file.endswith(".ipynb"): content.append(escape_xml(process_ipynb_file(file_path))) else: open(file_path, "r", encoding='utf-8', errors='ignore') f: content.append(escape_xml(f.read())) content.append('&lt;/file&gt;') content.append('&lt;/source&gt;') return '\n'.join(content) formatted_content = process_local_directory(local_path) print("all files processed.") return formatted_content def process_arxiv_pdf(arxiv_abs_url): pdf_url = arxiv_abs_url.replace("/abs/", "/pdf/") + ".pdf" response = requests.get(pdf_url) pdf_content = response.content open('temp.pdf', 'wb') pdf_file: pdf_file.write(pdf_content) text = [] open('temp.pdf', 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) page range(len(pdf_reader.pages)): text.append(pdf_reader.pages[page].extract_text()) formatted_text = f'&lt;source type="arxiv_paper" url="{arxiv_abs_url}"&gt;\n' formatted_text += '&lt;paper&gt;\n' formatted_text += escape_xml(' '.join(text)) formatted_text += '\n&lt;/paper&gt;\n' formatted_text += '&lt;/source&gt;' os.remove('temp.pdf') print("arxiv paper processed successfully.") return formatted_text def extract_links(input_file, output_file): url_pattern = re.compile(r'http[s]?://(?:[a-za-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fa-f][0-9a-fa-f]))+') open(input_file, 'r', encoding='utf-8') file: content = file.read() urls = re.findall(url_pattern, content) open(output_file, 'w', encoding='utf-8') output: url urls: output.write(url + '\n') def fetch_youtube_transcript(url): def extract_video_id(url): pattern = r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\s+\/|(?:v|e(?:mbed)?)\/|\s*?[?&amp;]v=)|youtu\.be\/)([a-za-z0-9_-]{11})' match = re.search(pattern, url) match: return match.group(1) return none video_id = extract_video_id(url) video_id: return f'&lt;source type="youtube_transcript" url="{escape_xml(url)}"&gt;\n&lt;error&gt;could extract video id url.&lt;/error&gt;\n&lt;/source&gt;' try: transcript_list = youtubetranscriptapi.get_transcript(video_id) formatter = textformatter() transcript = formatter.format_transcript(transcript_list) formatted_text = f'&lt;source type="youtube_transcript" url="{escape_xml(url)}"&gt;\n' formatted_text += '&lt;transcript&gt;\n' formatted_text += escape_xml(transcript) formatted_text += '\n&lt;/transcript&gt;\n' formatted_text += '&lt;/source&gt;' return formatted_text except exception e: return f'&lt;source type="youtube_transcript" url="{escape_xml(url)}"&gt;\n&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n&lt;/source&gt;' def preprocess_text(input_file, output_file): open(input_file, "r", encoding="utf-8") input_file: input_text = input_file.read() def process_text(text): text = re.sub(r"[\n\r]+", "\n", text) # update following line include apostrophes quotation marks text = re.sub(r"[^a-za-z0-9\s_.,!?:;@#$%^&amp;*()+\-=[\]{}|\\&lt;&gt;`~'\"/]+", "", text) text = re.sub(r"\s+", " ", text) text = text.lower() words = text.split() words = [word word words word stop_words] return " ".join(words) try: # try parse input xml root = et.fromstring(input_text) # process text content preserving xml structure elem root.iter(): elem.text: elem.text = process_text(elem.text) elem.tail: elem.tail = process_text(elem.tail) # write processed xml output file tree = et.elementtree(root) tree.write(output_file, encoding="utf-8", xml_declaration=true) print("text preprocessing completed xml structure preserved.") except et.parseerror: # xml parsing fails, process text without preserving xml structure processed_text = process_text(input_text) open(output_file, "w", encoding="utf-8") out_file: out_file.write(processed_text) print("xml parsing failed. text preprocessing completed without xml structure.") def get_token_count(text, disallowed_special=[], chunk_size=1000): enc = tiktoken.get_encoding("cl100k_base") # remove xml tags text_without_tags = re.sub(r'&lt;[^&gt;]+&gt;', '', text) # split text smaller chunks chunks = [text_without_tags[i:i+chunk_size] range(0, len(text_without_tags), chunk_size)] total_tokens = 0 chunk chunks: tokens = enc.encode(chunk, disallowed_special=disallowed_special) total_tokens += len(tokens) return total_tokens def is_same_domain(base_url, new_url): return urlparse(base_url).netloc == urlparse(new_url).netloc def is_within_depth(base_url, current_url, max_depth): base_parts = urlparse(base_url).path.rstrip('/').split('/') current_parts = urlparse(current_url).path.rstrip('/').split('/') current_parts[:len(base_parts)] != base_parts: return false return len(current_parts) - len(base_parts) &lt;= max_depth def process_pdf(url): response = requests.get(url) response.raise_for_status() open('temp.pdf', 'wb') pdf_file: pdf_file.write(response.content) text = [] open('temp.pdf', 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) page range(len(pdf_reader.pages)): text.append(pdf_reader.pages[page].extract_text()) os.remove('temp.pdf') return ' '.join(text) def crawl_and_extract_text(base_url, max_depth, include_pdfs, ignore_epubs): visited_urls = set() urls_to_visit = [(base_url, 0)] processed_urls = [] all_text = [f'&lt;source type="web_documentation" url="{escape_xml(base_url)}"&gt;'] urls_to_visit: current_url, current_depth = urls_to_visit.pop(0) clean_url = current_url.split('#')[0] clean_url visited_urls is_same_domain(base_url, clean_url) is_within_depth(base_url, clean_url, max_depth): ignore_epubs clean_url.endswith('.epub'): continue try: response = requests.get(current_url) soup = beautifulsoup(response.content, 'html.parser') visited_urls.add(clean_url) clean_url.endswith('.pdf') include_pdfs: text = process_pdf(clean_url) else: element soup(['script', 'style', 'head', 'title', 'meta', '[document]']): element.decompose() comments = soup.find_all(string=lambda text: isinstance(text, comment)) comment comments: comment.extract() text = soup.get_text(separator='\n', strip=true) all_text.append(f'&lt;page url="{escape_xml(clean_url)}"&gt;') all_text.append(escape_xml(text)) all_text.append('&lt;/page&gt;') processed_urls.append(clean_url) print(f"processed: {clean_url}") current_depth &lt; max_depth: link soup.find_all('a', href=true): new_url = urljoin(current_url, link['href']).split('#')[0] new_url visited_urls is_within_depth(base_url, new_url, max_depth) (include_pdfs new_url.endswith('.pdf')) (ignore_epubs new_url.endswith('.epub')): urls_to_visit.append((new_url, current_depth + 1)) except requests.requestexception e: print(f"failed retrieve {clean_url}: {e}") all_text.append('&lt;/source&gt;') formatted_content = '\n'.join(all_text) return { 'content': formatted_content, 'processed_urls': processed_urls } def process_doi_or_pmid(identifier): headers = { 'user-agent': 'mozilla/5.0 (windows nt 6.3) applewebkit/537.36 (khtml, like gecko) chrome/98.0.4758.102 safari/537.36', 'connection': 'keep-alive' } try: payload = { 'sci-hub-plugin-check': '', 'request': identifier } base_url = 'https://sci-hub.se/' response = requests.post(base_url, headers=headers, data=payload, timeout=60) soup = beautifulsoup(response.content, 'html.parser') pdf_element = soup.find(id='pdf') pdf_element none: raise valueerror(f"no pdf found identifier {identifier}. sci-hub might inaccessible document available.") content = pdf_element.get('src').replace('#navpanes=0&amp;view=fith', '').replace('//', '/') content.startswith('/downloads'): pdf_url = 'https://sci-hub.se' + content elif content.startswith('/tree'): pdf_url = 'https://sci-hub.se' + content elif content.startswith('/uptodate'): pdf_url = 'https://sci-hub.se' + content else: pdf_url = 'https:/' + content pdf_filename = f"{identifier.replace('/', '-')}.pdf" wget.download(pdf_url, pdf_filename) open(pdf_filename, 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) text = "" page range(len(pdf_reader.pages)): text += pdf_reader.pages[page].extract_text() formatted_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n' formatted_text += '&lt;paper&gt;\n' formatted_text += escape_xml(text) formatted_text += '\n&lt;/paper&gt;\n' formatted_text += '&lt;/source&gt;' os.remove(pdf_filename) print(f"identifier {identifier} processed successfully.") return formatted_text except (requests.requestexception, valueerror) e: error_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n' error_text += f'&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n' error_text += '&lt;/source&gt;' print(f"error processing identifier {identifier}: {str(e)}") print("sci-hub appears inaccessible document found. please try later.") return error_text def process_github_pull_request(pull_request_url): url_parts = pull_request_url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] pull_request_number = url_parts[-1] api_base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/pulls/{pull_request_number}" headers = {"authorization": f"token {token}"} response = requests.get(api_base_url, headers=headers) pull_request_data = response.json() diff_url = pull_request_data["diff_url"] diff_response = requests.get(diff_url, headers=headers) pull_request_diff = diff_response.text comments_url = pull_request_data["comments_url"] review_comments_url = pull_request_data["review_comments_url"] comments_response = requests.get(comments_url, headers=headers) review_comments_response = requests.get(review_comments_url, headers=headers) comments_data = comments_response.json() review_comments_data = review_comments_response.json() all_comments = comments_data + review_comments_data all_comments.sort(key=lambda comment: comment.get("position") float("inf")) formatted_text = f'&lt;source type="github_pull_request" url="{pull_request_url}"&gt;\n' formatted_text += '&lt;pull_request_info&gt;\n' formatted_text += f'&lt;title&gt;{escape_xml(pull_request_data["title"])}&lt;/title&gt;\n' formatted_text += f'&lt;description&gt;{escape_xml(pull_request_data["body"])}&lt;/description&gt;\n' formatted_text += '&lt;merge_details&gt;\n' formatted_text += f'{escape_xml(pull_request_data["user"]["login"])} wants merge {pull_request_data["commits"]} commit {repo_owner}:{pull_request_data["base"]["ref"]} {pull_request_data["head"]["label"]}\n' formatted_text += '&lt;/merge_details&gt;\n' formatted_text += '&lt;diff_and_comments&gt;\n' diff_lines = pull_request_diff.split("\n") comment_index = 0 line diff_lines: formatted_text += f'{escape_xml(line)}\n' comment_index &lt; len(all_comments) all_comments[comment_index].get("position") == diff_lines.index(line): comment = all_comments[comment_index] formatted_text += f'&lt;review_comment&gt;\n' formatted_text += f'&lt;author&gt;{escape_xml(comment["user"]["login"])}&lt;/author&gt;\n' formatted_text += f'&lt;content&gt;{escape_xml(comment["body"])}&lt;/content&gt;\n' formatted_text += f'&lt;path&gt;{escape_xml(comment["path"])}&lt;/path&gt;\n' formatted_text += f'&lt;line&gt;{comment["original_line"]}&lt;/line&gt;\n' formatted_text += '&lt;/review_comment&gt;\n' comment_index += 1 formatted_text += '&lt;/diff_and_comments&gt;\n' formatted_text += '&lt;/pull_request_info&gt;\n' repo_url = f"https://github.com/{repo_owner}/{repo_name}" repo_content = process_github_repo(repo_url) formatted_text += '&lt;repository&gt;\n' formatted_text += repo_content formatted_text += '&lt;/repository&gt;\n' formatted_text += '&lt;/source&gt;' print(f"pull request {pull_request_number} repository content processed successfully.") return formatted_text def escape_xml(text): return ( str(text) .replace("&amp;", "&amp;amp;") .replace("&lt;", "&amp;lt;") .replace("&gt;", "&amp;gt;") # remove following lines stop converting apostrophes quotes # .replace("\"", "&amp;quot;") # .replace("'", "&amp;apos;") ) def process_github_issue(issue_url): url_parts = issue_url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] issue_number = url_parts[-1] api_base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/issues/{issue_number}" headers = {"authorization": f"token {token}"} response = requests.get(api_base_url, headers=headers) issue_data = response.json() comments_url = issue_data["comments_url"] comments_response = requests.get(comments_url, headers=headers) comments_data = comments_response.json() formatted_text = f'&lt;source type="github_issue" url="{issue_url}"&gt;\n' formatted_text += '&lt;issue_info&gt;\n' formatted_text += f'&lt;title&gt;{escape_xml(issue_data["title"])}&lt;/title&gt;\n' formatted_text += f'&lt;description&gt;{escape_xml(issue_data["body"])}&lt;/description&gt;\n' formatted_text += '&lt;comments&gt;\n' comment comments_data: formatted_text += '&lt;comment&gt;\n' formatted_text += f'&lt;author&gt;{escape_xml(comment["user"]["login"])}&lt;/author&gt;\n' formatted_text += f'&lt;content&gt;{escape_xml(comment["body"])}&lt;/content&gt;\n' code_snippets = re.findall(r'https://github.com/.*#l\d+-l\d+', comment['body']) snippet_url code_snippets: url_parts = snippet_url.split("#") file_url = url_parts[0].replace("/blob/", "/raw/") line_range = url_parts[1] start_line, end_line = map(int, line_range.split("-")[0][1:]), map(int, line_range.split("-")[1][1:]) file_response = requests.get(file_url, headers=headers) file_content = file_response.text code_lines = file_content.split("\n")[start_line-1:end_line] code_snippet = "\n".join(code_lines) formatted_text += '&lt;code_snippet&gt;\n' formatted_text += f'&lt;![cdata[{code_snippet}]]&gt;\n' formatted_text += '&lt;/code_snippet&gt;\n' formatted_text += '&lt;/comment&gt;\n' formatted_text += '&lt;/comments&gt;\n' formatted_text += '&lt;/issue_info&gt;\n' repo_url = f"https://github.com/{repo_owner}/{repo_name}" repo_content = process_github_repo(repo_url) formatted_text += '&lt;repository&gt;\n' formatted_text += repo_content formatted_text += '&lt;/repository&gt;\n' formatted_text += '&lt;/source&gt;' print(f"issue {issue_number} repository content processed successfully.") return formatted_text def main(): console = console() intro_text = text("\ninput paths urls processed:\n", style="dodger_blue1") input_types = [ (" local folder path (flattens files text)", "bright_white"), (" github repository url (flattens files text)", "bright_white"), (" github pull request url (pr + repo)", "bright_white"), (" github issue url (issue + repo)", "bright_white"), (" documentation url (base url)", "bright_white"), (" youtube video url (to fetch transcript)", "bright_white"), (" arxiv paper url", "bright_white"), (" doi pmid search sci-hub", "bright_white"), ] input_type, color input_types: intro_text.append(f"\n{input_type}", style=color) intro_panel = panel( intro_text, expand=false, border_style="bold", title="[bright_white]copy file clipboard[/bright_white]", title_align="center", padding=(1, 1), ) console.print(intro_panel) len(sys.argv) &gt; 1: input_path = sys.argv[1] else: input_path = prompt.ask("\n[bold dodger_blue1]enter path url[/bold dodger_blue1]", console=console) console.print(f"\n[bold bright_green]you entered:[/bold bright_green] [bold bright_yellow]{input_path}[/bold bright_yellow]\n") output_file = "uncompressed_output.txt" processed_file = "compressed_output.txt" urls_list_file = "processed_urls.txt" progress( textcolumn("[bold bright_blue]{task.description}"), barcolumn(bar_width=none), timeremainingcolumn(), console=console, ) progress: task = progress.add_task("[bright_blue]processing...", total=100) try: "github.com" input_path: "/pull/" input_path: final_output = process_github_pull_request(input_path) elif "/issues/" input_path: final_output = process_github_issue(input_path) else: final_output = process_github_repo(input_path) elif urlparse(input_path).scheme ["http", "https"]: "youtube.com" input_path "youtu.be" input_path: final_output = fetch_youtube_transcript(input_path) elif "arxiv.org" input_path: final_output = process_arxiv_pdf(input_path) else: crawl_result = crawl_and_extract_text(input_path, max_depth=2, include_pdfs=true, ignore_epubs=true) final_output = crawl_result['content'] open(urls_list_file, 'w', encoding='utf-8') urls_file: urls_file.write('\n'.join(crawl_result['processed_urls'])) elif input_path.startswith("10.") "/" input_path input_path.isdigit(): final_output = process_doi_or_pmid(input_path) else: final_output = process_local_folder(input_path) progress.update(task, advance=50) # write uncompressed output open(output_file, "w", encoding="utf-8") file: file.write(final_output) # process compressed output preprocess_text(output_file, processed_file) progress.update(task, advance=50) compressed_text = safe_file_read(processed_file) compressed_token_count = get_token_count(compressed_text) console.print(f"\n[bright_green]compressed token count:[/bright_green] [bold bright_cyan]{compressed_token_count}[/bold bright_cyan]") uncompressed_text = safe_file_read(output_file) uncompressed_token_count = get_token_count(uncompressed_text) console.print(f"[bright_green]uncompressed token count:[/bright_green] [bold bright_cyan]{uncompressed_token_count}[/bold bright_cyan]") console.print(f"\n[bold bright_yellow]{processed_file}[/bold bright_yellow] [bold bright_blue]{output_file}[/bold bright_blue] created working directory.") pyperclip.copy(uncompressed_text) console.print(f"\n[bright_white]the contents [bold bright_blue]{output_file}[/bold bright_blue] copied clipboard.[/bright_white]") except exception e: console.print(f"\n[bold red]an error occurred:[/bold red] {str(e)}") console.print("\nplease check input try again.") raise # re-raise exception debugging purposes __name__ == "__main__": main()</file><file name="src/python/rag/create_vector_db.py">import os sentence_transformers import sentencetransformer langchain_community.vectorstores import faiss langchain.text_splitter import recursivecharactertextsplitter langchain_huggingface import huggingfaceembeddings, huggingfaceendpoint dotenv import load_dotenv load_dotenv() huggingfacehub_api_token = os.getenv('huggingfacehub_api_token') # step 1: read file def read_file(file_path): open(file_path, 'r', encoding='utf-8') file: return file.read() # step 2: preprocess data def preprocess_text(text): # clean split text chunks text_splitter = recursivecharactertextsplitter(chunk_size=1200, chunk_overlap=300) return text_splitter.split_text(text) # step 3: vectorization indexing def create_vector_store(docs): # use sentencetransformer embeddings # model = sentencetransformer('all-minilm-l6-v2') # embeddings = model.encode(docs) #, convert_to_tensor=true) # generate embeddings documents embeddings = huggingfaceembeddings() text_embeddings = embeddings.embed_documents(docs) text_embedding_pairs = zip(docs, text_embeddings) vector_store = faiss.from_embeddings(text_embedding_pairs, embeddings) vector_store.save_local("faiss_aidoc") # save index locally return vector_store # main function run program def main(): file_path = 'uncompressed_output.txt' text = read_file(file_path) docs = preprocess_text(text) vector_store = create_vector_store(docs) __name__ == "__main__": main()</file><file name="src/python/rag/prompt_llm.py">langchain_huggingface import huggingfaceembeddings, huggingfaceendpoint langchain_community.vectorstores import faiss # step 1: set llm def load_llm(): return huggingfaceendpoint(repo_id="mistralai/mixtral-8x7b-instruct-v0.1") # step 2: query function def query_llm(llm, vector_store, user_query): retriever = vector_store.as_retriever() # context = retriever.retrieve(user_query) # retrieve relevant documents relevant_docs = retriever.invoke(user_query) # combine context user query context = " ".join(doc.page_content doc relevant_docs) response = llm.generate([context + user_query]) return response # main function run program def main(): user_query = input("ask question: \n") llm = load_llm() embeddings = huggingfaceembeddings() vector_store = faiss.load_local("faiss_aidoc", embeddings, allow_dangerous_deserialization=true) answer = query_llm(llm, vector_store, user_query) print("answer:", answer) __name__ == "__main__": main()</file><file name="uncompressed_output.txt">&lt;source type="github_repository" url="https://github.com/simonplancke/local_llm"&gt; &lt;file name="readme.md"&gt; # local_llm &lt;/file&gt; &lt;file name="answer.txt"&gt; text='python sportcompetition program following potential shortcomings: 1. lack input validation: program perform input validation checks, could lead unexpected behavior errors input data expected format contains invalid values. 2. magic numbers: program contains several hard-coded "magic numbers" used calculations conditional statements. numbers defined constants variables descriptive names improve readability maintainability. 3. code duplication: program contains several instances duplicate code, could refactored reusable functions methods improve maintainability reduce risk bugs. 4. error handling: program handle errors gracefully could crash produce unexpected results error occurs. implementing proper error handling logging mechanisms could help identify address errors efficiently. 5. scalability: program designed handle large datasets, could impact performance efficiency. implementing optimizations best practices could improve scalability performance. 6. testing: program appear testing implemented, could lead undetected bugs errors. implementing testing validation checks could improve reliability robustness program. 7. code documentation: program lacks documentation, making difficult understand purpose, functionality, implementation. adding comments documentation could improve readability maintainability. 8. code organization: program could benefit better organization structure, breaking smaller functions modules, make easier understand, maintain, extend.' &lt;/file&gt; &lt;file name="debug.py"&gt; content = '/tree/thisworks?' content.startswith(('/downloads', '/tree', '/uptodate')): pdf_url = 'https://sci-hub.se' + content else: pdf_url = 'https:/' + content print(pdf_url) &lt;/file&gt; &lt;file name="requirements.txt"&gt; langchain==0.3.13 langchain_community weaviate-client&amp;gt;=3.26.7,&amp;lt;4.0.0 protobuf==3.20 huggingface-hub faiss-cpu sentence-transformers langchain_huggingface==0.1.2 &lt;/file&gt; &lt;file name="src/python/create_text_file/create_text_file.py"&gt; import sys urllib.parse import urlparse # import functions library rich (https://github.com/textualize/rich) rich import print rich.console import console rich.panel import panel rich.text import text rich.prompt import prompt rich.style import style rich.syntax import syntax rich.traceback import install rich.progress import progress, textcolumn, barcolumn, timeremainingcolumn # repository classes scripts git_methods import gitmethods import generic_functions generic_functions file_processing import textfilemethods, pdffilemethods, foldermethods, transcriptionmethods def safe_file_read(filepath, fallback_encoding:str='latin1') -&amp;gt; str: """ safely reads content file, attempting handle encoding issues. function first tries read file using utf-8 encoding. unicodedecodeerror occurs, attempts read file using specified fallback encoding. parameters: ---------- fallback_encoding : str, optional encoding use file cannot read utf-8. default 'latin1'. returns: ------- str content file string. raises: ------ filenotfounderror specified file exist. ioerror error reading file (e.g., permission issues). """ try: open(filepath, "r", encoding='utf-8') file: return file.read() except unicodedecodeerror: open(filepath, "r", encoding=fallback_encoding) file: return file.read() def main(): console = console() intro_text = text("\ninput paths urls processed:\n", style="dodger_blue1") input_types = [ (" local folder path (flattens files text)", "bright_white"), (" github repository url (flattens files text)", "bright_white"), (" github pull request url (pr + repo)", "bright_white"), (" github issue url (issue + repo)", "bright_white"), (" documentation url (base url)", "bright_white"), (" youtube video url (to fetch transcript)", "bright_white"), (" arxiv paper url", "bright_white"), (" doi pmid search sci-hub", "bright_white"), ] input_type, color input_types: intro_text.append(f"\n{input_type}", style=color) intro_panel = panel( intro_text, expand=false, border_style="bold", title="[bright_white]copy file clipboard[/bright_white]", title_align="center", padding=(1, 1), ) console.print(intro_panel) # argument passed, use it. otherwise, prompt user pass argument len(sys.argv) &amp;gt; 1: input_path = sys.argv[1] else: input_path = prompt.ask("\n[bold dodger_blue1]enter path url[/bold dodger_blue1]", console=console) console.print(f"\n[bold bright_green]you entered:[/bold bright_green] [bold bright_yellow]{input_path}[/bold bright_yellow]\n") output_file = "uncompressed_output.txt" processed_file = "compressed_output.txt" urls_list_file = "processed_urls.txt" progress( textcolumn("[bold bright_blue]{task.description}"), barcolumn(bar_width=none), timeremainingcolumn(), console=console, ) progress: task = progress.add_task("[bright_blue]processing...", total=100) # parse input path call correct function try: # git functions "github.com" input_path: gitmethods_object = gitmethods() final_output = gitmethods_object.handle_git_url() # url functions elif urlparse(input_path).scheme ["http", "https"]: "youtube.com" input_path "youtu.be" input_path: final_output = transcriptionmethods.fetch_youtube_transcript(input_path) elif "arxiv.org" input_path: final_output = pdffilemethods.process_arxiv_pdf(input_path) # else: # crawl_result = crawl_and_extract_text(input_path, max_depth=2, include_pdfs=true, ignore_epubs=true) # final_output = crawl_result['content'] # open(urls_list_file, 'w', encoding='utf-8') urls_file: # urls_file.write('\n'.join(crawl_result['processed_urls'])) # scientific papers https://sci-hub.se/ elif input_path.startswith("10.") "/" input_path input_path.isdigit(): final_output = pdffilemethods.process_doi_or_pmid(input_path) # local folder else: final_output = foldermethods.process_local_folder(input_path) progress.update(task, advance=50) # write uncompressed output open(output_file, "w", encoding="utf-8") file: file.write(final_output) # process compressed output textfilemethods.parse_text_as_xml(output_file, processed_file) progress.update(task, advance=50) compressed_text = safe_file_read(processed_file) compressed_token_count = get_token_count(compressed_text) console.print(f"\n[bright_green]compressed token count:[/bright_green] [bold bright_cyan]{compressed_token_count}[/bold bright_cyan]") uncompressed_text = safe_file_read(output_file) uncompressed_token_count = get_token_count(uncompressed_text) console.print(f"[bright_green]uncompressed token count:[/bright_green] [bold bright_cyan]{uncompressed_token_count}[/bold bright_cyan]") console.print(f"\n[bold bright_yellow]{processed_file}[/bold bright_yellow] [bold bright_blue]{output_file}[/bold bright_blue] created working directory.") pyperclip.copy(uncompressed_text) console.print(f"\n[bright_white]the contents [bold bright_blue]{output_file}[/bold bright_blue] copied clipboard.[/bright_white]") except exception e: console.print(f"\n[bold red]an error occurred:[/bold red] {str(e)}") console.print("\nplease check input try again.") raise # re-raise exception debugging purposes __name__ == "__main__": main() &lt;/file&gt; &lt;file name="src/python/create_text_file/file_processing.py"&gt; import import os import requests import wget generic_functions import get_stopword_list, escape_xml, is_allowed_filetype, process_ipynb_file, download_file import xml.etree.elementtree et bs4 import beautifulsoup pypdf2 import pdfreader youtube_transcript_api import youtubetranscriptapi youtube_transcript_api.formatters import textformatter class textfilemethods(): def __init__(self, filepath:str): self.filepath = filepath @staticmethod def clean_text(text): # remove new-line characters text = re.sub(r"[\n\r]+", "\n", text) # remove unwanted characters text = re.sub(r"[^a-za-z0-9\s_.,!?:;@#$%^&amp;amp;*()+\-=[\]{}|\\&amp;lt;&amp;gt;`~'\"/]+", "", text) # normalize whitespace text = re.sub(r"\s+", " ", text) # convert lowercase text = text.lower() # split words words = text.split() # drop stop words (the, is, in...) stop_words = get_stopword_list() words = [word word words word stop_words] return " ".join(words) def parse_text_as_xml(self, input_file, output_file): open(input_file, "r", encoding="utf-8") input_file: input_text = input_file.read() try: # try parse input xml root = et.fromstring(input_text) # process text content preserving xml structure elem root.iter(): elem.text: elem.text = self.clean_text(elem.text) elem.tail: elem.tail = self.clean_text(elem.tail) # write processed xml output file tree = et.elementtree(root) tree.write(output_file, encoding="utf-8", xml_declaration=true) print("text preprocessing completed xml structure preserved.") except et.parseerror: # xml parsing fails, process text without preserving xml structure processed_text = self.clean_text(input_text) open(output_file, "w", encoding="utf-8") out_file: out_file.write(processed_text) warning("xml parsing failed. text preprocessing completed without xml structure.") class pdffilemethods(): def add_xml_tags_for_paper(self, text): paper_xml_formatted_text = '&amp;lt;paper&amp;gt;\n' paper_xml_formatted_text += escape_xml(' '.join(text)) paper_xml_formatted_text += '\n&amp;lt;/paper&amp;gt;\n' return paper_xml_formatted_text def process_pdf(self, url): # download pdf content response = requests.get(url) response.raise_for_status() content = response.content open('temp.pdf', 'wb') pdf_file: pdf_file.write(content) text = [] open('temp.pdf', 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) page range(len(pdf_reader.pages)): text.append(pdf_reader.pages[page].extract_text()) os.remove('temp.pdf') return ' '.join(text) def process_arxiv_pdf(self, arxiv_abs_url): pdf_url = arxiv_abs_url.replace("/abs/", "/pdf/") + ".pdf" text = self.process_pdf(pdf_url) formatted_text = f'&amp;lt;source type="arxiv_paper" url="{arxiv_abs_url}"&amp;gt;\n' formatted_text += self.add_xml_tags_for_paper(text) formatted_text += '&amp;lt;/source&amp;gt;' os.remove('temp.pdf') print("arxiv paper processed successfully.") return formatted_text def process_doi_or_pmid(self, identifier): headers = { 'user-agent': 'mozilla/5.0 (windows nt 6.3) applewebkit/537.36 (khtml, like gecko) chrome/98.0.4758.102 safari/537.36', 'connection': 'keep-alive' } try: payload = { 'sci-hub-plugin-check': '', 'request': identifier } base_url = 'https://sci-hub.se/' response = requests.post(base_url, headers=headers, data=payload, timeout=60) soup = beautifulsoup(response.content, 'html.parser') pdf_element = soup.find(id='pdf') pdf_element none: raise valueerror(f"no pdf found identifier {identifier}. sci-hub might inaccessible document available.") content = pdf_element.get('src').replace('#navpanes=0&amp;amp;view=fith', '').replace('//', '/') content.startswith(('/downloads', '/tree', '/uptodate')): pdf_url = 'https://sci-hub.se' + content else: pdf_url = 'https:/' + content pdf_filename = f"{identifier.replace('/', '-')}.pdf" wget.download(pdf_url, pdf_filename) open(pdf_filename, 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) text = "" page range(len(pdf_reader.pages)): text += pdf_reader.pages[page].extract_text() formatted_text = f'&amp;lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&amp;gt;\n' formatted_text += self.add_xml_tags_for_paper(text) formatted_text += '&amp;lt;/source&amp;gt;' os.remove(pdf_filename) print(f"identifier {identifier} processed successfully.") return formatted_text except (requests.requestexception, valueerror) e: error_text = f'&amp;lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&amp;gt;\n' error_text += f'&amp;lt;error&amp;gt;{escape_xml(str(e))}&amp;lt;/error&amp;gt;\n' error_text += '&amp;lt;/source&amp;gt;' print(f"error processing identifier {identifier}: {str(e)}") print("sci-hub appears inaccessible document found. please try later.") return error_text # here!!!! class foldermethods(): def __init__(self, filepath): self.filepath = filepath def process_local_directory(self): content = [f'&amp;lt;source type="local_directory" path="{escape_xml(self.local_path)}"&amp;gt;'] root, files os.walk(self.local_path): file files: is_allowed_filetype(file): print(f"processing {os.path.join(root, file)}...") file_path = os.path.join(root, file) relative_path = os.path.relpath(file_path, self.local_path) content.append(f'&amp;lt;file name="{escape_xml(relative_path)}"&amp;gt;') file.endswith(".ipynb"): content.append(escape_xml(process_ipynb_file(file_path))) else: open(file_path, "r", encoding='utf-8', errors='ignore') f: content.append(escape_xml(f.read())) content.append('&amp;lt;/file&amp;gt;') content.append('&amp;lt;/source&amp;gt;') return '\n'.join(content) def process_local_folder(self, local_path): formatted_content = self.process_local_directory(local_path) print("all files processed.") return formatted_content def process_directory(self, url, output): response = requests.get(url, headers=headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file) output.write(f"# {'-' * 3}\n") output.write(f"# filename: {file['path']}\n") output.write(f"# {'-' * 3}\n\n") file["name"].endswith(".ipynb"): output.write(process_ipynb_file(temp_file)) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: output.write(f.read()) output.write("\n\n") os.remove(temp_file) elif file["type"] == "dir": self.process_directory(file["url"], output) class transcriptionmethods(): def __init__(self, url): self.url = url def extract_video_id(self, url): pattern = r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\s+\/|(?:v|e(?:mbed)?)\/|\s*?[?&amp;amp;]v=)|youtu\.be\/)([a-za-z0-9_-]{11})' match = re.search(pattern, url) match: return match.group(1) return none def fetch_youtube_transcript(self): video_id = self.extract_video_id(self.url) video_id: return f'&amp;lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&amp;gt;\n&amp;lt;error&amp;gt;could extract video id url.&amp;lt;/error&amp;gt;\n&amp;lt;/source&amp;gt;' try: transcript_list = youtubetranscriptapi.get_transcript(video_id) formatter = textformatter() transcript = formatter.format_transcript(transcript_list) formatted_text = f'&amp;lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&amp;gt;\n' formatted_text += '&amp;lt;transcript&amp;gt;\n' formatted_text += escape_xml(transcript) formatted_text += '\n&amp;lt;/transcript&amp;gt;\n' formatted_text += '&amp;lt;/source&amp;gt;' return formatted_text except exception e: return f'&amp;lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&amp;gt;\n&amp;lt;error&amp;gt;{escape_xml(str(e))}&amp;lt;/error&amp;gt;\n&amp;lt;/source&amp;gt;' &lt;/file&gt; &lt;file name="src/python/create_text_file/git_methods.py"&gt; import requests import os import dotenv import load_dotenv generic_functions import is_allowed_filetype, escape_xml, process_ipynb_file, download_file class gitmethods(): def __init__(self, url): self.url = url self.headers = self.get_github_token() @staticmethod def get_github_token(): load_dotenv() github_token = os.getenv('github_token') headers = {"authorization": f"token {github_token}"} return headers def process_github_main_branch(self, repo_owner, repo_name): repo_url = f"https://github.com/{repo_owner}/{repo_name}" repo_content = self.process_github_repo(repo_url) xml_formatted_repository = '&amp;lt;repository&amp;gt;\n' xml_formatted_repository += repo_content xml_formatted_repository += '&amp;lt;/repository&amp;gt;\n' xml_formatted_repository += '&amp;lt;/source&amp;gt;' return xml_formatted_repository def handle_git_url(self): "/pull/" self.url: final_output = self.process_github_pull_request() elif "/issues/" self.url: final_output = self.process_github_issue() else: final_output = self.process_github_repo() return final_output def process_git_directory(self, repo_content): response = requests.get(self.url, headers=self.headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file, self.headers) repo_content.append(f'&amp;lt;file name="{escape_xml(file["path"])}"&amp;gt;') file["name"].endswith(".ipynb"): repo_content.append(escape_xml(process_ipynb_file(temp_file))) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: repo_content.append(escape_xml(f.read())) repo_content.append('&amp;lt;/file&amp;gt;') os.remove(temp_file) elif file["type"] == "dir": self.process_git_directory(file["url"], repo_content) else: print(f"skipped {file}") def process_github_repo(self, base_url=none): # possible public repositories # pull repository locally use local path private base_url: base_url = self.url url_parts = base_url.split("/") api_base_url = "https://api.github.com/repos" repo_url_parts = url_parts.split("https://github.com/")[-1].split("/") repo_name = "/".join(repo_url_parts[:2]) # detect branch tag reference branch_or_tag = "" subdirectory = "" len(repo_url_parts) &amp;gt; 2 repo_url_parts[2] == "tree": # branch tag name index 3 len(repo_url_parts) &amp;gt; 3: branch_or_tag = repo_url_parts[3] # remaining parts branch/tag name form subdirectory len(repo_url_parts) &amp;gt; 4: subdirectory = "/".join(repo_url_parts[4:]) contents_url = f"{api_base_url}/{repo_name}/contents" subdirectory: contents_url = f"{contents_url}/{subdirectory}" branch_or_tag: contents_url = f"{contents_url}?ref={branch_or_tag}" # configure variable xml-like structure # variable exported .txt file repo_content = [f'&amp;lt;source type="github_repository" url="{url_parts}"&amp;gt;'] self.process_git_directory(contents_url, repo_content, self.headers) repo_content.append('&amp;lt;/source&amp;gt;') print("all files processed.") return "\n".join(repo_content) def process_github_pull_request(self): url_parts = self.url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] pull_request_number = url_parts[-1] api_base_url = "https://api.github.com/repos" pull_request_url = f"{api_base_url}/{repo_owner}/{repo_name}/pulls/{pull_request_number}" #region pr: get changes comments # 1. get metadata related pr response = requests.get(pull_request_url, headers=self.headers) pull_request_data = response.json() # 2. get changes made repository diff_url = pull_request_data["diff_url"] diff_response = requests.get(diff_url, headers=self.headers) pull_request_diff = diff_response.text # 3. get pr-level comments (general comments left submitted pr) comments_url = pull_request_data["comments_url"] comments_response = requests.get(comments_url, headers=self.headers) comments_data = comments_response.json() # 4. get code-level comments (comments changed files) review_comments_url = pull_request_data["review_comments_url"] review_comments_response = requests.get(review_comments_url, headers=self.headers) review_comments_data = review_comments_response.json() # 5. sort comments position all_comments = comments_data + review_comments_data all_comments.sort(key=lambda comment: comment.get("position") float("inf")) #endregion #region pr: add xml elements # add opening element: pull_request_info formatted_text = f'&amp;lt;source type="github_pull_request" url="{self.url}"&amp;gt;\n' formatted_text += '&amp;lt;pull_request_info&amp;gt;\n' # add generic elements: title, description, merge_details formatted_text += f'&amp;lt;title&amp;gt;{generic_functions.escape_xml(pull_request_data["title"])}&amp;lt;/title&amp;gt;\n' formatted_text += f'&amp;lt;description&amp;gt;{generic_functions.escape_xml(pull_request_data["body"])}&amp;lt;/description&amp;gt;\n' formatted_text += '&amp;lt;merge_details&amp;gt;\n' formatted_text += f'{generic_functions.escape_xml(pull_request_data["user"]["login"])} wants merge {pull_request_data["commits"]} commit {repo_owner}:{pull_request_data["base"]["ref"]} {pull_request_data["head"]["label"]}\n' formatted_text += '&amp;lt;/merge_details&amp;gt;\n' # iteratively add pr changes comments left pr text fields # parent element: diff_and_comments formatted_text += '&amp;lt;diff_and_comments&amp;gt;\n' diff_lines = pull_request_diff.split("\n") comment_index = 0 line diff_lines: formatted_text += f'{generic_functions.escape_xml(line)}\n' comment_index &amp;lt; len(all_comments) all_comments[comment_index].get("position") == diff_lines.index(line): comment = all_comments[comment_index] # comment element """ &amp;lt;review_comment&amp;gt; &amp;lt;author&amp;gt;xxx&amp;lt;/author&amp;gt; &amp;lt;content&amp;gt;xxx&amp;lt;/content&amp;gt; &amp;lt;path&amp;gt;xxx&amp;lt;/path&amp;gt; &amp;lt;line&amp;gt;xxx&amp;lt;/line&amp;gt; &amp;lt;/review_comment&amp;gt; """ formatted_text += f'&amp;lt;review_comment&amp;gt;\n' formatted_text += f'&amp;lt;author&amp;gt;{generic_functions.escape_xml(comment["user"]["login"])}&amp;lt;/author&amp;gt;\n' formatted_text += f'&amp;lt;content&amp;gt;{generic_functions.escape_xml(comment["body"])}&amp;lt;/content&amp;gt;\n' formatted_text += f'&amp;lt;path&amp;gt;{generic_functions.escape_xml(comment["path"])}&amp;lt;/path&amp;gt;\n' formatted_text += f'&amp;lt;line&amp;gt;{comment["original_line"]}&amp;lt;/line&amp;gt;\n' formatted_text += '&amp;lt;/review_comment&amp;gt;\n' comment_index += 1 # close open elements formatted_text += '&amp;lt;/diff_and_comments&amp;gt;\n' formatted_text += '&amp;lt;/pull_request_info&amp;gt;\n' # regular processing repo formatted_text += self.process_github_main_branch(repo_owner, repo_name) #endregion print(f"pull request {pull_request_number} repository content processed successfully.") return formatted_text def process_github_issue(self): url_parts = self.issue_url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] issue_number = url_parts[-1] api_base_url = "https://api.github.com/repos" issue_url = f"{api_base_url}/{repo_owner}/{repo_name}/issues/{issue_number}" #region issue: get metadata response = requests.get(issue_url, headers=self.headers) issue_data = response.json() comments_url = issue_data["comments_url"] comments_response = requests.get(comments_url, headers=self.headers) comments_data = comments_response.json() #endregion #region issue: add xml elements # add opening element: issue_info formatted_text = f'&amp;lt;source type="github_issue" url="{self.issue_url}"&amp;gt;\n' formatted_text += '&amp;lt;issue_info&amp;gt;\n' # add generic elements: title, description formatted_text += f'&amp;lt;title&amp;gt;{generic_functions.escape_xml(issue_data["title"])}&amp;lt;/title&amp;gt;\n' formatted_text += f'&amp;lt;description&amp;gt;{generic_functions.escape_xml(issue_data["body"])}&amp;lt;/description&amp;gt;\n' # iteratively add comments left pr text fields # parent element: comments formatted_text += '&amp;lt;comments&amp;gt;\n' comment comments_data: # comment element """ &amp;lt;comment&amp;gt; &amp;lt;author&amp;gt;xxx&amp;lt;/author&amp;gt; &amp;lt;content&amp;gt; &amp;lt;code_snippet&amp;gt;xxx&amp;lt;/code_snippet&amp;gt; &amp;lt;/content&amp;gt; &amp;lt;/comment&amp;gt; """ formatted_text += '&amp;lt;comment&amp;gt;\n' formatted_text += f'&amp;lt;author&amp;gt;{generic_functions.escape_xml(comment["user"]["login"])}&amp;lt;/author&amp;gt;\n' formatted_text += f'&amp;lt;content&amp;gt;{generic_functions.escape_xml(comment["body"])}&amp;lt;/content&amp;gt;\n' # extract code snippets comments github issue # regex: find occurrences urls comment body match pattern github file links line ranges code_snippets = re.findall(r'https://github.com/.*#l\d+-l\d+', comment['body']) snippet_url code_snippets: # split url two parts: base url line range. url_parts = snippet_url.split("#") # use file url get raw contents file try: file_url = url_parts[0].replace("/blob/", "/raw/") file_response = requests.get(file_url, headers=self.headers) file_content = file_response.text # using file content, get relevant code snippets back using start end indices line_range = url_parts[1] start_line, end_line = map(int, line_range.split("-")[0][1:]), map(int, line_range.split("-")[1][1:]) code_lines = file_content.split("\n")[start_line-1:end_line] code_snippet = "\n".join(code_lines) # format final code snippet using xml formatted_text += '&amp;lt;code_snippet&amp;gt;\n' formatted_text += f'&amp;lt;![cdata[{code_snippet}]]&amp;gt;\n' formatted_text += '&amp;lt;/code_snippet&amp;gt;\n' except requests.exceptions.requestexception e: warning(f"error fetching file: {e}") continue # skip next snippet there's error formatted_text += '&amp;lt;/comment&amp;gt;\n' # close open elements formatted_text += '&amp;lt;/comments&amp;gt;\n' formatted_text += '&amp;lt;/issue_info&amp;gt;\n' # regular processing repo formatted_text += self.process_github_main_branch(repo_owner, repo_name) #endregion print(f"issue {issue_number} repository content processed successfully.") return formatted_text &lt;/file&gt; &lt;file name="src/python/create_text_file/onefilellm.py"&gt; import requests bs4 import beautifulsoup, comment urllib.parse import urljoin, urlparse pypdf2 import pdfreader import os import sys import tiktoken import nltk nltk.corpus import stopwords import pathlib import path import nbformat nbconvert import pythonexporter youtube_transcript_api import youtubetranscriptapi youtube_transcript_api.formatters import textformatter import pyperclip import wget tqdm import tqdm time import sleep rich import print rich.console import console rich.panel import panel rich.text import text rich.prompt import prompt rich.style import style rich.syntax import syntax rich.traceback import install rich.progress import progress, textcolumn, barcolumn, timeremainingcolumn import xml.etree.elementtree et dotenv import load_dotenv load_dotenv() github_token = os.getenv('github_token') def safe_file_read(filepath, fallback_encoding='latin1'): try: open(filepath, "r", encoding='utf-8') file: return file.read() except unicodedecodeerror: open(filepath, "r", encoding=fallback_encoding) file: return file.read() nltk.download("stopwords", quiet=true) stop_words = set(stopwords.words("english")) token = os.getenv('github_token', github_token) token == 'default_token_here': raise environmenterror("github_token environment variable set.") headers = {"authorization": f"token {token}"} def download_file(url, target_path): response = requests.get(url, headers=headers) response.raise_for_status() open(target_path, "wb") f: f.write(response.content) def is_allowed_filetype(filename): allowed_extensions = ['.py', '.txt', '.js', '.tsx', '.ts', '.md', '.cjs', '.html', '.json', '.ipynb', '.h', '.localhost', '.sh', '.yaml', '.example'] # allowed_extensions = ['.md'] return any(filename.endswith(ext) ext allowed_extensions) def process_ipynb_file(temp_file): open(temp_file, "r", encoding='utf-8', errors='ignore') f: notebook_content = f.read() exporter = pythonexporter() python_code, _ = exporter.from_notebook_node(nbformat.reads(notebook_content, as_version=4)) return python_code def process_directory(url, output): response = requests.get(url, headers=headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file) output.write(f"# {'-' * 3}\n") output.write(f"# filename: {file['path']}\n") output.write(f"# {'-' * 3}\n\n") file["name"].endswith(".ipynb"): output.write(process_ipynb_file(temp_file)) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: output.write(f.read()) output.write("\n\n") os.remove(temp_file) elif file["type"] == "dir": process_directory(file["url"], output) def process_local_directory(local_path, output): root, dirs, files os.walk(local_path): file files: is_allowed_filetype(file): print(f"processing {os.path.join(root, file)}...") output.write(f"# {'-' * 3}\n") output.write(f"# filename: {os.path.join(root, file)}\n") output.write(f"# {'-' * 3}\n\n") file_path = os.path.join(root, file) file.endswith(".ipynb"): output.write(process_ipynb_file(file_path)) else: open(file_path, "r", encoding='utf-8', errors='ignore') f: output.write(f.read()) output.write("\n\n") def process_github_repo(repo_url): api_base_url = "https://api.github.com/repos/" repo_url_parts = repo_url.split("https://github.com/")[-1].split("/") repo_name = "/".join(repo_url_parts[:2]) # detect branch tag reference branch_or_tag = "" subdirectory = "" len(repo_url_parts) &amp;gt; 2 repo_url_parts[2] == "tree": # branch tag name index 3 len(repo_url_parts) &amp;gt; 3: branch_or_tag = repo_url_parts[3] # remaining parts branch/tag name form subdirectory len(repo_url_parts) &amp;gt; 4: subdirectory = "/".join(repo_url_parts[4:]) contents_url = f"{api_base_url}{repo_name}/contents" subdirectory: contents_url = f"{contents_url}/{subdirectory}" branch_or_tag: contents_url = f"{contents_url}?ref={branch_or_tag}" repo_content = [f'&amp;lt;source type="github_repository" url="{repo_url}"&amp;gt;'] def process_directory(url, repo_content): response = requests.get(url, headers=headers) response.raise_for_status() files = response.json() file files: file["type"] == "file" is_allowed_filetype(file["name"]): print(f"processing {file['path']}...") temp_file = f"temp_{file['name']}" download_file(file["download_url"], temp_file) repo_content.append(f'&amp;lt;file name="{escape_xml(file["path"])}"&amp;gt;') file["name"].endswith(".ipynb"): repo_content.append(escape_xml(process_ipynb_file(temp_file))) else: open(temp_file, "r", encoding='utf-8', errors='ignore') f: repo_content.append(escape_xml(f.read())) repo_content.append('&amp;lt;/file&amp;gt;') os.remove(temp_file) elif file["type"] == "dir": process_directory(file["url"], repo_content) process_directory(contents_url, repo_content) repo_content.append('&amp;lt;/source&amp;gt;') print("all files processed.") return "\n".join(repo_content) def process_local_folder(local_path): def process_local_directory(local_path): content = [f'&amp;lt;source type="local_directory" path="{escape_xml(local_path)}"&amp;gt;'] root, dirs, files os.walk(local_path): file files: is_allowed_filetype(file): print(f"processing {os.path.join(root, file)}...") file_path = os.path.join(root, file) relative_path = os.path.relpath(file_path, local_path) content.append(f'&amp;lt;file name="{escape_xml(relative_path)}"&amp;gt;') file.endswith(".ipynb"): content.append(escape_xml(process_ipynb_file(file_path))) else: open(file_path, "r", encoding='utf-8', errors='ignore') f: content.append(escape_xml(f.read())) content.append('&amp;lt;/file&amp;gt;') content.append('&amp;lt;/source&amp;gt;') return '\n'.join(content) formatted_content = process_local_directory(local_path) print("all files processed.") return formatted_content def process_arxiv_pdf(arxiv_abs_url): pdf_url = arxiv_abs_url.replace("/abs/", "/pdf/") + ".pdf" response = requests.get(pdf_url) pdf_content = response.content open('temp.pdf', 'wb') pdf_file: pdf_file.write(pdf_content) text = [] open('temp.pdf', 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) page range(len(pdf_reader.pages)): text.append(pdf_reader.pages[page].extract_text()) formatted_text = f'&amp;lt;source type="arxiv_paper" url="{arxiv_abs_url}"&amp;gt;\n' formatted_text += '&amp;lt;paper&amp;gt;\n' formatted_text += escape_xml(' '.join(text)) formatted_text += '\n&amp;lt;/paper&amp;gt;\n' formatted_text += '&amp;lt;/source&amp;gt;' os.remove('temp.pdf') print("arxiv paper processed successfully.") return formatted_text def extract_links(input_file, output_file): url_pattern = re.compile(r'http[s]?://(?:[a-za-z]|[0-9]|[$-_@.&amp;amp;+]|[!*\\(\\),]|(?:%[0-9a-fa-f][0-9a-fa-f]))+') open(input_file, 'r', encoding='utf-8') file: content = file.read() urls = re.findall(url_pattern, content) open(output_file, 'w', encoding='utf-8') output: url urls: output.write(url + '\n') def fetch_youtube_transcript(url): def extract_video_id(url): pattern = r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\s+\/|(?:v|e(?:mbed)?)\/|\s*?[?&amp;amp;]v=)|youtu\.be\/)([a-za-z0-9_-]{11})' match = re.search(pattern, url) match: return match.group(1) return none video_id = extract_video_id(url) video_id: return f'&amp;lt;source type="youtube_transcript" url="{escape_xml(url)}"&amp;gt;\n&amp;lt;error&amp;gt;could extract video id url.&amp;lt;/error&amp;gt;\n&amp;lt;/source&amp;gt;' try: transcript_list = youtubetranscriptapi.get_transcript(video_id) formatter = textformatter() transcript = formatter.format_transcript(transcript_list) formatted_text = f'&amp;lt;source type="youtube_transcript" url="{escape_xml(url)}"&amp;gt;\n' formatted_text += '&amp;lt;transcript&amp;gt;\n' formatted_text += escape_xml(transcript) formatted_text += '\n&amp;lt;/transcript&amp;gt;\n' formatted_text += '&amp;lt;/source&amp;gt;' return formatted_text except exception e: return f'&amp;lt;source type="youtube_transcript" url="{escape_xml(url)}"&amp;gt;\n&amp;lt;error&amp;gt;{escape_xml(str(e))}&amp;lt;/error&amp;gt;\n&amp;lt;/source&amp;gt;' def preprocess_text(input_file, output_file): open(input_file, "r", encoding="utf-8") input_file: input_text = input_file.read() def process_text(text): text = re.sub(r"[\n\r]+", "\n", text) # update following line include apostrophes quotation marks text = re.sub(r"[^a-za-z0-9\s_.,!?:;@#$%^&amp;amp;*()+\-=[\]{}|\\&amp;lt;&amp;gt;`~'\"/]+", "", text) text = re.sub(r"\s+", " ", text) text = text.lower() words = text.split() words = [word word words word stop_words] return " ".join(words) try: # try parse input xml root = et.fromstring(input_text) # process text content preserving xml structure elem root.iter(): elem.text: elem.text = process_text(elem.text) elem.tail: elem.tail = process_text(elem.tail) # write processed xml output file tree = et.elementtree(root) tree.write(output_file, encoding="utf-8", xml_declaration=true) print("text preprocessing completed xml structure preserved.") except et.parseerror: # xml parsing fails, process text without preserving xml structure processed_text = process_text(input_text) open(output_file, "w", encoding="utf-8") out_file: out_file.write(processed_text) print("xml parsing failed. text preprocessing completed without xml structure.") def get_token_count(text, disallowed_special=[], chunk_size=1000): enc = tiktoken.get_encoding("cl100k_base") # remove xml tags text_without_tags = re.sub(r'&amp;lt;[^&amp;gt;]+&amp;gt;', '', text) # split text smaller chunks chunks = [text_without_tags[i:i+chunk_size] range(0, len(text_without_tags), chunk_size)] total_tokens = 0 chunk chunks: tokens = enc.encode(chunk, disallowed_special=disallowed_special) total_tokens += len(tokens) return total_tokens def is_same_domain(base_url, new_url): return urlparse(base_url).netloc == urlparse(new_url).netloc def is_within_depth(base_url, current_url, max_depth): base_parts = urlparse(base_url).path.rstrip('/').split('/') current_parts = urlparse(current_url).path.rstrip('/').split('/') current_parts[:len(base_parts)] != base_parts: return false return len(current_parts) - len(base_parts) &amp;lt;= max_depth def process_pdf(url): response = requests.get(url) response.raise_for_status() open('temp.pdf', 'wb') pdf_file: pdf_file.write(response.content) text = [] open('temp.pdf', 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) page range(len(pdf_reader.pages)): text.append(pdf_reader.pages[page].extract_text()) os.remove('temp.pdf') return ' '.join(text) def crawl_and_extract_text(base_url, max_depth, include_pdfs, ignore_epubs): visited_urls = set() urls_to_visit = [(base_url, 0)] processed_urls = [] all_text = [f'&amp;lt;source type="web_documentation" url="{escape_xml(base_url)}"&amp;gt;'] urls_to_visit: current_url, current_depth = urls_to_visit.pop(0) clean_url = current_url.split('#')[0] clean_url visited_urls is_same_domain(base_url, clean_url) is_within_depth(base_url, clean_url, max_depth): ignore_epubs clean_url.endswith('.epub'): continue try: response = requests.get(current_url) soup = beautifulsoup(response.content, 'html.parser') visited_urls.add(clean_url) clean_url.endswith('.pdf') include_pdfs: text = process_pdf(clean_url) else: element soup(['script', 'style', 'head', 'title', 'meta', '[document]']): element.decompose() comments = soup.find_all(string=lambda text: isinstance(text, comment)) comment comments: comment.extract() text = soup.get_text(separator='\n', strip=true) all_text.append(f'&amp;lt;page url="{escape_xml(clean_url)}"&amp;gt;') all_text.append(escape_xml(text)) all_text.append('&amp;lt;/page&amp;gt;') processed_urls.append(clean_url) print(f"processed: {clean_url}") current_depth &amp;lt; max_depth: link soup.find_all('a', href=true): new_url = urljoin(current_url, link['href']).split('#')[0] new_url visited_urls is_within_depth(base_url, new_url, max_depth) (include_pdfs new_url.endswith('.pdf')) (ignore_epubs new_url.endswith('.epub')): urls_to_visit.append((new_url, current_depth + 1)) except requests.requestexception e: print(f"failed retrieve {clean_url}: {e}") all_text.append('&amp;lt;/source&amp;gt;') formatted_content = '\n'.join(all_text) return { 'content': formatted_content, 'processed_urls': processed_urls } def process_doi_or_pmid(identifier): headers = { 'user-agent': 'mozilla/5.0 (windows nt 6.3) applewebkit/537.36 (khtml, like gecko) chrome/98.0.4758.102 safari/537.36', 'connection': 'keep-alive' } try: payload = { 'sci-hub-plugin-check': '', 'request': identifier } base_url = 'https://sci-hub.se/' response = requests.post(base_url, headers=headers, data=payload, timeout=60) soup = beautifulsoup(response.content, 'html.parser') pdf_element = soup.find(id='pdf') pdf_element none: raise valueerror(f"no pdf found identifier {identifier}. sci-hub might inaccessible document available.") content = pdf_element.get('src').replace('#navpanes=0&amp;amp;view=fith', '').replace('//', '/') content.startswith('/downloads'): pdf_url = 'https://sci-hub.se' + content elif content.startswith('/tree'): pdf_url = 'https://sci-hub.se' + content elif content.startswith('/uptodate'): pdf_url = 'https://sci-hub.se' + content else: pdf_url = 'https:/' + content pdf_filename = f"{identifier.replace('/', '-')}.pdf" wget.download(pdf_url, pdf_filename) open(pdf_filename, 'rb') pdf_file: pdf_reader = pdfreader(pdf_file) text = "" page range(len(pdf_reader.pages)): text += pdf_reader.pages[page].extract_text() formatted_text = f'&amp;lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&amp;gt;\n' formatted_text += '&amp;lt;paper&amp;gt;\n' formatted_text += escape_xml(text) formatted_text += '\n&amp;lt;/paper&amp;gt;\n' formatted_text += '&amp;lt;/source&amp;gt;' os.remove(pdf_filename) print(f"identifier {identifier} processed successfully.") return formatted_text except (requests.requestexception, valueerror) e: error_text = f'&amp;lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&amp;gt;\n' error_text += f'&amp;lt;error&amp;gt;{escape_xml(str(e))}&amp;lt;/error&amp;gt;\n' error_text += '&amp;lt;/source&amp;gt;' print(f"error processing identifier {identifier}: {str(e)}") print("sci-hub appears inaccessible document found. please try later.") return error_text def process_github_pull_request(pull_request_url): url_parts = pull_request_url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] pull_request_number = url_parts[-1] api_base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/pulls/{pull_request_number}" headers = {"authorization": f"token {token}"} response = requests.get(api_base_url, headers=headers) pull_request_data = response.json() diff_url = pull_request_data["diff_url"] diff_response = requests.get(diff_url, headers=headers) pull_request_diff = diff_response.text comments_url = pull_request_data["comments_url"] review_comments_url = pull_request_data["review_comments_url"] comments_response = requests.get(comments_url, headers=headers) review_comments_response = requests.get(review_comments_url, headers=headers) comments_data = comments_response.json() review_comments_data = review_comments_response.json() all_comments = comments_data + review_comments_data all_comments.sort(key=lambda comment: comment.get("position") float("inf")) formatted_text = f'&amp;lt;source type="github_pull_request" url="{pull_request_url}"&amp;gt;\n' formatted_text += '&amp;lt;pull_request_info&amp;gt;\n' formatted_text += f'&amp;lt;title&amp;gt;{escape_xml(pull_request_data["title"])}&amp;lt;/title&amp;gt;\n' formatted_text += f'&amp;lt;description&amp;gt;{escape_xml(pull_request_data["body"])}&amp;lt;/description&amp;gt;\n' formatted_text += '&amp;lt;merge_details&amp;gt;\n' formatted_text += f'{escape_xml(pull_request_data["user"]["login"])} wants merge {pull_request_data["commits"]} commit {repo_owner}:{pull_request_data["base"]["ref"]} {pull_request_data["head"]["label"]}\n' formatted_text += '&amp;lt;/merge_details&amp;gt;\n' formatted_text += '&amp;lt;diff_and_comments&amp;gt;\n' diff_lines = pull_request_diff.split("\n") comment_index = 0 line diff_lines: formatted_text += f'{escape_xml(line)}\n' comment_index &amp;lt; len(all_comments) all_comments[comment_index].get("position") == diff_lines.index(line): comment = all_comments[comment_index] formatted_text += f'&amp;lt;review_comment&amp;gt;\n' formatted_text += f'&amp;lt;author&amp;gt;{escape_xml(comment["user"]["login"])}&amp;lt;/author&amp;gt;\n' formatted_text += f'&amp;lt;content&amp;gt;{escape_xml(comment["body"])}&amp;lt;/content&amp;gt;\n' formatted_text += f'&amp;lt;path&amp;gt;{escape_xml(comment["path"])}&amp;lt;/path&amp;gt;\n' formatted_text += f'&amp;lt;line&amp;gt;{comment["original_line"]}&amp;lt;/line&amp;gt;\n' formatted_text += '&amp;lt;/review_comment&amp;gt;\n' comment_index += 1 formatted_text += '&amp;lt;/diff_and_comments&amp;gt;\n' formatted_text += '&amp;lt;/pull_request_info&amp;gt;\n' repo_url = f"https://github.com/{repo_owner}/{repo_name}" repo_content = process_github_repo(repo_url) formatted_text += '&amp;lt;repository&amp;gt;\n' formatted_text += repo_content formatted_text += '&amp;lt;/repository&amp;gt;\n' formatted_text += '&amp;lt;/source&amp;gt;' print(f"pull request {pull_request_number} repository content processed successfully.") return formatted_text def escape_xml(text): return ( str(text) .replace("&amp;amp;", "&amp;amp;amp;") .replace("&amp;lt;", "&amp;amp;lt;") .replace("&amp;gt;", "&amp;amp;gt;") # remove following lines stop converting apostrophes quotes # .replace("\"", "&amp;amp;quot;") # .replace("'", "&amp;amp;apos;") ) def process_github_issue(issue_url): url_parts = issue_url.split("/") repo_owner = url_parts[3] repo_name = url_parts[4] issue_number = url_parts[-1] api_base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/issues/{issue_number}" headers = {"authorization": f"token {token}"} response = requests.get(api_base_url, headers=headers) issue_data = response.json() comments_url = issue_data["comments_url"] comments_response = requests.get(comments_url, headers=headers) comments_data = comments_response.json() formatted_text = f'&amp;lt;source type="github_issue" url="{issue_url}"&amp;gt;\n' formatted_text += '&amp;lt;issue_info&amp;gt;\n' formatted_text += f'&amp;lt;title&amp;gt;{escape_xml(issue_data["title"])}&amp;lt;/title&amp;gt;\n' formatted_text += f'&amp;lt;description&amp;gt;{escape_xml(issue_data["body"])}&amp;lt;/description&amp;gt;\n' formatted_text += '&amp;lt;comments&amp;gt;\n' comment comments_data: formatted_text += '&amp;lt;comment&amp;gt;\n' formatted_text += f'&amp;lt;author&amp;gt;{escape_xml(comment["user"]["login"])}&amp;lt;/author&amp;gt;\n' formatted_text += f'&amp;lt;content&amp;gt;{escape_xml(comment["body"])}&amp;lt;/content&amp;gt;\n' code_snippets = re.findall(r'https://github.com/.*#l\d+-l\d+', comment['body']) snippet_url code_snippets: url_parts = snippet_url.split("#") file_url = url_parts[0].replace("/blob/", "/raw/") line_range = url_parts[1] start_line, end_line = map(int, line_range.split("-")[0][1:]), map(int, line_range.split("-")[1][1:]) file_response = requests.get(file_url, headers=headers) file_content = file_response.text code_lines = file_content.split("\n")[start_line-1:end_line] code_snippet = "\n".join(code_lines) formatted_text += '&amp;lt;code_snippet&amp;gt;\n' formatted_text += f'&amp;lt;![cdata[{code_snippet}]]&amp;gt;\n' formatted_text += '&amp;lt;/code_snippet&amp;gt;\n' formatted_text += '&amp;lt;/comment&amp;gt;\n' formatted_text += '&amp;lt;/comments&amp;gt;\n' formatted_text += '&amp;lt;/issue_info&amp;gt;\n' repo_url = f"https://github.com/{repo_owner}/{repo_name}" repo_content = process_github_repo(repo_url) formatted_text += '&amp;lt;repository&amp;gt;\n' formatted_text += repo_content formatted_text += '&amp;lt;/repository&amp;gt;\n' formatted_text += '&amp;lt;/source&amp;gt;' print(f"issue {issue_number} repository content processed successfully.") return formatted_text def main(): console = console() intro_text = text("\ninput paths urls processed:\n", style="dodger_blue1") input_types = [ (" local folder path (flattens files text)", "bright_white"), (" github repository url (flattens files text)", "bright_white"), (" github pull request url (pr + repo)", "bright_white"), (" github issue url (issue + repo)", "bright_white"), (" documentation url (base url)", "bright_white"), (" youtube video url (to fetch transcript)", "bright_white"), (" arxiv paper url", "bright_white"), (" doi pmid search sci-hub", "bright_white"), ] input_type, color input_types: intro_text.append(f"\n{input_type}", style=color) intro_panel = panel( intro_text, expand=false, border_style="bold", title="[bright_white]copy file clipboard[/bright_white]", title_align="center", padding=(1, 1), ) console.print(intro_panel) len(sys.argv) &amp;gt; 1: input_path = sys.argv[1] else: input_path = prompt.ask("\n[bold dodger_blue1]enter path url[/bold dodger_blue1]", console=console) console.print(f"\n[bold bright_green]you entered:[/bold bright_green] [bold bright_yellow]{input_path}[/bold bright_yellow]\n") output_file = "uncompressed_output.txt" processed_file = "compressed_output.txt" urls_list_file = "processed_urls.txt" progress( textcolumn("[bold bright_blue]{task.description}"), barcolumn(bar_width=none), timeremainingcolumn(), console=console, ) progress: task = progress.add_task("[bright_blue]processing...", total=100) try: "github.com" input_path: "/pull/" input_path: final_output = process_github_pull_request(input_path) elif "/issues/" input_path: final_output = process_github_issue(input_path) else: final_output = process_github_repo(input_path) elif urlparse(input_path).scheme ["http", "https"]: "youtube.com" input_path "youtu.be" input_path: final_output = fetch_youtube_transcript(input_path) elif "arxiv.org" input_path: final_output = process_arxiv_pdf(input_path) else: crawl_result = crawl_and_extract_text(input_path, max_depth=2, include_pdfs=true, ignore_epubs=true) final_output = crawl_result['content'] open(urls_list_file, 'w', encoding='utf-8') urls_file: urls_file.write('\n'.join(crawl_result['processed_urls'])) elif input_path.startswith("10.") "/" input_path input_path.isdigit(): final_output = process_doi_or_pmid(input_path) else: final_output = process_local_folder(input_path) progress.update(task, advance=50) # write uncompressed output open(output_file, "w", encoding="utf-8") file: file.write(final_output) # process compressed output preprocess_text(output_file, processed_file) progress.update(task, advance=50) compressed_text = safe_file_read(processed_file) compressed_token_count = get_token_count(compressed_text) console.print(f"\n[bright_green]compressed token count:[/bright_green] [bold bright_cyan]{compressed_token_count}[/bold bright_cyan]") uncompressed_text = safe_file_read(output_file) uncompressed_token_count = get_token_count(uncompressed_text) console.print(f"[bright_green]uncompressed token count:[/bright_green] [bold bright_cyan]{uncompressed_token_count}[/bold bright_cyan]") console.print(f"\n[bold bright_yellow]{processed_file}[/bold bright_yellow] [bold bright_blue]{output_file}[/bold bright_blue] created working directory.") pyperclip.copy(uncompressed_text) console.print(f"\n[bright_white]the contents [bold bright_blue]{output_file}[/bold bright_blue] copied clipboard.[/bright_white]") except exception e: console.print(f"\n[bold red]an error occurred:[/bold red] {str(e)}") console.print("\nplease check input try again.") raise # re-raise exception debugging purposes __name__ == "__main__": main() &lt;/file&gt; &lt;file name="src/python/rag/create_vector_db.py"&gt; import os sentence_transformers import sentencetransformer langchain_community.vectorstores import faiss langchain.text_splitter import recursivecharactertextsplitter langchain_huggingface import huggingfaceembeddings, huggingfaceendpoint dotenv import load_dotenv load_dotenv() huggingfacehub_api_token = os.getenv('huggingfacehub_api_token') # step 1: read file def read_file(file_path): open(file_path, 'r', encoding='utf-8') file: return file.read() # step 2: preprocess data def preprocess_text(text): # clean split text chunks text_splitter = recursivecharactertextsplitter(chunk_size=1200, chunk_overlap=300) return text_splitter.split_text(text) # step 3: vectorization indexing def create_vector_store(docs): # use sentencetransformer embeddings # model = sentencetransformer('all-minilm-l6-v2') # embeddings = model.encode(docs) #, convert_to_tensor=true) # generate embeddings documents embeddings = huggingfaceembeddings() text_embeddings = embeddings.embed_documents(docs) text_embedding_pairs = zip(docs, text_embeddings) vector_store = faiss.from_embeddings(text_embedding_pairs, embeddings) vector_store.save_local("faiss_aidoc") # save index locally return vector_store # main function run program def main(): file_path = 'uncompressed_output.txt' text = read_file(file_path) docs = preprocess_text(text) vector_store = create_vector_store(docs) __name__ == "__main__": main() &lt;/file&gt; &lt;file name="src/python/rag/generic_functions.py"&gt; import requests import os import nbconvert import pythonexporter import nbformat nltk.corpus import stopwords import nltk def is_allowed_filetype(filename): allowed_extensions = ['.py', '.txt', '.js', '.tsx', '.ts', '.md', '.cjs', '.html', '.json', '.ipynb', '.h', '.localhost', '.sh', '.yaml', '.example', '.ps1', '.sql'] return any(filename.endswith(ext) ext allowed_extensions) def download_file(url, target_path, headers): response = requests.get(url, headers=headers) response.raise_for_status() open(target_path, "wb") f: f.write(response.content) def escape_xml(text): return ( str(text) .replace("&amp;amp;", "&amp;amp;amp;") .replace("&amp;lt;", "&amp;amp;lt;") .replace("&amp;gt;", "&amp;amp;gt;") # remove following lines stop converting apostrophes quotes # .replace("\"", "&amp;amp;quot;") # .replace("'", "&amp;amp;apos;") ) def process_ipynb_file(temp_file): open(temp_file, "r", encoding='utf-8', errors='ignore') f: notebook_content = f.read() exporter = pythonexporter() python_code, _ = exporter.from_notebook_node(nbformat.reads(notebook_content, as_version=4)) return python_code ## new: check def is_same_domain(base_url, new_url): return urlparse(base_url).netloc == urlparse(new_url).netloc def is_within_depth(base_url, current_url, max_depth): base_parts = urlparse(base_url).path.rstrip('/').split('/') current_parts = urlparse(current_url).path.rstrip('/').split('/') current_parts[:len(base_parts)] != base_parts: return false return len(current_parts) - len(base_parts) &amp;lt;= max_depth def extract_links(input_file, output_file): url_pattern = re.compile(r'http[s]?://(?:[a-za-z]|[0-9]|[$-_@.&amp;amp;+]|[!*\\(\\),]|(?:%[0-9a-fa-f][0-9a-fa-f]))+') open(input_file, 'r', encoding='utf-8') file: content = file.read() urls = re.findall(url_pattern, content) open(output_file, 'w', encoding='utf-8') output: url urls: output.write(url + '\n') def get_stopword_list(): nltk.download("stopwords", quiet=true) stop_words = set(stopwords.words("english")) return stop_words &lt;/file&gt; &lt;file name="src/python/rag/prompt_llm.py"&gt; langchain_huggingface import huggingfaceembeddings, huggingfaceendpoint langchain_community.vectorstores import faiss # step 1: set llm def load_llm(): return huggingfaceendpoint(repo_id="mistralai/mixtral-8x7b-instruct-v0.1") # step 2: query function def query_llm(llm, vector_store, user_query): retriever = vector_store.as_retriever() # context = retriever.retrieve(user_query) # retrieve relevant documents relevant_docs = retriever.invoke(user_query) # combine context user query context = " ".join(doc.page_content doc relevant_docs) response = llm.generate([context + user_query]) return response # main function run program def main(): user_query = input("ask question: \n") llm = load_llm() embeddings = huggingfaceembeddings() vector_store = faiss.load_local("faiss_aidoc", embeddings, allow_dangerous_deserialization=true) answer = query_llm(llm, vector_store, user_query) print("answer:", answer) __name__ == "__main__": main() &lt;/file&gt; &lt;/source&gt;</file></source>