<?xml version='1.0' encoding='utf-8'?>
<source type="local_directory" path="C:\Users\simon.plancke\OneDrive - Keyrus\Documents\repositories\dap_appl_dala_code"><file name="README.md"># dap_appl_dala_code ## description repo contains code related underlying architecture ingestion process. relates sql tables stored procedures, also synapse pipelines spark notebooks. two important things here, [notebooks](synapse/notebooks/) [sql-code](sql/). would like clearer overview one tickets, user stories related items new ingestion process, kindly refer [dap_appl_dala_config repo](https://dev.azure.com/lnlbe/data_platform/_git/dap_appl_dala_config). find configuration related documents well ongoing projects. ## synapse synapse short azure synapase workspace. find code related workspace, notebooks pipelines. ### notebooks notebooks repo structured stimulate object-oriented programming. #### functions functions include methods used notebooks, group several actions together specific method. here, find functions clean containers, well functions aid ingestion process. lastly, there's also unit test notebook present check functions still functioning properly. #### modules modules contains core notebooks make sure ingestion process works intended. leverages methods 'functions' folders well logic. here, also able find unit test check classes.ipynb still working properly. #### scrapnotebooks notebooks used test individual parts code, used ingestion process. #### workers workers executors core logic regarding ingestion process. leverages logic classes.ipynb, uses make sure ingestion process runs intended. ## sql sql umbrella contains sql-logic regarding new ingestion process. find general structures sql-database lnl_meta here, contains ingestion configuration &amp; logging tables, well stored procedures. ### sql-databases sql-databases provide tables used gather data start synapase pipeline run. #### configuration tables | name | purpose | | --- | --- | | meta.plan_configuration | contains available plans run pipeline. give plans description (dis/en)able them. | | meta.plan_task_configuration | contains combination plans tasks. contains overview tasks executed plan initiated, divided task groups. | | meta.source_column_configuration | contains available headers different configured datasets. look original name, sink name, order, dimension datatype. | | meta.source_configuration | contains metadata needed describe file successfully ingest system. | | meta.task_configuration | contains information expected source sync location data source, data file structure. | #### logging tables | name | purpose | | --- | --- | | meta.log_files | contains files moved copied least once, displays locations copies / movements file. | | meta.log_plan_tasks | contains log tasks need executed certain logged plan. | | meta.log_plans | contains long run plan lasted, status. last_pipeline_id refers latest id synapse pipeline tried running plan. | | meta.log_run_plans | contains plans run, pipeline was. | | meta.log_runs | contains long full pipeline run lasted, failed not. includes start-up time well run plan(s). | | meta.log_tasks | contains logs tasks need executed certain logged plan. check status see succeeded failed. | #### stored procedures stored procedure like function sql. mainly used add, update, and/or retrieve data existing sql tables. example, new synapse pipeline run started, stored procedure called add new rows logging tables. run completed, stored procedure update current status run. want know different tests, check [stored procedures](https://dev.azure.com/lnlbe/data_platform/_git/dap_appl_dala_code?path=/sql/metadb/stored_procedures) repo. | name | purpose | tests | | --- | --- | --- | | meta.usp_end_plan | end plan meta.log_plans completed (failed / succeeded) | fail bad parameters | | meta.usp_end_run | end run meta.log_runs completed (failed / succeeded) | fail bad parameters | meta.usp_end_task | end task meta.log_tasks completed (failed / succeeded)|fail bad parameters | | meta.usp_get_plan_metadata | get metadata configured plan | fail bad parameters | | meta.usp_get_plans | collect plans are: pending / failed &amp; plan_name activated plan | fail bad parameters | | meta.usp_get_task_metadata | get metadata task trying execute | fail bad parameters | | meta.usp_get_tasks | get tasks linked plan | fail bad parameters | | meta.usp_new_file | log new file meta.log_files table | fail bad parameters | | meta.usp_new_run | log new run logging tables &amp; new_plan = 1: add new plans tasks logging tables | fail bad parameters | | meta.usp_start_plan | set status plan "in progress" meta.log_plans | fail bad parameters | | meta.usp_start_run | set status run "in progress" meta.log_runs | fail bad parameters | | meta.usp_start_task | set status task "in progress" meta.log_tasks | fail bad parameters | | meta.usp_update_file_activity | update metadata file record (if file moving b, new location stored meta.log_files) | fail bad parameters |</file><file name="Technical_debt.md">welcome simon's brain * parametertemplate: check generate different linked services managed endpoints depending synapse target ws - synapse workspace deployment task source code: https://github.com/azure/synapse-workspace-deployment/tree/master - community question managed private endpoints: https://github.com/azure/synapse-workspace-deployment/issues/73 - guide: ci/cd w custom parameters: https://techcommunity.microsoft.com/t5/fasttrack-for-azure/azure-synapse-analytics-ci-cd-with-custom-parameters-made-easy/ba-p/3802517 - microsoft ci/cd page synapse: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/continuous-integration-delivery#create-custom-parameters-in-the-workspace-template =&gt; target: enable 'deleteartifactsnotintemplate' running task * extensive error-message xml files (check_test_files.ps1) * user guide new check introduced system - add check all_checks dict, add parameters allow_parameters list, etc. * decimal currently converted int, change decimal gets stored properly - unittest_spark_checks shows -&gt; price column decimal values, gets stored int * add advanced tsqlt-tests stored procedures - basics tests failed bad params, needs added * add extensive unit tests tsqlt -&gt; deploy.usp_set_task_configuration set_up file, used (keep save future tests) -&gt; deploy.deploy.usp_set_source_configuration set_up file, used (keep save future tests) -&gt; deploy.usp_set_source_column_configuration set_up file, used (keep save future tests) -&gt; deploy.set_source_check_config set_up file, used (keep save future tests) -&gt; deploy.set_plan_task_config set_up file, used (keep save future tests) -&gt; deploy.set_plan_config set_up file, used (keep save future tests) * meta.source_configuration.sql, [header] column set nvarchar, deploy.usp_set_source_configuration bit -&gt; needs changed nvarchar deploy... -&gt; consistency! -&gt; work, change bit inside meta... -&gt; option 1: changing nvarchar break logic inside data config module, possible anyways? -&gt; option 2: changing bit break logic notebooks, needs updated well (now extracted string, bit / int / bool, saved nvarchar. checked string code 'if header == "true"') -&gt; option 1 (changing nvarchar) easier do, option 2 (changing bit)? * delta table deployment currently allowed ingestionworker. needs changed future well. -&gt; see dev-note src/powershell/modules/dataconfig/public/set-ingestionmetadatafromjsonfile.ps1 deploydeltatables switch improvements: * write python file test result file, folder _success file * rewrite check_test_logs.ps1 -&gt; possible make script clean -&gt; already preprocessing python? -&gt; less steps? necessary use xml? -&gt; ... * rewrite try / except functions according best practices. general try / except, except exception: (https://python.land/deep-dives/python-try-except) * add sleep invoke_notebook.ps1 inside logs (while $notfinished) * create script / function closes spark pools everything done * ingestion.source_folder currently looks recursively folder that's provided (you need put /** it) fixed future (see ingestionworker_csv_recurse_search) * upload test scripts certain folder check folder empty/contains expected files running pipeline * update logic sqlserverinstance, currently gets overwritten inside localdeploy_synapse. name gets used multiple times, causes 'bug' variable parent script suddenly value passed / changed inside child script * handling config_params inside checks: currently string, really going change array flat json objects / dictionaries? * localbuild-powershell-module: unit tests fail, module still gets deployed locally. case. * create generic install-module step devops ci/cd (dataconfig, networking, tokenmodule) * recreate default managed private endpoints dev synapse workspace -&gt; still refers sbx * replace linked service parameters environment changes (dev, int, tst, acc, prd) using tokenization instead overwrite (if possible) * make notebook-tokenization spark configuration dynamic -&gt; current code give notebooks spark configuration (core_configuration) (tokenize_synapse_notebooks.ps1 tokenization/detokenize_synapse_arm.ps1) * allow 'true' 'hasheaders' parameter (class: calculatedfieldheaders, inside dataframe module) * use parameters start_index &amp; end_index (class: calculatedfieldheaders, called inside filworker). always 1 &amp; 3 respectively. * filter task_group using queries pipelines/scripts/sql/prepare_logging_tables.ps1 filter task_group, needed unit testing plan different task groups (e.g. "preprocess" &amp; "ingest" combined one plan -&gt; script return tasks probably mix them) * check possible rewrite synapse tests: use openrowset, read directly silver table * trabuf issues: nvarchar(8000) json-column cannot handle full json printing -&gt; sure issue writing table using 'select' statement clean up: * validate_ingestionworker: script finalised could improved based quick glance. * run_notebook: delete script replace references invoke_notebook * replace sandbox references managed private endpoint tokenization script (tokenize_managed_private_endpoints.ps1) * overwrite tokenmodule reference: called test artefact feed -&gt; call main * add documentation detokenize script devops: make sure devops users know done * savedataframe() method inside calculatedfieldheaders class, inside dataframes module -&gt; put inside ingestionfunctions seperate function - could handy future cases r&amp;d: * usage "import warnings" inside pyspark notebooks. necessary? -- https://stackoverflow.com/questions/3891804/raise-warning-in-python-without-interrupting-program</file><file name=".vscode\extensions.json">{ "recommendations": [ "ms-azure-devops.azure-pipelines", "ms-mssql.mssql" ] }</file><file name=".vscode\settings.json">{ "files.associations": { "**/pipelines/**/*.yaml": "azure-pipelines", "**/pipelines/**/*.yml": "azure-pipelines" }, "powershell.pester.debugoutputverbosity": "detailed", "powershell.pester.outputverbosity": "detailed", "powershell.pester.uselegacycodelens": false }</file><file name="configuration\synapse\parameters.json">{ "default": { "workspacename": "#{target_environment}#-dap-syn-core", "ls_dap_sql_meta_connectionstring": "integrated security=false;encrypt=true;connection timeout=30;data source=#{target_environment}#-dap-sql-core.database.windows.net;initial catalog=#{target_environment}#-dap-sqldb-core-meta", "ls_dap_adls_01_properties_typeproperties_url": "https://#{target_environment}#dapstdala1.dfs.core.windows.net/", "ls_dap_kv_core_properties_typeproperties_baseurl": "https://#{target_environment}#-dap-key-core-syn.vault.azure.net/", "(.*)_properties_bigdatapool_referencename": "#{target_environment}#dapsspcore", "pl_(.*)_properties_variables_env_code_defaultvalue": "#{target_environment}#", "pl_(.*)_properties_variables_spark_pool_defaultvalue": "#{target_environment}#dapsspcore", "(.*)_properties_sessionproperties_runasworkspacesystemidentity": "true", "core_configuration_properties_configs_environment_code": "#{target_environment}#", "#{target_environment}#-dap-syn-core-workspacedefaultsqlserver_connectionstring": "data source=tcp:#{target_environment}#-dap-syn-core.sql.azuresynapse.net,1433;initial catalog=@{linkedservice().dbname}", "#{target_environment}#-dap-syn-core-workspacedefaultstorage_properties_typeproperties_url": "https://#{target_environment}#dapstcoresyn.dfs.core.windows.net/" }, "dev": { "workspacename": "dev-dap-syn-core" } }</file><file name="configuration\synapse\synapse_workspace_tokens.json">{ "managed_private_endpoints": [ { "synapse-ws-dapstdala1": { "resource_group": "#{target_environment}#-dap-rg-dala", "storage_account": "#{target_environment}#dapstdala1" } }, { "synapse-ws-dap-sql_core": { "server": "#{target_environment}#-dap-sql-core" } }, { "synapse-ws-dap-key-core-syn": { "vault": "#{target_environment}#-dap-key-core-syn", "fqdns": "#{target_environment}#-dap-key-core-syn.vault.azure.net" } } ], "notebooks": [ { "default": { "targetsparkconfiguration": "core_configuration" } } ] }</file><file name="development\helper_scripts\delete_git_branches.ps1"># preview branches pruned # git remote prune origin --dry-run # real execution # git remote prune origin --dry-run # show branches status, shows "gone" deleted remote # git branch -vv # list ones showing "gone" # git branch -vv | select-string -pattern "(.*) ....... \[.*: gone\]" # remove local branches remote git remote prune origin # show list "gone" branches deleted locally [array] $branchestodeletelocally = git branch -vv | select-string -pattern "(.*) ........ \[.*: gone\]" | foreach-object {$_.matches} | foreach-object {$_.groups[1].value.trim()} ( $branchestodeletelocally.count -gt 0 ) { write-host "" write-host "list branches deleted" write-host "------------------------------" $branchestodeletelocally # confirm $continue = read-host -prompt "press continue" ($continue -ne "y") { write-output "cancelling" exit 0 } # delete (-d avoid warning/abort warnings, caused squash merges) $branchestodeletelocally | git branch -d $branchestodeletelocally } else { write-host "no branches deleted." }</file><file name="development\helper_scripts\install-modules.ps1">&lt;# .synopsis install main modules .description script installs following modules - sqlserver - az.accounts - az.keyvault - az.resources - az.storage - az.synapse - azuread - dataconfig (self-made module contained repository) script checks whether already ad account connected, requests connect case .parameter workspacefolder optionally provide path workspace folder. provided, value $env:reporoot used. #&gt; [cmdletbinding()] param( [string] $workspacefolder = $env:reporoot #run myenv_xx.ps1 script first ) write-output "check necessary modules installed" # install sqlserver module needed ( -not (get-module "sqlserver" -listavailable) ) { write-information "installing module: sqlserver ..." install-module -name "sqlserver" -scope currentuser -force } # install az.accounts module needed ( -not (get-module "az.accounts" -listavailable) ) { write-information "installing module: az.accounts ..." install-module -name "az.accounts" -scope currentuser -force } # install az.keyvault module needed ( -not (get-module "az.keyvault" -listavailable) ) { write-information "installing module: az.keyvault ..." install-module -name "az.keyvault" -scope currentuser -force } # install az.resources module needed ( -not (get-module "az.resources" -listavailable) ) { write-information "installing module: az.resources ..." install-module -name "az.resources" -scope currentuser -force } # install az.storage module needed ( -not (get-module "az.storage" -listavailable) ) { write-information "installing module: az.storage ..." install-module -name "az.storage" -scope currentuser -force } # install az.synapse module needed ( -not (get-module "az.synapse" -listavailable) ) { write-information "installing module: az.synapse ..." install-module -name "az.synapse" -scope currentuser -force } # install azuread module needed ( -not (get-module "azuread" -listavailable) ) { write-information "installing module: azuread ..." install-module -name "azuread" -scope currentuser -force } # install az.sql module needed ( -not (get-module "az.sql" -listavailable) ) { write-information "installing module: az.sql ..." install-module -name "az.sql" -scope currentuser -force } # install dataconfig module needed ( -not (get-module "dataconfig" -listavailable) ) { write-information "installing nalo module: dataconfig ..." import-module $env:reporoot\src\powershell\modules\dataconfig\dataconfig.psm1 -force }</file><file name="development\helper_scripts\startup.ps1"># check connected azure # not: make connection write-output "check azure ad connection..." $context = get-azcontext ($context){ write-output " already connected azure" } else { connect-azaccount -subscription "lnl-006" }</file><file name="development\helper_scripts\uninstall-old-az-modules.ps1">&lt;# .synopsis removes older az.* modules except latest (installed) version .description tip:the command support -whatif review impact first #&gt; [cmdletbinding(supportsshouldprocess)] param() #local settings $erroractionpreference = "stop" $informationpreference = "continue" set-strictmode -version "latest" foreach ($module (get-module -listavailable az*).name |get-unique) { $modules = [array]((get-module -listavailable $module) ?? @()) ($modules.count -gt 1) { $latest_version = ((get-module -listavailable $module | select-object version | sort-object version)[-1]).tostring() write-output "${module}: latest version ${latest_version}" get-module -listavailable $module | where-object {$_.version -ne $latest_version.version} | foreach-object { write-output " uninstalling: $($_.name) version $($_.version) ..." uninstall-module -name $_.name -requiredversion $_.version -errorvariable "errorvar" -erroraction "silentlycontinue" #-verbose ( $errorvar) { ( $errorvar | where-object { $_.exception.message -like "*no match found specified search criteria module names*"} ) { # known problem modules onedrive $modulepath = $_.path | split-path write-output " uninstall failed, probably known issue onedrive" write-output " try delete module folder: $modulepath ..." remove-item -path $modulepath -recurse -force -confirm:$false } else { write-output " unexpected error(s) uninstall: " $errorvar | foreach-object { write-error " $($_)"} } } } } else { write-output "${module}: one version installed $($modules[0].version.tostring())" } }</file><file name="development\local-env\myenv-template.ps1">&lt;# template local developer settings. copy file "myenv.ps1" /development/local-env folder update variables personal settings. myenv.ps1 file already foreseen .gitignore stored remote git repository. #&gt; write-output "$($env:username) environment" write-output "found powershell version $($psversiontable.psversion), edition $($psversiontable.psedition)" # user logging azure $env:userprincipalname = "&lt;firstname&gt;.&lt;lastname&gt;[.ext]@nationale-loterij.be" # folder git repo cloned $env:reporoot = "c:\my\path\to\dap_appl_dala_code" # sandbox (the environment part resources resourcgroups) $env:mydevenv = "dev" # full path sqlpackage.exe (if available path, leave "sqlpackage.exe") $env:sqlpackagepath = "sqlpackage.exe" # location dev database $env:sql_server = "dev-dap-sql-core.database.windows.net" $env:sql_database = "dev-dap-sqldb-core-meta" # parameters synapse workspace $env:synapse_ws = 'dev-dap-syn-core' $env:synapse_server = 'dev-dap-syn-core-ondemand.sql.azuresynapse.net' $env:synapse_database = 'default' # check modules installed . $env:reporoot\development\helper_scripts\install-modules.ps1 # common startup commands . $env:reporoot\development\helper_scripts\startup.ps1</file><file name="development\localdeploy\localbuild_dataconfig_module.ps1">&lt;# .synopsis script locally test build dataconfig powershell module generate nuget package. .description 1. runs unit tests 2. makes local copy module folder (because build process modifies files, changes want commit) 3. local copy: run build script prepare .psd1 file 4. packages module nuget package note: requires nuget.exe available path. * download nuget.exe (latest: 6.8.0) https://www.nuget.org/downloads * make sure added path .notes script use following environment variables parameters provided: * $env:reporoot #&gt; [cmdletbinding()] param ( [parameter(mandatory = $false, helpmessage = 'the folder repo cloned, leave empty use $env:reporoot') ] [string] $workspacefolder = $null , # local folder repo cloned # optional switches skip steps (useful running consecutively without real changes) [switch] $skipunittests ) # ================= # config # ================= # force error action set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = "continue" # check local environment configured ( -not $env:reporoot) { #throw "environment variable reporoot set, run local-env script first " . "$psscriptroot/../local-env/myenv.ps1" } (-not $workspacefolder) { $workspacefolder = $env:reporoot } $modulesfolder = join-path $workspacefolder "src" "powershell" "modules" ( -not (test-path -path $modulesfolder -pathtype "container")) { throw "could fine modulesfolder: $modulesfolder" } $modulename = "dataconfig" write-output "using modulesfolder: $modulesfolder" write-output "using modulename : $modulename" # run unit tests (-not $skipunittests){ $testscriptpath = join-path $modulesfolder $modulename ".build" "unit_tests.ps1" if( -not (test-path $testscriptpath) ) { throw "could locate script: $testscriptpath" } &amp; $testscriptpath } else { write-output "" write-output "(...skipping unit tests...)" } # make copy module write-output "" write-output "creating local copy module files..." $origfolder = join-path $modulesfolder $modulename $localroot = join-path $modulesfolder ".local" $localfolder = join-path $localroot $modulename write-output " using origfolder : $origfolder" write-output " using localroot : $localroot" write-output " using localfolder : $localfolder" # check ".local" folder exists ( -not ( test-path $localroot )) { write-output " creating localroot: $localroot" new-item -path $modulesfolder -name ".local" -itemtype directory } # remove exists, make clean copy ( test-path $localfolder) { write-output " removing local copy module" remove-item -path $localfolder -recurse -force } copy-item -path $origfolder -destination $localfolder -recurse # build (local copy the) module write-output "" write-output "run build script..." $buildscriptpath = join-path $localfolder ".build" "build.ps1" &amp; $buildscriptpath ` -modulename "dataconfig" ` -workingdir $localroot ` -buildversion "0.0.0" ` -localfeedpath (join-path $localroot "localfeed" ) ` -cleanfolder</file><file name="development\localdeploy\localbuild_networking_module.ps1">&lt;# .synopsis script locally test build networking powershell module generate nuget package. .description 1. runs unit tests 2. makes local copy module folder (because build process modifies files, changes want commit) 3. local copy: run build script prepare .psd1 file 4. packages module nuget package note: requires nuget.exe available path. * download nuget.exe (latest: 6.8.0) https://www.nuget.org/downloads * make sure added path .notes script use following environment variables parameters provided: * $env:reporoot #&gt; [cmdletbinding()] param ( [string] $workspacefolder = $env:reporoot, # local folder repo cloned # optional switches skip steps (useful running consecutively without real changes) [switch] $skipunittests ) # ================= # config # ================= # force error action set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = "continue" $modulesfolder = join-path $workspacefolder "src" "powershell" "modules" ( -not (test-path -path $modulesfolder -pathtype "container")) { throw "could fine modulesfolder: $modulesfolder" } $modulename = "networking" write-output "using modulesfolder: $modulesfolder" write-output "using modulename : $modulename" # run unit tests (-not $skipunittests){ $testscriptpath = join-path $modulesfolder $modulename ".build" "unit_tests.ps1" if( -not (test-path $testscriptpath) ) { throw "could locate script: $testscriptpath" } &amp; $testscriptpath } else { write-output "" write-output "(...skipping unit tests...)" } # make copy module write-output "" write-output "creating local copy module files..." $origfolder = join-path $modulesfolder $modulename $localroot = join-path $modulesfolder ".local" $localfolder = join-path $localroot $modulename write-output " using origfolder : $origfolder" write-output " using localroot : $localroot" write-output " using localfolder : $localfolder" # check ".local" folder exists ( -not ( test-path $localroot )) { write-output " creating localroot: $localroot" new-item -path $modulesfolder -name ".local" -itemtype directory } # remove exists, make clean copy ( test-path $localfolder) { write-output " removing local copy module" remove-item -path $localfolder -recurse } copy-item -path $origfolder -destination $localfolder -recurse # build (local copy the) module write-output "" write-output "run build script..." $buildscriptpath = join-path $localfolder ".build" "build.ps1" &amp; $buildscriptpath ` -modulename "networking" ` -workingdir $localroot ` -buildversion "0.0.0" ` -localfeedpath (join-path $localroot "localfeed" ) ` -cleanfolder</file><file name="development\localdeploy\localbuild_token_module.ps1">&lt;# .synopsis script locally test build tokenmodule powershell module generate nuget package. .description 1. runs unit tests 2. makes local copy module folder (because build process modifies files, changes want commit) 3. local copy: run build script prepare .psd1 file 4. packages module nuget package note: requires nuget.exe available path. * download nuget.exe (latest: 6.8.0) https://www.nuget.org/downloads * make sure added path .notes script use following environment variables parameters provided: * $env:reporoot #&gt; [cmdletbinding()] param ( [parameter( mandatory = $false, helpmessage="the local folder repo cloned")] [string] $workspacefolder = $env:reporoot, [parameter( mandatory = $false, helpmessage="optional switches skip steps (useful running consecutively without real changes)")] [switch] $skipunittests ) # force error action set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = "continue" # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "workspacefolder" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- $modulesfolder = join-path $workspacefolder "src" "powershell" "modules" ( -not (test-path -path $modulesfolder -pathtype "container")) { throw "could fine modulesfolder: $modulesfolder" } $modulename = "tokenmodule" write-output "using modulesfolder: $modulesfolder" write-output "using modulename : $modulename" # run unit tests (-not $skipunittests){ $testscriptpath = join-path $modulesfolder $modulename ".build" "unit_tests.ps1" if( -not (test-path $testscriptpath) ) { throw "could locate script: $testscriptpath" } &amp; $testscriptpath } else { write-output "" write-output "(...skipping unit tests...)" } # make copy module write-output "" write-output "creating local copy module files..." $origfolder = join-path $modulesfolder $modulename $localroot = join-path $modulesfolder ".local" $localfolder = join-path $localroot $modulename write-output " using origfolder : $origfolder" write-output " using localroot : $localroot" write-output " using localfolder : $localfolder" # check ".local" folder exists ( -not ( test-path $localroot )) { write-output " creating localroot: $localroot" new-item -path $modulesfolder -name ".local" -itemtype directory } # remove exists, make clean copy ( test-path $localfolder) { write-output " removing local copy module" remove-item -path $localfolder -recurse } copy-item -path $origfolder -destination $localfolder -recurse # build (local copy the) module write-output "" write-output "run build script..." $buildscriptpath = join-path $localfolder ".build" "build.ps1" &amp; $buildscriptpath ` -modulename "tokenmodule" ` -workingdir $localroot ` -buildversion "0.0.0" ` -localfeedpath (join-path $localroot "localfeed" ) ` -cleanfolder # write information script user write-output "*** ending script: $scriptname ***"</file><file name="development\localdeploy\localdeploy_sqldatabase.ps1">&lt;# .synopsis script locally validate development work metadata database issueing pr (pull request). .description 1. builds deploys database tests designated dev database 2. cleans dev database "fresh" start 2. runs tsqlt unit tests. 3. deploys test configuration data 4. &lt;todo&gt; runs integration tests .notes script use following environment variables parameters provided: * $env:reporoot * $env:sqlmeta_server * $env:sqlmeta_db #&gt; [cmdletbinding()] param ( [string] $workspacefolder = $env:reporoot, [string] $targetserver = $env:sql_server, [string] $targetdatabase = $env:sql_database, [string] $accesstoken = ((get-azaccesstoken -resourceurl https://database.windows.net -assecurestring -warningaction "ignore").token | convertfrom-securestring -asplaintext), # optional switches skip steps [string] $outputscriptpath = $null, # provide folder path create dacpac deploy scripts [switch] $skipbuild, [switch] $skipdeploy, [switch] $skipunittests ) # needs sqlserver module installed #requires -modules sqlserver # force error action $erroractionpreference = "stop" # path mssql $sqlfolder = join-path ${workspacefolder} "src" "sql" # build projects write-output "------------------------------" write-output "build main project ..." write-output "------------------------------" if( $skipbuild) { write-output "" write-output "&lt;skipping build requested&gt;" } else { . $sqlfolder/_build/db_build.ps1 -projectpath "$sqlfolder/metadb/metadb.sqlproj" -erroraction "stop" } write-output "" write-output "------------------------------" write-output "build test project ..." write-output "------------------------------" if( $skipbuild) { write-output "" write-output "&lt;skipping build requested&gt;" } else { . $sqlfolder/_build/db_build.ps1 -projectpath "$sqlfolder/metadb_test/metadb_test.sqlproj" $buildresult = $lastexitcode ( $buildresult -ne 0 ) { write-error "build succesful (exit code: $buildresult), aborting..." return } } # clean database # todo (there test.usp_reset_database clear config tables, requires project deployed first ...) ( $outputscriptpath -or (-not $skipdeploy) ) { write-output "" write-output "------------------------------" write-output "connecting sql server ..." write-output "------------------------------" # test connection sql server $params = @{ serverinstance = $targetserver database = $targetdatabase accesstoken = $accesstoken } $rows = invoke-sqlcmd @params -query "select 'connected database: ' + db_name() message;" $rows.message # default parameters sqlpackage $deployparams = @{ sqlpackageexepath = $env:sqlpackagepath publishprofilepath = (join-path $sqlfolder "/_build/generic.publish.xml") targetserver = $targetserver targetdatabase = $targetdatabase accesstoken = $accesstoken } write-output "" write-output "------------------------------" write-output "main project ..." write-output "------------------------------" $dacpacname = "metadb" $dacpacpath = join-path $sqlfolder $dacpacname "bin" "debug" "$dacpacname.dacpac" write-output $dacpacpath write-output $sqlfolder ( $outputscriptpath ) { write-output "creating $dacpacname deploy script $outputscriptpath ..." $outputfilepath = join-path $outputscriptpath "$dacpacname.sql" &amp; $sqlfolder/_build/db_deploy.ps1 @deployparams -dacpacpath $dacpacpath -action "script" -outputscriptpath $outputfilepath } ($skipdeploy) { write-output "" write-output "&lt;skipping deploy requested&gt;" } else { write-output "deploying..." &amp; $sqlfolder/_build/db_deploy.ps1 @deployparams -dacpacpath $dacpacpath -action "publish" } write-output "" write-output "------------------------------" write-output "deploy tsqlt project ..." write-output "------------------------------" ( $skipdeploy) { write-output "" write-output "&lt;skipping deploy requested&gt;" } else { $dacpacname = "metadb_test" $dacpacpath = join-path $sqlfolder $dacpacname "dependencies" "tsqlt_azurev12_v1.0.5873.27393.dacpac" write-output $dacpacpath write-output $sqlfolder &amp; $sqlfolder/_build/db_deploy.ps1 @deployparams -dacpacpath $dacpacpath -action "publish" } write-output "" write-output "------------------------------" write-output "test project ..." write-output "------------------------------" $dacpacname = "metadb_test" $dacpacpath = join-path $sqlfolder $dacpacname "bin" "debug" "$dacpacname.dacpac" ( $outputscriptpath ) { write-output "creating $dacpacname deploy script $outputscriptpath ..." $outputfilepath = join-path $outputscriptpath "$dacpacname.sql" &amp; $sqlfolder/_build/db_deploy.ps1 @deployparams -dacpacpath $dacpacpath -action "script" -outputscriptpath $outputfilepath } ($skipdeploy) { write-output "" write-output "&lt;skipping deploy requested&gt;" } else { write-output "deploying..." &amp; $sqlfolder/_build/db_deploy.ps1 @deployparams -dacpacpath $dacpacpath -action "publish" } } # now, run reset proc? # todo ( -not $skipunittests) { # construct parameters run_testclasses.ps1 $testparams = @{ targetserver = $targetserver targetdatabase = $targetdatabase accesstoken = $accesstoken } # run design rules &amp; unit tests write-output "" write-output "-----------------------------------" write-output "run unit tests ..." write-output "-----------------------------------" . $sqlfolder/_test/run_testclasses.ps1 @testparams -testsets "unit" -informationaction "continue" -verbose # run design rules write-output "" write-output "-----------------------------------" write-output "run design rules tests ..." write-output "-----------------------------------" . $sqlfolder/_test/run_testclasses.ps1 @testparams -testsets "design" -informationaction "continue" -verbose } # run integration tests # todo none yet :) # write-output "" # write-output "-----------------------------------" # write-output "run integration ..." # write-output "-----------------------------------" # . $mssqlfolder/_test/run_testclasses.ps1 -targetserver $targetserver -targetdatabase $targetdatabase -testsets "integration"</file><file name="development\localdeploy\localdeploy_synapse.ps1">&lt;# .synopsis script run unit tests worker tests azure synapse workspace .description using existing 'synapse live' artefacts, script run tests synapse workspace. include testnotebook synapse environment, unit test pyspark code also includes tests workers (pipelines notebooks) defined workspace .notes script use following environment variables parameters provided: * $env:reporoot * $env:mydevenv #&gt; [cmdletbinding()] param ( ## general params [parameter( mandatory = $false, helpmessage="the local folder repo cloned")] [string] $workspacefolder = $env:reporoot, [parameter( mandatory = $false, helpmessage="the local environment script called")] [string] $targetenvironment = $env:mydevenv, [parameter( mandatory = $false, helpmessage="switch skip running tests synapse pipelines")] [switch] $skipsynapsepipelines, [parameter( mandatory = $false, helpmessage="switch skip running test synapse notebooks")] [switch] $skipsynapsenotebooks ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "workspacefolder" "targetenvironment" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # local deploy pipeline allowed run development environment ( $targetenvironment -notin ('dev')) { throw "invalid targetenvironment '$targetenvironment'. localdeploy allowed run development environment." } # script requires use dataconfig powershell module (= module created dap team) #requires -modules dataconfig # target environment valid required modules installed, set script variables ## sql variables $sqlservername = "$($targetenvironment)-dap-sql-core" $sqlserverinstance = $env:sql_server $sqldatabase = $env:sql_database $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net -debug:$false).token ## synapse variables $storageaccount = "$($targetenvironment)dapstdala1" $synapseserverinstance = $env:synapse_server $synapsedatabase = $env:synapse_database ## spark session variables $synapsewsname = $env:synapse_ws $sparkpoolname = "$($targetenvironment)dapsspcore" $sparksessionname = 'localdeploy' $sparkparams = @{ workspacename = $synapsewsname poolname = $sparkpoolname sessionname = $sparksessionname } ## general variables $targetresourcegroup = "$($targetenvironment)-dap-rg-core" # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- ## general preparation ## running tests, make sure "reset" environment happened ## clean sql logging tables adls containers ## upload test files adls containers ## deploy test-metadata configuration sql meta db silver container ## make sure sql meta db online . $workspacefolder/pipelines/scripts/sql/resume_sqldb.ps1 ` -servername $sqlservername ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -resourcegroupname $targetresourcegroup ## clean sql logging tables . $workspacefolder/pipelines/scripts/sql/clean_sql_tables.ps1 ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ` -targetenvironment $targetenvironment ## clean adls containers . $workspacefolder/pipelines/scripts/datalake/clean_all_containers.ps1 ` -storageaccount $storageaccount ` -targetenvironment $targetenvironment # --------------------------------------------------------------------------------------------------------- . $workspacefolder/pipelines/scripts/datalake/upload_to_container.ps1 ` -storageaccount $storageaccount ` -containername 'unittest' ` -testtype 'unittest_files' . $workspacefolder/pipelines/scripts/datalake/upload_to_container.ps1 ` -storageaccount $storageaccount ` -containername 'landing' ` -testtype 'fulltest_files' . $workspacefolder/pipelines/scripts/sql/upload_to_tables.ps1 ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ` -filtertype 'all' ` -folderpath "$workspacefolder/src/synapse/_test/metadata" ` -deltatableconfigurationfile "src/synapse/_test/delta_tables/delta_tables.json" # --------------------------------------------------------------------------------------------------------- # tests synapse pipelines need run (-not $skipsynapsepipelines){ # --------------------------------------------------------------- # synapse pipeline test: pl_dummy_worker # # description: invoke pipeline pl_start_run 'synapse_pipeline' task type, 'pl_dummy_worker' workername # expected result: pl_dummy_worker pipeline gets invoked sets current_status task 'succeeded' # purpose: validate orchestration synapse pipelines # #--------------------------------------------------------------- # synapse pipeline test: pl_unzip_worker # # description: invoke pipeline pl_start_task_group 'synapse_pipeline' task type, 'pl_unzip_worker' workername # expected result: pl_unzip_worker pipeline gets invoked, unzips test file logs actions logging tables # purpose: validate functionalities pl_unzip_worker pipeline: unzip file/folder log new status task file/folder # #--------------------------------------------------------------- # synapse pipeline test: dummyworker # # description: invoke pipeline pl_start_task_group 'spark_notebook' task type, 'dummyworker' workername # expected result: pipeline invokes metanotebook notebook, invokes dummyworker notebook logs executed tasks # purpose: validate orchestration synapse pipeline environment synapse notebook environment # #--------------------------------------------------------------- ### prepare ### log new instance unittest_synapse_unzip plan $unzipworkervariables = . $workspacefolder/pipelines/scripts/sql/prepare_logging_tables.ps1 ` -targetserver $sqlserverinstance ` -targetdatabase $sqldatabase ` -planname 'unittest_synapse_unzip' ` -informationaction 'continue' ### log new instance unittest_spark_dummy plan $dummworkervariables = . $workspacefolder/pipelines/scripts/sql/prepare_logging_tables.ps1 ` -targetserver $sqlserverinstance ` -targetdatabase $sqldatabase ` -planname 'unittest_spark_dummy' ### execute ### run set pipelines integration testing ### - pl_start_task_group dummyworker ### - pl_start_task_group pl_unzip_worker ### - pl_start_run pl_dummy_worker . $workspacefolder/pipelines/scripts/synapse/invoke_parallel_synapse_pipelines.ps1 ` -synapsewsname $synapsewsname ` -pipelines @( @{ pl_start_task_group = @{ task_group = 'dummy'; plan_id = $dummworkervariables.planid; run_id = $dummworkervariables.runid}}, @{ pl_start_task_group = @{ task_group = 'preprocess'; plan_id = $unzipworkervariables.planid; run_id = $unzipworkervariables.runid}}, @{ pl_start_run = @{plan_name = 'unittest_synapse_dummy'; new_plan = $true}}) ### evaluate ### run sql tests scripts pl_dummy_worker . $workspacefolder/pipelines/scripts/sql/gather_sql_test_scripts.ps1 ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ` -filtertype 'path' ` -filterpath '*synapse_dummy_pl*' ### run sql tests scripts pl_unzip_worker . $workspacefolder/pipelines/scripts/sql/gather_sql_test_scripts.ps1 ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ` -filtertype 'path' ` -filterpath '*synapse_unzip*' ### run sql tests scripts dummyworker . $workspacefolder/pipelines/scripts/sql/gather_sql_test_scripts.ps1 ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ` -filtertype 'path' ` -filterpath '*spark_dummy_notebook*' } # tests synapse notebooks need run (-not $skipsynapsenotebooks){ #--------------------------------------------------------------- # synapse notebook tests: testnotebook # # description: invoke notebook 'testnotebook' initiate unittests defined test-notebooks # expected result: .txt file pushed adls 'logs' container results unit tests # purpose: make sure individual function synapse notebooks works expected # #--------------------------------------------------------------- # synapse notebook tests: filworker # # description: invoke notebook metanotebook list tasks execute using filworker notebook # expected result: file .fil extension gets turned csv. .fil-file moves archive, .csv-file remains # purpose: validate functionalities filworker # #--------------------------------------------------------------- # synapse notebook tests: ingestionworker # # description: invoke notebook metanotebook list tasks execute using ingestionworker notebook # expected result: set files landing zone get successfully moved landing silver using ingestionworker notebook # purpose: validate functionalities ingestionworker: move files landing zone delta lake log process # #--------------------------------------------------------------- ### prepare ### log new instance unittest_filworker plan $filworkervariables =. $workspacefolder/pipelines/scripts/sql/prepare_logging_tables.ps1 ` -targetserver $sqlserverinstance ` -targetdatabase $sqldatabase ` -planname 'unittest_filworker' ### log new instance unittest_ingestionworker plan $ingestionworkervariables =. $workspacefolder/pipelines/scripts/sql/prepare_logging_tables.ps1 ` -targetserver $sqlserverinstance ` -targetdatabase $sqldatabase ` -planname 'unittest_ingestionworker' ` ### execute ### run set notebooks integration testing ### - metanotebook ingestionworker ### - metanotebook filworker ### - testnotebook unit tests . $workspacefolder/pipelines/scripts/synapse/invoke_parallel_notebooks.ps1 ` -poolname $sparkpoolname ` -synapseworkspac $synapsewsname ` -targetenvironment $targetenvironment ` -notebooks @(@{ metanotebook = @{ folder = 'workers'; 'parametermap' = @{plan_list = "$($ingestionworkervariables.tasklist)"; plan_id = $ingestionworkervariables.planid; env_code = $targetenvironment }}}, @{ metanotebook = @{ folder = 'workers'; 'parametermap' = @{ plan_list = "$($filworkervariables.tasklist)"; plan_id = $filworkervariables.planid; env_code = $targetenvironment }}}, @{ testnotebook = @{ folder = 'workers'}}) ` -sessionname: $sparksessionname ### evaluate ### check logs posted 'logs' container . $workspacefolder/pipelines/scripts/synapse/check_test_logs.ps1 ` -storageaccount $storageaccount ` -containername 'logs' ` -outpath 'src/synapse/_test/output' ` -islocaldeploy ` -erroraction continue ### run tests filworker . $workspacefolder/pipelines/scripts/synapse/validate_workers/validate_filworker.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ### run tests ingestionworker . $workspacefolder/pipelines/scripts/synapse/validate_workers/validate_ingestionworker.ps1 ` -sqlserverinstance $sqlserverinstance ` -sqldatabase $sqldatabase ` -accesstoken $accesstoken ` -storageaccount $storageaccount ` -synapseworkspace $synapsewsname ` -synapseserver $synapseserverinstance ` -synapsedatabase $synapsedatabase ` -workspacefolder: $workspacefolder ` -plan_id $ingestionworkervariables.planid }</file><file name="pipelines\scripts\datalake\clean_all_containers.ps1">&lt;# .synopsis script clean containers adls account .description development integration environment, might necessary start clean slate. script clean containers adls account. #&gt; [cmdletbinding()] param ( [parameter( mandatory = $true, helpmessage="name storage account land files")] [string] $storageaccount = 'devdapstdala1', [parameter( mandatory = $true, helpmessage="environment script run (dev, int)")] [string] $targetenvironment = $env:mydevenv, [parameter( mandatory = $false, helpmessage="exclude containers clean. type: [array]" )] [array] $excludecontainers = @() ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "storageaccount" "targetenvironment" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # logging tables dev int environments allowed cleaned. ( $targetenvironment -notin ('dev', 'int')) { throw "invalid targetenvironment '$targetenvironment'. dev int deployments allowed." } # set storage context able access containers adls account $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount # get list containers [array] $storagecontainers = (get-azstoragecontainer -context $context) ($storagecontainers){ $storagecontainerlist = $storagecontainers.name # check containers exclude # not: clean containers ([bool] $excludecontainers){ write-warning "container(s) '$($excludecontainers -join ', ')' is/are skipped $scriptname" $containerlist = $storagecontainerlist | where-object { $_ -notin $excludecontainers } } else{ [array] $containerlist = $storagecontainerlist } # loop container remove contents foreach ($container $containerlist){ write-information "clean container $container..." # container empty, continue retrying remove items # container empty, keep going :retry_loop while($true){ # get list blobs container $blobs = get-azstorageblob -context $context -container $container | sort-object -property @{expression ='length'; descending= $true}, @{expression='name'; descending=$true} # iteratively remove blobs ([bool] $blobs){ foreach($blob $blobs.name){ write-information " removing blob: $blob" remove-azstorageblob -container $container -context $context -blob $blob } } # check blobs container $blobs = get-azstorageblob -context $context -container $container | sort-object -property @{expression ='length'; descending= $false}, @{expression='name'; descending=$true} # blobs: break loop if(-not [bool]$blobs){ break retry_loop } } } } # write information script user write-output "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\datalake\file_container_existence.ps1">&lt;# .synopsis script used check existence absence certain files inside certain container inside certain storage account .description pass array files expect expect present certain container inside certain storage account .notes $filelist parameter empty, use function look files inside specific container #&gt; [cmdletbinding()] param( [parameter( mandatory = $false, helpmessage="the local folder repo cloned")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="name environment table needs deployed to" )] [string] $targetenvironment = $env:mydevenv, [parameter(mandatory=$true, helpmessage="container name inside storage account (e.g. silver)")] [string] $containername, [parameter(mandatory=$false, helpmessage="the prefix actual file name (= full path excl. container name)")] [string] $blobprefix, [parameter(mandatory=$false, helpmessage="array files expected absent certain container")] [array] $filelist, [parameter(mandatory=$false, helpmessage="flag check absence presence files. set false want check absence files list")] [bool] $presencecheck = $true ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "workspacefolder" "targetenvironment" "containername" "filelist" "presencecheck" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # step 1: define parameters # using target-environment, define names different azure services $storageaccount = "$($targetenvironment)dapstdala1" ([bool]$presencecheck){ write-information "checking existence files $("'" + ($filelist -join "','") + "'") container $($containername)..." } else{ write-information "checking absence files $("'" + ($filelist -join "','") + "'") container $($containername)..." } $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount # get possible blobs inside container according specific filter $blobs = get-azstorageblob -context $context -container $containername | sort-object -property @{expression ='length'; descending= $true}, @{expression='name'; descending=$true} # contents inside azure container, boolean false ([bool]$blobs){ # collect files (contain '.' -&gt; file extension) # split-path -leaf extracts last part path [array]$foundfiles = $blobs | foreach-object { if($_.name -match '\.') {$_.name}} write-information "found files: $("'" + ($foundfiles -join "','") + "'")" # loop files foreach($file $filelist){ # prefix / parent path file, include search if([bool]$blobprefix){ # make sure actual path, turn backslashes way around # storage account path made like "path/to/something" # vs join-path "path\to\something" $fullpath = (join-path $blobprefix $file).replace('\', '/') } else{ # case file root container $fullpath = $file } # see file container, throw error checking presence file found (($fullpath -notin $foundfiles) -and ([bool]$presencecheck)){ throw "file $($fullpath) expected container $($containername) found" } # see file container, throw error checking absence file found elseif (($fullpath -in $foundfiles) -and (-not [bool]$presencecheck)) { throw "file $($fullpath) expected container $($containername) found" } } } else{ write-warning "the container '$($containername)' empty storage account '$($storageaccount)'" # throw error supposed looking certain files, container appears empty ([bool]$presencecheck){ throw "container $($containername) empty. files $("'" + ($filelist -join "','") + "'") found" } } write-information "" write-information "***end script: $($scriptname)***"</file><file name="pipelines\scripts\datalake\get_blob_content.ps1">&lt;# .synopsis script used check file contents stored blob .description pass container look in, blob trying access #&gt; [cmdletbinding()] param( [parameter( mandatory = $false, helpmessage="the local folder repo cloned")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="name environment table needs deployed to" )] [string] $targetenvironment = $env:mydevenv, [parameter(mandatory=$true, helpmessage="container name inside storage account (e.g. silver)")] [string] $containername, [parameter(mandatory=$false, helpmessage="the actual blob trying extract")] [string] $blob ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "workspacefolder" "targetenvironment" "containername" "blob" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # step 1: define parameters # using target-environment, define names different azure services $storageaccount = "$($targetenvironment)dapstdala1" # step 2: set content storage account &amp; extract container $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount ## note: container exist, throw error "can find container 'xxx'." $container = get-azstoragecontainer -name $containername -context $context # step 3: create virtual client &amp; extract blob (full path excl. container name) $client = $container.cloudblobcontainer.getblockblobreference($blob) $file = $client.downloadtext() # $csvdata = $file | convertfrom-csv -delimiter "," # specify tab delimiter # $csvdata | format-table -autosize # display data table format # step 4: return file &amp; end script write-information "" write-information "***end script: $($scriptname)***" return $file</file><file name="pipelines\scripts\datalake\setup_storage_account_containers.ps1">&lt;# .synopsis checks existence certain containers certain storage account. exist, containers created .description deploying complete new environment, containers mandatory make sure checks processes succeed. script makes sure every necessary containers, based parameter input, present created present. .parameter containernames powershell scripts intended run azure devops need accept arrays either expect single string parse internally, iterate multiple occurrences parameter. so: put string, directly array #&gt; [cmdletbinding()] param ( [parameter( mandatory = $false)] [string] $storageaccount = 'devdapstdala1', # name storage account land files [parameter( mandatory = $false)] [string] $containernames = 'archive, landing, raw, silver' # array containers checked new environment ) # enforce erroraction informationpreference $erroractionpreference = 'continue' $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" $inputparameters = @( "storageaccount" "containernames" ) $scriptname = $script:myinvocation.mycommand.path $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # prep: string containers array ## turn string containers array containers $containers = $containernames -split ',' | foreach-object { $_.trim() } # step 1: validation ## currently validation necessary ## dev-note: future: array allowed containers checked created? # step 2: check current containers storage account ## a: get existing containers already present storage account $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount $existingcontainers = get-azstoragecontainer -context $context write-information "containers: " write-information $existingcontainers ## b: current storage account empty? ### empty, create containers inside array (-not [bool]$existingcontainers){ write-information "storage account '$($storageaccount)' empty. creating containers:" foreach ($container $containers) { write-information " create container: $($container)..." new-azstoragecontainer -name $container -context $context } } else{ ### empty, check first container already present inside storage account ### present, create it, else output information message write-information "storage account '$($storageaccount)' containers. checking presence containers:" foreach ($container $containers){ ($container -notin $existingcontainers.name){ write-information " create container: $($container)..." new-azstoragecontainer -name $container -context $context } else { write-information "container $($container) already present storage account '$($storageaccount)'. action required." } } } # step 3: end script # write information script user write-output "*** end script: $scriptname ***"</file><file name="pipelines\scripts\datalake\upload_to_container.ps1">&lt;# .synopsis run spark notebook synapse workspace .description development, might necessary run specific notebook invoking notebook powershell. relevant unit test process development, script allows run notebook (without parameters) .parameter notebookname name notebook want run? .parameter poolname name spark pool want use run notebook? .parameter synapseworkspace name synapse workspace pool notebook located? #&gt; [cmdletbinding()] param ( [parameter( mandatory = $false)] [string] $storageaccount = 'devdapstdala1', # name storage account land files [parameter( mandatory = $false)] [string] $containername = 'landing', # container name land files [parameter( mandatory = $false)] [string] $testtype = "fulltest_files", # dealing unit tests full tests? [parameter( mandatory = $false)] [string] $workspacefolder = $env:reporoot # local folder repo cloned ) # enforce erroraction informationpreference set-strictmode -version "latest" $erroractionpreference = 'stop' $informationpreference = 'continue' # start region: print variables $inputparameters = @( "storageaccount" "containername" "testtype" "workspacefolder" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } #endregion # step 1: validation # check folder test files exists $datafolder = join-path $workspacefolder "src" "synapse" "_test" "test_files" $testtype ( -not ( test-path $datafolder -pathtype "container" )) { throw "data folder integration tests found: $datafolder " } # check $containername container exists $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount [array] $existingcontainers = get-azstoragecontainer -context $context write-information "containers: " write-information $existingcontainers # create container exist # dev-note: add allowed containers (to discussed) ((-not [bool]$existingcontainers) -or ($containername -notin $existingcontainers.name)){ write-information " create container: $($containername)..." new-azstoragecontainer -name $containername -context $context } # upload files datafolder azure blob container '$containername' write-information " upload data $($containername) container" $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount $allfiles = get-childitem -path $datafolder -recurse | where-object { ! $_.psiscontainer } $allfiles | foreach-object{ $filepath = [system.io.path]::getrelativepath($datafolder, $_.fullname) set-azstorageblobcontent -container $containername -context $context -file $_.fullname -blob $filepath -force } # commented: clean containers -&gt; use script this: clean_all_containers # elseif($containername -eq "landing"){ # $container_list = "landing", "raw", "silver", "archive" # } # else{ # write-error "unknown container; stop execution" # } # foreach ($container $container_list){ # ( $container -notin $existingcontainers.name){ # write-information " create container: $container..." # new-azstoragecontainer -name $container -context $context # } # else { # $notempty = $true # # container empty, keep going # while($notempty){ # # container exists data lake, clean # ($container -in $existingcontainers.name){ # write-information " clean container $container..." # # get list blobs container # $blobs = get-azstorageblob -context $context -container $container | sort-object -property @{expression ='length'; descending= $true}, @{expression='name'; descending=$true} # # iteratively remove blobs # foreach($blob $blobs.name){ # write-information " removing blob: $blob" # remove-azstorageblob -container $container -context $context -blob $blob # } # # check blobs container # $blobs = get-azstorageblob -context $context -container $container | sort-object -property @{expression ='length'; descending= $false}, @{expression='name'; descending=$true} # if($blobs -eq $null){ # $notempty = $false # } # } # } # } # }</file><file name="pipelines\scripts\general\check_environment_code.ps1">[cmdletbinding()] param ( [parameter()] [string] $environment_code, [parameter()] [string] $target_environment ) write-output "checking target environment ..." write-output " environment_code : $environment_code" write-output " target_environment: $target_environment" write-output "using new version ..." # environment sandbox( "sbx"), target_environment match sandbox naming convention ( $environment_code -eq "sbx") { ( $target_environment -notmatch '^sbx[a-z0-9]{2,3}$') { write-error "sandbox environments start 'sbx' followed 2 3 digits lowercase letters eg 'sbxab1" return } write-output "looks okay $environment_code environment!" return } # cases, environment_code must equal target_environemnt, must match one accepted ones ( @( 'dev', 'int', 'tst', 'tmp', 'acc', 'prd') -contains $environment_code ) { ( $target_environment -ne $environment_code) { write-error "for non-sandbox environments, target_environment equal environment_code" return } write-output "looks okay $environment_code environment!" return } write-error "invalid environment_code provided: $environment_code"</file><file name="pipelines\scripts\general\install_powershell_requirements.ps1">&lt;# .synopsis script installs required modules specified .description script relies getting list modules available. - module installed, script install required version (optional) - module exists older version, required version installed - module exists newer version, action taken - versions specified (just module name) script assume version ok modules always installed currentuser scope. .parameter requirements json array string, array member following properties: - name : name module (mandatory) - version : semantic versioning (semver) minimum version module e.g. [ { "name": "sqlserver" }, { "name": "pester", "version" : "5.5" } ] #&gt; [cmdletbinding()] param ( [parameter(mandatory = $false, helpmessage = "json array required module information")] [string]$requirements = "[]", [parameter(mandatory = $false, helpmessage = "custom directory script install modules")] [string]$custominstallpath = $null ) #region local script settings $erroractionpreference = "stop" set-strictmode -version "latest" #endregion #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { ($_ -notin [system.management.automation.pscmdlet]::commonparameters ) -and ($_ -notin [system.management.automation.pscmdlet]::optionalcommonparameters ) } $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "" write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "----------------------------------------------------------------------" #endregion script info $requiredmodules = [array] ( $requirements | convertfrom-json ) :reqloop foreach ($req $requiredmodules) { $reqname = $req.name $reqversion = ( $req | get-member -name "version" ) ? $req.version : $null write-information "" write-information "processing requirement: $reqname $($reqversion ?? "(any)") ..." # validate version number ( $reqversion -and $reqversion -ne "latest") { ( $reqversion -match "^\d+\.\d+\.\d+$" -or $reqversion -match "^\d+\.\d+\.\*$" -or $reqversion -match "^\d+\.\*$" ) { # valid } elseif ( $reqversion -match "^\d+$" -or $reqversion -match "^\d+\.\d+$" ) { # add "*" minor rev $reqversion = $reqversion + ".*" write-information " updated semver string '$reqversion'" } else { throw "invalid version number '$reqversion' module '$reqname' requirements parameter" } } # currently installed module versions $installedversions = [array] (get-module $reqname -listavailable) | select-object @{name = 'version'; expression = {$_.version.tostring()}}, path write-information " installed versions:" $installedversions | foreach-object { write-information " $($_.version) $($_.path)" } $currentmaxversion = ($installedversions | sort-object -property "version" -descending | select-object -first 1).version # continue module already exists requirements specify version (-not $reqversion -and $installedversions) { write-information " module '$reqname' already installed particular version required." continue } # evaluate version installed (($reqversion -eq "latest") -or (-not $reqversion)) { # specified latest -&gt; get latest version try { $remotemodules = find-module -name $reqname -erroraction "stop" | sort-object -property "version" -descending write-information " found available modules:" $remotemodules | foreach-object { write-information " $($_.version) repository: $($_.repository)" } $remotemodule = $remotemodules | select-object -first 1 $remotemodule } catch { $erroractionpreference = "continue" write-error "error getting module '$reqname' psrepository, message: $($_.exception.message)" $erroractionpreference = "stop" continue reqloop } $installversion = $remotemodule.version.tostring() write-information " latest version module '$reqname' psgallery '$installversion'" write-debug " remote versions module ${reqname}:`n$($remotemodule | out-string)" } elseif ($reqversion -match "\*") { # wildcards specified, find latest matching version try { $remotemodule = find-module $reqname -allversions | ` where-object { $_.version -like $reqversion } | ` sort-object -property "version" -descending -top 1 } catch { $erroractionpreference = "continue" write-error "error getting module '$reqname' psrepository, message: $($_.exception.message)" $erroractionpreference = "stop" continue reqloop } $installversion = $remotemodule.version.tostring() write-information " latest matching version module '$reqname' psgallery '$installversion'" write-debug " remote versions module ${reqname}:`n$($remotemodule | out-string)" } else { # exact version $installversion = $reqversion } # continue module already exists required higher version #if ( $installedversions | where-object version -ge $installversion) { ( $currentmaxversion -ge $installversion ) { write-information " module '$reqname' version '$currentmaxversion' already required version '$installversion' higher." continue reqloop } # get here, need install somthing... $moduleconfig = @{ name = $reqname requiredversion = $installversion } ( -not $custominstallpath) { # path provided use install-module user scope $moduleconfig.add("scope", "currentuser") $moduleconfig.add("force", $true) $moduleconfig.add("allowclobber", $true) install-module @moduleconfig write-information " installed module $($moduleconfig.name) version $($moduleconfig.requiredversion)" } else { # path provided need save specified folder find-module @moduleconfig | save-module -path $custominstallpath -force write-information " saved module '$($moduleconfig.name)' version '$($moduleconfig.requiredversion)' directory '$custominstallpath'" } } #region end script write-information "" write-information "--- end script: $($myinvocation.mycommand.name) ---" write-information "" #endregion</file><file name="pipelines\scripts\general\show_config_vars.ps1">[cmdletbinding()] param ( [parameter()] [string] $variables = "", [parameter()] [bool] $showfullenv = $false ) # calling devops task creating temporary env variable starting "param_" pipeline parameter write-output "pipeline parameters:" $paramlist = (get-childitem env: | where-object name -like "param_*") $maxlength = ($paramlist.key | measure-object -property length -maximum).maximum -6 # substract "param_" foreach ( $p $paramlist) { write-output ( " " + $p.name.replace("param_", "").padright($maxlength) + " : " + $p.value ) } # show variables listes $variables parameter write-output "" write-output "variables:" if( $variables) { $varlist = $variables.split(",").trim() $maxlength = ( $varlist | measure-object -maximum -property length).maximum foreach ( $v $varlist) { $envvar = $v.toupper().replace(".","_") write-output ( " " + $v.padright($maxlength) + " : " + [environment]::getenvironmentvariable($envvar) ) } } else { write-output " (none)" } # show standard entries write-output "" write-output "system configuration:" write-output " build.sourcebranch : $env:build_sourcebranch" write-output " build.sourcebranchname : $env:build_sourcebranchname" write-output " system.defaultworkingdirectory : $env:system_defaultworkingdirectory" write-output " build.artifactstagingdirectory : $env:build_artifactstagingdirectory" write-output " system.artifactsdirectory : $env:system_artifactsdirectory" write-output " pipeline.workspace : $env:pipeline_workspace" # show standard entries write-output "" write-output "path:" ($env:path).split([io.path]::pathseparator) | foreach-object { write-output " $_"} # requested, show full list environment variables ($showfullenv) { write-output "" write-output "-------------------------------------------------" write-output "full list environment variables:" write-output "-------------------------------------------------" (get-childitem env: ) | format-table -hidetableheaders }</file><file name="pipelines\scripts\general\show_folder_contents.ps1">[cmdletbinding()] param ( [parameter(helpmessage="provide directory, leave empty current dir")] [string] $path = "", [parameter()] [bool] $showfoldersonly = $false ) #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { $_ -notin [system.management.automation.pscmdlet]::commonparameters} $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "----------------------------------------------------------------------" #endregion script info # local script settings set-strictmode -version "latest" # provided current working dir (-not $path) { $path = (get-item -path ".").fullname } # check path exists elseif (-not (test-path $path) ) { write-warning "provided folder `"$path`" exist." } write-output "path: `"$path`"... " write-output ("showing : " + ($showfoldersonly) ? "folders only" : "files folders") write-output "-----------------------------------------------------------------------" $data = (get-childitem -path $path -recurse -directory:$showfoldersonly ) | select-object -expandproperty fullname $data | sort-object | foreach-object { write-output (" " + $_.replace($path + [io.path]::directoryseparatorchar, ""))}</file><file name="pipelines\scripts\sql\clean_sql_tables.ps1">&lt;# .synopsis clean logging tables sql meta db .description able start scratch, logging tables sql meta db cleaned. script executes stored procedure test.usp_reset_database truncate logging tables truncating, script checks dev int environment #&gt; [cmdletbinding()] param( [parameter(mandatory = $true, helpmessage = "the sql server database found")] [string] $sqlserverinstance = $env:sql_server, [parameter(mandatory = $true, helpmessage = "the name sql database needs cleaned")] [string] $sqldatabase = $env:sql_database, [parameter(mandatory = $false, helpmessage="personal accesstoken linked microsoft account")] [string] $accesstoken, [parameter(mandatory = $false, helpmessage="the environment script run (dev, int)")] [string] $targetenvironment = $env:mydevenv ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" $inputparameters = @( "sqlserverinstance" "sqldatabase" "targetenvironment" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # logging tables dev int environments allowed cleaned. ( $targetenvironment -notin ('dev', 'int')) { throw "invalid targetenvironment '$targetenvironment'. dev int deployments allowed." } # get accesstoken provided (-not $accesstoken){ write-information "token provided, obtaining..." $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net -debug:$false).token } # group sql parameters invoke sql command $sqlparams = @{ serverinstance = $sqlserverinstance database = $sqldatabase accesstoken = $accesstoken } # invoke sql command clean logging tables invoke-sqlcmd @sqlparams -query "exec test.usp_reset_database @database_name = '$($sqldatabase)', @sure = 1" # write information script user write-output "*** end script: $scriptname ***"</file><file name="pipelines\scripts\sql\deploy_data_configuration.ps1">#requires -modules dataconfig &lt;# .synopsis deploy data configurationfor plans, tasks, datasets contained json files .description using configurationfolderpath, deploy configuration files necessary: - plan_tasks - datasets #&gt; [cmdletbinding()] param ( [parameter(mandatory = $false, helpmessage="sql server access")] [string] $sqlserverinstance = $env:sql_server, [parameter(mandatory = $false, helpmessage="sql db access")] [string] $sqldatabase=$env:sql_database, [parameter(mandatory = $false, helpmessage="accesstoken sql db currentuser")] [string] $accesstoken = $null, [parameter(mandatory = $false, helpmessage="folder configuration files located")] [string] $configurationfolderpath = "$($env:reporoot)/src/synapse/_test/metadata", [parameter(mandatory = $false, helpmessage="folder delta table configuration file located")] [string] $deltatableconfigurationfile, #= "src/synapse/_test/delta_tables/delta_tables.json", [parameter(mandatory = $false, helpmessage="boolean: deploy plan_tasks configuration files?")] [boolean] $deployplantasks = $true, [parameter(mandatory = $false, helpmessage="boolean: deploy datasets configuration files?")] [boolean] $deploydatasets = $true, [parameter(mandatory = $false, helpmessage="boolean: deploy datasets configuration files?")] [boolean] $deploydeltatables = $true, [parameter(mandatory = $false, helpmessage="selection files deploy")] [validateset('all','path','filelist')] [string]$filtertype = 'all', [parameter(mandatory = $false, helpmessage="if filtertype = path: '-like' style path filter (regex!)")] [string]$filterpath = $null, [parameter(mandatory = $false, helpmessage="if filtertype = filelist: path file list files")] [string]$filelist = $null, [parameter(mandatory = $false, helpmessage="root folder git repo local device")] [string] $workspacefolder, #= $env:reporoot, [parameter(mandatory = $false, helpmessage="environment deploy configuration files")] [string] $targetenvironment, #= 'dev', [parameter(mandatory = $false, helpmessage="name session used, annexed user identifier")] [string] $sessionname = "deploydeltatables", [parameter(mandatory = $false, helpmessage="boolean: look recursively?")] [boolean] $recurse = $true, [parameter(mandatory = $false, helpmessage="execute parallel &gt; 1")] [int32] $threads = 1 ) #region local script settings set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' #endregion #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { $_ -notin [system.management.automation.pscmdlet]::commonparameters} | where-object { $_ -notin [system.management.automation.pscmdlet]::optionalcommonparameters} $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { $paramvalue = (get-variable -name $param).value ( $param -eq "accesstoken" -and $paramvalue) { $paramvalue = $paramvalue.substring(0,20) + ( ($paramvalue.length -gt 20 ) ? "[...] (length: $($paramvalue.length))" : "" ) } write-information (" | " + $param.padright( $maxparameterlength ) + ": " + $paramvalue ) } write-information "----------------------------------------------------------------------" #endregion script info #region step 1: check provided parameters valid write-information "validate parameters..." # check 1: validate configurationfolderpath exists current repository ( -not ( test-path $configurationfolderpath -pathtype container )) { throw "configuration folder exist: $configurationfolderpath" } # check 2: filtertype = path: make sure filterpath empty ( $filtertype -eq "path") { ( -not $filterpath ) { throw "filterpath parameter mandatory filtertype 'path'" } } # check 3: filtertype = filelist: make sure filelist empty ( $filtertype -eq "filelist") { ( -not $filelist ) { throw "filelist parameter mandatory filtertype 'filelist'" } } # check 4: check access token provided. create false ( -not $accesstoken) { write-information "token provided, obtaining..." $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net -debug:$false).token # write-information "accesstoken: $accesstoken" } # check 5: make sure relevant modules set accesstoken parameter empty ( -not (get-module "sqlserver" -listavailable) ) { write-information "installing module: sqlserver ..." install-module -name "sqlserver" -scope currentuser -force } # check 6: deltatableconfigurationfile exists, remove (hard reset) (test-path (join-path $workspacefolder $deltatableconfigurationfile)) { # create file write-information "deletion pre-existing deltatableconfigurationfile: $(join-path $workspacefolder $deltatableconfigurationfile)" remove-item (join-path $workspacefolder $deltatableconfigurationfile) -force } # initialize session variable if($env:agent_id){ $annex="agent" } else{ $annex = $env:username } $identifiedsessionname = "$($sessionname)_$($annex)" #endregion step 1 #region step 2: collect configuration files configuration path $configurationfolderpath execute filter required write-information "collect configuration files..." # get json-files configurationfolderpath $allfiles = get-childitem -path $configurationfolderpath -recurse:$recurse -filter "*.json" write-information "found $($allfiles.count) json file(s) filtering" write-information " first 10:"# print sample files" $allfiles | select-object -first 10 | foreach-object { write-information " $_"} # filter collected json_files based filtertype switch( $filtertype) { 'all' { $selectedfiles = $allfiles } 'path' { $selectedfiles = @($allfiles | where-object {$_ -match $filterpath}) } 'filelist' { write-information "adjusted contents ${filelist}:" # incoming paths relative workspace, start "./" # prepend workspace path &amp; artifact download path, resolve get rid "/./" middle # allow matching later $filelistcontents = @( get-content -path $filelist | foreach-object { resolve-path (join-path "${env:pipeline_workspace}" "dataconfig" $_) } ).path $filelistcontents | foreach-object { write-information " $_"} write-information "applying filter..." $selectedfiles = $allfiles | where-object { $filelistcontents -contains $_ } } } write-information "retained $($selectedfiles.count) json file(s) filtering [$filtertype] mode" # selection configuration files passed filter section $selectedfiles | foreach-object { write-information " $_"} #endregion step 2 #region step 3: deploy configuration files # set parameters needed successfully deploy (filtered) configuration files $deployparams = @{ sqlserverinstance = $sqlserverinstance sqldatabase = $sqldatabase accesstoken = $accesstoken deployplantasks = $deployplantasks deploydatasets = $deploydatasets deploydeltatable = $deploydeltatables deltatableconfigurationfile = $deltatableconfigurationfile workspacefolder = $workspacefolder targetenvironment = $targetenvironment informationaction = $informationpreference erroraction = $erroractionpreference debug = $debugpreference } write-information "start deploying json metadata..." # sequentially deploy configuration files sql database fill configuration file ( $threads -eq 1) { $i = 1 foreach ( $file $selectedfiles) { write-output "" write-output "deploying configuration file ${i}/$($selectedfiles.count): $file" set-ingestionmetadatafromjsonfile @deployparams -path $file $i++ } } # parallel processing # else { # #$modulepath = (get-module -name "mbbaz").path # $selectedfiles | foreach-object -parallel { # #import-module $using:modulepath | out-null # set-ingestionmetadatafromjsonfile @using:deployparams -path $_ # } -throttlelimit $threads # } #endregion #region step 4: deploy delta tables # deploy delta tables ($deploydeltatables){ $deltatableconfigurationpath = join-path $workspacefolder $deltatableconfigurationfile ( -not (test-path -path $deltatableconfigurationpath -pathtype "leaf")) { throw "could find delta table configuration file: $deltatableconfigurationpath" } set-ingestiondeltatableconfiguration ` -deltatableconfigurationfile "$($workspacefolder)/$($deltatableconfigurationfile)" ` -sessionname $identifiedsessionname ` -targetenvironment $targetenvironment ` -informationaction 'continue' } #endregion write-output "*** end script: $($myinvocation.mycommand.name) ***"</file><file name="pipelines\scripts\sql\execute_sqldb_query.ps1">&lt;# .synopsis execute queries sql db tables .description make connection sql db. allows query delta lake powershell. script expects filename: file contain query execute sql #&gt; [cmdletbinding()] param( [parameter(mandatory=$true)] [string] $sqlserverinstance, # sql server trying connect [parameter(mandatory=$true)] [string] $sqldatabase, # sql database trying write [parameter(mandatory=$true)] [string] $accesstoken, # personal accesstoken user [parameter(mandatory=$true)] [string] $path, # file trying execute [parameter(mandatory=$false)] [string] $targetenvironment # environment (dev, int) sql scripts executed ) $scriptname = $script:myinvocation.mycommand.path write-output "*** start script: $scriptname ***" # force error action $erroractionpreference = "stop" $informationpreference = "continue" #region print parameters # define list input parameters $inputparameters = @( "sqlserverinstance" "sqldatabase" "path" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # endregion # set sql parameters $sql_parameters = @{ serverinstance = $sqlserverinstance database = $sqldatabase accesstoken = $accesstoken includesqlusererrors = $true } $querycontent = get-content -path $path -raw ($sqlserverinstance -like '*sql.azuresynapse.net'){ $storageaccount = "$($targetenvironment)dapstdala1" invoke-sqlcmd @sql_parameters -query $querycontent -connectiontimeout 60 -outputsqlerrors $true -variable @{storage_account = $storageaccount} } else{ invoke-sqlcmd @sql_parameters -query $querycontent -connectiontimeout 60 -outputsqlerrors $true }</file><file name="pipelines\scripts\sql\gather_sql_test_scripts.ps1">&lt;# .synopsis gather sql scripts needed executed #&gt; [cmdletbinding()] param ( # full server url [parameter(mandatory = $true)] [string] $sqlserverinstance, # sql server access [parameter(mandatory = $true)] [string] $sqldatabase, # sql db access [parameter(mandatory = $true)] [string] $accesstoken = $null, # accesstoken sql db currentuser [parameter()] [validateset('all','path')] [string]$filtertype = 'all', # selection files gathered (all, path = regex folderpath) [string]$filterpath = $null, # filtertype = path: "-like" style path filter (regex!) [string] $workspacefolder = $env:reporoot, # root folder git repo local device [boolean] $recurse = $true, # look recursively? [int32] $threads = 1 # execute parallel &gt; 1 ) $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" #region print parameters $inputparameters = @( "sqlserverinstance" "sqldatabase" "workspacefolder" "filtertype" "filterpath" "recurse" "threads" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } #endregion write-information "validate parameters..." $fullpath = join-path $workspacefolder "src" "synapse" "_test" "test_scripts" "sql" # check folderpath exists ( -not ( test-path $fullpath -pathtype container )) { throw "sql-folder exist: $fullpath" } # filtertype = path: make sure filterpath empty ( $filtertype -eq "path") { ( -not $filterpath ) { throw "filterpath parameter mandatory filtertype 'path'" } } write-information "collect sql-files..." # get sql-files folder path [array] $allfiles = get-childitem -path $fullpath -recurse:$recurse -filter "*.sql" write-information "found $($allfiles.count) sql-file(s) filtering" write-information " first 10:"# print sample files" $allfiles | select-object -first 10 | foreach-object { write-information " $_"} # filter collected json_files based filtertype switch( $filtertype) { 'all' { [array] $selectedfiles = $allfiles } 'path' { [array] $selectedfiles = $allfiles | where-object {$_ -like $filterpath} } } write-information "retained $($selectedfiles.count) sql file(s) filtering [$filtertype] mode" # selection configuration files passed filter section $selectedfiles | foreach-object { write-information " $_"} # make sure relevant modules set accesstoken parameter empty ( -not (get-module "sqlserver" -listavailable) ) { write-information "installing module: sqlserver ..." install-module -name "sqlserver" -scope currentuser -force } #create token specified ( -not $accesstoken) { write-information "token provided, obtaining..." $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net -debug:$false).token } else { write-information "using provided token sql." } write-debug "accesstoken: $accesstoken" write-information "starting executing sql-scripts..." # # simple sequential processing ( $threads -eq 1) { $i = 1 foreach ( $file $selectedfiles) { write-output "" write-output "executing script: ${i}/$($selectedfiles.count): $file" $sqlparams = @{ sqlserverinstance = $sqlserverinstance sqldatabase = $sqldatabase accesstoken = $accesstoken path = $file targetenvironment = $targetenvironment } . $workspacefolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @sqlparams $i++ } } # parallel processing # else { # #$modulepath = (get-module -name "mbbaz").path # $selectedfiles | foreach-object -parallel { # #import-module $using:modulepath | out-null # set-ingestionmetadatafromjsonfile @using:deployparams -path $_ # } -throttlelimit $threads # }</file><file name="pipelines\scripts\sql\prepare_logging_tables.ps1">&lt;# .synopsis start new run certain plan expose ids pipeline variables .description running devops pipeline test orchestration workers, running ingestion framework scratch every time make sense script start new run using meta.usp_new_run stored procedure next this, plan_id, run_id, task_list exposed pipeline variables (adovariable) called tasks #&gt; [cmdletbinding()] param ( [parameter(mandatory=$false, helpmessage="full url sql server, eg xxxxxxx.database.windows.net")] [string] $targetserver = $env:sql_server, [parameter(mandatory=$false, helpmessage="full name sql database, eg gm6dwh-xxxxx-sqldb-meta")] [string] $targetdatabase = $env:sql_database, [parameter(mandatory=$true, helpmessage="name plan needs logged")] [string] $planname, [parameter(mandatory=$false, helpmessage="personal accesstoken linked microsoft account")] [string] $accesstoken, [parameter(mandatory=$false, helpmessage = "name azure devops variable expose, used tasks devops pipeline")] [string[]] $adovariable ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" | out-null # start region: print variables $inputparameters = @( "targetserver" "targetdatabase" "planname" "accesstoken" "adovariable" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "" # --------------------------------------------------------------------------------------------------------- # get accesstoken provided (!$accesstoken){ $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net).token } # group sql parameters overarching variable $sqlparams = @{ serverinstance = $targetserver database = $targetdatabase accesstoken = $accesstoken includesqlusererrors = $true } # invoke stored procedure 'meta.usp_new_run' relevant parameters write-information "" write-information "creating plan: $planname..." $query = "exec meta.usp_new_run @pipeline_id = '$($planname)', @plan_name = '$($planname)', @new_plan = 1, @no_select = 0" # return run_id select statement $runplan = invoke-sqlcmd @sqlparams -query $query # use run_id get plan_id: use highest plan_id given run_id # dev-note: filter task_group, needed unit testing plan different task groups $query = @" select max(lp.plan_id) plan_id meta.log_plans lp inner join meta.log_run_plans lrp (lrp.plan_id = lp.plan_id) 1=1 lrp.run_id = $($runplan.run_id) lp.plan_name = '$($planname)' "@ # get list plans (should one) $planlist = invoke-sqlcmd @sqlparams -query $query # using plan_id, get list tasks execute plan # dev-note: filter task_group, needed unit testing plan different task groups write-information "" write-information "getting tasks plan: $planname ($($planlist.plan_id))..." $query = @" select lpt.*, lt.current_status meta.log_plan_tasks lpt inner join meta.log_tasks lt ( lt.task_id = lpt.task_id) 1=1 lpt.plan_id = $($planlist.plan_id) lpt.original_plan_id = $($planlist.plan_id) "@ $tasklist = invoke-sqlcmd @sqlparams -query $query # tasks pending plan # not, throw error $missconfiguredtasks = $tasklist | where-object {$_.current_status -notin ("pending")} ($missconfiguredtasks) { $missconfiguredstatus = ($missconfiguredtasks | select-object -expandproperty "current_status" -unique) -join "', '" write-error "aborting execution missconfigured tasks detected statuses ['${missconfiguredstatus}']" } # tasks execute, write warning shell (-not $tasklist) { write-warning "could find task executions associated plan_id $($runplan.plan_id)" write-host "##vso[task.logissue type=warning;sourcepath=$pscommandpath;linenumber=60;]could find task executions associated plan_id $($runplan.plan_id)" } # return list tasks list json objects # dev-note: filter task_group, needed unit testing plan different task groups $query = @" declare @tasks nvarchar(max); set @tasks = ( select lpt.*, lt.current_status meta.log_plan_tasks lpt inner join meta.log_tasks lt ( lt.task_id = lpt.task_id) 1=1 lpt.plan_id = $($planlist.plan_id) lpt.original_plan_id = $($planlist.plan_id) order task_id json path ); select @tasks tasks "@ $jsontasklist = invoke-sqlcmd @sqlparams -query $query # add files meta.log_files prevent 'file exist' errors logging file movements # write-information "" # write-information "------------------------------------" # write-information "prepare file tracking table..." # write-information "------------------------------------" # foreach ($task $tasklist) { # write-information "" # $filestoupload = get-childitem "_test/unittest-files/ingestion_pipeline/$($task.task_name)" -recurse -file # write-information "files register meta.file_tracking task [$($task.task_name)]:" # $filestoupload | foreach-object { write-information " $_"} # foreach ($file $filestoupload.name){ # write-information "registering file: $file" # $query = " # insert meta.file_tracking (task_id, plan_id, registration_ts, original_filename, upload_ts, source_folder, extended_filename, landing_container, landing_folder, copied_status, copied_ts) # values ($($task.task_id), $($task.plan_id), getdate(), '$file', getdate(), 'test/unit_test', '$file', 'landing', 'unit_test', 'true', getdate()); # " # exec-query($query) # } # } # adovariables given, loop expose # dev-note: expose run_id, plan_id, task_list ([bool]$adovariable) { write-information "writing ado variables..." foreach ($variable $adovariable){ write-information "checking variable $variable..." ($variable -like '*run_id*'){ write-information "set run_id variable" write-host "##vso[task.setvariable variable=${variable};]$($runplan.run_id)" } elseif($variable -like '*plan_id*'){ write-information "set plan_id variable" write-host "##vso[task.setvariable variable=${variable};]$($planlist.plan_id)" } elseif($variable -like '*task_list*'){ write-information "set task list variable" write-host "##vso[task.setvariable variable=${variable};]$($jsontasklist.tasks)" } } } else{ [hashtable] $variablelist = @{ runid = $($runplan.run_id) planid = $($planlist.plan_id) tasklist = $($jsontasklist.tasks) } write-host $variablelist.gettype() write-host $variablelist write-host $variablelist.runid return $variablelist } write-information "" write-information "***end script: $($scriptname)***"</file><file name="pipelines\scripts\sql\resume_sqldb.ps1">&lt;# .synopsis start az sql database currently paused .description check current state sql database sql server. status "paused: launch test query every (sleepseconds) seconds connection successful, (maxattempts) reached. maxattempts reached, exception error raised. #&gt; [cmdletbinding()] param ( [parameter(mandatory = $true, helpmessage="name sql server ")] [string] $servername, [parameter(mandatory = $true, helpmessage="name sql server host")] [string] $sqlserverinstance = $env:sql_server, [parameter(mandatory = $true, helpmessage= "name sql database check")] [string] $sqldatabase = $env:sql_database, [parameter(mandatory = $true, helpmessage = "name azure resource group sql server located")] [string] $resourcegroupname = 'dev-dap-rg-core', [parameter(mandatory = $false, helpmessage = "token gives access database")] [string] $accesstoken = $null, [parameter(mandatory = $false, helpmessage="number attempts start sql pool going error")] [int32] $maxattempts = 4, [parameter(mandatory = $false, helpmessage="wait time start-attempts")] [int32] $sleepseconds = 20 ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" $inputparameters = @( "servername" "sqlserverinstance" "sqldatabase" "resourcegroupname" "maxattempts" "sleepseconds" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # get database instance provided servername write-information "get database information..." $databaseinstance = get-azsqldatabase -resourcegroupname $resourcegroupname -servername $servername -databasename $sqldatabase -warningaction "ignore" ( -not $databaseinstance) { throw "database '$sqldatabase' exist resource group $resourcegroupname permission access it." } # get status database (paused, started...) $status = $databaseinstance.status write-information ("current database state: " + $status) ( $status -eq "paused") { # install sqlserver module already installed ( -not (get-module sqlserver -listavailable) ) { write-information "installing module: sqlserver ..." install-module -name sqlserver -scope currentuser -force } write-information "resuming database..." # get accesstoken provided ( -not ($accesstoken) ) { write-information "token provided, obtaining..." $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net).token } # group sql parameters invoke sql command $sqlparams = @{ serverinstance = $sqlserverinstance database = $sqldatabase accesstoken = $accesstoken query = "select 'hello ' + db_name() test" } $numtries = 0 :retry_loop while( $true ) { # try query database # error thrown, break loop try { invoke-sqlcmd @sqlparams write-information " success!" break retry_loop } # error thrown catch { # timeout-error: continue script ( $_.exception.message -like "*the timeout period elapsed post-login phase*") { write-information (' failed: database yet available ... ("the timeout period elapsed post-login phase")') } elseif ( $_.exception.message -like "*is currently available*") { write-information (' failed: database yet available ... ("database currently available")') } else { # error: throw error stop script throw $_.exception.message } } # numtries &lt; maxattempts: wait continue loop ( $numtries -lt $maxattempts ) { $numtries ++ write-information "retrying $sleepseconds seconds..." start-sleep -seconds $sleepseconds } # else: throw error else { throw "connection successful $maxattempts attempts." } } } else { write-information "no action take" } write-output "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\sql\upload_to_tables.ps1">&lt;# .synopsis upload configuration data sql db tables .description initiate deploy_data_configuration script (set of) configuration files #&gt; [cmdletbinding()] param( [parameter(mandatory=$true)] [string] $sqlserverinstance, # sql server trying connect [parameter(mandatory=$true)] [string] $sqldatabase, # sql database trying connect [parameter(mandatory=$true)] [string] $accesstoken, # accesstoken executor script [parameter(mandatory=$true)] [string] $folderpath, # path find configuration files repo [parameter(mandatory=$true)] [string] $filtertype, # configuration files upload: ['all', 'path', 'filelist'] [parameter(mandatory=$false)] [string] $path, # filtertype = path: give path configuration files stored [parameter(mandatory=$false)] [array] $filelist, # filtertype = filelist: give list configuration files upload [parameter(mandatory=$false)] [string] $deltatableconfigurationfile, # filepath delta_tables.json file repository [parameter(mandatory=$false)] [string] $workspacefolder = $env:reporoot, # directory git repo stored device [parameter(mandatory=$false)] [string] $targetenvironment = $env:mydevenv ) $scriptname = $script:myinvocation.mycommand.path write-output "*** start script: $scriptname ***" # force error action $erroractionpreference = "stop" $informationpreference = "continue" #region print parameters # define list input parameters $inputparameters = @( "sqlserverinstance" "sqldatabase" "folderpath" "filtertype" "path" "filelist" "deltatableconfigurationfile" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # endregion $params = @{ sqlserverinstance = $sqlserverinstance sqldatabase = $sqldatabase accesstoken = $accesstoken targetenvironment = $targetenvironment workspacefolder = $workspacefolder configurationfolderpath = $folderpath deltatableconfigurationfile = $deltatableconfigurationfile filtertype = $filtertype filterpath = $path filelist = $filelist recurse = $true threads = 1 } . $workspacefolder/pipelines/scripts/sql/deploy_data_configuration.ps1 @params -informationaction 'continue' # $synapseparams = @{ # serverinstanceinstance = $serverinstance # database = "lnl_meta" # accesstoken = $accesstoken # } # ### gather data insert sql tables # $files = get-childitem -path $folderpath # foreach($file $files){ # $querycontent = get-content -path $file -raw # write-output "execute query $file :" # write-output " $querycontent" # invoke-sqlcmd @synapseparams -query $querycontent -outputsqlerrors $true # }</file><file name="pipelines\scripts\synapse\check_test_logs.ps1">&lt;# .synopsis grab log files azure container convert xml formatted set files .description testing process, testnotebook spark notebook generates log file tests executed. file published azure storage container 'logs'. script grabs file container converts xml format accepted azure devops. resulting xml files later published devops another script .notes script also allows localdeploy option. switch present, xml files generated content output file printed terminal window #&gt; [cmdletbinding()] param ( [parameter( mandatory = $true)] [string] $storageaccount = 'devdapstdala1', # storage account find log file [parameter( mandatory = $true)] [string] $containername = 'logs', # container name log file found [parameter( mandatory = $false)] [string] $workspacefolder = $env:reporoot, # local folder repo cloned [parameter( mandatory = $true)] [string] $outpath = 'src\synapse\_test\output', # path output generated xml files [parameter( mandatory = $false)] [switch] $islocaldeploy # deploying locally -&gt; avoid generating xml files print results terminal ) # force informationpreference continue $informationpreference = 'continue' # start region: print variables $inputparameters = @( "storageaccount" "containername" "workspacefolder" "outpath" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } #endregion # step 1: validation write-information "executing validity checks" # check outpath exists workspace. false, create path. true, remove create # dev-note: remove create replaced clean function sorts: get rid files folder $fulloutpath = join-path $workspacefolder $outpath write-information "full path: $fulloutpath" if(!(test-path $fulloutpath)) { new-item -itemtype directory -force -path $fulloutpath | out-null } else{ remove-item -path $fulloutpath -recurse new-item -itemtype directory -force -path $fulloutpath | out-null } # check storage account exists (!($storageaccount -in (get-azstorageaccount).storageaccountname)){ throw "cannot find $storageaccount given current azcontext" } $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount (!($containername -in (get-azstoragecontainer -context $context).name)){ throw "cannot find container '$containername' storage account '$storageaccount'" } # step 2: get test log file # test logs file grabbed adls landed testresultpath $testresultpath = join-path $fulloutpath 'output.txt' write-information "grab test logs container '$containername' export '$testresultpath'" # get list blobs container $blobs = get-azstorageblob -container $containername -context $context # loop blobs: one non-empty file # write non-empty file testresultpath $blobs | foreach-object { ($_.length -gt 0){ get-azstorageblobcontent -blob $_.name -container $containername -context $context -destination $testresultpath -force } } # step 3: convert test log file csv table write-information "reconfigure test-file table" # get content test file $content = get-content -path $testresultpath $table_content= @() # loop line write content array table_content foreach( $line $content){ # hardcoded: output file built different sections separated empty line. first section converted table-like structure # test file contains specific error messages (if any). whenever first empty line read, for-loop break # dev-note: analyze piece code cleaner ($line -eq ''){ break } $table_content += $line } # convert array table column names 'name', 'class', 'skip', 'status' # name -&gt; contains name test run (= name function python) # class -&gt; contains name class contains function # skip -&gt; contains relevant information # status -&gt; contains status test: ok, error, fail $csv = $table_content | convertfrom-csv -delimiter " " -header name, class, skip, status # step 4: analyze test results # local deploy, print terminal create xml-files # else, create xml-files based csv-table ($islocaldeploy){ # create empty lists save names broken functions user-friendly output $list_errors = new-object collections.generic.list[string] $list_fails = new-object collections.generic.list[string] # add objects actually contain error fail status $csv | where-object { $_.status -eq 'error'} | foreach-object { $list_errors.add($_.name) } $csv | where-object { $_.status -eq 'fail'} | foreach-object { $list_fails.add($_.name) } # items list, know error ($list_errors.count -ne 0){ write-error "detection broken function(s): $list_errors" } if($list_fails.count -ne 0){ write-error "detection wrongly configured test(s): $list_fails" } } else { # row table: create xml file following specific template read publishtestresults task azure devops $csv | foreach-object { # get rid (__main__\.*) structure around name class # see test log file: (__main__\.&lt;class_name&gt;) -&gt; keep &lt;class_name&gt; $pattern = '\(__main__\.|\)' $replacement = '' $new_value = $_.class -replace $pattern, $replacement #write-information " $($new_value)" $_.class = $new_value } write-information "reconfigure table set xml files" # group rows class-name # create testsuite individual class, containing test results test class # note: 1 xml file per class (containing results tests class), per individual test $groups = $csv | group-object -property class write-output $groups foreach ($group $groups) { $xml = new-object -typename system.xml.xmldocument $root = $xml.createelement('testsuites') $xml.appendchild($root) $suite = $xml.createelement('testsuite') $root.appendchild($suite) # dev-note: add code populate xml document group data # set attributes testsuite element $suite.setattribute('name', $group.name) # name class $suite.setattribute('tests', $($group.count)) # number tests ran class # loop tests group # determine number failures errors $failures = 0 $errors = 0 # make individual key function/test # error/failure, create sub-key gives results # dev-note: sub-key give much information -&gt; check error message also returned foreach ($row $group.group) { $case = $xml.createelement('testcase') $suite.appendchild($case) # set attributes testcase element $case.setattribute('classname', $row.class) # name class running function $case.setattribute('name', $row.name) # name function # dev-note: error fail: also get error given python message -&gt; message contained output file nothing done ($row.status -eq 'error'){ $errors += 1 $error_element = $xml.createelement('error') $case.appendchild($error_element) $error_element.setattribute('message', 'test failed') $error_element.setattribute('type', 'unittest.mock') } elseif ($row.status -eq 'fail'){ $failures += 1 $failure_element = $xml.createelement('failure') $case.appendchild($failure_element) $failure_element.setattribute('message', 'test failed') $failure_element.setattribute('type', 'unittest.mock') } } $suite.setattribute('failures', $failures) $suite.setattribute('errors', $errors) # write generated xml template xml file name class $outfile = join-path $workspacefolder $outpath "mock_result_$($group.name).xml" $xml.save($outfile) } }</file><file name="pipelines\scripts\synapse\invoke_parallel_notebooks.ps1">&lt;# .synopsis run multiple spark notebooks synapse workspace using mssparkutils.notebook.run() .description whenever would needed invoke (list of) notebook(s) (with without parameters) powershell/devops, script used. first, check happens see (for given spark pool session name) whether existing spark session already active. not, one started. second, notebooklist array looped over, invoking notebooks potentially hashtable parameters lastly, check makes sure commands completed validates whether error .parameter notebooks parameter important parameter contains information successfully invoke notebook. array expected contain set objects look like this: @{ &lt;notebookname&gt; = @{ folder = &lt;name folder synapse notebook stored&gt; parametermap = @{ &lt;parameter_name_1&gt; = &lt;parameter_value_1&gt;; &lt;parameter_name_2&gt; = &lt;parameter_value_2&gt;; etc. } } } notebookname reference notebook needs run. key, set parameters defined: - folder: name folder notebook located. check executed validate - parametermap: hashtable-object containing list parameters values need passed notebook example [array] $notebooks = @( @{ metanotebook = @{ folder = "workers" parametermap = @{ plan_list = @(); plan_id = -1; env_code = 'dev' } } }, @{ metanotebook = @{ folder = "workers" } } ) .notes dev_note: look implementation mssparkutils.notebook.runmultiple() https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python #&gt; [cmdletbinding()] param ( [parameter( mandatory = $false, helpmessage="name pool used run notebook")] [string] $poolname = 'devdapsspcore', [parameter( mandatory = $false, helpmessage="name synapse workspace")] [string] $synapseworkspace = $env:synapse_ws, [parameter( mandatory = $false, helpmessage="name session used, annexed user identifier")] [string] $sessionname = 'localinvoke', [parameter( mandatory = $false, helpmessage="list parameters taken notebook")] [array[]] $notebooks = @(), [parameter(mandatory=$false, helpmessage="environment deploy database")] [string] $targetenvironment = $env:mydevenv, [parameter(mandatory=$false, helpmessage="environment deploy database")] [string] $statusupdateseconds = 10 ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-information "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "poolname" "synapseworkspace" "targetenvironment" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # validate environment start new spark session # --------------------------------------------------------------------------------------------------------- #region locate create spark session # initialize session variable if($env:agent_id){ $annex="agent" } else{ $annex = $env:username } $identifiedsessionname = "$($sessionname)_$($annex)" $sessionobj = get-ingestionsparksessionobject -synapseworkspace $synapseworkspace -synapsesparkpool $poolname -sessionname $identifiedsessionname -waitforsession $true -setupsession $false #endregion #region initialize session write-information " run session setup..." # make sure mssparkutils loaded $code = "from notebookutils import mssparkutils" $statement = $sessionobj | invoke-azsynapsesparkstatement -language 'pyspark' -code $code -asjob ($statement.output.count -ne 1){ ($statement.output.count -gt 1){ throw "too many items. something went wrong" } start-sleep -seconds 1 # wait async object load } $statementid = $statement.output.id ( $statement.state -notin ("available", "completed") ) { write-information " statement state [$($statement.state)], check 10 seconds..." start-sleep -seconds 10 $statement = get-azsynapsesparkstatement -workspacename $workspacename -sparkpoolname $poolname -sessionid $sessionobj.id | where-object { $_.id -eq $statementid } } write-information " statement ready!" #endregion #region execute spark statements # print current active session id # useful manually tracking errors write-information("==========================================") write-information("current active id: " + $sessionobj.id) write-information("==========================================") # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # code execution: use mssparkutils.notebook.run start notebook set parameters # --------------------------------------------------------------------------------------------------------- # $startdate = ([datetime]($runinfo).runstart.tostring('yyyy-mm-dd hh:mm:ss')).add($tolocaltimeoffset) # synapse workspace exists: run code $synapweworkspaceexists = get-azsynapsepipeline -workspacename $synapseworkspace if($synapweworkspaceexists) { # notebook, run given parameters $($notebooks) | foreach-object { # define set variables [pscustomobject] $object = $_ [string] $notebookname = $object.keys [string] $notebookfolder = $object.values.folder # notebook exists given folder: ((get-azsynapsenotebook -workspacename $synapseworkspace -name $notebookname ).properties.folder.name -eq $notebookfolder ){ write-information "executing notebook $notebookname..." # parametermap exists object, convert json. else, pass empty variable if( $object.values.containskey("parametermap") ){ $jsonparameterobject = $object.values.parametermap | convertto-json } else{ $jsonparameterobject = "" } # define code execute spark session [string] $code = "mssparkutils.notebook.run ('$($notebookfolder)/$($notebookname)', 3600, $($jsonparameterobject))" write-information "executing code sessionid $($sessionobj.id):" write-information $code # execute code $sessionobj | invoke-azsynapsesparkstatement -language 'pyspark' -code $code -asjob } else{ throw "notebook $($notebookname) exist located given foldername $($notebookfolder)" } } } else{ throw "synapse workspace $($synapseworkspace) cannot found..." } #endregion #region wait code execute # --------------------------------------------------------------------------------------------------------- # code completion: make sure statements executed going next step # --------------------------------------------------------------------------------------------------------- # status statement different 'waiting' 'running' $stillrunning = $true ($stillrunning){ $stillrunning = $false $logs = [array] @(get-azsynapsesparkstatement -workspacename $synapseworkspace -sparkpoolname $poolname -sessionid $sessionobj.id) ($logs.count -eq 0){ start-sleep -seconds 1 # wait async object load } $logs | foreach-object { ( ($_.state -eq 'waiting') -or ($_.state -eq 'running') ) { $stillrunning = $true } else{ # $duration = new-timespan start $startdate end (get-date) # write-output " progress... running time: $($duration.tostring('hh\:mm\:ss')) | started: $($startdate.tostring('hh:mm:ss')) | current time: $((get-date).tostring('hh:mm:ss'))" start-sleep $statusupdateseconds } } } #endregion #region validate code executions # --------------------------------------------------------------------------------------------------------- # code validation: check statements raised error # --------------------------------------------------------------------------------------------------------- # logs-object contains status = error: throw error print error statement ($logs.output.status.contains("error")){ $logs | foreach{ $errors = $logs | where-object { $_.state -eq 'error' } $errors | foreach-object { #write-information "error statement: $($_.code) `n" write-information "errorvalue: $($_.output.errorvalue) `n`n" } } # error, current spark session cannot used anymore current setup always error # stop session completely throw error stop-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $poolname -livyid $sessionobj.id throw "$notebookname errors..." } #endregion # write information script user write-information "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\synapse\invoke_parallel_synapse_pipelines.ps1">&lt;# .synopsis invoke pipeline synapse certain parameters .description start synapse pipeline, need know synapse workspace pipeline located. next that, also need give name pipeline want start parameters needed start pipeline. note: pipelines build metadata sql. make sure metadata deployed (correctly) sql db running pipeline full run orchestration, use pipeline 'pl_start_run' parameters 'plan_name' 'new_plan' .parameter pipelines() parameter important parameter contains information successfully invoke synapse pipeline. array expected contain set objects look like this: @{ &lt;pipelinename&gt; = @{ &lt;parameter_name_1&gt; = &lt;parameter_value_1&gt;; &lt;parameter_name_2&gt; = &lt;parameter_value_2&gt;; etc. } } pipelinename reference pipeline needs run. key, hashtable object defined: - object contains list parameters values need passed pipeline example $pipelines = @( @{ pl_start_run= @{ plan_name= 'unittest_synapse_dummy' new_plan= "true" } }, @{ pl_start_run = @{ plan_name= 'unittest_spark_dummy' new_plan= "true" } }, @{ pl_start_unzip= @{ taskid= 3 planid= 4 } } ) .notes possibility run pipelines without parameter-object defined using script #&gt; [cmdletbinding()] param ( [parameter(mandatory=$false, helpmessage="name synapse workspace environment pipeline located")] [string] $synapsewsname = 'dev-dap-syn-core', [parameter(mandatory=$false, helpmessage="hashtable parameters needed execute pipeline")] [array[]] $pipelines = @(), [parameter(mandatory=$false, helpmessage="how often want status update running pipeline")] [int] $statusupdateseconds = 30 ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-information "*** start script: $scriptname ***" # start region: print variables # define list input parameters $inputparameters = @( "synapsewsname" ## synapse workspace name (e.g. sbx-dap-syn-01) "pipelines" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # end region # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # pipeline execution: start pipelines sequence # --------------------------------------------------------------------------------------------------------- # define set variables [array] $runids = @() [bool] $failedpipelineflag = $false [array] $failedpipelines = @() # offset convert synapse pipeline utc local time [timespan] $tolocaltimeoffset = (get-timezone).baseutcoffset # synapweworkspace exists, invoke pipelines $synapweworkspaceexists = get-azsynapsepipeline -workspacename $synapsewsname if($synapweworkspaceexists) { # loop pipeline-objects $($pipelines) | foreach-object { # get relevant values pipeline-object [pscustomobject] $object = $_ [string] $pipelinename = $object.keys [hashtable] $pipelineparameters = $object.values write-information "" write-information "starting pipeline $pipelinename ..." # invoke pipeline $runid = invoke-azsynapsepipeline -workspacename $synapsewsname -pipelinename "$pipelinename" -parameter $pipelineparameters # get metadata pipeline $runinfo = get-azsynapsepipelinerun -workspacename $synapsewsname -pipelinerunid $runid.runid # add runid list runids check completion $newrunid = [pscustomobject] @{pipelinename=$pipelinename; runid=$($runinfo.runid)} $runids += ($newrunid) # user information write-information " pipeline triggered!" write-information " runid: $($runinfo.runid)" write-information " started: $($runinfo.runstart)`n" write-information "with parameters:" write-information ($pipelineparameters.keys | foreach-object { "$_ $($pipelineparameters[$_])" }) #-join "|" } # user information: print pipeline names runid $($runids) | foreach-object { [string] $pipelinename = $_.pipelinename [string] $pipelinerunid = $_.runid write-information "$($pipelinename): $($pipelinerunid)" } # loop runids validate completed # dev-note: 1 pipeline analysed time. # since pipelines initiated time, need check pipelines time # total time taken for-loop time taking longest pipeline $($runids) | foreach-object { [string] $pipelinename = $_.pipelinename [string] $pipelinerunid = $_.runid $runinfo = get-azsynapsepipelinerun -workspacename $synapsewsname -pipelinerunid $pipelinerunid $runstatus = $runinfo.status; $retries = 0; $maxretries = 5; # pipeline running, sleep check status completed (($runstatus -eq "inprogress" ) -or ($runstatus -eq "queued" ) ) { try { $runinfo = get-azsynapsepipelinerun -workspacename $synapsewsname -pipelinerunid $pipelinerunid $retries = 0 } catch { $retries += 1 ( $retries -le $maxretries) { write-information "an error occured getting runinfo:"; write-information $_; write-information "retrying $statusupdateseconds ..."; continue; } else { throw } } $runstatus = $runinfo.status $startdate = ([datetime]($runinfo).runstart.tostring('yyyy-mm-dd hh:mm:ss')).add($tolocaltimeoffset) #$lastupdated = ([datetime]($runinfo).lastupdated.tostring('yyyy-mm-dd hh:mm:ss')).add($tolocaltimeoffset) $duration = new-timespan start $startdate end (get-date).add($tolocaltimeoffset) write-information " progress... running time: $($duration.tostring('hh\:mm\:ss')) | started: $($startdate.tostring('hh:mm:ss')) | current time: $((get-date).tostring('hh:mm:ss')).add($tolocaltimeoffset)" start-sleep $statusupdateseconds } # finished: print status total duraction pipeline $duration = ([timespan]::frommilliseconds($runinfo.durationinms)).tostring('hh\:mm\:ss') write-information "finished status $runstatus, duration: $duration" ($runinfo.status -ne "succeeded"){ $failedpipelineflag = $true $newfailedpipeline = [pscustomobject] @{pipelinename=$pipelinename; message=$($runinfo.message)} $failedpipelines += $newfailedpipeline } } # pipeline failed: print return message ($failedpipelineflag){ $($failedpipelines) | foreach-object { write-information "there error running pipeline $($_.pipelinename). returned message was: ""$($_.message)"" " } throw "error running pipelines..." } } else{ throw "synapse workspace $($synapseworkspace) cannot found..." } # write information script user write-information "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\synapse\invoke_sparkpools.ps1">#requires -modules "az.synapse" &lt;# .synopsis invoke, run, resume one spark pools specific azure synapse workspace .description script looks active sparkpools, and/or starts sparkpools specific azure synapse workspace sessionname provided, scripts existing (re)usable session name. new sessions started active session found. .parameter waitforsession set $true (default), script wait session start, run initial configuration. set $false, wait session available. .outputs pssynapsesparksession object. #&gt; [cmdletbinding()] param ( [parameter(mandatory = $false, helpmessage = "name synapse workspace")] [string] $workspacename = 'dev-dap-syn-core', [parameter(mandatory = $false, helpmessage = "name pool available inside workspace")] [string] $poolname = 'devdapsspcore', [parameter(mandatory = $false, helpmessage = "name sessions used, annexed user identifier")] [array] $sessionnames = @("localtestsimon"), [parameter(helpmessage = "controls whethere script wait session ready (default), not")] [bool] $waitforsession = $true, [parameter(helpmessage = "environment synapse workspace")] [string] $targetenvironment ) # local script settings set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { $_ -notin [system.management.automation.pscmdlet]::commonparameters } | where-object { $_ -notin [system.management.automation.pscmdlet]::optionalcommonparameters } $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "----------------------------------------------------------------------" #endregion script info # loop list sessionnames start session one $sessionnames | foreach-object { #region locate create spark session # initialize session variable if($env:agent_id){ $annex="agent" } else{ $annex = $env:username } $identifiedsessionname = "$($_)_$($annex)" $sessionobj = get-ingestionsparksessionobject -synapseworkspace $workspacename -synapsesparkpool $poolname -sessionname $identifiedsessionname -waitforsession $waitforsession -setupsession $false #endregion #region initialize session # waitforsession enabled (in case sure valid session) ( $waitforsession) { write-information " run session setup..." # make sure mssparkutils loaded $code = "from notebookutils import mssparkutils" $statement = $sessionobj | invoke-azsynapsesparkstatement -language 'pyspark' -code $code -asjob ($statement.output.count -ne 1){ ($statement.output.count -gt 1){ throw "too many items. something went wrong" } start-sleep -seconds 1 # wait async object load } $statementid = $statement.output.id ( $statement.state -notin ("available", "completed") ) { write-information " statement state [$($statement.state)], check 10 seconds..." start-sleep -seconds 10 $statement = get-azsynapsesparkstatement -workspacename $workspacename -sparkpoolname $poolname -sessionid $sessionobj.id | where-object { $_.id -eq $statementid } } write-information " statement ready!" } #endregion # return pssparksession caller $sessionobj } # write information script user write-information "" write-information "----------------------------------------------------------------------" write-information "end script: $($myinvocation.mycommand.name)" write-information "----------------------------------------------------------------------"</file><file name="pipelines\scripts\synapse\overwrite_synapse_params.ps1">&lt;# .synopsis overwrite parameters templateparametersforworkspace file deploy synapse workspace correct parameters .description devops task synapse workspace deployment@2 generates arm template synapse workspace, well parameters file templateparametersforworkspace. parameters file need overwritten fit target environment deployment. example, linked services pointing dev-environment data lake storage converted point int-environment adls. script analyse templateparametersforworkspace file compare configuration/synapse/parameters.json file: 1. match keys, overwrite parameter configured parameter 2. parameter refers bigdatapool (= spark pool), overwrite (normally) empty value name spark pool used target environment 3. throw warnings: 1. templateparametersforworkspace contains non-empty value configuration parameter parameters.json file 2. parameters.json file contains configuration parameter, parameter templateparametersforworkspace file #&gt; [cmdletbinding()] param( [parameter( mandatory = $true)] [string] $targetenvironment = 'sbx', [parameter( mandatory = $true)] [string] $armtemplatepath = 'c:\users\pla917\dap_appl_dala_code\src\synapse\_test\workspace_templates\templateparametersforworkspace.json', [parameter( mandatory = $false)] [string] $workspacefolder = $env:reporoot ) # force information action 'continue' $informationpreference = "continue" # start region: print parameters $inputparameters = @( "targetenvironment" "armtemplatepath" "workspacefolder" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } #endregion # step 1: validation # check configfile default parameters exists $configfile = join-path $workspacefolder "/configuration/synapse/parameters.json" write-information "checking file exists: $configfile ..." $exists = test-path $configfile ( -not $exists) { throw "configfile found: $configfile" } $configdata = get-content -path $configfile -raw | convertfrom-json # validate parameters replace templateparametersforworkspace (!(get-content -path $armtemplatepath -raw | convertfrom-json).parameters){ # check parameters template need overwritten throw "the parameter file contain parameters" } #region step 2: build dictionary replacement values $synapsereplacements = @{} # first load "default" configuration $defaultconfig = $configdata.default write-information "loading default configuration..." foreach ( $parameter ($defaultconfig | get-member -membertype noteproperty).name ) { write-information " $parameter : $($defaultconfig.$parameter)" $synapsereplacements.add($parameter, $defaultconfig.$parameter) } # apply target_environment specific values $environmentconfigexists = ($configdata.psobject.properties.name -match $targetenvironment) ( ! $environmentconfigexists ) { write-information "no environment-specific parameter section found." } else { write-information "apply environment-specific parameters..." $specificconfig = $configdata.$targetenvironment foreach ( $parameter ($specificconfig | get-member -membertype noteproperty).name ) { write-information " $parameter : $($specificconfig.$parameter)" if( $synapsereplacements.containskey($parameter) ) { $synapsereplacements[$parameter] = $specificconfig.$parameter } else { $synapsereplacements.add($parameter, $specificconfig.$parameter) } } } #endregion #region step 3: override parameters target environment write-information "overwrite parameters..." # gather arm-template start working variables $parametertemplate = get-content -path $armtemplatepath -raw | convertfrom-json # loop parameters arm parameter template (= templateforworkspace) # match default config data, overwrite target environment values # match: # warning: arm contains parameter non-empty value # warning: default config contains parameter arm template # information: empty parameter value found default configuration given foreach ( $parameter ($parametertemplate.parameters | get-member -membertype noteproperty).name) { # tell user parameter checking current value inside arm-template write-information "=======================" write-information " checking parameter: $parameter" write-information " current arm-template value: $($parametertemplate.parameters.$parameter.value)" # generate empty array make sure save updated parameters somewhere $changedparameters = @() # then, inside parameters.json, go parameters need overwritten foreach ($defaultkeypair $synapsereplacements.getenumerator()){ # regex match default arm-template variable, overwrite ($parameter -match $($defaultkeypair.name)){ write-information " overwriting parameter '$($parameter)' value '$($defaultkeypair.value)'" $parametertemplate.parameters.$parameter.value = $defaultkeypair.value # store parameter that's changed inside array, check later warnings $changedparameters += $parameter } } # elements array, know something changed # there's nothing there, nothing changed things need checked if($changedparameters.count -eq 0){ # warning: default config contains parameter arm template (($synapsereplacements | get-member -membertype noteproperty).name -contains $parameter){ $value = $parametertemplate.parameters.$parameter.value write-warning " value $value configured parameter $parameter. however, arm template contain parameter none overwritten" } # warning: arm contains parameter non-empty value elseif ($parametertemplate.parameters.$parameter.value){ write-warning " arm template contains non-empty value '$($parametertemplate.parameters.$parameter.value)' parameter $parameter. value however configured overwritten." } # information: empty parameter value found default configuration given else{ write-information " overwrite needed $parameter" } } } #endregion #region step 4: save overwritten parameter template write-information "writing updated arm template test.json" $parametertemplate | convertto-json -depth 32 | set-content -path $armtemplatepath -force #endregion</file><file name="pipelines\scripts\synapse\tokenization\detokenize_synapse_arm.ps1">&lt;# .synopsis detokenize set json files .description build synapse workspace arm template, managed private endpoints tokenized. script detokenize values arm deploy. .notes tokens string-values certain (set of) start end character(s). strings need replaced depending deployment time, strings resemble meaningful value. .example #{storage_account}# replaced variable storage_account (which set script) #&gt; [cmdletbinding()] param( [parameter(mandatory=$false, helpmessage="azure subscription id")] [string] $subscriptionid = "07f1b2e8-52dc-4020-967e-3eacb668d07a", [parameter(mandatory=$false, helpmessage="resource group synapse workspace")] [string] $coreresourcegroup = "dev-dap-core-rg", [parameter(mandatory=$true, helpmessage="azure subscription id")] [psobject] $armtemplate = "$($env:reporoot)\templateforworkspace (20).json", [parameter(mandatory=$true, helpmessage="resource group synapse workspace")] [psobject] $configurationpath = "$($env:reporoot)\configuration\synapse\synapse_workspace_tokens.json", [parameter(mandatory=$false, helpmessage="resource group synapse workspace")] [psobject] $targetenvironment = $env:mydevenv ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = 'continue' # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "subscriptionid" "coreresourcegroup" "armtemplate" "configurationpath" "targetenvironment" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # check path exists workspace ( -not (test-path -path $configurationpath -pathtype "leaf")) { throw "could fine synapse workspace configuration file: $configurationpath" } # read contents managed_private_endpoints.json configuration file [psobject] $configurationobjects = get-content -raw -path $configurationpath &lt;# joachim: deployment process, tokens configuration files already handled logic removed. #&gt; # replace tokens configurationobjects script (configuration\synapse\synapse_workspace_tokens.json) [psobject] $detokenizedconfigobjects = get-detokenizedstring -stringvalue $configurationobjects -starttoken "#{" -endtoken "}#" # convert detokenized configuration extract members [psobject] $detokenizedjsonconfig = $detokenizedconfigobjects | convertfrom-json # loop managed private endpoint (mpe) configuration objects # note: code expects configuration mpes. mpe replacement values configured, get-detokenizedstring function fail. ( get-member -inputobject $detokenizedjsonconfig -name "managed_private_endpoints" ){ $detokenizedjsonconfig.managed_private_endpoints | foreach-object { # create set variable used replace tokens # reason: get-detokenizedstring replace tokens arm template file. file contains tokens #{&lt;token_name&gt;}# # -&gt; module look variable name replace &lt;token_name&gt; # tokens configured use '_' instead '-' variables (see further) cannot use '-' # create additional variable '-' replaced '_' [string] $mpename = $_.psobject.properties.name write-information "`nchecking mpe: $($mpename)" [string] $mpename_ = $mpename -replace "-", "_" # get configured tokens [object] $mpeconfiguration = $_.psobject.properties.value # every mpe reference subscription resource group, regardless configuration # none configured, use one passed parameter if( ! ( get-member -inputobject $mpeconfiguration -name "subscription" -membertype "properties") ){ new-variable -name "$($mpename_)_subscription" -value "$($subscriptionid)" -force -scope script write-information " new variable '$($mpename_)_subscription' value '$($subscriptionid)'" } if( ! (get-member -inputobject $mpeconfiguration -name "resource_group" -membertype "properties" ) ){ new-variable -name "$($mpename_)_resource_group" -value "$($coreresourcegroup)" -force -scope script write-information " new variable '$($mpename_)_resource_group' value '$($coreresourcegroup)'" } # expose configured properties variables $mpeconfiguration | get-member -membertype noteproperty | foreach-object{ $property = $_.name $propertyvalue = $mpeconfiguration.$property new-variable -name "$($mpename_)_$($property)" -value "$($propertyvalue)" -force -scope script write-information " new variable '$($mpename_)_$($property)' value '$($propertyvalue)'" } } } # replace targetsparkconfiguration tokens actual default value configured synapse_workspace_token.json file (get-member -inputobject $detokenizedjsonconfig -name "notebooks" -membertype "properties"){ # dev-note: whenever would need multiple spark configurations, code need made dynamic. new-variable -name targetsparkconfiguration -value $detokenizedjsonconfig.notebooks.default.targetsparkconfiguration -force -scope script write-information "new variable 'targetsparkconfiguration' value '$($detokenizedjsonconfig.notebooks.default.targetsparkconfiguration)'" } # use exposes variables overwrite tokens arm template $armcontent = get-content -raw -path $armtemplate #| convertfrom-json $detokenizedarm = get-detokenizedstring -stringvalue $armcontent -starttoken "#{" -endtoken "}#" # overwrite arm template detokenized one $detokenizedarm | set-content -path $armtemplate -force # --------------------------------------------------------------------------------------------------------- # write information script user write-output "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\synapse\tokenization\tokenize_managed_private_endpoints.ps1">&lt;# .synopsis tokenize set json files .description building arm template synapse workspace, managed private endpoint values reference names need overwritten. also includes subscription id resource group. script tokenize values arm template build. another script overwrite tokens correct value depending environment arm template deployed to. #&gt; [cmdletbinding()] param( [parameter(mandatory=$false, helpmessage="root working directory")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="path json files need tokenized")] [string] $path = "src\synapse\studio\managedvirtualnetwork\default\managedprivateendpoint" ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = "continue" # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print parameters $inputparameters = @( "workspacefolder" "path" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- # convention: environment code always 3 characters, unknown build stage. regex experession complies conventions $environment_code = '[a-z]{3}' # synapse workspace name will, according conventions, always look like xxx-dap-syn-core xxx target environment $synapseworkspace = "$($environment_code)-dap-syn-core" # adls instance linked synapse workspace will, according conventions, always look like xxxdapstcoresyn xxx target environment $synapsecontainer = "$($environment_code)dapstcoresyn" # array: list default mpes created synapse workspace infrastructure deployment # dev-note: explicit sbx-references need removed, dev-environment still contains references [array] $defaultmpes = @("synapse-ws-custstgacct--$($synapseworkspace)-$($synapsecontainer)", "synapse-ws-sql--$($synapseworkspace)", "synapse-ws-sqlondemand--$($synapseworkspace)", "synapse-ws-custstgacct--sbx-dap-syn-01-sbxdapstadls01", "synapse-ws-sql--sbx-dap-syn-01", "synapse-ws-sqlondemand--sbx-dap-syn-01") # entire path folder, including workspace $folderpath = join-path $workspacefolder $path # check path exists workspace ( -not (test-path -path $folderpath -pathtype "container")) { throw "could fine modulesfolder: $folderpath" } # get list mpes files $jsonfiles = get-childitem -path $folderpath -filter "*.json" # loop file foreach ($file $jsonfiles){ # get content files convert json $mpecontent = get-content -raw -path $file $mpejsoncontent = $mpecontent | convertfrom-json # get name mpe properties [string] $mpename = $mpejsoncontent.name [psobject] $properties = $mpejsoncontent.properties # mpe $defaultmpe array, tokenize properties if( ! $defaultmpes.where({ $mpename -match $_ })){ write-information "managed private endpoint: $($mpename)" # replace '-' '_': causes issues detokenization $mpename = $mpename -replace "-", "_" # create variable privatelinkresource [string] $privatelinkresource = $properties.privatelinkresourceid # replace subscription id #{subscription}# $privatelinkresource = $privatelinkresource -replace "/subscriptions/[\w-]+", "/subscriptions/#{$($mpename)_subscription}#" # replace resource group name #{resource_group}# $privatelinkresource = $privatelinkresource -replace "/resourcegroups/[\w-]+", "/resourcegroups/#{$($mpename)_resource_group}#" # provider replacements # dev-note: might need future. $privatelinkresource = $privatelinkresource -replace "/storageaccounts/[\w-]+", "/storageaccounts/#{$($mpename)_storage_account}#" $privatelinkresource = $privatelinkresource -replace "/servers/[\w-]+", "/servers/#{$($mpename)_server}#" $privatelinkresource = $privatelinkresource -replace "/vaults/[\w-]+", "/vaults/#{$($mpename)_vault}#" $privatelinkresource = $privatelinkresource -replace "/workspaces/[\w-]+", "/workspaces/#{$($mpename)_workspace}#" # overwrite private link mpe tokenized value $properties.privatelinkresourceid = $privatelinkresource # mpe contains property 'fqdns': tokenize if( get-member -inputobject $properties -name "fqdns"){ # overwrite fqdns mpe tokenized value $properties.fqdns = @("#{$($mpename)_fqdns}#") } } # overwrite current content mpe files tokenized content $mpejsoncontent | convertto-json | set-content $file } # --------------------------------------------------------------------------------------------------------- # write information script user write-output "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\synapse\tokenization\tokenize_synapse_notebooks.ps1">&lt;# .synopsis tokenize set json files .description script tokenize notebooks synapse workspace. currently, following references tokenized: - target spark configuration sparkconfiguration configured notebook, overwritten token #{targetsparkconfiguration}# configuration configured notebook, reference made token #{targetsparkconfiguration}# #&gt; [cmdletbinding()] param( [parameter(mandatory=$false, helpmessage="root working directory")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="path json files synapse notebooks need tokenized")] [string] $path = "src\synapse\studio\notebook" ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = "continue" # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "workspacefolder" "path" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # entire path folder, including workspace $folderpath = join-path $workspacefolder $path # check path exists workspace ( -not (test-path -path $folderpath -pathtype "container")) { throw "could fine modulesfolder: $folderpath" } # get list files folder $jsonfiles = get-childitem -path $folderpath -filter "*.json" # loop notebook file foreach ($file $jsonfiles){ # get content files convert json $notebookcontent = get-content -raw -path $file $notebookjsoncontent = $notebookcontent | convertfrom-json # get name notebook properties [string] $notebookname = $notebookjsoncontent.name [psobject] $properties = $notebookjsoncontent.properties # write name notebook found write-information "notebook: $($notebookname)" # create token targetsparkconfiguration # dev-note: whenever would need multiple spark configurations, code need made dynamic. # already reference spark configuration: overwrite reference token "#{targetsparkconfiguration}# if( get-member -inputobject $properties.metadata -name "targetsparkconfiguration" -membertype "properties" ){ $properties.metadata.targetsparkconfiguration = "#{targetsparkconfiguration}#" } # reference spark configuration: create reference give token value "#{targetsparkconfiguration}# else{ $properties.metadata | add-member -name "targetsparkconfiguration" -value "#{targetsparkconfiguration}#" -membertype noteproperty } # overwrite current content notebook file tokenized content $notebookjsoncontent | convertto-json -depth 32 | set-content $file } # --------------------------------------------------------------------------------------------------------- # write information script user write-output "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\synapse\tokenization\tokenize_synapse_workspace.ps1">&lt;# .synopsis tokenize set json files .description script go different powershell scripts, tokenizing specific set synapse artefact files. powershell scripts stored directory: pipelines/scripts/synapse/tokenization #&gt; [cmdletbinding()] param( [parameter(mandatory=$false, helpmessage="root working directory")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="path json files need tokenized")] [string] $studiopath = "src\synapse\studio\" ) # force switch parameters set-strictmode -version "latest" $erroractionpreference = "stop" $informationpreference = "continue" # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print parameters $inputparameters = @( "workspacefolder" "studiopath" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } # --------------------------------------------------------------------------------------------------------- # invoke script: tokenize_managed_private_endpoints.ps1 # script tokenize managed private endpoint artefacts synapse workspace write-information "tokenize managed private endpoints..." $mpepath = join-path $studiopath "managedvirtualnetwork\default\managedprivateendpoint" . $workspacefolder/pipelines\scripts\synapse\tokenization\tokenize_managed_private_endpoints.ps1 -workspacefolder $workspacefolder -path $mpepath # invoke script: tokenize_synapse_notebooks.ps1 # script tokenize notebook artefacts synapse workspace write-information "tokenize synapse notebooks..." $notebookpath = join-path $studiopath "notebook" . $workspacefolder/pipelines\scripts\synapse\tokenization\tokenize_synapse_notebooks.ps1 -workspacefolder $workspacefolder -path $notebookpath # --------------------------------------------------------------------------------------------------------- # write information script user write-output "*** ending script: $scriptname ***"</file><file name="pipelines\scripts\synapse\validate_workers\validate_filworker.ps1">&lt;# .synopsis script executes validation checks make sure filworker behaved expected .description script execute following checks: 1. check landing absence original file &amp; presence newly converted file 2. check archive container presence original file 3. check contents converted file .fil -&gt; csv-file see everything converted properly 4. check logs metadb #&gt; [cmdletbinding()] param( [parameter( mandatory = $false, helpmessage="the local folder script based")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="name environment we're currently in" )] [string] $targetenvironment = $env:mydevenv, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $sqlserverinstance = $env:sql_server, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $sqldatabase = $env:sql_database, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $accesstoken ) # force switch parameters set-strictmode -version "latest" $informationpreference = 'continue' $erroractionpreference = 'continue' # --------------------------------------------------------------------------------------------------------- $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" #startregion # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $inputparameters = @( "workspacefolder" "targetenvironment" "sqlserverinstance" "sqldatabase" "accesstoken" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "" #endregion # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- ### prep: validation (!$accesstoken){ write-information "access token provided, obtaining..." $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net).token } ### metadb-params $sqlparams = @{ sqlserverinstance = $sqlserverinstance sqldatabase = $sqldatabase accesstoken = $accesstoken } ## storage account checks [array] $originalfilelist = @('firstrow_to_headers.fil') [array] $convertedfilelist = @('firstrow_to_headers.csv') ### 1a. original file removed landing . $workspacefolder/pipelines/scripts/datalake/file_container_existence.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -containername 'landing' ` -blobprefix 'preprocess_test/filworker_test/prd/20240802_112046' ` -filelist $originalfilelist ` -presencecheck $false ` -informationaction 'continue' ### 1b. landing contain converted .fil -&gt; .csv-file . $workspacefolder/pipelines/scripts/datalake/file_container_existence.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -containername 'landing' ` -blobprefix 'preprocess_test/filworker_test/preprocess/prd/20240802_112046/firstrow_to_headers' ` -filelist $convertedfilelist ` -informationaction 'continue' ### 2. archive contain original .fil-file . $workspacefolder/pipelines/scripts/datalake/file_container_existence.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -containername 'archive' ` -blobprefix 'preprocess_test/filworker_test/prd/20240802_112046' ` -filelist $originalfilelist ` -informationaction 'continue' ### 3. check contents converted file see everything converted properly $convertedfile = . $workspacefolder/pipelines/scripts/datalake/get_blob_content.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -containername 'landing' ` -blob "preprocess_test/filworker_test/preprocess/prd/20240802_112046/firstrow_to_headers/$($convertedfilelist[0])" # dev-note: index 0, output 'write-output "start script: $($myinvocation.mycommand.name) parameters:' (if ever removed, index) $dataframe = $convertedfile[1] | convertfrom-csv -delimiter "`t" $expecteddataframe = @" 999998 75 0003 1 1642.60 19800101 19800102 82 999998 75 0004 3 2463.90 19800101 19800102 82 999998 75 0005 12 720.00 19800101 19800102 82 999998 75 0006 66 1980.00 19800101 19800102 82 "@ | convertfrom-csv -delimiter "`t" # simple comparison rows expected order ($i = 0; $i -lt $dataframe.count; $i++) { $actualrow = $dataframe[$i].psobject.properties.value -join "`t" $expectedrow = $expecteddataframe[$i].psobject.properties.value -join "`t" ($actualrow -ne $expectedrow) { throw "mismatch found row $($i): expected '$expectedrow', got '$actualrow'. validation check filworker, step 3" } } ### 4. check file logged suffix _preprocess $loggedscripts = get-childitem -path $workspacefolder/src/synapse/_test/test_scripts/sql | where-object -property name -like 'filworker*' foreach ($script $loggedscripts){ . $workspacefolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @sqlparams -path $script.fullname } write-information "" write-information "***end script: $($scriptname)***"</file><file name="pipelines\scripts\synapse\validate_workers\validate_ingestionworker.ps1">&lt;# .synopsis script executes validation checks make sure ingestionworker behaved expected .description script execute following checks: 1. landing container contain files 2. file moved archive 3. delta table created silver container 4. parquet file raw 5. file logged logging tables 6. ingestionworker_csv_task executed successfully 7. delta table contain data csv file #&gt; [cmdletbinding()] param( [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $storageaccount = 'devdapstdala1', [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $sqlserverinstance = $env:sql_server, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $sqldatabase = $env:sql_database, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $accesstoken, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $synapseworkspace = $env:synapse_ws, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $synapseserver = $env:synapse_server, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $synapsedatabase = $env:synapse_database, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $targetenvironment = $env:mydevenv, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $workspacefolder = $env:reporoot, [parameter(mandatory=$false, helpmessage="name storage account files expected")] [string] $plan_id=2 ) # force switch parameters set-strictmode -version "latest" $informationpreference = 'continue' $erroractionpreference = 'continue' # --------------------------------------------------------------------------------------------------------- $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" #startregion # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $inputparameters = @( "storageaccount" "sqlserverinstance" "sqldatabase" "synapseworkspace" "synapseserver" "synapsedatabase" "targetenvironment" "workspacefolder" "plan_id" ) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "" #endregion # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- ## parameters ### access token (!$accesstoken){ $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net).token } $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount ### metadb-params $sqlparams = @{ sqlserverinstance = $sqlserverinstance sqldatabase = $sqldatabase accesstoken = $accesstoken } ### on-demand params $synapseparams = @{ sqlserverinstance = $synapseserver sqldatabase = $synapsedatabase accesstoken = $accesstoken targetenvironment = $targetenvironment } ## storage account checks [array]$files = @("csv_test.csv", "json_test.json", "headerless_header_check.csv", ` "prd/20240802_112046/recurse.csv", "parquet_test.parquet", "csv_skiplines.csv", "csv_partitioning.csv", ` "datatype_test.csv") [string]$blobprefix = 'ingestion_test/20240802_112048' ### 1. files remain landing # check files supposed removed landing, actually removed. not, write error . $workspacefolder/pipelines/scripts/datalake/file_container_existence.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -containername 'landing' ` -blobprefix $blobprefix ` -filelist $files ` -presencecheck $false ` -informationaction 'continue' write-output "" write-output "" ### 2. files moved archive # check files supposed moved archive, actually moved. not, write error . $workspacefolder/pipelines/scripts/datalake/file_container_existence.ps1 ` -workspacefolder $workspacefolder ` -targetenvironment $targetenvironment ` -containername 'archive' ` -blobprefix $blobprefix ` -filelist $files ` -informationaction 'continue' ### 3. delta tables exist silver container $expecteddeltatables = @("csv_integration", "json_integration", "datatype_table", "unittest_spark_checks_headerless", ` "parquet_integration", "skiplines", "options_partitioning") $containername = 'silver' # get possible blobs inside container according specific filter $blobs = (get-azstorageblob -context $context -container $containername).name # | sort-object -property @{expression ='length'; descending= $true}, @{expression='name'; descending=$true} $expecteddeltatables | foreach-object{ ($_ -notin $blobs){ throw "$($_) found container $($containername) ingestion..." } } # 4. # 5. file logged meta.log_files # 6. ingestionworker_csv_task executed successfully $test_scripts = get-childitem -path $workspacefolder/src/synapse/_test/test_scripts/sql | where-object -property name -like 'ingestionworker*' foreach ($script $test_scripts){ . $workspacefolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @sqlparams -path $script.fullname } # 7. delta table contain data csv file $test_scripts = get-childitem -path $workspacefolder/src/synapse/_test/test_scripts/synapse | where-object -property name -like 'ingestionworker*' foreach ($script $test_scripts){ . $workspacefolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @synapseparams -path $script.fullname } # 8. special case: partitioning: expect year month folders defined delta table # get possible blobs inside container according specific filter $containername = 'silver' $expecteddateparts = @( 'options_partitioning/p_year=1900', 'options_partitioning/p_year=1900/p_month=01' # task: ingestionworker_options_partitioning 'options_filename_partitioning/p_year=2024', 'options_filename_partitioning/p_year=2024/p_month=10', 'options_filename_partitioning/p_year=2024/p_month=10/p_day=24') # task: ingestionworker_options_partitioning_on_filename $partitionedblobs = get-azstorageblob -context $context -container $containername | where-object {$_.name -like 'options_partitioning*' -or $_.name -like 'options_filename_partitioning*'} $expecteddateparts | foreach-object{ ($_ -notin $partitionedblobs.name){ throw "$($_) found container $($containername) folders 'options_partioning'/'options_filename_partitioning' ingestion..." } }</file><file name="pipelines\templates\steps\install_dataconfig_module.yaml">parameters: # file nugets found eg "$(build.artifactstagingdirectory)/nuget" - name: nugetdirectory type: string # provided, evaluate null, skips tasks relying azure connection - name: service_connection_name type: string default: '' steps: - ${{ parameters.service_connection_name }}: # check module azurepowershell - task: azurepowershell@5 displayname: "[debug] check module azurepowershell" # executes debug pipeline executed option 'enable system diagnostics' condition: | ( succeeded(), eq(variables['system.debug'], true) ) inputs: azuresubscription: "${{ parameters.service_connection_name }}" workingdirectory: "$(system.defaultworkingdirectory)" scripttype: inlinescript inline: | write-output "psmodulepath: " write-output $env:psmodulepath write-output "-------------------------" write-output '' write-output '( get-module -name "dataconfig" ).path:' ( get-module -name "dataconfig" ).path write-output "-------------------------" write-output '' write-output '( get-module -name "dataconfig" -listavailable ).path:' ( get-module -name "dataconfig" -listavailable ).path write-output '------------------------------' write-output '' write-output 'unload modules' if( get-module -name "dataconfig" ) { remove-module -name "dataconfig" } write-output '------------------------------' write-output '' write-output 'delete modules' foreach ( $mod ( get-module -name "dataconfig" -listavailable).path ) { $folder = split-path -path $mod write-output "delete: $folder" remove-item -path $folder -force -recurse } write-output '------------------------------' # check write-output 'get-module -name "dataconfig":' get-module -name "dataconfig" write-output '' write-output 'get-module -name "dataconfig" -listavailable:' get-module -name "dataconfig" -listavailable write-output '' write-output '------------------------------' write-output 'loaded modules path(s):' ( get-module -name "dataconfig" ).path erroractionpreference: 'stop' failonstandarderror: true azurepowershellversion: 'latestversion' pwsh: true # install dataconfig module - task: powershell@2 displayname: "install dataconfig module" inputs: workingdirectory: "$(pipeline.workspace)" targettype: inline script: | write-output 'get-module -name "dataconfig" :' get-module -name "dataconfig" write-output 'get-module -name "dataconfig" -listavailable :' get-module -name "dataconfig" -listavailable write-output "----------------------------------" $moduledest = ($env:psmodulepath -split [io.path]::pathseparator)[0] write-output "moduledest: $moduledest" ( test-path -path "${moduledest}/dataconfig") { write-output 'deleting "${moduledest}/dataconfig"...' remove-item -path "${moduledest}/dataconfig" -recurse -force # uninstall-package -name dataconfig -allversions #-erroraction "ignore" } write-output "----------------------------------" write-output 'install-package...' install-package -name dataconfig -source "${{ parameters.nugetdirectory }}" -destination $moduledest -excludeversion -informationaction "continue" -force erroractionpreference: "stop" failonstderr: true pwsh: true - ${{ parameters.service_connection_name }}: # check module azurepowershell - task: azurepowershell@5 displayname: "[debug] check module azurepowershell" # executes debug pipeline executed option 'enable system diagnostics' condition: | ( succeeded(), eq(variables['system.debug'], true) ) inputs: azuresubscription: "${{ parameters.service_connection_name }}" workingdirectory: "$(system.defaultworkingdirectory)" scripttype: inlinescript inline: | write-output "psmodulepath: " write-output $env:psmodulepath write-output "-------------------------" write-output '( get-module -name "dataconfig" ).path:' ( get-module -name "dataconfig" ).path write-output "-------------------------" write-output '( get-module -name "dataconfig" -listavailable ).path:' ( get-module -name "dataconfig" -listavailable ).path write-output '------------------------------' write-output '' # check write-output 'get-module -name "dataconfig":' get-module -name "dataconfig" write-output '' write-output 'get-module -name "dataconfig" -listavailable:' get-module -name "dataconfig" -listavailable write-output '' write-output '------------------------------' write-output 'loaded modules path(s):' ( get-module -name "dataconfig" ).path erroractionpreference: 'stop' failonstandarderror: true azurepowershellversion: 'latestversion' pwsh: true</file><file name="pipelines\templates\steps\install_required_modules.yaml">parameters: # json string holding required module names versions (no version = "any") # format '[ { "name": "&lt;modulename&gt;", "version" : "latest|x.y.z|x.*|x.y.*" }]' - name: requirements type: string # provide custom install path "download" module instead performing normal install currentuser scope - name: custom_install_path type: string default: "" # working directory (if default) - name: workingdirectory type: string default: $(system.defaultworkingdirectory) steps: # install powershell modules order able execute sql queries - task: powershell@2 displayname: 'install required modules' inputs: informationpreference: continue failonstderr: true targettype: filepath pwsh: true workingdirectory: "${{ parameters.workingdirectory }}" filepath: "${{ parameters.workingdirectory }}/pipelines/scripts/general/install_powershell_requirements.ps1" arguments: &gt; -requirements "${{ parameters.requirements }}" -custominstallpath "${{ parameters.custom_install_path }}"</file><file name="pipelines\templates\steps\show_config_vars.yaml">parameters: # comma-separated list variables show - name: variables type: string default: "" # object parameters calling template - name: parametersobj type: object default: - name: "abc" value: "def" # working directory (if default) - name: workingdirectory type: string default: $(system.defaultworkingdirectory) steps: # display job configuration values - task: powershell@2 displayname: '[info] display job configuration values' env: ${{ p parameters.parametersobj }}: param_${{ p.key }}: "${{ p.value }}" inputs: pwsh: true targettype: filepath filepath: "${{ parameters.workingdirectory }}/pipelines/scripts/general/show_config_vars.ps1" erroractionpreference: "continue" failonstderr: true informationpreference: "continue" arguments: &gt; -variables "${{ parameters.variables }}" workingdirectory: "${{ parameters.workingdirectory }}"</file><file name="pipelines\templates\steps\show_folder_contents.yaml">parameters: - name: showfoldersonly type: boolean default: false # folder show contents (default system.defaultworkingdirectory) - name: folderpath type: string default: $(system.defaultworkingdirectory) # working directory (default system.defaultworkingdirectory) - name: workingdirectory type: string default: $(system.defaultworkingdirectory) steps: # defaultworkingdirectory directory listing - task: powershell@2 displayname: "[info] show working directory contents" inputs: pwsh: true targettype: filepath filepath: "${{ parameters.workingdirectory }}/pipelines/scripts/general/show_folder_contents.ps1" erroractionpreference: "continue" failonstderr: true informationpreference: "continue" arguments: &gt; -showfoldersonly $${{ parameters.showfoldersonly }} -path "${{ parameters.folderpath }}" workingdirectory: ${{ parameters.workingdirectory }}</file><file name="pipelines\templates\steps\validate_target_environment.yaml">parameters: # - name: environment_code type: string # - name: target_environment type: string # working directory (if default) - name: workingdirectory type: string default: $(system.defaultworkingdirectory) steps: # validate environment code target environment - task: powershell@2 displayname: "[info] check target environment" enabled: true inputs: targettype: filepath filepath: "${{ parameters.workingdirectory }}/pipelines/scripts/general/check_environment_code.ps1" arguments: &gt; -environment_code "${{ parameters.environment_code }}" -target_environment "${{ parameters.target_environment }}" pwsh: true erroractionpreference: stop informationpreference: "continue" failonstderr: true workingdirectory: "${{ parameters.workingdirectory }}"</file><file name="src\powershell\modules\DataConfig\.build\build.ps1">[cmdletbinding()] param ( [parameter(helpmessage = "name module")] [string] $modulename = "dataconfig", [parameter(helpmessage = "version number major.minor.path format")] [string] $buildversion, [parameter(helpmessage = "base directory module folder")] [string] $workingdir, [parameter(helpmessage = "directory localfeed psrepository")] [string] $localfeedpath, [parameter(helpmessage = "set switch enable cleaning module folder packaging")] [switch] $cleanfolder ) #local settings set-strictmode -version "latest" #requires -version 7.2 #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { $_ -notin [system.management.automation.pscmdlet]::commonparameters} $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "psversion: $($psversiontable.psversion.tostring())" write-information "----------------------------------------------------------------------" #endregion script info write-output "" write-output "package providers:" get-packageprovider write-output "----------------------------------------------------------------------" #unload module currently loaded $currentmodules = get-module -name $modulename -erroraction "ignore" if( $currentmodules) { write-output "unloading currently loaded '$modulename' modules ..." $currentmodules.version | foreach-object { write-information " $_"} $currentmodules | remove-module | out-null } else { write-output "no currently loaded '$modulename' modules (ok)" } #uninstall package currently installed $currentpackages = get-package -name $modulename -erroraction "ignore" if( $currentpackages) { write-output "uninstalling currently installed packages ..." $currentpackages.version | foreach-object { write-information " $_"} $currentpackages | uninstall-package | out-null } else { write-output "no currently installed '$modulename' packages (ok)" } #build check path module folder $modulepath = join-path $workingdir $modulename write-information "" write-information "using modulepath: $modulepath" ( -not (test-path $modulepath -pathtype "container") ) { throw "module '$modulename' subfolder '$workingdir'" } else { write-information " modulepath: ok, folder exists" $modulepath = (resolve-path $modulepath).path write-information " resolved to: $modulepath" } #build check path module manifest $psdfile = $modulename + ".psd1" write-information "using psdfile: $psdfile" $manifestpath = join-path $modulepath $psdfile write-information "using manifestpath: $manifestpath" (-not (test-path $manifestpath) ) { throw "manifest exist at: $manifestpath" } #region prepare localfeed psrepository write-output "preparing localfeed psrepository ..." #check resolve localfeedpath (-not (test-path $localfeedpath)) { new-item -path $localfeedpath -itemtype "directory" | out-null } $localfeedpathresolved = ($localfeedpath | resolve-path -erroraction ignore) ( -not $localfeedpathresolved ) { throw "could resolve localfeedpath: '$localfeedpath'" } else { $localfeedpathresolved = $localfeedpathresolved.path write-information " resolved localfeedpath to: $localfeedpathresolved" } #check create "localfeed" psrepository $localfeed = get-psrepository -name localfeed -erroraction "ignore" ( $localfeed ) { write-output " found localfeed psrepository" # $localfeed # $localfeed.scriptsourcelocation # $localfeed.scriptpublishlocation # write-output "(members)" # $localfeed | get-member } else { write-output " localfeed psrepository found" } ( $localfeed -and ($localfeed.sourcelocation -ne $localfeedpathresolved) ) { write-warning " localfeed repository wrong path ($($localfeed.sourcelocation)), removing ..." #write-output "unregister existing localfeed repository ..." unregister-psrepository -name "localfeed" $localfeed = $null } ( -not $localfeed) { write-output " registering 'localfeed' psrepository path: $localfeedpathresolved ..." register-psrepository -name localfeed -sourcelocation $localfeedpathresolved -publishlocation $localfeedpathresolved -installationpolicy trusted } else { write-output " psrepository 'localfeed' already present path: $localfeedpathresolved" } # remove existing package current version (main avoid conflict (re)building version 0.0.0 locally) $existingpackages = get-childitem -path $localfeedpathresolved -filter "${modulename}.${buildversion}.nupkg" if( -not $existingpackages) { write-information " deleting existing package files : none found [ok]" } else { write-information " deleting existing package files ..." $existingpackages.fullname | foreach-object { write-information " $_"} $existingpackages | remove-item | out-null } #endregion #region create manifest write-output "" write-output "prepare module manifest..." # remove files required nuget package ( $cleanfolder) { write-information " remove unneeded files nuget package..." get-childitem -path $modulepath -include "*.tests.ps1" -recurse | remove-item -force get-childitem -path $modulepath -filter ".build" -directory | remove-item -recurse -force get-childitem -path $modulepath -filter ".test_config" -directory | remove-item -recurse -force get-childitem -path $modulepath -filter "coverage.xml" | remove-item -force get-childitem -path $modulepath -filter "testresults_unit.xml" | remove-item -force } else { write-information " (...skipping cleanfolder section...)" } # update build version manifest write-information " updating version number manifest..." $manifestcontent = get-content -path $manifestpath -raw $manifestcontent = $manifestcontent -replace '&lt;moduleversion&gt;', $buildversion # find public functions add cmdlets $publicfuncfolderpath = join-path $modulepath public $itemparams = @{ path = $publicfuncfolderpath file = $true recurse = $true filter = '*.ps1' exclude = '*.tests.ps1' } ((test-path -path $publicfuncfolderpath) -and ($publicfunctionnames = get-childitem @itemparams | select-object -expandproperty basename)) { $funcstrings = "'$($publicfunctionnames -join "',$([system.environment]::newline) '")'" $funcstrings = $([system.environment]::newline) + " " + $funcstrings + $([system.environment]::newline) } else { $funcstrings = $null } ## add public functions functionstoexport attribute $manifestcontent = $manifestcontent -replace "'&lt;functionstoexport&gt;'", $funcstrings write-information " " write-information " functions export:" $funcstrings | foreach-object { write-output " $_" } ## save updated manifest $manifestcontent | set-content -path "$manifestpath" write-information " updated manifest: $manifestpath" #endregion # create nuget package write-output "" write-output "create publish nuget package localfeed repository..." publish-module -path $modulepath -repository localfeed -nugetapikey 'dummy' -force # list available packages write-output "" write-output "list available packages localfeed repository..." #find-package -source "localfeed" | select-object version, providername, source, fullpath find-package -source "localfeed" | foreach-object {write-output " $($_.version) -- $($_.providername)"} # install package write-output "" write-output "install module localfeed..." install-package -name $modulename -source "localfeed" -providername "powershellget" -force | foreach-object {write-output " $($_.version)"} # review installed modules $modules = get-module -name $modulename -listavailable | select-object name, version, repositorysourcelocation, path write-output "" write-output "available module versions:" $modules | foreach-object {write-output " $($_.version)"} # check "current" version write-output "" write-output "check current version available..." $currentmodule = $modules | where-object {$_.version.tostring() -eq $buildversion } ( -not $currentmodule) { throw "module version '$buildversion' found available modules" } else { write-information " found available module '$modulename' version '$buildversion' (ok)" } # import module write-output "" write-output "import module ..." import-module -name $modulename get-module -name $modulename | foreach-object {write-output " $($_.version)"}</file><file name="src\powershell\modules\DataConfig\.build\unit_tests.ps1">#requires -modules az.keyvault #requires -modules az.accounts #requires -modules az.resources #requires -modules az.synapse #requires -modules @{ modulename="pester"; moduleversion="5.5.0" } [cmdletbinding()] param ( ) #region section: prerequisites write-output "" write-output "using pester version: $((get-module pester).version)" write-output "" write-output "module test:" $modulepath = resolve-path ( join-path "$psscriptroot" "..") $modulename = (split-path $modulepath -leaf) + '.psm1' write-output " modulepath: $modulepath" write-output " modulename: $modulename" # preload module first time ( get-module -name $modulename) { remove-module $modulename } import-module -name (join-path $modulepath $modulename ) -force #endregion #region section: pester configuration # start default configuration $pesterconfig = [pesterconfiguration]::default # create testresults.xml output $pesterconfig.testresult.enabled = $true $pesterconfig.testresult.outputformat = "nunitxml" $pesterconfig.testresult.outputpath = join-path $modulepath "testresults_unit.xml" # create code coverage output $pesterconfig.codecoverage.enabled = $true $pesterconfig.codecoverage.outputformat = "jacoco" $pesterconfig.codecoverage.outputpath = join-path $modulepath "coverage.xml" $pesterconfig.codecoverage.path = "$modulepath/[p]*/*.ps1" # public, private $pesterconfig.codecoverage.usebreakpoints = $false # filters (the fullname filter powershell "like" statement, regex) # important: fullname tag filters work combined "or", "and" $pesterconfig.filter.fullname = "*.unittests.*" $pesterconfig.filter.tag = 'unittest' $pesterconfig.filter.excludetag = "draft" # run parameters $pesterconfig.run.path = join-path $modulepath "public" $pesterconfig.run.exit = $false $pesterconfig.run.passthru = $false # show describe/context/it details $pesterconfig.output.verbosity = "detailed" #endregion #region section: run tests write-output "run tests..." invoke-pester -configuration $pesterconfig #endregion write-output "*** end script: $($myinvocation.mycommand.name)"</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets-extra_key.json">{ "datasets": [ { "name": "test_dataset", "description": "additional description2 key exist", "kind": "csv", "task_type": "synapse_pipeline", "description2": "test" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets-mandatory_keys.json">{ "datasets": [ { "name": "test_dataset_passes_with_all_mandatory_keys", "kind": "csv", "task_type": "synapse_pipeline", "worker": "pl_dummy_worker" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_datatypes.json">{ "datasets": [ { "name": "test_datatypes", "description": "dataset testing possible data types", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "test", "source_folder": "test", "match_pattern": "test", "extension": "csv" }, "columns": [ { "sequence": 1, "name": "array", "data_type": "array" }, { "sequence": 2, "name": "binary", "data_type": "binary" }, { "sequence": 3, "name": "boolean", "data_type": "boolean" }, { "sequence": 4, "name": "date", "data_type": "date" }, { "sequence": 5, "name": "string", "data_type": "string" }, { "sequence": 6, "name": "timestamp", "data_type": "timestamp" }, { "sequence": 7, "name": "decimal", "data_type": "decimal(x,y)" }, { "sequence": 8, "name": "float", "data_type": "float" }, { "sequence": 9, "name": "byte", "data_type": "byte" }, { "sequence": 10, "name": "integer", "data_type": "integer" }, { "sequence": 11, "name": "long_integer", "data_type": "long_integer" } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_defaults.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "dataset variables default left out", "kind": "csv", "task_type": "spark_notebook", "worker": "ingestionworker", "ingestion": { "target_table": "tablename", "source_folder": "test/", "match_pattern": "mypattern", "extension": "txt" }, "checks": [ { "name": "header" } ], "columns": [ { "sequence": 1, "name": "first_column" } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_filename_precond_all_params.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "this tests paramers assigned correctly", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "tablename", "source_folder": "test/", "match_pattern": "mypattern", "extension": ".txt", "container": "test", "preconditions": [ { "name":"required_name", "type":"file_exists", "frequency":"daily", "expected_file_mask":".*{yyyymmdd}.*", "enabled":true, "description":"my_description" } ] }, "columns": [ { "sequence": 1, "name": "first_column", "data_type": "integer", "dimension": "pk" } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_filename_precond_required_params.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "this tests required paramers verifies default values assigned correctly", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "tablename", "source_folder": "test/", "match_pattern": "mypattern", "extension": ".txt", "container": "test", "preconditions": [ { "name":"required_name", "type":"file_exists", "frequency":"daily" } ] }, "columns": [ { "sequence": 1, "name": "first_column", "data_type": "integer", "dimension": "pk" } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_formatting.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "this tests fully configured dataset passes", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "tablename", "source_folder": "test/", "match_pattern": "mypattern", "extension": "txt", "encoding": "utf-8", "column_delimiter": "|", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true, "container": "test", "skip_first_lines": 2 }, "columns": [ { "sequence": 1, "name": "first_column", "data_type": "decimal", "dimension": "pk", "column_options": { "locale": "en-us" } }, { "sequence": 2, "name": "second_column", "data_type": "date", "dimension": "scd2", "column_options": { "format": "yyyy-mm-dd" } }, { "sequence": 3, "name": "third_column", "data_type": "timestamp", "dimension": "scd2", "column_options": { "format": "yyyy-mm-dd't'hh:mm:ss" } }, { "sequence": 4, "name": "fourth_column", "data_type": "decimal", "dimension": "scd2", "column_options": { "thousand_separator": ",", "decimal_separator": "." } } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_full.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "this tests fully configured dataset passes", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "tablename", "target_options": { "partitioning": [ { "name": "non_timestamp_field", "sequence": 1 }, { "name": "timestamp_field", "datepart": "year", "sequence": 2 }, { "name": "timestamp_field", "datepart": "month", "sequence": 3 }, { "name": "timestamp_field", "timestamp": "day", "sequence": 4 } ], "extract_date":{ "column_name": "t_file_name", "regex_expression": "_(\\d{8}_\\d{6})", "extract_date_format": "ddmmyyyy" } }, "source_folder": "test/", "match_pattern": "mypattern", "extension": "txt", "encoding": "utf-8", "column_delimiter": "|", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true, "container": "test", "skip_first_lines": 2 }, "checks": [ { "name": "header", "enabled": false, "config_params": "{\"test\":\"dummy\", \"test2\":\"dummy2\"}" }, { "name": "data_type" } ], "columns": [ { "sequence": 1, "name": "first_column", "data_type": "integer", "dimension": "pk" }, { "sequence": 2, "name": "second_column", "data_type": "string", "dimension": "scd2" }, { "sequence": 3, "name": "third_column", "data_type": "date", "column_info": { "optional": true } } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_invalid-extension.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "this tests ingestion fails ingestion.extension enum", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "tablename", "source_folder": "test/", "match_pattern": "mypattern", "extension": ".txt", "container": "test" }, "columns": [ { "sequence": 1, "name": "first_column", "data_type": "integer", "dimension": "pk" } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_invalid-tableoptions.json">{ "datasets": [ { "name": "test_dataset_csv", "description": "this tests fully configured dataset passes", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker", "ingestion": { "target_table": "tablename", "target_options": { "partitioning": [ { "name": "non_timestamp_field", "sequence": 1 }, { "name": "timestamp_field", "datepart": "plus", "sequence": 2 } ] }, "source_folder": "test/", "match_pattern": "mypattern", "extension": "txt", "encoding": "utf-8", "column_delimiter": "|", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true, "container": "test" }, "checks": [ { "name": "header", "enabled": false, "config_params": "{\"test\":\"dummy\", \"test2\":\"dummy2\"}" }, { "name": "data_type" } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_invalid.json">{ "datasets": [ { "name": "test_dataset", "description": "tests json fails missing mandatory basic keys" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_tasktype_error.json">{ "datasets": [ { "name": "pl_unzip_worker", "description": "this used test json-file fails task_type enum", "kind": "csv", "task_type": "xxx" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_tasktype_SPARK_NOTEBOOK.json">{ "datasets": [ { "name": "test_json_tasktype_spark_notebook", "description": "this tests spark_notebook task_type allowed", "kind": "csv", "task_type": "spark_notebook", "worker": "dummyworker" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_tasktype_SYNAPSE_PIPELINE.json">{ "datasets": [ { "name": "test_json_tasktype_synapse_pipeline", "description": "this tests synapse_pipeline task_type allowed", "kind": "csv", "task_type": "synapse_pipeline", "worker": "pl_dummy_worker" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_worker_pl_dummy_worker.json">{ "datasets": [ { "name": "test_json_tasktype_spark_notebook", "description": "this tests pl_dummy_worker worker allowed", "kind": "csv", "task_type": "synapse_pipeline", "worker": "pl_dummy_worker" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_worker_pl_unzip_worker.json">{ "datasets": [ { "name": "test_json_tasktype_spark_notebook", "description": "this tests pl_unzip_worker worker allowed", "kind": "csv", "task_type": "synapse_pipeline", "worker": "pl_unzip_worker", "ingestion": { "target_table": "unzipped", "source_folder": "zipped", "container": "landing", "match_pattern": "(.*)", "extension": "gz" } } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-delta_tables.json">{ "table_configurations": [ { "table_description": "test description", "kind": "csv", "table_name": "table_name", "container_name": "silver", "column_info": "'[{}]'", "storage_account": "devdapstdala1" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks-mandatory_keys.json">{ "plans": [ { "name": "test_plan_passes_with_all_mandatory_keys" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks_defaults.json">{ "plans": [ { "name": "plan_name_defaults", "description": "plan variables default left out", "task_groups": [ { "name": "ingest", "tasks": [ {"name": "task_name"} ] } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks_full.json">{ "plans": [ { "name": "plan_name", "description": "this test fully configured plan passes", "environments" : ".*", "enabled": true, "task_groups": [ { "name": "preprocess", "tasks":[ {"name": "task_name", "sequence": 1, "enabled": true} ] }, { "name": "ingest", "tasks": [ {"name": "task_name", "sequence": 1, "enabled": true} ] } ] } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks_skip_environment.json">{ "plans": [ { "name": "skip_plan", "description": "this test plan deployed environment matched", "environments" : "none" } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-preprocess_valid.json">{ "preprocess": [ { "name": "preprocess_test", "description": "run preprocessing template worker", "task_type" : "synapse_pipeline", "worker_pipeline": "pl_preprocess_template", "source_folder": "", "source_pattern": "adobe_analytics/%/adobe_analytics/%/finished", "worker_properties": { "foo": "bar" } } ] }</file><file name="src\powershell\modules\DataConfig\.test_config\testInvalidJson.json">{ "invalid" : valid json }</file><file name="src\powershell\modules\DataConfig\public\Compare-IngestionMetadataFromJsonFile.ps1">function compare-ingestionmetadatafromjsonfile { [cmdletbinding()] param( [parameter (mandatory=$true, helpmessage="path file checked")] [string] $path, [parameter (mandatory=$false, helpmessage="name schema-file schema folder compare to-be-checked file to")] [string] $schemafile ) # set default schemafile none given (-not $schemafile){ $schemafile = "ingestionmetadata.schema.json" } $schemapath = join-path $psscriptroot "../schemas" $schemafile ( -not ( test-path $schemapath) ) { write-error "cannot locate ingestionmetadata schema file: $schemapath" return } #load assignments data $ingestionjson = get-content -raw -path $path # verify schema definition ( -not (test-json -json $ingestionjson -erroraction "silentlycontinue") ) { throw "configuration file '$path' valid json file" } $valid = test-json -json $ingestionjson -schemafile $schemapath -erroraction "silentlycontinue" -errorvariable "jsonerrors" ( -not $valid) { (get-member -inputobject $jsonerrors[0] -name "message" -membertype properties) { $message = $jsonerrors[0].message } elseif ((get-member -inputobject $jsonerrors[0] -name "errordetails" -membertype properties) ` -and ($jsonerrors[0].errordetails) ){ $message = $jsonerrors[0].errordetails.message } elseif (get-member -inputobject $jsonerrors[0] -name "exception" -membertype properties) { $message = $jsonerrors[0].exception.message } else { $message = "uknown error" } throw ("configuration file '$path' valid json file schema: $schemafile `n" + $message ) } }</file><file name="src\powershell\modules\DataConfig\public\Compare-IngestionMetadataFromJsonFile.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "compare-ingestionmetadatafromjsonfile" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } } "dataset pass mandatory parameters configured" { $invalidfile = join-path $testfilepath "testingestionmetadata-datasets-mandatory_keys.json" { compare-ingestionmetadatafromjsonfile -path $invalidfile -erroraction "stop" } | -not -throw } "plan pass mandatory parameters configured" { $invalidfile = join-path $testfilepath "testingestionmetadata-plantasks-mandatory_keys.json" { compare-ingestionmetadatafromjsonfile -path $invalidfile -erroraction "stop" } | -not -throw } "preprocessing pass mandatory parameters configured" { $validfile = join-path $testfilepath "testingestionmetadata-preprocess_valid.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should fail non-configured additional key" { $invalidfile = join-path $testfilepath "testingestionmetadata-datasets-extra_key.json" { compare-ingestionmetadatafromjsonfile -path $invalidfile -erroraction "stop" } | -throw "*not valid json file schema*" } "should fail missing mandatory basic keys" { $invalidfile = join-path $testfilepath "testingestionmetadata-datasets_invalid.json" { compare-ingestionmetadatafromjsonfile -path $invalidfile -erroraction "stop" } | -throw "*not valid json file schema*" } "should fail extension enum"{ $invalidfile = join-path $testfilepath "testingestionmetadata-datasets_invalid-extension.json" # support powershell &lt; 7.4 ( $psversiontable.psversion.tostring() -ge "7.4") { $expectederrormessage = "*should match one values specified enum*" } else { $expectederrormessage = "*notinenumeration*" } { compare-ingestionmetadatafromjsonfile -path $invalidfile -erroraction "stop" } | -throw $expectederrormessage } "should fail incorrect datepart partitioning" { $invalidfile = join-path $testfilepath "testingestionmetadata-datasets_invalid-tableoptions.json" $expectederrormessage = "*should match one values specified enum*datepart*" { compare-ingestionmetadatafromjsonfile -path $invalidfile -erroraction "stop" } | -throw $expectederrormessage } "should succeed valid datasets-schema" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_full.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed valid plantask-schema" { $validfile = join-path $testfilepath "testingestionmetadata-plantasks_full.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" }| -not -throw } "should fail task_type enum" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_tasktype_error.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -throw "*is*not*a*valid*json*" } "should succeed task_type synapse_pipeline" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_tasktype_synapse_pipeline.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed task_type spark_notebook"{ $validfile = join-path $testfilepath "testingestionmetadata-datasets_tasktype_spark_notebook.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed worker pl_unzip_worker" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_worker_pl_unzip_worker.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed worker pl_dummy_worker" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_worker_pl_dummy_worker.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed datatypes expected" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_datatypes.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed expected column_options given" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_formatting.json" { compare-ingestionmetadatafromjsonfile -path $validfile -erroraction "stop" } | -not -throw } "should succeed delta table metadata expected" { $validfile = join-path $testfilepath "testingestionmetadata-delta_tables.json" { compare-ingestionmetadatafromjsonfile -path $validfile -schema "deltatabledata.schema.json" -erroraction "stop" } | -not -throw } } }</file><file name="src\powershell\modules\DataConfig\public\Get-CompletedSparkStatements.ps1">function get-completedsparkstatements { &lt;# .synopsis loop spark statements running spark session add completed ones completedjobs array .description status report, often interesting know many initiated jobs completed function loop jobs running spark session append list completed ones. current functionality focused giving information user, could extended future with: - stopping process job failed - getting insights jobs running spark session - ... .parameter startstatementid id first statement initiated spark pool relevant search. statements id higher startstatementid taken consideration .outputs return list ids spark jobs completed. used later user information #&gt; [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="name synapse workspace invoke session" )] [string] $workspacename, [parameter(mandatory=$true, helpmessage="name synapse spark pool running jobs" )] [string] $poolname, [parameter(mandatory=$true, helpmessage="pssynapsesparksession object")] [psobject] $sessionobj, [parameter(mandatory=$false, helpmessage="array jobs completed past")] [array] $completedjobs = @(), [parameter(mandatory=$false, helpmessage="id first spark statement needs checked. ignore statements lower id longer relevant")] [string] $startstatementid = 0 ) # local scripts settings set-strictmode -version "latest" #region get list completed spark statements # get list spark statements executed sessionobject $sparkstatements = get-azsynapsesparkstatement -workspacename $workspacename -sparkpoolname $poolname -sessionid $sessionobj.id # filter list statements id higher startstatementid [array] $sparkstatements = ($sparkstatements | where-object id -ge $startstatementid) ?? @() # (remaining) statement: status, means completed # add id statement $completedjobs array $sparkstatements | foreach-object { ($_.output.status) { (-not ($completedjobs -contains $_.id)){ $completedjobs += $_.id } } } #endregion # return array completed jobs return $completedjobs }</file><file name="src\powershell\modules\DataConfig\public\Get-CompletedSparkStatements.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "get-completedsparkstatements" { context "unittests" { beforeall{ # mock get-azsynapsesparkstatement cmdlet # define outputted result: 2 succeeeded 1 failed spark statement mock -commandname get-azsynapsesparkstatement -mockwith { return @( [pscustomobject]@{ id = "1"; output = [pscustomobject]@{ status = "succeeded" } }, [pscustomobject]@{ id = "2"; output = [pscustomobject]@{ status = "failed" } }, [pscustomobject]@{ id = "3"; output = [pscustomobject]@{ status = "succeeded" } }, [pscustomobject]@{ id = "4"; output = [pscustomobject]@{ status = $null} } ) } } "should return jobs non-empty status property" { # define parameters used call function $completedjobs = @(1) $workspacename = "testworkspace" $poolname = "testpool" $sessionobj = [pscustomobject]@{ id =123 } # call function test $result = get-completedsparkstatements -completedjobs $completedjobs -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj # expect completed jobs array $result | -be @(1, 2, 3) } "should return array new completed jobs added" { # define parameters used call function $completedjobs = @(4, 5) $workspacename = "testworkspace" $poolname = "testpool" $sessionobj = [pscustomobject]@{ id =123 } # call function test $result = get-completedsparkstatements -completedjobs $completedjobs -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj # expect result input completed jobs $result | -be @(4, 5, 1, 2, 3) } "should handle empty completedjobs list" { # define parameters used call function $completedjobs = @() $workspacename = "testworkspace" $poolname = "testpool" $sessionobj = [pscustomobject]@{ id =123 } # call function test $result = get-completedsparkstatements -completedjobs $completedjobs -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj # expect result contain job ids non-empty status $result | -be @(1, 2, 3) } "should return jobs id lower startstatementid" { # define parameters used call function $completedjobs = @(4) $workspacename = "testworkspace" $poolname = "testpool" $sessionobj = [pscustomobject]@{ id =123 } $startstatementid = 2 $result = get-completedsparkstatements -completedjobs $completedjobs -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj -startstatementid $startstatementid # expect result contain job ids higher startstatementid previously completedjobs array $result | -be @(4, 2, 3) } } }</file><file name="src\powershell\modules\DataConfig\public\Get-FailedSparkStatements.ps1">function get-failedsparkstatements { &lt;# .synopsis return error message spark session failed jobs, close session .description execution spark jobs happens without validation whether jobs completed successfully. function loop states jobs executed. .outputs $failedstatement: boolean value indicating whether failed spark statement .parameter startstatementid id first statement initiated spark pool relevant search. statements id higher startstatementid taken consideration #&gt; [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="name synapse workspace invoke session" )] [string] $workspacename, [parameter(mandatory=$true, helpmessage="name synapse spark pool invoke session from" )] [string] $poolname, [parameter(mandatory=$true, helpmessage="pssynapsesparksession object")] [psobject] $sessionobj, [parameter(mandatory=$false, helpmessage="id first spark statement needs checked. ignore statements lower id longer relevant")] [psobject] $startstatementid = 0, [parameter(mandatory=$false, helpmessage="boolean value indicating whether raise error")] [psobject] $raiseerror = $false ) # local scripts settings set-strictmode -version "latest" #region loop completed spark statements print error message failure write-verbose "checking failed jobs..." $logs = get-azsynapsesparkstatement -workspacename $workspacename -sparkpoolname $poolname -sessionid $sessionobj.id # list statements initiated startstatementid [array] $logs = ($logs | where-object id -gt $startstatementid) ?? @() [boolean] $failedstatement = $false # set $failedstatement = $true least one statement status = error $logs | foreach-object { ($_.output.status -eq 'error'){ write-information "errorvalue: $($_.output.errorvalue) `n`n" write-verbose "error statement: $($_.code)" $failedstatement = $true $failedstatement | out-null #get rid syntax-warning } } #endregion #region stop process error raiseerror = $true ($raiseerror -and $failedstatement){ throw "stop process..." } #endregion # return boolean value use caller function return $failedstatement }</file><file name="src\powershell\modules\DataConfig\public\Get-FailedSparkStatements.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe 'get-failedsparkstatements' { context "unittests" { # mock cmdlets used function beforeall { mock get-azsynapsesparkstatement {} } # test case successful execution errors 'should return $false failed statements' { # define function parameters $workspacename = 'testworkspace' $poolname = 'testpool' $sessionobj = [pscustomobject]@{ id = 123 } # mock output get-azsynapsesparkstatement mock -commandname get-azsynapsesparkstatement -mockwith { return @( [pscustomobject]@{ id = 1; output = [pscustomobject]@{ status = 'success' } }, [pscustomobject]@{ id = 2; output = [pscustomobject]@{ status = 'success' } } ) } # call function test $result = get-failedsparkstatements -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj # expect $failedstatements = $result = $false $result | -be $false # expect following function calls assert-mockcalled get-azsynapsesparkstatement -exactly 1 -scope } # test case execution one failed statement 'should return $true least one failed statement' { # define function parameters $workspacename = 'testworkspace' $poolname = 'testpool' $sessionobj = [pscustomobject]@{ id = 123 } # mock output get-azsynapsesparkstatement mock -commandname get-azsynapsesparkstatement -mockwith { return @( [pscustomobject]@{ id = 1; output = [pscustomobject]@{ status = 'error'; errorvalue = 'some error'}; code = "unittest" }, [pscustomobject]@{ id = 2; output = [pscustomobject]@{ status = 'success' } } ) } # call function test $result = get-failedsparkstatements -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj # expect $failedstatements = $result = $true $result | -be $true # expect following function calls assert-mockcalled get-azsynapsesparkstatement -exactly 1 -scope } # test case logs returned 'should return $false logs returned' { # define function parameters $workspacename = 'testworkspace' $poolname = 'testpool' $sessionobj = [pscustomobject]@{ id = 123 } # mock output get-azsynapsesparkstatement return empty array mock -commandname get-azsynapsesparkstatement -mockwith { return @() } # call function test $result = get-failedsparkstatements -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj # expect $failedstatements = $result = $false $result | -be $false # expect following function calls assert-mockcalled get-azsynapsesparkstatement -exactly 1 -scope } 'should return $false failed statements lower id startstatementid' { # define function parameters $workspacename = 'testworkspace' $poolname = 'testpool' $sessionobj = [pscustomobject]@{ id = 123 } $startstatementid = 124 # mock output get-azsynapsesparkstatement return empty array mock -commandname get-azsynapsesparkstatement -mockwith { [pscustomobject]@{ id = 122; output = [pscustomobject]@{ status = 'error'; errorvalue = 'some error'}; code = "unittest" }, [pscustomobject]@{ id = 124; output = [pscustomobject]@{ status = 'success' } } } # call function test $result = get-failedsparkstatements -workspacename $workspacename -poolname $poolname -sessionobj $sessionobj -startstatementid $startstatementid # expect $failedstatements = $result = $false $result | -be $false # expect following function calls assert-mockcalled get-azsynapsesparkstatement -exactly 1 -scope } } }</file><file name="src\powershell\modules\DataConfig\public\Get-FilteredFileList.ps1">&lt;# .synopsis filters files directory based filtertype parameter .description evaluates files sourcepath directory (recursive not) filters based filtertype set. options filtertype: - "all" : loop files found sourcepath (recursively not) - "filelist" : limit list files sourcepath ones listed file specifieid filtervalue parameter - "path" : limit files appling regex match using filtervalue parameter - "branch" : limit files based git diff branch specified filtervalue parameter - "tag" : use tag instead branch. look "latest" tag starting value filtervalue parameter descending sort tags. example, specifying "mytag" select "mytag_1500" "mytag_1400" also never "prefix_mytag" note: filelist mode, paths files must either absolute, evaluated relative sourcepath. .parameter outputfilepath specified, output written file specified parameter. not, function write list files found normal output stream. .parameter outputrelativepath set, output (either file output stream) paths relative relativebasepath. .parameter relativebasepath used outputrelativepath set. specified, output paths relative path. specified, output paths relative current working directory (get-location). example: sourcepath "c:\src\myfolder" outputrelativepath "c:\src" output paths look like './myfolder/file1.txt' instead 'c:\src\myfolder\file1.txt' #&gt; function get-filteredfilelist { [cmdletbinding()] param ( [parameter(mandatory=$true, helpmessage="filter type: all, path, branch, tag")] [validateset('all','filelist','path','branch','tag')] [string]$filtertype, # support "*.json" example [parameter(mandatory=$false, helpmessage="regex filter apply files, top filtertype")] [string]$basicfilter = $null, [parameter(mandatory=$false, helpmessage="filter value, eitherthe path, branch name, tag name")] [string]$filtervalue = $null, [parameter(mandatory=$true, helpmessage="path source directory")] [string]$sourcepath, [parameter(mandatory=$false, helpmessage="recursive find sourcepath, default true")] [boolean]$recurse = $true, [parameter(mandatory=$false, helpmessage="if specified, output file list matching files")] [string]$outputfilepath = $null, [parameter(mandatory=$false, helpmessage="if set, output path relative relativebasepath parameter")] [switch]$outputrelativepath, [parameter(mandatory=$false, helpmessage="if specified, relative paths relative path. required outputrelativepath set")] [string]$relativebasepath = (get-location) ) #region local script settings set-strictmode -version "latest" #endregion #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { $_ -notin [system.management.automation.pscmdlet]::commonparameters} | where-object { $_ -notin [system.management.automation.pscmdlet]::optionalcommonparameters} $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "----------------------------------------------------------------------" write-information "starting function: $($myinvocation.mycommand.name)" write-information " parameters:" foreach ($param $inputparameters) { $paramvalue = (get-variable -name $param).value write-information (" | " + $param.padright( $maxparameterlength ) + ": " + $paramvalue ) } write-information "----------------------------------------------------------------------" #endregion script info #region parameter validation # check sourcepath ( -not (test-path -path $sourcepath -pathtype "container" ) ) { throw "sourcepath exist directory: $sourcepath" } # check: basic filter ( $basicfilter ) { try { [void]([regex]::new($basicfilter)) } catch { throw "basicfilter valid regex expression: $basicfilter" } } # check type: path ( $filtertype -eq "path") { ( -not $filtervalue ) { throw "filtervalue parameter mandatory filtertype 'path'" } try { [void]([regex]::new($basicfilter)) } catch { throw "basicfilter valid regex expression: $basicfilter" } } # check type: filelist ( $filtertype -eq "filelist" ) { # file must exist ( -not (test-path -path $filtervalue -pathtype leaf) ) { throw "the file provided filtervalue parameter must exist filtertype 'filelist'" } } # check output file path ( $outputfilepath ) { # create folder exist $outputfolder = split-path -path $outputfilepath ( -not (test-path -path $outputfolder -pathtype "container") ) { new-item -path $outputfolder -itemtype "directory" -force } } #endregion parameter validation #region get file list basic filter [array] $files = get-childitem -path $sourcepath -recurse:$recurse -file # -relative ( -not $files) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-warning "no files found sourcepath: $sourcepath" return } [array] $files = $files.fullname write-information "found $($files.count) files filtering" ( $files.count -gt 10) { write-verbose "(showing first 10 files only)" } $files | select-object -first 10 | foreach-object { write-verbose " $_"} # apply basic filter (if specified) if($basicfilter) { [array] $files = $files | where-object { $_ -match $basicfilter } ( -not $files) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "no files left applying basic filter: $basicfilter" return } write-information "retained $($files.count) files basic filtering" ( $files.count -gt 10) { write-verbose "(showing first 10 files only)" } $files | select-object -first 10 | foreach-object { write-verbose " $_"} } #endregion #region filelist filter if($filtertype -eq "filelist") { # read filelist file $filelistraw = (get-content -path $filtervalue) ?? @() ( -not $filelistraw) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "the filelist empty, files selected" return } # convert files absolute paths (based $sourcepath) # paths absolute, left [array] $filelist = ($filelistraw | resolve-path -relativebasepath $sourcepath).path # filter $files [array] $files = $files | where-object { $filelist -contains $_ } ( -not $files) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "no files filter applying filelist filter" return } } #endregion #region path filter if($filtertype -eq "path") { [array] $files = $files | where-object { $_ -match $filtervalue } ( -not $files) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "no files filter applying path filter: $filtervalue" return } } #endregion #region branch filter if( $filtertype -eq "branch" ) { # first get list changed files branch (exclude deleted files) [array] $changedfiles = @(git diff --name-only --diff-filter "$filtervalue" $sourcepath/* ) | resolve-path ( -not $changedfiles) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "there files different branch [$filtervalue] source path [$sourcepath]" return } else { [array] $changedfiles = $changedfiles.path write-verbose "branch filter: detected $($changedfiles.count) file(s) changed compared branch [$filtervalue]:" ( $changedfiles.count -gt 10) { write-verbose "(showing first 10 files only)" } $changedfiles | select-object -first 10 | foreach-object { write-verbose " $_"} } # limit files filtered far [array] $files = $files | where-object {$changedfiles -contains $_} ( -not $files) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "no applicable files left applying branch comparison branch [$filtervalue]" return } } #endregion #region tag filter if( $filtertype -eq "tag" ) { # first find latest tag $latesttag = git tag | where-object { $_ -like "${filtervalue}*"} | sort-object -descending | select-object -first 1 ( $latesttag) { write-information "found latest tag: $latesttag" # get files changed compared tag (exclude deleted files) [array] $changedfiles = @(git diff --name-only --diff-filter "$latesttag" $sourcepath/* ) | resolve-path ( -not $changedfiles) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "there files different tag [$latesttag] source path [$sourcepath]" return } else { [array] $changedfiles = $changedfiles.path write-verbose "tag filter: detected $($changedfiles.count) file(s) changed compared branch [$latesttag]:" ( $changedfiles.count -gt 10) { write-verbose "(showing first 10 files only)" } $changedfiles | foreach-object { write-verbose " $_"} } # limit files filtered far [array] $files = $files | where-object {$changedfiles -contains $_} ( -not $files) { if( $outputfilepath) { new-item -path $outputfilepath -type file -force | out-null } write-information "no applicable files left applying branch comparison tag [$filtervalue]" return } } else { # tag found , first time? we'll keep files write-warning "could locate existing tag starting '$filtervalue', applying filter" } } #endregion #region output file output stream write-information "retained $($files.count) file(s) filtering [$filtertype] mode" # make output relative, outputrelativepath specified if( $outputrelativepath ) { $files = $files | resolve-path -relative -relativebasepath $relativebasepath } # write selected output file... ( $outputfilepath ) { $files | out-file -filepath $outputfilepath -encoding "utf8" -force # still useful sample information stream ( $files.count -gt 10) { write-information "showing first 10 lines (only):" } else { $files | select-object -first 10 | foreach-object { write-information " $_"} } } # ... output stream else { $files | write-output } #endregion write-information "*** end function: $($myinvocation.mycommand.name) ***" }</file><file name="src\powershell\modules\DataConfig\public\Get-FilteredFileList.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) # (none) # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest $erroractionpreference = "stop" } describe "get-filteredfilelist" { context "unittests" { # setup tests inside given context beforeall { # get location testdrive # get-childitem cmdlet returns absolute path testdrive files # need store path testdrive able compare results $testfolder = (get-item -path "testdrive:").fullname $testfolder | out-null # suppress warning # filename outputfile$ $outputfilepath = join-path "testdrive:" "filelist.txt" $outputfilepath | out-null # suppress warning # prepare empty source folder $null = new-item -path (join-path "testdrive:" "emptyfolder") -itemtype directory # prepare non-empty sourcefolder $null = new-item -path 'testdrive:/src' -itemtype directory $null = new-item -path 'testdrive:/src/aaa.json' -itemtype file $null = new-item -path 'testdrive:/src/aab.json' -itemtype file $null = new-item -path 'testdrive:/src/bbb.json' -itemtype file $null = new-item -path 'testdrive:/src/readme.md' -itemtype file $null = new-item -path 'testdrive:/src/sub' -itemtype directory $null = new-item -path 'testdrive:/src/sub/aac.json' -itemtype file $null = new-item -path 'testdrive:/src/sub/test.json.obsolete' -itemtype file } context "empty nonexisting source path" { "should fail source folder exist" { # test invoke-sqlcmd invoked provided right params $params = @{ sourcepath = join-path "testdrive:" "badfolder" filtertype = 'all' } { get-filteredfilelist @params } | -throw -expectedmessage "*sourcepath*" test-path $outputfilepath | -be $false } "should return files source folder empty" { $params = @{ sourcepath = join-path "testdrive:" "emptyfolder" filtertype = 'all' warningaction = 'silentlycontinue' } [array] $response = get-filteredfilelist @params $response | -benullorempty } "should create empty file source folder empty" { $params = @{ sourcepath = join-path "testdrive:" "emptyfolder" filtertype = 'all' outputfilepath = $outputfilepath } [array] $response = get-filteredfilelist @params $response | -benullorempty test-path $outputfilepath | -be $true get-content $outputfilepath | -benullorempty } } context "basic filtering" { "should return files source folder (recursive default)" { $params = @{ sourcepath = join-path "testdrive:" "src" filtertype = 'all' } [array] $response = get-filteredfilelist @params $response.count | -be 6 } "should return nothing, create filelist files" { $params = @{ sourcepath = 'testdrive:\src' filtertype = 'all' outputfilepath = $outputfilepath } [array] $response = get-filteredfilelist @params $response | -benullorempty test-path $outputfilepath | -be $true [array] $response = get-content $outputfilepath $response.count | -be 6 } "should return files source folder (non-recursive)" { $params = @{ sourcepath = join-path "testdrive:" "src" filtertype = 'all' recurse = $false } [array] $response = get-filteredfilelist @params $response.count | -be 4 $response | -contain (join-path "$testfolder" "src" "aaa.json") $response | -contain (join-path "$testfolder" "src" "aab.json") $response | -contain (join-path "$testfolder" "src" "bbb.json") } "should return files source folder (recursive) basic filter" { $params = @{ sourcepath = join-path "testdrive:" "src" filtertype = 'all' recurse = $true basicfilter = '\.json$' } [array] $response = get-filteredfilelist @params $response.count | -be 4 $response | -contain (join-path "$testfolder" "src" "aaa.json") $response | -contain (join-path "$testfolder" "src" "aab.json") $response | -contain (join-path "$testfolder" "src" "bbb.json") $response | -contain (join-path "$testfolder" "src" "sub" "aac.json") } context "relative paths" { "should return relative paths relative specified path" { $relativebasepath = join-path $testfolder "src" $params = @{ sourcepath = join-path "testdrive:" "src" filtertype = 'all' recurse = $true basicfilter = '\.json$' outputrelativepath = $true relativebasepath = $relativebasepath } [array] $response = get-filteredfilelist @params $response.count | -be 4 $response | -contain (join-path "." "aaa.json") $response | -contain (join-path "." "aab.json") $response | -contain (join-path "." "bbb.json") $response | -contain (join-path "." "sub" "aac.json") } } "should return relative paths relative current working dir" { # we're mocking get-location return root testdrive $testdriveroot = ($testfolder | split-path -qualifier) + [io.path]::directoryseparatorchar mock get-location { return $testdriveroot } $params = @{ sourcepath = join-path "testdrive:" "src" filtertype = 'all' recurse = $true basicfilter = '\.json$' outputrelativepath = $true } [array] $response = get-filteredfilelist @params $response.count | -be 4 $expectedbasepath = join-path "." ($testfolder.replace($testdriveroot, "")) $response | -contain (join-path $expectedbasepath "src" "aaa.json") $response | -contain (join-path $expectedbasepath "src" "aab.json") $response | -contain (join-path $expectedbasepath "src" "bbb.json") $response | -contain (join-path $expectedbasepath "src" "sub" "aac.json") } } "should return files source path mentioned filefilter path" { # create input filelist $filefilterpath = join-path "testdrive:" "input-filelist.txt" (join-path $testfolder "src" "sub" "aac.json") | set-content -path $filefilterpath # run function $params = @{ sourcepath = 'testdrive:/src' recurse = $true basicfilter = '\.json$' filtertype = 'filelist' filtervalue = $filefilterpath } [array] $response = get-filteredfilelist @params $response.count | -be 1 $response | -contain (join-path "$testfolder" "src" "sub" "aac.json") } "should return files source path mentioned filefilter path contains relative paths" { # create input filelist $filefilterpath = join-path "testdrive:" "input-filelist.txt" (join-path "." "sub" "aac.json") | set-content -path $filefilterpath # run function $params = @{ sourcepath = join-path 'testdrive:' 'src' recurse = $true basicfilter = '\.json$' filtertype = 'filelist' filtervalue = $filefilterpath } [array] $response = get-filteredfilelist @params $response.count | -be 1 $response | -contain (join-path "$testfolder" "src" "sub" "aac.json") } "should return files source folder (recursive) basic path filter" { $params = @{ sourcepath = 'testdrive:\src' recurse = $true basicfilter = '\.json$' filtertype = 'path' filtervalue = 'sub' } [array] $response = get-filteredfilelist @params $response.count | -be 1 $response | -contain (join-path "$testfolder" "src" "sub" "aac.json") } "should return files different main branch" { # mock git diff report differences mock git { return @( join-path $testfolder "src" "aaa.json" join-path $testfolder "src" "readme.md" join-path $testfolder "src" "sub" "aac.json" join-path $testfolder "src" "sub" "test.json.obsolete" ) } $params = @{ sourcepath = join-path "testdrive:" "src" recurse = $true basicfilter = '\.json$' filtertype = 'branch' filtervalue = 'origin/main' } [array] $response = ( get-filteredfilelist @params ) ?? @() $response.count | -be 2 $response | -contain (join-path "$testfolder" "src" "aaa.json") $response | -contain (join-path "$testfolder" "src" "sub" "aac.json") } "should return files different tag" { # mock git diff report differences mock git { return @( join-path $testfolder "src" "aaa.json" join-path $testfolder "src" "readme.md" join-path $testfolder "src" "sub" "aac.json" join-path $testfolder "src" "sub" "test.json.obsolete" ) } $params = @{ sourcepath = join-path "testdrive:" "src" recurse = $true basicfilter = '\.json$' filtertype = 'branch' filtervalue = 'origin/main' } [array] $response = ( get-filteredfilelist @params ) ?? @() $response.count | -be 2 $response | -contain (join-path "$testfolder" "src" "aaa.json") $response | -contain (join-path "$testfolder" "src" "sub" "aac.json") } } }</file><file name="src\powershell\modules\DataConfig\public\Get-IngestionSparkSessionObject.ps1">function get-ingestionsparksessionobject { &lt;# .synopsis return object class pssynapsesparksession .description using name synapse workspace, spark pool, session name, look existing spark session session exists, validate session usable state. so, return object session (in usable state), create new session waitforsession: wait session started setupsession: session usable, run basic comments .notes #&gt; [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="name synapse workspace invoke session" )] [string] $synapseworkspace, [parameter(mandatory=$true, helpmessage="name synapse spark pool invoke session from" )] [string] $synapsesparkpool, [parameter(mandatory=$true, helpmessage="name spark session invoked (or looked for)")] [string] $sessionname, [parameter(mandatory=$false, helpmessage="controls whether script wait session ready (default), not")] [bool] $waitforsession = $true, [parameter(mandatory=$false, helpmessage="controls whether script set-up spark session (default), not")] [bool] $setupsession = $true ) # local scripts settings set-strictmode -version "latest" #region step 1: define parameters # states sessions reused (in order preference) # idle = busy active # busy = something moment # success = indicates previous job completed successfully # not_started starting = 2 stages starting cluster $reusablestates = "idle", "success", "starting", "busy", "not_started" $readystates = "idle", "success", "busy" #endregion step 1 #region step 2: retrieve list existing active spark sessions # check active session based status existing sessions write-information("checking active spark sessions session name $($sessionname)...") # get full list usable sessions specified sessionname [array] $activesessions = (get-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $synapsesparkpool | where-object { $_.name -eq $sessionname } ) ?? @() # filter sort state if( $activesessions) { $activesessions = $activesessions | where-object { $reusablestates -contains $_.state } $activesessions = $activesessions | sort-object { $reusablestates.indexof($_.state) } } # debug output # $activesessions | select-object name, id, state | foreach-object { write-information (" " + $_ ) } #endregion #region step 3: return active spark session create new one # sessions left filtering &amp; sorting, select first one ( $activesessions) { write-information "found [$($activesessions.count)] usable session(s)" $sessionobj = $activesessions | select-object -first 1 write-information " selected first session id [$($sessionobj.id)], state [$($sessionobj.state)]" # wait cluster ready $currentstate = $sessionobj.state ( $readystates -contains $currentstate ){ $sessionobj = get-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $synapsesparkpool -id $sessionobj.id ($setupsession){ $code = 'from notebookutils import mssparkutils' start-ingestionsparksessionjobs $synapseworkspace $synapseworkspace -synapsesparkpool $synapsesparkpool -sessionobj $sessionobj -code $code -job $false | out-null # catch statementobj return well } return $sessionobj } elseif ($waitforsession){ :waitforreadyloop while( $readystates -notcontains $currentstate) { ( $reusablestates -notcontains $currentstate) { # unexpected (unusuable) state throw "cluster unusable state '$currentstate'" } ( $readystates -contains $currentstate) { write-information "cluster state: $currentstate, ready use!" break waitforreadyloop } write-information "cluster state: $currentstate, ready, checking 30 seconds.." start-sleep -seconds 30; $sessionobj = get-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $synapsesparkpool -id $sessionobj.id $currentstate = $sessionobj.state } } else{ return $null } } # active sessions: start new session else { # start new session wait start if( $waitforsession ) { write-information("no active session found, creating new session...") $processtimer = [system.diagnostics.stopwatch]::startnew() $sessionobj = start-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $synapsesparkpool -name $sessionname -language 'python' -executorcount 1 -executorsize small $processtimer.stop() $ts = $processtimer.elapsed $elapsedstring = "{0:0}m {1:0}s" -f $ts.minutes, $ts.seconds #write-information(" elapsed time: $elapsedstring") $newsessionid = $sessionobj.id write-information("new session created " + $elapsedstring + " id: " + $newsessionid) #for reason, session object returned incomplete, get using id $sessionobj = get-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $synapsesparkpool -id $newsessionid write-information "using session object: " write-information $sessionobj } # want wait, fire start-session request "asjob" mode else { write-information("no active session found, creating new session (background) ...") # asjob mode, returns azurelongrunningjob object (and session object) $null = start-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $synapsesparkpool -name $sessionname -language 'python' -executorcount 1 -executorsize small -asjob return $null } } # dev-info: get $waitforsession also true. condiditions already returned value ($setupsession){ $code = 'from notebookutils import mssparkutils' start-ingestionsparksessionjobs $synapseworkspace $synapseworkspace -synapsesparkpool $synapsesparkpool -sessionobj $sessionobj -code $code -job $false | out-null # catch statementobj return well } return $sessionobj #$null pssynapsesparksession #endregion }</file><file name="src\powershell\modules\DataConfig\public\Get-IngestionSparkSessionObject.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "get-ingestionsparksessionobject" { context "unittests" { beforeall { mock get-azcontext { '{ "account": "unittest" }' | convertfrom-json } mock start-ingestionsparksessionjobs{} mock start-sleep {} } context "execute functions idle spark session"{ beforeall{ $sparkobjectget = [hashtable]@{ state = 'idle' name = 'unittest' id = '123' appid = 'application_0000000000000_0000' livyinfo = @{ startingat = get-date } submitter = "unittest" } $sparkobjectstart = [hashtable]@{ state = 'idle' name = 'unittest' id = '456' appid = 'application_0000000000000_0000' livyinfo = @{ startingat = get-date } } $mockgetsession = new-mockobject -type microsoft.azure.commands.synapse.models.pssynapsesparksession -properties $sparkobjectget $mockstartsession = new-mockobject -type microsoft.azure.commands.synapse.models.pssynapsesparksession -properties $sparkobjectstart # mock az.synapse functions called start-azsynapsesparksession function mock start-azsynapsesparksession {$mockstartsession} mock get-azsynapsesparksession {$mockgetsession} } "should start new spark session active session found"{ $localsessionobj = get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $false -setupsession $false -not -invoke start-azsynapsesparksession -invoke get-azsynapsesparksession $localsessionobj.id | -be $sparkobjectget.id } "should install mssparkutils module executing invoke-azsynapsesparkstatement call new session"{ get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $true -setupsession $true -invoke start-ingestionsparksessionjobs -times 1 -parameterfilter { $code -eq 'from notebookutils import mssparkutils' } } "should install mssparkutils module setupsession disabled"{ get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $true -setupsession $false -not -invoke start-ingestionsparksessionjobs } } context "execute functions inactive spark session"{ beforeall{ $sparkobjectget = [hashtable]@{ state = 'unusablestate' name = 'unittest' id = '123' appid = 'application_0000000000000_0000' livyinfo = @{ startingat = get-date } submitter = "unittest" } $sparkobjectstart = [hashtable]@{ state = 'idle' name = 'unittest' id = '456' appid = 'application_0000000000000_0000' livyinfo = @{ startingat = get-date } } $mockgetsession = new-mockobject -type microsoft.azure.commands.synapse.models.pssynapsesparksession -properties $sparkobjectget $mockstartsession = new-mockobject -type microsoft.azure.commands.synapse.models.pssynapsesparksession -properties $sparkobjectstart # mock az.synapse functions called start-azsynapsesparksession function mock start-azsynapsesparksession {$mockstartsession} mock get-azsynapsesparksession {$mockgetsession} } "should start new spark session active session found"{ $localsessionobj = get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $true -setupsession $true -invoke start-azsynapsesparksession -invoke get-azsynapsesparksession -times 2 $localsessionobj.id | -be $sparkobjectget.id } "should return session object waitforsession disabled"{ $localsessionobj = get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $false -setupsession $true -invoke start-azsynapsesparksession -invoke get-azsynapsesparksession -times 1 -not -invoke start-ingestionsparksessionjobs $localsessionobj | -be $null } "should install mssparkutils module executing invoke-azsynapsesparkstatement call new session"{ get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $true -setupsession $true -invoke start-ingestionsparksessionjobs -times 1 -parameterfilter { $code -eq 'from notebookutils import mssparkutils' } } "should install mssparkutils module setupsession disabled"{ get-ingestionsparksessionobject -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -sessionname 'unittest' -waitforsession $true -setupsession $false -not -invoke start-ingestionsparksessionjobs } } } }</file><file name="src\powershell\modules\DataConfig\public\Get-SQLQueryResults.ps1">function get-sqlqueryresults{ &lt;# .description execute sql-command, returns result many inserts, updates, deletes, ... happened #&gt; # first set parameters [cmdletbinding()] param( [parameter(mandatory = $true)] [string] $serverinstance, # server instance trying access [parameter(mandatory = $true)] [string] $database, # database inside server trying access [parameter(mandatory = $true)] [string] $accesstoken, # accesstoken [parameter(mandatory=$true)] [string] $query, # query want execute [parameter(mandatory=$false)] [int64] $numtotalparam = 1, # amount actions expecting [parameter(mandatory=$false)] [bool] $outputsqlerrors = $true, # want output errors? [parameter(mandatory=$false)] [bool] $includesqlusererrors = $true, # want include sql user errors? [parameter(mandatory=$false)] [bool] $abortonerror = $true, # want abort error found? [parameter(mandatory=$false)] [switch] $printdetailedresults # want detailed output not? ) # going construct sql params well $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $outputsqlerrors includesqlusererrors = $includesqlusererrors abortonerror = $abortonerror } # execute query &amp; capture result variable $sqlresult = invoke-sqlcmd @sqlparameters -query $query -erroraction stop ($printdetailedresults){ $sqlresult | format-table | out-string | write-information } $numtotal = $numtotalparam $numinsert = ($sqlresult | where-object -property "action" -eq "insert") ? ($sqlresult | where-object -property "action" -eq "insert" ).count : 0 $numupdate = ($sqlresult | where-object -property "action" -eq "update") ? ($sqlresult | where-object -property "action" -eq "update" ).count : 0 $numdelete = ($sqlresult | where-object -property "action" -eq "delete") ? ($sqlresult | where-object -property "action" -eq "delete" ).count : 0 $result = [pscustomobject]@{ total = $numtotal inserted = $numinsert updated = $numupdate deleted = $numdelete } return $result }</file><file name="src\powershell\modules\DataConfig\public\Get-SQLQueryResults.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "get-sqlqueryresults" { context "unittests" { # setup tests inside given context beforeall { # first all, configure supposed output invoke-sqlcmd $resultset = @( [pscustomobject]@{ action = "insert" file_layout = "dummy" } [pscustomobject]@{ action = "update" file_layout = "dummy" } [pscustomobject]@{ action = "delete" file_layout = 'dummy' } ) # mock invoke-sqlcmd command, else: try make call sql mock invoke-sqlcmd { $resultset } } "should invoke invoke-sqlcmd right params" { # test invoke-sqlcmd invoked provided right params $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' query = 'exec dummy_sp @dummy_var = test' numtotalparam = 3 } get-sqlqueryresults @params -invoke -commandname invoke-sqlcmd -times 1 } "should return total, inserted, updated &amp; deleted object" { # test that, based known output invoke-sqlcmd, right object returned $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' query = 'exec dummy_sp @dummy_var = test' numtotalparam = 3 } # capture result get-sqlqueryresults check $result = get-sqlqueryresults @params # assertions $result.total | -be $params.numtotalparam $result.inserted | -be 1 $result.updated | -be 1 $result.deleted | -be 1 } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionDeltaTableConfiguration.ps1">function set-ingestiondeltatableconfiguration { &lt;# .synopsis function deploy empty delta tables json configuration file .description function calls upon deltatableconfigurationfile 'delta_tables.json' metadata delta table function start spark pool execute calls notebook 'deployorchestrator'. object file (representing delta table configuration) passed separate spark job spark session. function loop jobs complete succeeded successfully .notes disabled solution issue another fix implemented error returns, enable code many calls made active spark session, following error thrown: status: 429 {"code":"toomanyrequestsforidentifier","message":"request limit exceeded identifiers"} error states currently many requests made spark session, mean failure deployment. therefore, function contains error handling regarding error. idea: retry ping spark session $retryseconds $maxretrycount times. successful call made, $retrycount reset 0 successful call made within set parameters, error thrown anyway #&gt; [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="name environment tables needs deployed to" )] [string] $targetenvironment, [parameter(mandatory=$true, helpmessage="configuration file containing list delta table objects deploy delta lake")] [string] $deltatableconfigurationfile, [parameter(mandatory=$false, helpmessage="name spark session invoked (or looked for)")] [string] $sessionname='deploydeltatables', [parameter(mandatory=$false, helpmessage="number seconds wait api calls spark session")] [string] $statusupdateseconds = 30, # [parameter(mandatory=$false, helpmessage="maximum number api call retries throttling occurs session")] # [string] $maxretrycount = 5, # [parameter(mandatory=$false, helpmessage="time api call retries throtteling")] # [string] $retryseconds = 60, [parameter(mandatory=$false, helpmessage="show spark statements")] [switch] $showsparkstatements ) # local scripts settings set-strictmode -version "latest" #region step 1: define parameters # using targetenvironment, define names different azure services [string] $synapseworkspace = "$targetenvironment-dap-syn-core" # name synapse workspace deploy tables [string] $poolname = "$($targetenvironment)dapsspcore" # name spark pool used deployment # set synapse parameters needed deploy delta tables [string] $notebookdirectory = "deploy" # name folder deployment notebooks stored [string] $notebookname = "deployorchestrator" # name notebook orchestrate deployment [timespan] $tolocaltimeoffset = (get-timezone).baseutcoffset # set variables spark job tracking [int] $maxconcurrentjobs = 5 [array] $jobs = @() [array] $completedjobs = @() #endregion step 1 #region step 2: validate $deltatableconfigurationfile # 1. check file exists (-not (test-path $deltatableconfigurationfile)){ throw "delta table configuration file $($deltatableconfigurationfile) exist. cannot deploy anything." } # 2. check configuration file matches schema compare-ingestionmetadatafromjsonfile -path $deltatableconfigurationfile -schema "deltatabledata.schema.json" $deltaconfigfilecontent = get-content $deltatableconfigurationfile -raw | convertfrom-json # configuration file empty: work todo if(-not $deltaconfigfilecontent.table_configurations){ write-information "there delta tables deploy configuration file, exiting..." return } #endregion step 2 #region step 3: retrieve start spark session # check active session based status existing sessions $sessionobj = get-ingestionsparksessionobject -synapseworkspace $synapseworkspace -synapsesparkpool $poolname -sessionname $sessionname -waitforsession $true -setupsession $true [datetime] $startdate = (get-date).add($tolocaltimeoffset).tostring('hh:mm:ss') write-information "run session setup..." # dev-note: execute statement separate sessionobj-setup function returns statement object $code = "from notebookutils import mssparkutils" $help = [array] @(get-azsynapsesparkstatement -workspacename $synapseworkspace -sparkpoolname $poolname -sessionid $sessionobj.id) $debug = [array] @($help | sort-object id -descending) $statement = $debug[0] #save statement starting point later $startstatementid = $statement.id #endregion step 3 #region step 4: deploy delta tables using configuration file # iteratively launch spark-jobs sessionobj # allow $maxconcurrentjobs active time sessionobj write-information "launch delta table creation spark-jobs..." # function wait available job slots # loop delta table configurations invoke spark jobs accordingly $tablenumber = 0 $jobcount = $deltaconfigfilecontent.table_configurations.count $deltaconfigfilecontent.table_configurations | foreach-object{ $tablenumber += 1 wait-availablejobs -maxconcurrentjobs $maxconcurrentjobs -jobs $jobs | out-null # wait available job slots $completedjobs = @(get-completedsparkstatements -startstatementid $startstatementid -workspacename $synapseworkspace -poolname $poolname -sessionobj $sessionobj -completedjobs $completedjobs) # ping see jobs completed already # print progress information $duration = new-timespan start $startdate end (get-date).add($tolocaltimeoffset) $starttime = $startdate.tostring('hh:mm:ss') $currenttime = (get-date).add($tolocaltimeoffset).tostring('hh:mm:ss') write-information " progress... running time: $($duration.tostring('hh\:mm\:ss')) | started: $starttime | current time: $currenttime | running jobs: $($jobs.count)" # execute spark-job sessionobj $tabledata = $_ | convertto-json -depth 5 -asarray $code = "mssparkutils.notebook.run ('$notebookdirectory/$notebookname', 3600, { 'table_definition_list' : '''$($tabledata)''' })" if( $showsparkstatements) { write-information "invoke code statement:" write-information $code } else { write-information " deploying $($_.table_name)..." } $job = start-ingestionsparksessionjobs -code $code -synapseworkspace $synapseworkspace -synapsesparkpool $poolname -sessionobj $sessionobj -job $true $jobs += $job # jobcount = total number jobs # tablenumber = last added job # completedjobs = list finished jobs write-verbose "started jobs: $($tablenumber)/$($jobcount) | completed jobs: $($completedjobs.count)/$($jobcount)" } # wait jobs complete write-verbose "waiting jobs complete..." $jobs | wait-job $completedjobs = @(get-completedsparkstatements -startstatementid $startstatementid -workspacename $synapseworkspace -poolname $poolname -sessionobj $sessionobj -completedjobs $completedjobs) # ping see jobs completed already write-verbose "completed jobs: $($completedjobs.count)/$($jobcount)" #endregion step 4 #region step 5: validate deployments successfull # retrieve results spark (job) statements write-verbose "checking failed jobs..." $failed = get-failedsparkstatements -workspacename $synapseworkspace -poolname $poolname -sessionobj $sessionobj -startstatementid $startstatementid ($failed){ # stop spark session avoid using future (before timeout) stop-azsynapsesparksession -workspacename $synapseworkspace -sparkpoolname $poolname -livyid $sessionobj.id throw "table deployment errors" } write-information "all tables deployed successfully" #endregion 5 } # obsolete code: deal throttling issue # solution (hopefully): allow maxconcurrentjobs time # # reset retrycount ping succeeds # $retrycount = 0 # # error gets thrown: # catch { # # log exception debugging purposes # write-output $_.exception # # validate exception status property # ($_.exception.psobject.properties['status']) { # # check status indicates rate limit issue # ($_.exception.status -eq 429) { # $retrycount++ # write-output "rate limit exceeded. retrying $retryseconds seconds..." # start-sleep -seconds $retryseconds # } # else { # throw $_ # rethrow exception rate limit error # } # } # else { # throw $_ # rethrow exception status property # } # } # } # evaluate deployment successful # $retrycount = 0 # $finished = $false # ($retrycount -gt $maxretrycount){ # throw "number retries exceeded maximum allowed number ($maxretrycount)" # } # elseif (-not $finished){ # throw "the status deploy set finished. something went wrong deployment..." # } # # success! # write-information "all tables deployed successfully"</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionDeltaTableConfiguration.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) . $psscriptroot/get-completedsparkstatements.ps1 . $psscriptroot/wait-availablejobs.ps1 . $psscriptroot/get-failedsparkstatements.ps1 # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestiondeltatableconfiguration" { context "unittests" { beforeall { # mock spark object $sparkobject = [hashtable]@{ state = 'idle' name = 'deploydeltatables' id = '123' appid = 'application_0000000000000_0000' livyinfo = @{ startingat = get-date } } $mocksession = new-mockobject -type microsoft.azure.commands.synapse.models.pssynapsesparksession -properties $sparkobject # mock job object $jobobject = @([pscustomobject]@{ id = '123' state = "waiting" }) # mock output successful spark statement $startstatementoutput = @( @{ output = @{ status = 'ok' } id = 124 }, @{ output = @{ status = 'ok' } id = 125 } ) $newstatementoutput = @{ output = @{ status = 'ok' } id = 126 } $script:mockcalled = 0 $mocktestpath = { $script:mockcalled++ if($script:mockcalled -eq 1) { return $startstatementoutput } else { return $newstatementoutput } } mock -commandname get-azsynapsesparkstatement -mockwith $mocktestpath # mock az.synapse functions called set-ingestiondeltatableconfiguration function mock get-ingestionsparksessionobject {$mocksession} # mock get-azsynapsesparkstatement {$startstatementoutput} mock invoke-azsynapsesparkstatement {$startstatementoutput} mock start-ingestionsparksessionjobs {$jobobject} mock stop-azsynapsesparksession {} mock start-sleep {} mock wait-job } context "empty configuration file call function"{ beforeall{ # configuration object empty object $configurationcontent = [pscustomobject]@{ table_configurations = @( ) } # write empty object json file testdrive # set-ingestiondeltatableconfiguration function call upon json execute functionalities $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/delta_tables.json' # mock az.synapse function: function called mocked assertion } "should anything configurations deploy"{ # run: call function configuration file testdrive-file empty configuration located set-ingestiondeltatableconfiguration -targetenvironment 'dev' -deltatableconfigurationfile 'testdrive:/delta_tables.json' -warningaction 'silentlycontinue' # assert #should -invoke write-information -times 1 -parameterfilter { $message -like "*no delta tables*"} # since configuration file empty, spark activities expected -not -invoke get-ingestionsparksessionobject -not -invoke invoke-azsynapsesparkstatement -not -invoke start-ingestionsparksessionjobs -not -invoke stop-azsynapsesparksession } } context "non-empty configuration start spark session jobs"{ beforeall{ # configuration object empty object $configurationcontent = [pscustomobject]@{ table_configurations = @( @{ table_description = 'test description' table_name = 'table_name' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' } ) } # write empty object json file testdrive # set-ingestiondeltatableconfiguration function call upon json execute functionalities $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/delta_tables.json' } "should call functions correct parameters"{ # run: call function configuration file testdrive-file empty configuration located set-ingestiondeltatableconfiguration -targetenvironment 'dev' -deltatableconfigurationfile 'testdrive:/delta_tables.json' -warningaction 'silentlycontinue' # assert #should -invoke write-information -times 1 -parameterfilter { $message -like "*no delta tables*"} # since configuration file empty, spark activities expected -invoke get-ingestionsparksessionobject -invoke start-ingestionsparksessionjobs -times 1 -parameterfilter{ $code -match "mssparkutils.notebook.run" $code -match "deploy/deployorchestrator" $code -match $configurationcontent.table_configurations } -invoke wait-job } } context "multi configurations start individual jobs"{ beforeall{ # configuration object empty object $configurationcontent = [pscustomobject]@{ table_configurations = @( @{ table_description = 'test description' table_name = 'table_name' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' }, @{ table_description = 'test description' table_name = 'table_name' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' } ) } # write empty object json file testdrive # set-ingestiondeltatableconfiguration function call upon json execute functionalities $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/delta_tables.json' } "should call functions correct parameters"{ # run: call function configuration file testdrive-file empty configuration located set-ingestiondeltatableconfiguration -targetenvironment 'dev' -deltatableconfigurationfile 'testdrive:/delta_tables.json' -warningaction 'silentlycontinue' # assert #should -invoke write-information -times 1 -parameterfilter { $message -like "*no delta tables*"} # since configuration file empty, spark activities expected -invoke start-ingestionsparksessionjobs -times 1 -parameterfilter{ $code -match "mssparkutils.notebook.run" $code -match "deploy/deployorchestrator" $code -match $configurationcontent.table_configurations[0] } -invoke start-ingestionsparksessionjobs -times 1 -parameterfilter{ $code -match "mssparkutils.notebook.run" $code -match "deploy/deployorchestrator" $code -match $configurationcontent.table_configurations[1] } } } context "job error fail"{ beforeall{ # configuration object empty object $configurationcontent = [pscustomobject]@{ table_configurations = @( @{ table_description = 'test description' table_name = 'table_name' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' } ) } # write empty object json file testdrive # set-ingestiondeltatableconfiguration function call upon json execute functionalities $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/delta_tables.json' $newstatementoutput = @( @{ output = @{ status = 'error' errorvalue = "thrown spark error" } id = 126 code = "executed code statement" }, @{ output = @{ status = 'completed' errorvalue = "thrown spark error" } id = 127 code = "executed code statement" } ) $script:mockcalled = 0 $mocktestpath = { $script:mockcalled++ if($script:mockcalled -eq 4) { return $newstatementoutput } else { return $startstatementoutput } } mock -commandname get-azsynapsesparkstatement -mockwith $mocktestpath } "should throw error statement failed"{ # run: call function configuration file testdrive-file empty configuration located {set-ingestiondeltatableconfiguration -targetenvironment 'dev' -deltatableconfigurationfile 'testdrive:/delta_tables.json' -warningaction 'silentlycontinue'} | -throw -expectedmessage "table deployment errors" } } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionMetadataFromJsonFile.ps1">function set-ingestionmetadatafromjsonfile { [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="path configuration file deployed" )] [string] $path, [parameter(mandatory=$false, helpmessage="path configuration file delta tables delta_tables.json" )] [string] $deltatableconfigurationfile = "configuration/delta_tables/delta_tables.json", [parameter(mandatory=$false, helpmessage="boolean: deploy metadata plan-task configuration exists" )] [boolean] $deployplantasks = $true, [parameter(mandatory=$false, helpmessage="boolean: deploy metadata dataset exists" )] [boolean] $deploydatasets = $true, [parameter(mandatory=$false, helpmessage="boolean: deploy metadata preprocessing tasks table exists" )] [boolean] $deploypreprocessing = $true, [parameter(mandatory=$false, helpmessage="boolean: deploy metadata delta table exists" )] [boolean] $deploydeltatables = $true, [parameter(mandatory=$false, helpmessage="name sql server deploy metadata to" )] [string] $sqlserverinstance, [parameter(mandatory=$false, helpmessage="name sql database deploy metadata to" )] [string] $sqldatabase, [parameter(mandatory=$false, helpmessage="accesstoken sql server database fo user" )] [string] $accesstoken, [parameter(mandatory=$true, helpmessage="name environment table needs deployed to" )] [string] $targetenvironment, [parameter(mandatory=$false, helpmessage="path repository function called from" )] [string] $workspacefolder, [parameter(mandatory=$false, helpmessage="switch parameter used executing sql commands" )] [switch] $printdetailedresults ) #region preparation validation write-information "run function: set-ingestionmetadatafromjsonfile" # file must exist ( -not ( test-path -path $path) ) { throw [system.io.filenotfoundexception]::new("provided path '$path' exist.") } # filename part $path, used logging $jsonfile = split-path -path $path -leaf $sqlparams = @{ serverinstance = $sqlserverinstance database = $sqldatabase accesstoken = $accesstoken } # compare files configuration schema ../schemas compare-ingestionmetadatafromjsonfile -path $path #endregion $ingestionjson = get-content -raw -path $path $ingestionmetadata = $ingestionjson | convertfrom-json ##region helper functions function get-propertyvalue{ param( [psobject] $object , [string] $name , [allownull()] [psobject] $defaultvalue ) ($object.psobject.properties.name -contains $name) { $propertyvalue = $object.psobject.properties[$name].value return $propertyvalue } return $defaultvalue } #endregion if( $deployplantasks ) { if( get-member -inputobject $ingestionmetadata -name "plans" -membertype "properties") { #loop plans json file (if present) write-information "[$jsonfile] found $($ingestionmetadata.plans.count) plan(s) file" :plantaskloop foreach ($plan $ingestionmetadata.plans ) { $planname = $plan.name.trim() write-information "[$jsonfile] processing plan: $planname ..." # check environment valid deployment (if provided assume "any") ( get-member -inputobject $plan -name "environments" -membertype "properties" ) { if( $targetenvironment -notmatch $plan.environments ) { write-information "[$jsonfile].[$planname] environments filter '[$($plan.environments)]' valid target environment [$targetenvironment], skipping" continue :plantaskloop } } # plan_configuration write-information "[$jsonfile].[$planname] plan_configuration ..." $plandescription = $plan.description.trim() if( get-member -inputobject $plan -name "enabled" -membertype "properties" ) { $planenabled = $plan.enabled } else { $planenabled = $true } $planresult = set-ingestionplanconfiguration @sqlparams -planname $planname -plandescription $plandescription -planenabled $planenabled -verbose:$verbosepreference -printdetailedresults:$printdetailedresults write-information "[$jsonfile].[$planname] plan_configuration succeeded - total: $($planresult.total), inserted: $($planresult.inserted), updated : $($planresult.updated), deleted : $($planresult.deleted)" # plan_task_configuration write-information "[$jsonfile].[$planname] plan_task_configuration ..." # building required table manually, get working nested select-object ... $plantasks = [system.collections.arraylist]@() foreach ( $taskgroup $plan.task_groups) { foreach ( $task $taskgroup.tasks) { # optional params if( get-member -inputobject $task -name "sequence" -membertype "properties" ){ $task_sequence = $task.sequence } else{ $task_sequence = 1 } if(get-member -inputobject $task -name "enabled" -membertype "properties"){ $enabled = $task.enabled } else{ $enabled = $true } $plantasks.add( [pscustomobject]@{ task_group = $taskgroup.name task_name = $task.name task_sequence = $task_sequence enabled = $enabled }) | out-null } } $plantaskjson = ( $plantasks | convertto-json -asarray ) ?? "[]" $plantaskresult = set-ingestionplantaskconfiguration @sqlparams -planname $planname -plantask_json $plantaskjson -verbose:$verbosepreference -printdetailedresults:$printdetailedresults write-information "[$jsonfile].[$planname] plan_task_configuration succeeded - total: $($plantaskresult.total), inserted: $($plantaskresult.inserted), updated : $($plantaskresult.updated), deleted : $($plantaskresult.deleted)" } } else { write-information "[$jsonfile] plans detected file" } } ($deploydatasets) { #loop datasets json file (if present) if( get-member -inputobject $ingestionmetadata -name "datasets" -membertype "properties" ) { write-information "[$jsonfile] found $($ingestionmetadata.datasets.count) dataset(s) file" :datasetloop foreach ($dataset $ingestionmetadata.datasets ) { # mandatory params $datasetname = $dataset.name write-information "[$jsonfile] processing dataset: $datasetname ..." $taskcategory = "ingest" # $kind = $dataset.kind write-information "[$jsonfile].[$datasetname] task_configuration (ingestion) ..." # mandatory params $task_type = $dataset.task_type # optional params (get-member -inputobject $dataset -name "worker" -membertype "properties"){ $worker_name = $dataset.worker } else{ $worker_name = 'ingestionworker' } (get-member -inputobject $dataset -name "ingestion" -membertype "properties"){ # mandatory params $target_table = $dataset.ingestion.target_table $source_folder = $dataset.ingestion.source_folder # optional params (get-member -inputobject $dataset.ingestion -name "container" -membertype "properties"){ $container_name = $dataset.ingestion.container } else{ $container_name = 'landing' } (get-member -inputobject $dataset.ingestion -name "target_options" -membertype "properties"){ [psobject] $target_options = $dataset.ingestion.target_options # | convertto-json -depth 5 } else{ $target_options = $null } } else{ # ingestion exist, still set $source_folder = $null $target_table = $null $container_name = 'landing' $target_options = $null } # set meta.task_configuration $taskresult = set-ingestiontaskconfiguration @sqlparams ` -taskname "$datasetname" ` -taskcategory $taskcategory ` -taskdescription "ingest files layout ''$datasetname''" ` -tasktype $task_type ` -workername $worker_name ` -filelayout $datasetname ` -sourcefolder $source_folder ` -containername $container_name ` -tablename $target_table ` -targetoptions $target_options ` -verbose:$verbosepreference -printdetailedresults:$printdetailedresults write-information "[$jsonfile].[$datasetname] task_configuration (ingestion) succeeded - total: $($taskresult.total), inserted: $($taskresult.inserted), updated : $($taskresult.updated), deleted : $($taskresult.deleted)" # "ingestion" present json file (get-member -inputobject $dataset -name "ingestion" -membertype "properties"){ $ingestion_data = $dataset.ingestion write-information "[$jsonfile].[$datasetname] source_configuration ..." # check/calculate optional members if( get-member -inputobject $ingestion_data -name "column_delimiter" -membertype "properties" ) { $column_delimiter = $ingestion_data.column_delimiter } else { $column_delimiter = "," } if( get-member -inputobject $ingestion_data -name "row_delimiter" -membertype "properties" ) { $row_delimiter = $ingestion_data.row_delimiter } else { $row_delimiter = '\r\n' } if( get-member -inputobject $ingestion_data -name "escape_character" -membertype "properties" ) { $escape_character = $ingestion_data.escape_character } else { $escape_character = '' } if( get-member -inputobject $ingestion_data -name "quote_character" -membertype "properties" ) { $quote_character = $ingestion_data.quote_character } else { $quote_character = '' } if( get-member -inputobject $ingestion_data -name "header_line" -membertype "properties" ) { $header_line = $ingestion_data.header_line } else { $header_line = $true } if( get-member -inputobject $ingestion_data -name "encoding" -membertype "properties" ) { $encoding = $ingestion_data.encoding } else { $encoding = 'utf-8' } if( get-member -inputobject $ingestion_data -name "skip_first_lines" -membertype "properties" ) { $skip_first_lines = $ingestion_data.skip_first_lines } else { $skip_first_lines = $null } $source_conditions = [system.collections.arraylist]@() $source_preconditions_raw = get-propertyvalue -object $ingestion_data -name 'preconditions' -defaultvalue $null ($null -ne $source_preconditions_raw) { foreach ($precond $source_preconditions_raw) { $source_conditions.add( [pscustomobject]@{ #mandatory name = $precond.name type = $precond.type frequency = $precond.frequency #optional expected_file_mask = (get-propertyvalue -object $precond -name 'expected_file_mask' -defaultvalue $null) enabled = get-propertyvalue -object $precond -name 'enabled' -defaultvalue $true description = (get-propertyvalue -object $precond -name 'description' -defaultvalue $null) } ) | out-null } } $source_conditions_json = $source_conditions | convertto-json -asarray # set meta.source_configuration $taskresult = set-ingestionsourceconfiguration @sqlparams ` -filelayout $datasetname ` -matchpattern $ingestion_data.match_pattern ` -filekind $dataset.kind ` -extension $ingestion_data.extension ` -columndelimiter $column_delimiter ` -rowdelimiter $row_delimiter ` -escapecharacter $escape_character ` -quotecharacter $quote_character ` -headerline $header_line ` -encoding $encoding ` -skipfirstlines $skip_first_lines ` -preconditions $source_conditions_json ` -printdetailedresults:$printdetailedresults -verbose:$verbosepreference write-information "[$jsonfile].[$datasetname] source_configuration succeeded - total: $($taskresult.total), inserted: $($taskresult.inserted), updated : $($taskresult.updated), deleted : $($taskresult.deleted)" } else{ write-information "no ingestion properties defined" } # "columns" present json file (get-member -inputobject $dataset -name "columns" -membertype "properties"){ write-information "[$jsonfile].[$datasetname] source_column_configuration ..." # check/calculate optional members # building required table manually, get working nested select-object ... $columndata = [system.collections.arraylist]@() # special case: fixedlength files 1 column defined foreach ( $column ($dataset.columns) ) { # get/calculate optional properties if( get-member -inputobject $column -name "sink_name" -membertype "properties" ) { $sink_column_name = $column.sink_name.tolower() } else { $sink_column_name = $column.name.tolower() } if( get-member -inputobject $column -name "dimension" -membertype "properties" ) { $dimension = $column.dimension } else { $dimension = "scd2" } if( get-member -inputobject $column -name "data_type" -membertype "properties" ) { $data_type = $column.data_type } else { $data_type = "string" } if( get-member -inputobject $column -name "column_info" -membertype "properties" ) { [string] $column_info = $column.column_info | convertto-json -depth 5 -compress } else { [string] $column_info = "null" } # create entry json $columndata.add( [pscustomobject]@{ column_sequence = $column.sequence source_column_name = $column.name sink_column_name = $sink_column_name dimension = $dimension data_type = $data_type column_info = $column_info }) | out-null } $columndatajson = $columndata | convertto-json # write-information $columndatajson # set meta.source_column_configuration $jobresult = set-ingestionsourcecolumnconfiguration @sqlparams ` -filelayout $datasetname ` -columndatajson $columndatajson ` -printdetailedresults:$printdetailedresults -verbose:$verbosepreference write-information "[$jsonfile].[$datasetname] source_file_column_def succeeded - total: $($jobresult.total), inserted: $($jobresult.inserted), updated : $($jobresult.updated), deleted : $($jobresult.deleted)" } # "checks" present json file (get-member -inputobject $dataset -name "checks" -membertype "properties"){ write-information "[$jsonfile].[$datasetname] source_check_configuration ..." # check/calculate optional members # building required table manually, get working nested select-object ... $checksdata = [system.collections.arraylist]@() # special case: fixedlength files 1 column defined foreach ( $column ($dataset.checks) ) { # get/calculate optional properties # check_name $check_name = $column.name.tolower() # enabled_flag if( get-member -inputobject $column -name "enabled" -membertype "properties"){ $enabled_flag = $column.enabled } else{ $enabled_flag = $true } # config_params ( get-member -inputobject $column -name "config_params" -membertype "properties" ){ $config_params = $column.config_params } else{ $config_params = $null } # create entry json $checksdata.add( [pscustomobject]@{ name = $check_name enabled = $enabled_flag config_params = $config_params }) | out-null } $checksdataarray = $checksdata | convertto-json # write-information $checksdataarray # set meta.source_check_configuration $checkresult = set-ingestionsourcecheckconfiguration @sqlparams ` -filelayout $datasetname ` -checksdataarrayjson $checksdataarray write-information "[$jsonfile].[$datasetname] source_check_config succeeded - total: $($checkresult.total), inserted: $($checkresult.inserted), updated : $($checkresult.updated), deleted : $($checkresult.deleted)" } else{ write-information "no checks defined [$datasetname]" } } #foreach dataset } else { write-information "[$jsonfile] found dataset(s) file" } } ($deploydeltatables){ if( get-member -inputobject $ingestionmetadata -name "datasets" -membertype "properties" ) { write-information "[$jsonfile] found $($ingestionmetadata.datasets.count) dataset(s) file" :datasetloop foreach ($dataset $ingestionmetadata.datasets ) { ((get-member -inputobject $dataset -name "columns" -membertype "properties") -and ($dataset.worker -eq "ingestionworker")){ $datasetname = $dataset.name write-information "[$jsonfile].[$datasetname] delta_table configuration ..." # check/calculate optional members # building required table manually, get working nested select-object ... $columndata = [system.collections.arraylist]@() # special case: fixedlength files 1 column defined foreach ( $column ($dataset.columns) ) { # get/calculate optional properties if( get-member -inputobject $column -name "sink_name" -membertype "properties" ) { $column_name = $column.sink_name.tolower() } else { $column_name = $column.name.tolower() } if( get-member -inputobject $column -name "dimension" -membertype "properties" ) { $dimension = $column.dimension } else { $dimension = "scd2" } if( get-member -inputobject $column -name "data_type" -membertype "properties" ) { $data_type = $column.data_type } else { $data_type = "string" } # create entry json $columndata.add( [pscustomobject]@{ column_name = $column_name dimension = $dimension data_type = $data_type }) | out-null } (get-member -inputobject $dataset.ingestion -name "target_options" -membertype "properties"){ [psobject] $target_options = $dataset.ingestion.target_options } else{ $target_options = @{} } # everything done, go create delta table #$columndatajson = $columndata | convertto-json # write-information $columndata # set delta_tables.json update-ingestiondeltatableconfiguration ` -tablename $dataset.ingestion.target_table ` -columndata $columndata ` -targetenvironment $targetenvironment ` -containername 'silver' ` -deltatableconfigurationfile "$($workspacefolder)/$($deltatableconfigurationfile)" ` -targetoptions $target_options ` -informationaction "continue" ` write-information "[$jsonfile].[$datasetname] configuration written delta_tables.json" } else{ write-information "deploying delta table turned on, worker $($dataset.worker) $($jsonfile) found. delta table created." } } } } ($deploypreprocessing) { #loop preprocessing tasks json file (if present) if( get-member -inputobject $ingestionmetadata -name "preprocess" -membertype "properties" ) { write-information "[$jsonfile] found $($ingestionmetadata.preprocess.count) preprocessing task(s) file" :datasetloop foreach ($dataset $ingestionmetadata.preprocess ) { # mandatory params $taskname = $dataset.name write-information "[$jsonfile].[$taskname] task_configuration (preprocess) ..." # mandatory params $taskcategory = "preprocess" $task_type = $dataset.task_type $worker_pipeline = $dataset.worker_pipeline $preprocesspattern = $dataset.source_pattern # optional params (get-member -inputobject $dataset -name "description" -membertype "properties"){ $task_description = $dataset.description } else { $task_description = $taskname } (get-member -inputobject $dataset -name "source_container" -membertype "properties"){ $container_name = $dataset.source_container } else { $container_name = 'preprocessing' } (get-member -inputobject $dataset -name "source_folder" -membertype "properties"){ $source_folder = $dataset.source_folder } else { $source_folder = 'preprocessing' } (get-member -inputobject $dataset -name "worker_properties" -membertype "properties"){ $preprocessworkeroptions = $dataset.worker_properties | convertto-json -depth 5 } else { $preprocessworkeroptions = $null } # set meta.task_configuration $taskresult = set-ingestiontaskconfiguration @sqlparams ` -taskname "$taskname" ` -taskcategory $taskcategory ` -taskdescription $task_description ` -tasktype $task_type ` -workername $worker_pipeline ` -filelayout $null ` -sourcefolder $source_folder ` -containername $container_name ` -tablename $null ` -targetoptions $null ` -preprocesspattern $preprocesspattern ` -preprocessworkeroptions $preprocessworkeroptions ` -verbose:$verbosepreference -printdetailedresults:$printdetailedresults write-information "[$jsonfile].[$taskname] task_configuration (preprocess) succeeded - total: $($taskresult.total), inserted: $($taskresult.inserted), updated : $($taskresult.updated), deleted : $($taskresult.deleted)" } #foreach dataset } else { write-information "[$jsonfile] found preprocessing task(s) file" } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionMetadataFromJsonFile.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) # . $psscriptroot/set-mbbazingestionbatchjobs.ps1 # . $psscriptroot/set-mbbazingestiondatasets.ps1 # . $psscriptroot/get-mbbazdetokenizedstring.ps1 # . $psscriptroot/set-ingestiontaskconfiguration.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-mbbazingestionmetadatafromjsonfile" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } mock write-information { param([string]$messagedata) write-output $messagedata } mock compare-ingestionmetadatafromjsonfile {} mock set-ingestionplanconfiguration { $resultset} mock set-ingestionplantaskconfiguration { $resultset} mock set-ingestiontaskconfiguration { $resultset } mock set-ingestionsourceconfiguration { $resultset} mock set-ingestionsourcecolumnconfiguration { $resultset } mock set-ingestionsourcecheckconfiguration { $resultset } mock update-ingestiondeltatableconfiguration {} $resultset = [pscustomobject]@{ total = 0 inserted = 0 updated = 0 deleted = 0 } $resultset | out-null # avoid unused variable warning # deploy dummy unit test environment $commonparams = @{ targetenvironment = "unittest" sqlserverinstance = "dummy" sqldatabase = "dummy" accesstoken = "dummy" erroraction = "stop" } $commonparams | out-null # avoid unused variable warning } context "test invalid input file behaviour" { beforeall { } "should fail missing file" { { set-ingestionmetadatafromjsonfile @commonparams -path "unexisting" } | -throw -exceptiontype system.io.filenotfoundexception } "should fail invalid json" { # dev-note: actually, checking invalid files detected asu unit tested # compare function test, check errors compare function # correctly handled mock compare-ingestionmetadatafromjsonfile {throw} $invalidfile = join-path $testfilepath "testinvalidjson.json" { set-ingestionmetadatafromjsonfile @commonparams -path $invalidfile } | -throw -invoke -commandname compare-ingestionmetadatafromjsonfile -times 1 } } context "test input file orchestration behaviour" { beforeall { } "should invoke plantask plan configuration functions" { $validfile = join-path $testfilepath "testingestionmetadata-plantasks_full.json" set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname compare-ingestionmetadatafromjsonfile -times 1 -parameterfilter {$path -eq $validfile} -invoke -commandname set-ingestionplanconfiguration -times 1 -invoke -commandname set-ingestionplantaskconfiguration -times 1 } "should invoke task, source, source column configuration functions" { $validfile = join-path $testfilepath "testingestionmetadata-datasets_full.json" set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname compare-ingestionmetadatafromjsonfile -times 1 -parameterfilter {$path -eq $validfile} -invoke -commandname set-ingestiontaskconfiguration -times 1 -invoke -commandname set-ingestionsourceconfiguration -times 1 -invoke -commandname set-ingestionsourcecolumnconfiguration -times 1 -invoke -commandname set-ingestionsourcecheckconfiguration -times 1 } "should invoke preprocess task configuration functions" { $validfile = join-path $testfilepath "testingestionmetadata-preprocess_valid.json" set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname compare-ingestionmetadatafromjsonfile -times 1 -parameterfilter {$path -eq $validfile} -invoke -commandname set-ingestionplanconfiguration -times 0 -invoke -commandname set-ingestionplantaskconfiguration -times 0 -invoke -commandname set-ingestiontaskconfiguration -times 1 } "should invoke preprocess task configuration functions" { $validfile = join-path $testfilepath "testingestionmetadata-preprocess_valid.json" # call -deploypreprocessing $false set-ingestionmetadatafromjsonfile @commonparams -path $validfile -deploypreprocessing $false -invoke -commandname compare-ingestionmetadatafromjsonfile -times 1 -parameterfilter {$path -eq $validfile} -invoke -commandname set-ingestiontaskconfiguration -times 0 } "should deploy plan environment match"{ $skippedplanfile = join-path $testfilepath "testingestionmetadata-plantasks_skip_environment.json" set-ingestionmetadatafromjsonfile @commonparams -path $skippedplanfile -invoke -commandname compare-ingestionmetadatafromjsonfile -times 1 -parameterfilter {$path -eq $skippedplanfile} -invoke -commandname set-ingestionplanconfiguration -times 0 -invoke -commandname set-ingestionplantaskconfiguration -times 0 -invoke -commandname write-information -parameterfilter {$messagedata -like "*not valid target environment*"} } } context "test default usage optional variables dataset present"{ beforeall{ $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } } "should invoke task, source, source column, source check defaults"{ $validfile = join-path $testfilepath "testingestionmetadata-datasets_defaults.json" $checksdataarray = '{"name": "header", "enabled": true}' $columndatajson = '{column_sequence": 1, "source_column_name": "first_column", "sink_column_name": "first_column", "dimension": "scd2", "data_type": "string"}' set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname set-ingestionsourceconfiguration -times 1 -parameterfilter{ $columndelimiter -like ',' -and $rowdelimiter -like '\r\n' -and $escapecharacter -like '' -and $quotecharacter -like '' -and $headerline -like $true -and $encoding -like 'utf-8' -and $skipfirstlines -like 0 -and $preconditions -like $null } -invoke -commandname set-ingestiontaskconfiguration -times 1 -parameterfilter{ $containername -like 'landing' } -invoke -commandname set-ingestiontaskconfiguration -times 1 -parameterfilter{ $targetoptions -like $null } -invoke -commandname set-ingestionsourcecheckconfiguration -times 1 -parameterfilter{ $checksdataarray -like "$checksdataarray" } -invoke -commandname set-ingestionsourcecolumnconfiguration -times 1 -parameterfilter{ $columndatajson -like "$columndatajson" } } "filename preconditions: load parameters"{ $validfile = join-path $testfilepath "testingestionmetadata-datasets_filename_precond_all_params.json" $expected_condition_text = ('[ { "name": "required_name", "type": "file_exists", "frequency": "daily", "expected_file_mask": ".*{yyyymmdd}.*", "enabled": true, "description": "my_description" } ]') set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname set-ingestionsourceconfiguration -times 1 -parameterfilter{ $columndelimiter -like ',' -and $rowdelimiter -like '\r\n' -and $escapecharacter -like '' -and $quotecharacter -like '' -and $headerline -like $true -and $encoding -like 'utf-8' -and $skipfirstlines -like 0 -and $preconditions -eq $expected_condition_text } } "filename preconditions: load required params load"{ $validfile = join-path $testfilepath "testingestionmetadata-datasets_filename_precond_required_params.json" $expected_condition_text = ('[ { "name": "required_name", "type": "file_exists", "frequency": "daily", "expected_file_mask": null, "enabled": true, "description": null } ]') set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname set-ingestionsourceconfiguration -times 1 -parameterfilter{ $columndelimiter -like ',' -and $rowdelimiter -like '\r\n' -and $escapecharacter -like '' -and $quotecharacter -like '' -and $headerline -like $true -and $encoding -like 'utf-8' -and $skipfirstlines -like 0 -and $preconditions -eq $expected_condition_text } } } context "test default usage optional variables plans present"{ beforeall{ $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } } "should invoke plan, plan task config defaults"{ $validfile = join-path $testfilepath "testingestionmetadata-plantasks_defaults.json" set-ingestionmetadatafromjsonfile @commonparams -path $validfile -invoke -commandname set-ingestionplanconfiguration -times 1 -parameterfilter{ $planenabled -like $true } -invoke -commandname set-ingestionplantaskconfiguration -times 1 -parameterfilter{ $plantask_json -like '*"task_sequence": 1*' -and $plantask_json -like '*"enabled": true*' } } } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanConfiguration.ps1">function set-ingestionplanconfiguration { [cmdletbinding()] param ( [parameter(mandatory = $true)] [string] $serverinstance, [parameter(mandatory = $true)] [string] $database, [parameter(mandatory = $true)] [string] $accesstoken, [parameter(mandatory = $true)] [string] $planname, [parameter(mandatory = $true)] [string] $plandescription, [parameter(mandatory = $false)] [boolean] $planenabled = $true, [parameter(mandatory = $false)] [switch] $printdetailedresults ) $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $true includesqlusererrors = $true abortonerror = $true } $plan_description = ($plandescription) ? ("'" + $plandescription.replace("'", "''") + "'") : "null" $enabled = ($planenabled) ? "1" : "0" $generatedsql = [system.text.stringbuilder]::new() [void]$generatedsql.appendline("exec [deploy].[usp_set_plan_configuration] ") [void]$generatedsql.appendline(" @plan_name = '$planname', ") [void]$generatedsql.appendline(" @plan_description = $plan_description, ") [void]$generatedsql.appendline(" @enabled = $enabled ") $query = $generatedsql.tostring() # write-verbose "executing query:" # write-verbose $query $sqlresult = get-sqlqueryresults @sqlparameters -query $query -printdetailedresults:$printdetailedresults return $sqlresult }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanConfiguration.Tests.ps1"># dev-note: basic test function really lot -&gt; potentially pull apart reusable pieces new function rewrite part module modulair beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestionplanconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } mock get-sqlqueryresults { $resultset } } "should invoke get-sqlqueryresults function" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' planname = 'dummy' plandescription = "description 'dummy' plan" planenabled = $false } set-ingestionplanconfiguration @params -erroraction "stop" -invoke -commandname get-sqlqueryresults -times 1 -parameterfilter { $query -like "*deploy*set_plan_configuration*" -and $query -like "*@plan_description = 'description ''dummy'' plan'*" -and $query -like "*@enabled = 0*" } # dev-note: testing possibility # main function: use $script:query # test function: use $query | -belike "*@enabled*" } "should return object get-sqlqueryresults"{ $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' planname = 'dummy' plandescription = "description 'dummy' plan" planenabled = $false } $result = set-ingestionplanconfiguration @params -erroraction "stop" $result.total | -be 1 $result.inserted | -be 1 $result.updated | -be 0 $result.deleted | -be 0 } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanTaskConfiguration.ps1">function set-ingestionplantaskconfiguration { [cmdletbinding()] param ( [parameter(mandatory = $true)] [string] $serverinstance, [parameter(mandatory = $true)] [string] $database, [parameter(mandatory = $true)] [string] $accesstoken, [parameter(mandatory = $true)] [string] $planname, [parameter(mandatory = $true, helpmessage = "json array plan_task_configurations")] [string] $plantask_json, [parameter(mandatory = $false)] [switch] $printdetailedresults ) $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $true includesqlusererrors = $true abortonerror = $true } $generatedsql = [system.text.stringbuilder]::new() [void]$generatedsql.appendline("exec [deploy].[usp_set_plan_task_configuration] ") [void]$generatedsql.appendline(" @plan_name = '$planname', ") [void]$generatedsql.appendline(" @plantask_data = '$plantask_json' ") $query = $generatedsql.tostring() write-verbose "executing query:" write-verbose $query ( $plantask_json) { # dev-info: -noenumerate keeps "array" idea json, otherwise we'll get null single item $numplantasks = ($plantask_json | convertfrom-json -noenumerate).count } else { $numplantasks = 0 } $sqlresult = get-sqlqueryresults @sqlparameters -query $query -numtotalparam $numplantasks -printdetailedresults:$printdetailedresults return $sqlresult }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanTaskConfiguration.Tests.ps1"># dev-note: basic test function really lot -&gt; potentially pull apart reusable pieces new function rewrite part module modulair beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestionplantaskconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } mock get-sqlqueryresults { $resultset } } "should invoke get-sqlqueryresults function" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' planname = 'dummy' plantask_json = '{"dummy_key": "dummy_value"}' } # needed, else: unable pass json -like statement $plantask_json = '{"dummy_key": "dummy_value"}' set-ingestionplantaskconfiguration @params -erroraction "stop" -invoke -commandname get-sqlqueryresults -times 1 -parameterfilter { $query -like "*deploy*set_plan_task_configuration*" -and $query -like "*@plan_name = 'dummy'*" -and $query -like "*@plantask_data = '$plantask_json'*" } } "should return object get-sqlqueryresults"{ $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' planname = 'dummy' plantask_json = '{"dummy_key": "dummy_value"}' } $result = set-ingestionplantaskconfiguration @params -erroraction "stop" $result.total | -be 1 $result.inserted | -be 1 $result.updated | -be 0 $result.deleted | -be 0 } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceCheckConfiguration.ps1">function set-ingestionsourcecheckconfiguration { [cmdletbinding()] param ( [parameter(mandatory = $true)] [string] $serverinstance, [parameter(mandatory = $true)] [string] $database, [parameter(mandatory = $true)] [string] $accesstoken, [parameter(mandatory = $true)] [string] $filelayout, # file_layout used link checks certain dataset [parameter(mandatory = $true)] [string] $checksdataarrayjson, # array json-objects [parameter(mandatory = $false)] [switch] $printdetailedresults ) $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $true includesqlusererrors = $true abortonerror = $true } # create string new sql-script executed $generatedsql = [system.text.stringbuilder]::new() [void]$generatedsql.appendline("exec [deploy].[usp_set_source_check_configuration] ") [void]$generatedsql.appendline(" @file_layout = '$filelayout', ") [void]$generatedsql.appendline(" @checks_data = '$checksdataarrayjson' ") # make sure script string format (to prevent errors) $query = $generatedsql.tostring() # write-verbose "executing query:" # write-information $query # execute query generated $sqlresult = get-sqlqueryresults @sqlparameters -query $query -numtotalparam ($checksdataarrayjson | convertfrom-json).count -printdetailedresults:$printdetailedresults return $sqlresult }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceCheckConfiguration.Tests.ps1"># dev-note: basic test function really lot -&gt; potentially pull apart reusable pieces new function rewrite part module modulair beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestionsourcecheckconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } mock get-sqlqueryresults { $resultset } } "should invoke get-sqlqueryresults function" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' filelayout = 'dummy' checksdataarrayjson = '{"dummy_key": "dummy_value"}' } # needed, else: unable pass json -like statement $checksdataarrayjson = '{"dummy_key": "dummy_value"}' set-ingestionsourcecheckconfiguration @params -erroraction "stop" -invoke -commandname get-sqlqueryresults -times 1 -parameterfilter { $query -like "*deploy*set_source_check_configuration*" -and $query -like "*@file_layout = 'dummy'*" -and $query -like "*@checks_data = '$checksdataarrayjson'*" } } "should return object get-sqlqueryresults"{ $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' filelayout = 'dummy' checksdataarrayjson = '{"dummy_key": "dummy_value"}' } $result = set-ingestionsourcecheckconfiguration @params -erroraction "stop" $result.total | -be 1 $result.inserted | -be 1 $result.updated | -be 0 $result.deleted | -be 0 } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceColumnConfiguration.ps1">function set-ingestionsourcecolumnconfiguration { [cmdletbinding()] param ( [parameter(mandatory = $true)] [string] $serverinstance, [parameter(mandatory = $true)] [string] $database, [parameter(mandatory = $true)] [string] $accesstoken, [parameter(mandatory = $true)] [string] $filelayout, [parameter(mandatory = $true)] [string] $columndatajson, [parameter(mandatory = $false)] [switch] $printdetailedresults ) $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $true includesqlusererrors = $true abortonerror = $true } $generatedsql = [system.text.stringbuilder]::new() [void]$generatedsql.appendline("exec [deploy].[usp_set_source_column_configuration] ") [void]$generatedsql.appendline(" @file_layout = '$filelayout', ") [void]$generatedsql.appendline(" @column_data = '$columndatajson' ") $query = $generatedsql.tostring() # write-verbose "executing query:" # write-information $query $sqlresult = get-sqlqueryresults @sqlparameters -query $query -numtotalparam ($columndatajson | convertfrom-json).count -printdetailedresults:$printdetailedresults return $sqlresult }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceColumnConfiguration.Tests.ps1"># dev-note: basic test function really lot -&gt; potentially pull apart reusable pieces new function rewrite part module modulair beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestionsourcecolumnconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } mock get-sqlqueryresults { $resultset } } "should invoke get-sqlqueryresults function" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' filelayout = 'dummy' columndatajson = '{"dummy_key": "dummy_value"}' } # needed, else: unable pass json -like statement $columndatajson = '{"dummy_key": "dummy_value"}' set-ingestionsourcecolumnconfiguration @params -erroraction "stop" -invoke -commandname get-sqlqueryresults -times 1 -parameterfilter { $query -like "*deploy*set_source_column_configuration*" -and $query -like "*@file_layout = 'dummy'*" -and $query -like "*@column_data = '$columndatajson'*" } } "should return object get-sqlqueryresults"{ $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' filelayout = 'dummy' columndatajson = '{"dummy_key": "dummy_value"}' } $result = set-ingestionsourcecolumnconfiguration @params -erroraction "stop" $result.total | -be 1 $result.inserted | -be 1 $result.updated | -be 0 $result.deleted | -be 0 } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceConfiguration.ps1">function set-ingestionsourceconfiguration { [cmdletbinding()] param ( [parameter(mandatory = $true)] [string] $serverinstance, [parameter(mandatory = $true)] [string] $database, [parameter(mandatory = $true)] [string] $accesstoken, [parameter(mandatory = $true)] [string] $filelayout, [parameter(mandatory = $true)] [string] $matchpattern, [parameter(mandatory = $true)] [string] $filekind, [parameter(mandatory = $true)] [string] $extension, [parameter(mandatory = $false)] [string] $columndelimiter, [parameter(mandatory = $false)] [string] $rowdelimiter, [parameter(mandatory = $false)] [string] $escapecharacter, [parameter(mandatory = $false)] [string] $quotecharacter, [parameter(mandatory = $true)] [bool] $headerline, [parameter(mandatory = $false)] [string] $sheetname, [parameter(mandatory = $false)] [string] $datarange, [parameter(mandatory = $true)] [string] $encoding, [parameter(mandatory = $false)] [int16] $skipfirstlines, [parameter(mandatory = $false)] [switch] $printdetailedresults, [parameter(mandatory= $false)] [string] $preconditions ) $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $true includesqlusererrors = $true abortonerror = $true } $file_layout = "'$filelayout'" $match_pattern = "'$matchpattern'" $file_kind = "'$filekind'" $extension = ($extension) ? ("'" + $extension + "'") : "null" $column_delimiter = ($columndelimiter) ? ($columndelimiter) : "null" # work enclosed single quotes, leave $row_delimiter = ($rowdelimiter) ? ("'" + $rowdelimiter + "'") : "null" $escape_character = ($escapecharacter) ? ("'" + $escapecharacter + "'") : "null" $quote_character = ($quotecharacter) ? ("'" + $quotecharacter + "'") : "null" $headers = ($headerline) $skip_first_lines = ($skipfirstlines) ? ($skipfirstlines) : "null" $preconditions = ($preconditions) ? ("'" + ($preconditions -replace "'", "''") + "'") : "null" $generatedsql = [system.text.stringbuilder]::new() [void]$generatedsql.appendline("exec [deploy].[usp_set_source_configuration] ") [void]$generatedsql.appendline(" @file_layout = $file_layout, ") [void]$generatedsql.appendline(" @file_pattern = $match_pattern, ") [void]$generatedsql.appendline(" @file_kind = $file_kind, ") [void]$generatedsql.appendline(" @file_extension = $extension, ") [void]$generatedsql.appendline(" @column_delimiter = '$column_delimiter', ") [void]$generatedsql.appendline(" @row_delimiter = $row_delimiter, ") [void]$generatedsql.appendline(" @escape_character = $escape_character, ") [void]$generatedsql.appendline(" @quote_character = $quote_character, ") [void]$generatedsql.appendline(" @header = $headers, ") [void]$generatedsql.appendline(" @encoding = '$encoding', ") [void]$generatedsql.appendline(" @skip_first_lines = $skip_first_lines, ") [void]$generatedsql.appendline(" @source_conditions = $preconditions ") $query = $generatedsql.tostring() # write-verbose "executing query:" # write-information $query $sqlresult = get-sqlqueryresults @sqlparameters -query $query -printdetailedresults:$printdetailedresults return $sqlresult }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceConfiguration.Tests.ps1"># dev-note: basic test function really lot -&gt; potentially pull apart reusable pieces new function rewrite part module modulair beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestionsourceconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } mock get-sqlqueryresults { $resultset } } "should invoke get-sqlqueryresults function" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' filelayout = 'dummy' matchpattern = '(dummy)' filekind = 'dummy' extension = '.dummy' columndelimiter = 'd' rowdelimiter = 'd' escapecharacter = 'd' quotecharacter = 'd' headerline = $true sheetname = 'dummy' encoding = 'utf-dummy' skipfirstlines = -1 } set-ingestionsourceconfiguration @params -erroraction "stop" -invoke -commandname get-sqlqueryresults -times 1 -parameterfilter { $query -like "*deploy*set_source_configuration*" -and $query -like "*@file_layout = 'dummy'*" -and $query -like "*@file_pattern = '(dummy)'*" -and $query -like "*@file_kind = 'dummy'*" -and $query -like "*@file_extension = '.dummy'*" -and $query -like "*@column_delimiter = 'd'*" -and $query -like "*@row_delimiter = 'd'*" -and $query -like "*@escape_character = 'd'*" -and $query -like "*@quote_character = 'd'*" -and $query -like "*@header = true*" -and $query -like "*@encoding = 'utf-dummy'*" $query -like "*@skip_first_lines = -1*" $query -like "*@source_conditions = 'dummy'*" } } "should return object get-sqlqueryresults"{ $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' filelayout = 'dummy' filekind = 'dummy' matchpattern = '(dummy)' extension = '.dummy' columndelimiter = 'd' rowdelimiter = 'd' escapecharacter = 'd' quotecharacter = 'd' headerline = $true sheetname = 'dummy' encoding = 'utf-dummy' skipfirstlines = -1 preconditions = 'dummy' } $result = set-ingestionsourceconfiguration @params -erroraction "stop" $result.total | -be 1 $result.inserted | -be 1 $result.updated | -be 0 $result.deleted | -be 0 } } }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionTaskConfiguration.ps1">function set-ingestiontaskconfiguration { [cmdletbinding()] param ( [parameter(mandatory = $true)] [string] $serverinstance, [parameter(mandatory = $true)] [string] $database, [parameter(mandatory = $true)] [string] $accesstoken, [parameter (mandatory = $true)] [string] $taskname, [parameter (mandatory = $true, helpmessage = "task category required. valid values 'ingest' 'preprocess'")] [validateset('ingest', 'preprocess')] [string] $taskcategory, [parameter (mandatory = $true, helpmessage = "task description required.")] [string] $taskdescription, [parameter (mandatory = $true)] [string] $tasktype, [parameter (mandatory = $true)] [string] $workername, [parameter (mandatory = $false)] [string] $filelayout, [parameter (mandatory = $true)] [allowemptystring()] [string] $sourcefolder, [parameter (mandatory = $true)] [string] $containername, [parameter (mandatory = $false)] [string] $tablename, [parameter (mandatory = $false)] [allowemptystring()] [psobject] $targetoptions = @{}, [parameter (mandatory = $false)] [allowemptystring()] [string] $preprocesspattern = $null, [parameter (mandatory = $false)] [allowemptystring()] [psobject] $preprocessworkeroptions = $null, [parameter(mandatory = $false)] [switch] $printdetailedresults ) #region additional checks: ingest ($taskcategory -eq "ingest") { (-not $filelayout) { throw "filelayout required ingest task category" } (-not $tablename) { throw "tablename required ingest task category" } } #endregion #region additional checks: preprocess ($taskcategory -eq "preprocess") { (-not $preprocesspattern) { throw "preprocesspattern required preprocess task category" } } #endregion $sqlparameters = @{ serverinstance = $serverinstance database = $database accesstoken = $accesstoken outputsqlerrors = $true includesqlusererrors = $true abortonerror = $true } $table_name = ($tablename) ? ("'" + $tablename.trim() + "'") : "null" $source_folder = ($sourcefolder) ? ("'" + $sourcefolder+ "'") : "null" $file_layout = ($filelayout) ? ("'" + $filelayout.trim() + "'") : "null" ($targetoptions){ $targetoptions = "'" + ($targetoptions | convertto-json -depth 5 ) + "'" } else { $targetoptions = "null" } ($preprocessworkeroptions){ $preprocessworkeroptions = "'" + ($preprocessworkeroptions | convertto-json -depth 5) + "'" } else { $preprocessworkeroptions = "null" } $generatedsql = [system.text.stringbuilder]::new() [void]$generatedsql.appendline("exec [deploy].[usp_set_task_configuration] ") [void]$generatedsql.appendline(" @task_name = '$taskname', ") [void]$generatedsql.appendline(" @task_category = '$taskcategory', ") [void]$generatedsql.appendline(" @task_type = '$tasktype', ") [void]$generatedsql.appendline(" @worker_name = $workername, ") [void]$generatedsql.appendline(" @task_description = '$taskdescription', ") [void]$generatedsql.appendline(" @table_name = $table_name, ") [void]$generatedsql.appendline(" @target_options = $targetoptions, ") [void]$generatedsql.appendline(" @file_layout = $file_layout, ") [void]$generatedsql.appendline(" @source_folder = $source_folder, ") [void]$generatedsql.appendline(" @container_name = '$containername', ") [void]$generatedsql.appendline(" @preprocess_pattern = '$preprocesspattern', ") [void]$generatedsql.appendline(" @preprocess_worker_options = $preprocessworkeroptions, ") [void]$generatedsql.appendline(" @enabled = $true ") $query = $generatedsql.tostring() write-verbose "executing query:" write-verbose $query $sqlresult = get-sqlqueryresults @sqlparameters -query $query -printdetailedresults:$printdetailedresults return $sqlresult }</file><file name="src\powershell\modules\DataConfig\public\Set-IngestionTaskConfiguration.Tests.ps1"># dev-note: basic test function really lot -&gt; potentially pull apart reusable pieces new function rewrite part module modulair beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # load depending functions (they need loaded mocked) #. $psscriptroot/set-mbbazingestionbatchjobs.ps1 #. $psscriptroot/set-mbbazingestiondatasets.ps1 #. $psscriptroot/get-mbbazdetokenizedstring.ps1 # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "set-ingestiontaskconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } $resultset = [pscustomobject]@{ total = 1 inserted = 1 updated = 0 deleted = 0 } mock get-sqlqueryresults { $resultset } } "should invoke get-sqlqueryresults function" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' taskname = 'dummy' taskcategory = 'ingest' taskdescription = 'dummy dummy' tasktype = 'dummy' workername = 'dummyworker' filelayout = 'dummy_file' sourcefolder = '/dummy/dummy' containername = 'dummy_container' tablename = 'dummy_table' targetoptions = $null } set-ingestiontaskconfiguration @params -erroraction "stop" -invoke -commandname get-sqlqueryresults -times 1 -parameterfilter { $query -like "*deploy*set_task_configuration*" -and $query -like "*@task_name = 'dummy'*" -and $query -like "*@task_type = 'dummy'*"-and $query -like "*@worker_name = dummyworker*" -and $query -like "*@task_description = 'dummy dummy'*" -and $query -like "*@table_name = 'dummy_table'*" -and $query -like "*@target_options = null*" -and $query -like "*@file_layout = 'dummy_file'*" -and $query -like "*@source_folder = '/dummy/dummy'*" -and $query -like "*@container_name = 'dummy_container'*" -and $query -like "*@enabled = true*" } } "should return object get-sqlqueryresults"{ $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' taskname = 'dummy' taskcategory = 'ingest' taskdescription = 'dummy dummy' tasktype = 'dummy' workername = 'dummyworker' filelayout = 'dummy_file' sourcefolder = '/dummy/dummy' containername = 'dummy_container' tablename = 'dummy_table' targetoptions = $null } $result = set-ingestiontaskconfiguration @params -erroraction "stop" $result.total | -be 1 $result.inserted | -be 1 $result.updated | -be 0 $result.deleted | -be 0 } "should fail preprocess task preprocesspattern" { $params = @{ serverinstance = 'dummy' database = 'dummy' accesstoken = 'dummy' taskname = 'dummy' taskcategory = 'preprocess' taskdescription = 'dummy dummy' tasktype = 'dummy' workername = 'dummyworker' sourcefolder = '/dummy/dummy' containername = 'dummy_container' } {set-ingestiontaskconfiguration @params -erroraction "stop"} | -throw "*preprocesspattern*" } } }</file><file name="src\powershell\modules\DataConfig\public\Start-IngestionSparkSessionJobs.ps1">function start-ingestionsparksessionjobs { &lt;# .synopsis invoke code statement spark session return jobobject statementobject .description function uses pssynapsesparksession gets object using sessionname launches code statement object. done job -job: wait statement execute continuing -&gt; return jobobject -else: wait statement complete continuing -&gt; return statementobject .notes - need define either name session pass pssynapsesparksession object #&gt; [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="code execute spark session")] [string] $code, [parameter(mandatory=$true, helpmessage="name synapse workspace invoke session" )] [string] $synapseworkspace, [parameter(mandatory=$true, helpmessage="name synapse spark pool invoke session from" )] [string] $synapsesparkpool, [parameter(mandatory=$false, helpmessage="pssynapsesparksession object")] [psobject] $sessionobj, [parameter(mandatory=$false, helpmessage="name spark session")] [string] $sessionname, [parameter(mandatory=$false, helpmessage="run code job (-asjob) not")] [boolean] $job ) # local scripts settings set-strictmode -version "latest" #region step 1: execute data quality checks # either session sessionname needs defined (-not $sessionobj -and -not $sessionname ){ throw "need define either sessionobject give name session" } #endregion #region step 2: get spark session object (-not $sessionobj){ $sessionobj = get-ingestionsparksessionobject -synapseworkspace $synapseworkspace -synapsesparkpool $synapsesparkpool -sessionname $sessionname -waitforsession $true -setupsession $true } # make sur ethe type object correct ($sessionobj.gettype().name -ne "pssynapsesparksession"){ throw "given session object type 'pssynapsesparksession'" } #endregion #region step 3: execute code-block spark session ($job){ $jobobj = $sessionobj | invoke-azsynapsesparkstatement -language 'pyspark' -code $code -asjob return $jobobj } else{ $statementobj = $sessionobj | invoke-azsynapsesparkstatement -language 'pyspark' -code $code return $statementobj } #endregion }</file><file name="src\powershell\modules\DataConfig\public\Start-IngestionSparkSessionJobs.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "start-ingestionsparksessionjobs" { context "unittests" { beforeall { # mock session object $sparkobjectget = [hashtable]@{ state = 'idle' name = 'unittest' id = '123' appid = 'application_0000000000000_0000' livyinfo = @{ startingat = get-date } } $mocksession = new-mockobject -type microsoft.azure.commands.synapse.models.pssynapsesparksession -properties $sparkobjectget mock get-ingestionsparksessionobject {$mocksession} } context "execute functions usable spark session"{ beforeall { # mock output -asjob used $joboutput = @([pscustomobject]@{ id = '123' state = "waiting" }) # mock output -asjob used $statementoutput = @{ output = @{ status = 'ok' } id = 2 } # mock function return different result based code-statement mock invoke-azsynapsesparkstatement { param([string]$code) ($code -eq 'statement') {return $statementoutput} elseif ($code -eq 'job') {return $joboutput} } # }return $joboutput} # mock invoke-azsynapsesparkstatement { param([string]$asjob=$false) return $statementoutput} } "should get new session object sessionname passed"{ start-ingestionsparksessionjobs -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -code "localtest" -sessionname "localtest" -invoke get-ingestionsparksessionobject } "should run code job"{ $result = start-ingestionsparksessionjobs -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -code "job" -sessionname "localtest" -job $true -invoke invoke-azsynapsesparkstatement -times 1 -parameterfilter {$asjob -eq $true} $result | -be $joboutput } "should run code statement"{ $result = start-ingestionsparksessionjobs -synapseworkspace 'dev-dap-syn-core' -synapsesparkpool 'devdapsspcore' -code "statement" -sessionname "localtest" -job $false -invoke invoke-azsynapsesparkstatement -times 0 -parameterfilter {$asjob -eq $true} $result | -be $statementoutput } } } }</file><file name="src\powershell\modules\DataConfig\public\Update-IngestionDeltaTableConfiguration.ps1">function update-ingestiondeltatableconfiguration { &lt;# .synopsis function create configuration file missing delta tabes .description function creates configuration file delta tables currently present silver lake database. function checks exists folder silver container name table. case, metadata delta table (table name, column names datatypes...) added configuration file. later process, set-ingestiondeltatableconfiguration deploy configuration file .notes time writing, function add tables silver database yet. future, might useful include tables execute validation checks. help flag issues like schema drift, column renames, etc. #&gt; [cmdletbinding()] param( # metadata delta table [parameter(mandatory=$true, helpmessage="name delta table created" )] [string] $tablename, [parameter(mandatory=$true, helpmessage="list json objects containing column metadata")] [psobject] $columndata, [parameter(mandatory=$false, helpmessage="metadata description table resembles" )] [string] $tabledescription, [parameter(mandatory=$true, helpmessage="container name inside storage account (e.g. silver)")] [string] $containername, # logical parameters needed function logic [parameter(mandatory=$false, helpmessage="name environment table needs deployed to" )] [string] $targetenvironment, [parameter(mandatory=$false, helpmessage="a switch turn need check existence delta tables container. provided, precheck always executed")] [switch] $checktableexistence, [parameter(mandatory=$false, helpmessage="configuration file containing list delta table objects deploy delta lake")] [string] $deltatableconfigurationfile, # "$($env:reporoot)/configuration/delta_tables/delta_tables.json" [parameter(mandatory=$false, helpmessage="additional options-object delta table assigned deployment")] [psobject] $targetoptions = @{} ) $storageaccount = "$($targetenvironment)dapstdala1" # name storage account store tables # use regex-function validate existence delta table $regex = "^($($tablename)\/_delta_log)$" # silver container contain delta tables current set-up # therefore, container 'silver', error thrown ($containername -ne 'silver'){ throw "only 'silver' container contain delta tables, $($containername)" } (-not (test-path $deltatableconfigurationfile)){ write-information "creating new configuration file" new-item -path $deltatableconfigurationfile -itemtype file -force # create custom powershell object $jsonobject = new-object psobject # add table_configurations property empty array $jsonobject | add-member -membertype noteproperty -name table_configurations -value @() # convert object json format $jsonstring = $jsonobject | convertto-json # save json string file $jsonstring | out-file -filepath $deltatableconfigurationfile } # (see notes) # first function checks whether delta table already exists silver database. # done checking if, 'silver' container, folder name table. # case, table already exists need redeploy table. # $containercontent = new-object collections.generic.list[string] [bool]$foundflag = $false ($checktableexistence){ write-information "checking existence delta table $($tablename) container $($containername)..." # set context azure storage account $context = new-azstoragecontext -storageaccountname $storageaccount -useconnectedaccount # get blobs inside container according specific filter $blobs = get-azstorageblob -context $context -container $containername # blobs inside container, boolean false ([bool]$blobs){ # found blob, check matches regex pattern foreach($blob $blobs.name){ # match, delta table exists lake database ($blob -match $regex){ $foundflag = $true # $containercontent.add($blob) break } } } } if((-not $foundflag) -or (-not $checktableexistence)){ (-not $checktableexistence){ # user-friendly output based scenario happening if-structure # write-warning "existence delta table $tablename disabled. continuing caution..." write-information "adding metadata delta table $($tablename) configuration file..." } else{ write-information "table $($tablename) found. adding metadata configuration file..." } # convert column-data json parameter # replace " ' colum data: necessary later deployments $jsoncolumndata = ($columndata | convertto-json -compress).replace('"', "'") # $targetoptions = ($targetoptions | convertto-json) # create parameter object containing parameters deploydeltatable script $params = @{ table_name = $tablename container_name = $containername column_info = $jsoncolumndata storage_account = $storageaccount table_description = $tabledescription target_options = $targetoptions } # open configuration file add parameter object file $deltaconfigfilecontent = get-content $deltatableconfigurationfile | convertfrom-json $tableconfigurations = $deltaconfigfilecontent.table_configurations # check object already exists configuration $tablenameobjects = $tableconfigurations | where-object { $_.table_name -eq $tablename } # exist, add ($tablenameobjects -eq $null) { # found, add new object $deltaconfigfilecontent.table_configurations += $params $deltaconfigfilecontent | convertto-json -depth 5 | out-file $deltatableconfigurationfile compare-ingestionmetadatafromjsonfile $deltatableconfigurationfile -schema "deltatabledata.schema.json" } else{ write-information "configuration table $($tablename) already exists" } } else{ write-information "delta table table $tablename found $($containername). new deployment needed." } }</file><file name="src\powershell\modules\DataConfig\public\Update-IngestionDeltaTableConfiguration.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "update-ingestiondeltatableconfiguration" { context "unittests" { beforeall { mock get-azcontext { '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx" } }' | convertfrom-json } } context "skip configuration update table exists"{ beforeall{ $storageobject = @([pscustomobject]@{ name = "table_name/_delta_log" }) mock new-azstoragecontext { } mock get-azstorageblob { $storageobject } mock write-information { param([string]$messagedata) } # write-host "info: $messagedata" mock new-item { } mock out-file { } } "table exists silver table. skip configuration"{ update-ingestiondeltatableconfiguration ` -tablename 'table_name' ` -columndata "[{}]" ` -tabledescription 'test description' ` -containername 'silver' ` -targetenvironment 'dev' ` -deltatableconfigurationfile 'mock' ` -checktableexistence ` -targetoptions @{} ` -warningaction silentlycontinue -invoke new-azstoragecontext -times 1 -invoke get-azstorageblob -times 1 -invoke write-information -times 1 -parameterfilter { $message -eq "delta table table table_name found silver. new deployment needed." } -invoke new-item -times 1 -parameterfilter { $path -eq 'mock' &amp;&amp; $itemtype -eq 'file'} } } context "add configuration empty configuration file"{ beforeall{ # mock functions called update-ingestiondeltatableconfiguration function call ## write-information write-warning: write message host manual validation mock write-information { param([string]$messagedata) } # write-host "info: $messagedata" mock write-warning { param([string]$messagedata) } # write-host "info: $messagedata" # mock azure-native functions call storage contents $storageobject = @([pscustomobject]@{ name = "table_name/_delta_log" }) mock new-azstoragecontext { } mock get-azstorageblob { $storageobject } # object try added configuration file $newobject = @{ table_description = 'test description' table_name = 'table_name_2' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' target_options = @{} } # configuration file empty file $configurationcontent = [pscustomobject]@{ table_configurations = @() } $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/user.json' } "should add configuration configuration file empty checktableexistence disabled"{ # ---------------------------------------------------------- ## call function parameters update-ingestiondeltatableconfiguration ` -tablename 'table_name_2' ` -columndata "[{}]" ` -tabledescription 'test description' ` -containername 'silver' ` -targetenvironment 'dev' ` -deltatableconfigurationfile 'testdrive:/user.json' ` -targetoptions @{} ` -warningaction silentlycontinue # ---------------------------------------------------------- # ---------------------------------------------------------- ## assert functions: validate certain methods (not) called function-call ### call: new-azstoragecontext (-not checktableexistence) -not -invoke new-azstoragecontext ### call: write-information write-warning -invoke write-information -times 1 -parameterfilter { $message -eq "adding metadata delta table table_name_2 configuration file..." } # -invoke write-warning -times 1 -parameterfilter { $message -eq "existence delta table table_name_2 disabled. continuing caution..." } ### validate: check output file correct schema compare-ingestionmetadatafromjsonfile 'testdrive:/user.json' -schemafile "deltatabledata.schema.json" ### validate 1 object file $configuration_contents = get-content 'testdrive:/user.json' -raw $objectcount = (($configuration_contents | convertfrom-json).table_configurations).count $objectcount | -be 1 } "should add configuration configuration file empty checktableexistence enabled"{ # ---------------------------------------------------------- ## call function parameters update-ingestiondeltatableconfiguration ` -tablename 'table_name_2' ` -columndata "[{}]" ` -tabledescription 'test description' ` -containername 'silver' ` -targetenvironment 'dev' ` -deltatableconfigurationfile 'testdrive:/user.json' ` -targetoptions @{} ` -checktableexistence ` -warningaction silentlycontinue # ---------------------------------------------------------- # ---------------------------------------------------------- ## assert functions: validate certain methods (not) called function-call ### call: write-warning (checktableexistence disabled) # -not -invoke write-warning ### call: write-information warning -invoke new-azstoragecontext -times 1 -invoke get-azstorageblob -times 1 -invoke write-information -times 1 -parameterfilter { $message -eq "table table_name_2 found. adding metadata configuration file..." } ### validate: check output file correct schema compare-ingestionmetadatafromjsonfile 'testdrive:/user.json' -schemafile "deltatabledata.schema.json" ### validate 1 object file $configuration_contents = get-content 'testdrive:/user.json' -raw $objectcount = (($configuration_contents | convertfrom-json).table_configurations).count $objectcount | -be 1 } } context "add configuration non-empty configuration file"{ beforeall{ # mock functions called update-ingestiondeltatableconfiguration function call ## write-information write-warning: write message host manual validation mock write-information { param([string]$messagedata) } # write-host "info: $messagedata" mock write-warning { param([string]$messagedata) } # write-host "info: $messagedata" # mock azure-native functions call storage contents $storageobject = @([pscustomobject]@{ name = "table_name_3/_delta_log" }) mock new-azstoragecontext { } mock get-azstorageblob { $storageobject } # object try added configuration file $newobject = @{ table_description = 'test description' table_name = 'table_name_2' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' target_options = @{} } # object already configuration file $existingobject = @{ table_description = 'test description' table_name = 'table_name' column_info = "'[{}]'" storage_account = 'devdapstdala1' container_name = 'silver' target_options = @{} } $configurationcontent = [pscustomobject]@{ table_configurations = @( $existingobject ) } } "should add configuration metadata configuration checktableexistence disabled"{ # configuration file non-empty file $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/user.json' # ---------------------------------------------------------- ## call function parameters update-ingestiondeltatableconfiguration ` -tablename 'table_name_2' ` -columndata "[{}]" ` -tabledescription 'test description' ` -containername 'silver' ` -targetenvironment 'dev' ` -deltatableconfigurationfile 'testdrive:/user.json' ` -targetoptions @{} ` -warningaction silentlycontinue # ---------------------------------------------------------- # ---------------------------------------------------------- ## assert functions: validate certain methods (not) called function-call ### call: new-azstoragecontext (checktableexistence) -not -invoke new-azstoragecontext ### call: write-information write-warning -invoke write-information -times 1 -parameterfilter { $message -eq "adding metadata delta table table_name_2 configuration file..." } # -invoke write-warning -times 1 -parameterfilter { $message -eq "existence delta table table_name_2 disabled. continuing caution..." } ### validate: check output file correct schema compare-ingestionmetadatafromjsonfile 'testdrive:/user.json' -schemafile "deltatabledata.schema.json" ### validate 1 object file $configuration_contents = get-content 'testdrive:/user.json' -raw $objectcount = (($configuration_contents | convertfrom-json).table_configurations).count $objectcount | -be 2 } "should add configuration metadata configuration checktableexistence enabled"{ # configuration file non-empty file $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/user.json' # ---------------------------------------------------------- ## call function parameters update-ingestiondeltatableconfiguration ` -tablename 'table_name_2' ` -columndata "[{}]" ` -tabledescription 'test description' ` -containername 'silver' ` -targetenvironment 'dev' ` -deltatableconfigurationfile 'testdrive:/user.json' ` -targetoptions @{} ` -checktableexistence ` -warningaction silentlycontinue # ---------------------------------------------------------- # ---------------------------------------------------------- ## assert functions: validate certain methods (not) called function-call ### call: write-warning (checktableexistence disabled) # -not -invoke write-warning ### call: write-information warning -invoke new-azstoragecontext -times 1 -invoke get-azstorageblob -times 1 -invoke write-information -times 1 -parameterfilter { $message -eq "table table_name_2 found. adding metadata configuration file..." } ### validate: check output file correct schema compare-ingestionmetadatafromjsonfile 'testdrive:/user.json' -schemafile "deltatabledata.schema.json" ### validate 1 object file $configuration_contents = get-content 'testdrive:/user.json' -raw $objectcount = (($configuration_contents | convertfrom-json).table_configurations).count $objectcount | -be 2 } "should add configuration metadata configuration"{ $configurationcontent | convertto-json -depth 5 | out-file 'testdrive:/user.json' # ---------------------------------------------------------- ## call function parameters update-ingestiondeltatableconfiguration ` -tablename 'table_name' ` -columndata "[{}]" ` -tabledescription 'test description' ` -containername 'silver' ` -targetenvironment 'dev' ` -deltatableconfigurationfile 'testdrive:/user.json' ` -targetoptions @{} ` -checktableexistence ` -warningaction silentlycontinue # ---------------------------------------------------------- # ---------------------------------------------------------- ## assert functions: validate certain methods (not) called function-call ### call: write-warning (checktableexistence disabled) # -not -invoke write-warning ### call: write-information warning -invoke new-azstoragecontext -times 1 -invoke get-azstorageblob -times 1 -invoke write-information -times 1 -parameterfilter { $message -eq "configuration table table_name already exists" } ### validate new configuration added configuration file $configuration_contents = get-content 'testdrive:/user.json' -raw $objectcount = (($configuration_contents | convertfrom-json).table_configurations).count $objectcount | -be 1 } } } }</file><file name="src\powershell\modules\DataConfig\public\Wait-AvailableJobs.ps1">function wait-availablejobs { &lt;# .synopsis given array pslongrunningjobs, wait $maxconcurrentjobs still active .description executing long running jobs, good practice overload system. script introduces "pause" new jobs introduced system. user passes array pslongrunningjobs, script wait less $maxconrrurentjobs still active (in state 'running' 'waiting'). whenever case, updated jobs-array returned containing still active jobs. every "ping" system, wait time statusupdateseconds introduced overload system .outputs return list active pslongrunningjobs objects .parameter startdate etc time process started. time offset local time #&gt; [cmdletbinding()] param( [parameter(mandatory=$true, helpmessage="maximum number jobs run time")] [int] $maxconcurrentjobs, [parameter(mandatory=$false, helpmessage="array pslongrunningjob-objects")] [array] $jobs = @(), [parameter(mandatory=$false, helpmessage="etc time process began, information purposes only")] [datetime] $startdate = (get-date), [parameter(mandatory=$false, helpmessage="time wait status updates")] [int] $statusupdateseconds = 30 ) # local scripts settings set-strictmode -version "latest" [timespan] $tolocaltimeoffset = (get-timezone).baseutcoffset #region # get list pslongrunningjobs still running waiting ($jobs){ $jobs = [array] @($jobs | where-object { $_.state -in ('running', 'waiting') }) # more/equal number jobs $maxconcurrentjobs, wait $statusupdateseconds check ($jobs.count -ge $maxconcurrentjobs) { # write user information $duration = new-timespan start $startdate.add($tolocaltimeoffset).tostring('hh:mm:ss') end (get-date).add($tolocaltimeoffset).tostring('hh:mm:ss') write-information " progress... running time: $($duration.tostring('hh\:mm\:ss')) | $($maxconcurrentjobs) jobs running, next check $($statusupdateseconds) seconds..." # wait $statusupdateseconds start-sleep -seconds $statusupdateseconds # get list pslongrunningjobs still running waiting $jobs = [array] @($jobs | where-object { $_.state -in ('running', 'waiting') }) } write-information "jobs progress: $($jobs.count)/$maxconcurrentjobs | $($maxconcurrentjobs - $jobs.count) slot(s) available" } #endregion return $jobs }</file><file name="src\powershell\modules\DataConfig\public\Wait-AvailableJobs.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "wait-availablejobs" { context "unittests" { # mocking get-date cmdlet control current time tests beforeall{ mock get-date { return [datetime]::parse("2023-10-01 12:00:00") } } "should return null null passed" { # call function test $result = wait-availablejobs -maxconcurrentjobs 5 -jobs $null # expect return value empty $result | -be $null } "should return empty array empty array passed" { # call function test # force array returned $result = @(wait-availablejobs -maxconcurrentjobs 5 -jobs @()) # expect returned array empty $result.count | -be 0 } "should return running jobs" { # define function parameters $jobs = @( [pscustomobject]@{ state = 'running' }, [pscustomobject]@{ state = 'completed' } ) # call function test $result = [array] ((wait-availablejobs -jobs $jobs -maxconcurrentjobs 5) ?? @()) # expect one item returned state = running $result.count | -be 1 $result[0].state | -be 'running' } "should wait number running jobs less maxconcurrentjobs" { # define function parameters # define list initial jobs $job1 = [pscustomobject] @{state = "running"} $job2 = [pscustomobject] @{state = "running"} $job3 = [pscustomobject] @{state = "running"} $jobs = @($job1, $job2, $job3) # mock start-sleep function simulate time passing without actually sleeping # use start-sleep mock change state 2 running jobs "completed" mock -commandname start-sleep -mockwith { # simulate changing state 2 jobs 'completed' $job1.state = 'rompleted' $job2.state = 'rompleted' # leave job3 'running' } # call function test $remainingjobs = [array] @(wait-availablejobs -jobs $jobs -maxconcurrentjobs 2 -statusupdateseconds 1) # assert: check 1 job remains jobs array $remainingjobs.count | -be 1 $remainingjobs[0].state | -be 'running' } "should return empty array jobs completed" { # define function parameters $jobs = @( [pscustomobject]@{ state = 'completed' }, [pscustomobject]@{ state = 'completed' } ) # call function test $result = [array] ((wait-availablejobs -jobs $jobs -maxconcurrentjobs 1) ?? @()) # jobs complete, expect result empty array $result.count | -be 0 } } }</file><file name="src\powershell\modules\DataConfig\schemas\DeltaTableData.schema.json">{ "$schema": "http://json-schema.org/draft-07/schema#", "type": "object", "title": "ingestion configuration files schema", "description": "schema keyrus ingestion framework configuration files", "required": [], "optional": [ "table_configurations" ], "additionalproperties": false, "properties": { "table_configurations": { "type": "array", "description": "array delta tables deploy", "items": { "type": "object", "description": "an explanation purpose instance.", "required": [ "table_description", "table_name", "column_info", "storage_account", "container_name" ], "optional": [ "target_options" ], "properties": { "table_description": { "type": "string", "description": "description delta table synapse workspace" }, "table_name": { "type": "string", "description": "name te delta table deploy synapse workspace" }, "column_info": { "type": "string", "description": "json object containing names columns data types" }, "storage_account": { "type": "string", "description": "name storage account silver database stored", "enum": ["devdapstdala1", "intdapstdala1", "tstdapstdala1", "accdapstdala1", "prddapstdala1"] }, "container_name": { "type": "string", "description": "name lake database deploy delta table to", "enum": ["silver"] }, "target_options": { "type": "object", "description": "additional options target table", "additionalitems": false, "items": { "type": "object", "required": [ ], "optional": [ "partitioning" ], "properties": { "partitioning": { "type": "array", "description": "properties used creation target_table deployment", "additionalproperties": false, "items": { "type": "object", "required": [ "name", "sequence" ], "optional": [ "datepart" ], "properties": { "name": { "type": "string", "description": "name column deploy partition" }, "sequence": { "type": "integer", "description": "sequence number partition column" }, "datepart": { "type": "string", "description": "part date needs partitioned on", "enum": ["year", "month", "day"] } } } } } } } } } } } }</file><file name="src\powershell\modules\DataConfig\schemas\IngestionMetadata.schema.json">{ "$schema": "http://json-schema.org/draft-07/schema#", "type": "object", "title": "ingestion configuration files schema", "description": "schema keyrus ingestion framework configuration files", "required": [], "optional": [ "plans", "datasets", "table_configurations" ], "additionalproperties": false, "properties": { "plans": { "type": "array", "description": "array plan definitions resources, one task_groups", "additionalitems": false, "items": { "type": "object", "description": "an explanation purpose instance.", "required": [ "name" ], "optional": [ "description", "environments", "enabled", "task_groups" ], "properties": { "name": { "type": "string", "description": "name plan" }, "description": { "type": "string", "description": "freetext description tasks plan run" }, "environments": { "type": "string", "description": "optional regex filter used evaluate environments plan deployed too. omit 'any' (.*)", "default": ".*" }, "enabled": { "type": "boolean", "description": "enable plan, not", "default": true }, "task_groups": { "type": "array", "description": "array task_group definitions, one tasks", "additionalitems": false, "items": { "type": "object", "required": [ "name" ], "optional": [ "tasks" ], "properties": { "name": { "type": "string", "description": "name task group", "enum": ["preprocess","ingest", "dummy"] }, "tasks": { "type": "array", "description": "array tasks executed (in parallel interdependencies possible) task group", "additionalitems": false, "items": { "type": "object", "required": [ "name" ], "optional": [ "sequence", "enabled" ], "properties": { "name": { "type": "string", "description": "name task" }, "sequence": { "type": "integer", "description": "sequence number, optional used managing/representation", "default": 1 }, "enabled": { "type": "boolean", "description": "enable task, not", "default": true } } } } } } } } } }, "datasets": { "type": "array", "description": "array plan definitions resources, one task_groups", "items": { "type": "object", "description": "an explanation purpose instance.", "required": [ "name", "kind", "task_type", "worker" ], "optional": [ "description", "ingestion", "checks", "columns" ], "additionalproperties": false, "properties": { "name": { "type": "string", "description": "name dataset, used creating tasks main identifiers" }, "description": { "type": "string", "description": "freetext description dataset" }, "kind": { "type": "string", "description": "description dataset's file type", "enum": ["csv", "json", "parquet", "zip"] }, "task_type": { "type": "string", "description": "describes tool/service used execute task", "enum": ["spark_notebook", "synapse_pipeline"] }, "worker": { "type": "string", "description": "depending used service, diffeent workers exist execute specific task", "enum": ["ingestionworker", "dummyworker", "filworker", "pl_unzip_worker", "pl_dummy_worker"] }, "ingestion": { "type": "object", "description": "properties used 'ingestion' phase landing area ods", "additionalproperties": false, "required": [ "target_table", "source_folder", "match_pattern", "extension" ], "optional": [ "target_options", "container", "encoding", "column_delimiter", "row_delimiter", "escape_character", "quote_character", "header_line", "skip_first_lines", "preconditions" ], "properties": { "target_table": { "type": "string", "description": "synapse table drop to-be-ingested files" }, "target_options":{ "type": "object", "description": "properties used creation target_table deployment", "additionalproperties": false, "required": [ ], "optional": [ "partitioning", "extract_date", "retention_time" ], "properties": { "extract_date": { "type": "object", "description": "properties used define technical field 't_extract_date'", "additionalproperties": false, "required": [ "column_name" ], "optional": [ "regex_expression", "extract_date_format" ], "properties": { "column_name": { "type": "string", "description": "name column use extract timestamp" }, "regex_expression": { "type": "string", "description": "regex expression allow extract timestamp given column" }, "extract_date_format": { "type": "string", "description": "format timestamp" } } }, "partitioning": { "type": "array", "description": "properties used creation target_table deployment", "additionalproperties": false, "items": { "type": "object", "required": [ "name", "sequence" ], "optional": [ "datepart" ], "properties": { "name": { "type": "string", "description": "name column deploy partition" }, "sequence": { "type": "integer", "description": "sequence number partition column" }, "datepart": { "type": "string", "description": "part date needs partitioned on", "enum": ["year", "month", "day"] } } } }, "retention_time":{ "type":"integer", "description": "properties used vacuum process determine long data retained" } } }, "source_folder": { "type": "string", "description": "folder relative ftproot folder set environment_configuration, without leading slash, eg 'myfolder/'" }, "match_pattern": { "type": "string", "description": "a simple 'contains' style match pattern, wildcards, eg 'some_file' would match 'xxxx_some_file_123.csv'" }, "extension": { "type": "string", "description": "extension excluding dot, eg 'csv'", "enum": ["csv", "json", "txt", "parquet", "gz", "zip", "fil"] }, "container": { "type": "string", "description": "azure container dataset expected found", "default": "landing" }, "column_delimiter": { "type": "string", "maxlength": 1, "description": "a single character column_delimiter, empty 'none'. important: limitation azure data factory, we'll use tab character 'empty' column_delimiter supported.", "default": "," }, "row_delimiter": { "type": "string", "maxlength": 5, "description": "a character row_delimiter, empty 'none'. important: limitation azure data factory, we'll use tab character 'empty' row_delimiter supported.", "default": "" }, "escape_character": { "type": "string", "maxlength": 1, "description": "optionally provide escape character, empty 'none' (default)", "default": "" }, "quote_character": { "type": "string", "maxlength": 1, "description": "optionally provide quote character, empty 'none' (default)", "default": "" }, "header_line": { "type": "boolean", "description": "optionally mark file header line columns names (default), not", "default": true }, "encoding": { "type": "string", "description": "", "enum": ["utf-8", "ansi"], "default": "utf-8" }, "skip_first_lines": { "type": "integer", "description": "total number lines need skipped beginning source file", "default": 0 } , "preconditions": { "type": "array", "description": "preconditions allow task start processing", "items": { "type": "object", "description": "the allowed precondition configuration items.", "required": [ "name", "type", "frequency" ], "optional": [ "expected_file_mask", "enabled", "description" ], "properties": { "name" : { "type": "string" , "description": "descriptive name validation perform (free text)" } , "type" : { "type": "string" , "description": "verification type currently default file_exists" , "enum": ["file_exists"] } , "enabled" : { "type":"boolean" , "description": "perform check? (default= true)" , "default": true } , "description" : { "type":"string" , "description": "the description check (in addition name)" } , "expected_file_mask" : { "type":"string" , "description": "the filemask must found found file. expected context deduced last loaded file" } , "frequency" : { "type":"string" , "description": "frequency expections previous current file. influence definition expected file mask. file_exists " , "enum": ["daily","hourly"] , "default" : "daily" } } } } } }, "checks": { "type": "array", "description": "checks need executed dataset", "additionalitems": false, "items": { "type": "object", "required": [ "name" ], "optional": [ "enabled", "config_params" ], "properties": { "name": { "type": "string", "description": "name check needs executed", "enum": ["header", "default_replace", "primary_keys", "data_type", "landing_rows","duplicate_primary_keys","replace_primary_key"] }, "enabled": { "type": "boolean", "description": "is check enabled disabled?", "default": true }, "config_params": { "type": "string", "description": "flat json object, saved string" } } } }, "columns": { "type": "array", "description": "column metadata control ingestion process", "additionalitems": false, "items": { "type": "object", "required": [ "sequence", "name" ], "optional": [ "dimension", "sink_name", "data_type", "column_info" ], "properties": { "sequence": { "type": "integer", "description": "sequence number column" }, "name": { "type": "string", "description": "name column appears source (ignored dataset kind set 'fixedlength_file')" }, "dimension": { "type": "string", "description": "describes dimension column", "enum": ["pk", "scd2"], "default": "scd2" }, "sink_name": { "type": "string", "description": "if source name needs changed, sink name specifies new column name" }, "data_type": { "type": "string", "description": "the expected data type column", "oneof": [ { "pattern": "^array$"}, { "pattern": "^binary$"}, { "pattern": "^boolean$"}, { "pattern": "^date$"}, { "pattern": "^string$"}, { "pattern": "^timestamp$"}, { "pattern": "^float$"}, { "pattern": "^byte$"}, { "pattern": "^integer$"}, { "pattern": "^long_integer$"}, { "pattern": "^decimal"}, { "pattern": "^varchar"} ], "default": "string" }, "column_info": { "type": "object", "description": "column metadata control ingestion process", "additionalitems": false, "items": { "type": "object", "required": [ ], "optional": [ "locale", "format", "thousand_sepatator", "decimal_separator", "optional" ,"replace_value" ], "properties": { "locale":{ "type": "string", "description": "indication locale setting file (en-us, fr-fr, etc)", "default": "en-us" }, "format": { "type": "string", "description": "format date timestamp (yyyy-mm-dd, dd-mm-yyyy, etc)", "default": "yyyy-mm-dd" }, "thousand_sepatator":{ "type": "string", "description": "separator value used numbers thousands", "enum": [",", "."], "default": "," }, "decimal_separator": { "type": "string", "description": "separator value used decimal numbers", "enum": [",", "."] }, "optional": { "type": "boolean", "description": "boolean value indicating column optional" }, "replace_value": { "type": "string", "description": "the name used replace null value primary key" } } } } } } } } } }, "preprocess": { "type": "array", "description": "array preprocess definition resources", "items": { "type": "object", "required": [ "name", "task_type", "worker_pipeline", "source_pattern" ], "optional": [ "description", "source_container", "source_folder", "worker_properties" ], "additionalproperties": false, "properties": { "name": { "type": "string", "description": "name preprocessing task, used creating tasks main identifiers" }, "description": { "type": "string", "description": "freetext description task" }, "task_type": { "type": "string", "description": "type worker task, 'synapse_pipeline' type task", "enum": ["synapse_pipeline"] }, "worker_pipeline": { "type": "string", "description": "name synapse pipeline run exucuting preprocessing task" }, "source_pattern": { "type": "string", "description": "a simple 'contains' style match pattern, wildcards, eg 'some_file' would match 'xxxx_some_file_123.csv'" }, "source_container":{ "type": "string", "description": "name container source files located, specified 'preprocess' used" }, "source_folder": { "type": "string", "description": "optional: path relative source_countainer filter source files, support wildcards. provided match_pattern used filtering." }, "worker_properties": { "type": "object", "description": "properties used preprocessing task", "additionalproperties": true, "required": [], "optional": [], "properties": { } } } } } } }</file><file name="src\powershell\modules\Networking\.build\build.ps1">[cmdletbinding()] param ( [parameter(helpmessage = "name module")] [string] $modulename = "networking", [parameter(helpmessage = "version number major.minor.path format")] [string] $buildversion, [parameter(helpmessage = "base directory module folder")] [string] $workingdir, [parameter(helpmessage = "directory localfeed psrepository")] [string] $localfeedpath, [parameter(helpmessage = "set switch enable cleaning module folder packaging")] [switch] $cleanfolder ) #local settings set-strictmode -version "latest" #requires -version 7.2 #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { $_ -notin [system.management.automation.pscmdlet]::commonparameters} $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "psversion: $($psversiontable.psversion.tostring())" write-information "----------------------------------------------------------------------" #endregion script info write-output "" write-output "package providers:" get-packageprovider write-output "----------------------------------------------------------------------" #unload module currently loaded $currentmodules = get-module -name $modulename -erroraction "ignore" if( $currentmodules) { write-output "unloading currently loaded '$modulename' modules ..." $currentmodules.version | foreach-object { write-information " $_"} $currentmodules | remove-module | out-null } else { write-output "no currently loaded '$modulename' modules (ok)" } #uninstall package currently installed $currentpackages = get-package -name $modulename -erroraction "ignore" if( $currentpackages) { write-output "uninstalling currently installed packages ..." $currentpackages.version | foreach-object { write-information " $_"} $currentpackages | uninstall-package | out-null } else { write-output "no currently installed '$modulename' packages (ok)" } #build check path module folder $modulepath = join-path $workingdir $modulename write-information "" write-information "using modulepath: $modulepath" ( -not (test-path $modulepath -pathtype "container") ) { throw "module '$modulename' subfolder '$workingdir'" } else { write-information " modulepath: ok, folder exists" $modulepath = (resolve-path $modulepath).path write-information " resolved to: $modulepath" } #build check path module manifest $psdfile = $modulename + ".psd1" write-information "using psdfile: $psdfile" $manifestpath = join-path $modulepath $psdfile write-information "using manifestpath: $manifestpath" (-not (test-path $manifestpath) ) { throw "manifest exist at: $manifestpath" } #region prepare localfeed psrepository write-output "preparing localfeed psrepository ..." #check resolve localfeedpath (-not (test-path $localfeedpath)) { new-item -path $localfeedpath -itemtype "directory" | out-null } $localfeedpathresolved = ($localfeedpath | resolve-path -erroraction ignore) ( -not $localfeedpathresolved ) { throw "could resolve localfeedpath: '$localfeedpath'" } else { $localfeedpathresolved = $localfeedpathresolved.path write-information " resolved localfeedpath to: $localfeedpathresolved" } #check create "localfeed" psrepository $localfeed = get-psrepository -name localfeed -erroraction "ignore" ( $localfeed ) { write-output " found localfeed psrepository" # $localfeed # $localfeed.scriptsourcelocation # $localfeed.scriptpublishlocation # write-output "(members)" # $localfeed | get-member } else { write-output " localfeed psrepository found" } ( $localfeed -and ($localfeed.sourcelocation -ne $localfeedpathresolved) ) { write-warning " localfeed repository wrong path ($($localfeed.sourcelocation)), removing ..." #write-output "unregister existing localfeed repository ..." unregister-psrepository -name "localfeed" $localfeed = $null } ( -not $localfeed) { write-output " registering 'localfeed' psrepository path: $localfeedpathresolved ..." register-psrepository -name localfeed -sourcelocation $localfeedpathresolved -publishlocation $localfeedpathresolved -installationpolicy trusted } else { write-output " psrepository 'localfeed' already present path: $localfeedpathresolved" } # remove existing package current version (main avoid conflict (re)building version 0.0.0 locally) $existingpackages = get-childitem -path $localfeedpathresolved -filter "${modulename}.${buildversion}.nupkg" if( -not $existingpackages) { write-information " deleting existing package files : none found [ok]" } else { write-information " deleting existing package files ..." $existingpackages.fullname | foreach-object { write-information " $_"} $existingpackages | remove-item | out-null } #endregion #region create manifest write-output "" write-output "prepare module manifest..." # remove files required nuget package ( $cleanfolder) { write-information " remove unneeded files nuget package..." get-childitem -path $modulepath -include "*.tests.ps1" -recurse | remove-item get-childitem -path $modulepath -filter ".build" -directory | remove-item -recurse get-childitem -path $modulepath -filter ".test_config" -directory | remove-item -recurse get-childitem -path $modulepath -filter "coverage.xml" | remove-item get-childitem -path $modulepath -filter "testresults_unit.xml" | remove-item } else { write-information " (...skipping cleanfolder section...)" } # update build version manifest write-information " updating version number manifest..." $manifestcontent = get-content -path $manifestpath -raw $manifestcontent = $manifestcontent -replace '&lt;moduleversion&gt;', $buildversion # find public functions add cmdlets $publicfuncfolderpath = join-path $modulepath public $itemparams = @{ path = $publicfuncfolderpath file = $true recurse = $true filter = '*.ps1' exclude = '*.tests.ps1' } ((test-path -path $publicfuncfolderpath) -and ($publicfunctionnames = get-childitem @itemparams | select-object -expandproperty basename)) { $funcstrings = "'$($publicfunctionnames -join "',$([system.environment]::newline) '")'" $funcstrings = $([system.environment]::newline) + " " + $funcstrings + $([system.environment]::newline) } else { $funcstrings = $null } ## add public functions functionstoexport attribute $manifestcontent = $manifestcontent -replace "'&lt;functionstoexport&gt;'", $funcstrings write-information " " write-information " functions export:" $funcstrings | foreach-object { write-output " $_" } ## save updated manifest $manifestcontent | set-content -path "$manifestpath" write-information " updated manifest: $manifestpath" #endregion # create nuget package write-output "" write-output "create publish nuget package localfeed repository..." publish-module -path $modulepath -repository localfeed -nugetapikey 'dummy' -force # list available packages write-output "" write-output "list available packages localfeed repository..." #find-package -source "localfeed" | select-object version, providername, source, fullpath find-package -source "localfeed" | foreach-object {write-output " $($_.version) -- $($_.providername)"} # install package write-output "" write-output "install module localfeed..." install-package -name $modulename -source "localfeed" -providername "powershellget" -force | foreach-object {write-output " $($_.version)"} # review installed modules $modules = get-module -name $modulename -listavailable | select-object name, version, repositorysourcelocation, path write-output "" write-output "available module versions:" $modules | foreach-object {write-output " $($_.version)"} # check "current" version write-output "" write-output "check current version available..." $currentmodule = $modules | where-object {$_.version.tostring() -eq $buildversion } ( -not $currentmodule) { throw "module version '$buildversion' found available modules" } else { write-information " found available module '$modulename' version '$buildversion' (ok)" } # import module write-output "" write-output "import module ..." import-module -name $modulename get-module -name $modulename | foreach-object {write-output " $($_.version)"}</file><file name="src\powershell\modules\Networking\.build\unit_tests.ps1">[cmdletbinding()] param ( ) # -------------------------------------------------------------- # section: prerequisites # -------------------------------------------------------------- #requires -modules az.keyvault #requires -modules az.accounts #requires -modules az.resources #requires -modules az.synapse #requires -modules @{ modulename="pester"; moduleversion="5.5.0" } # install pester needed # note joachim: actually done upfront, simple use 'requires' statement script # write-output "available pester versions:" # $pesterrequiredversion = "5.5" # $pestercheckregex = "^" + $pesterrequiredversion.replace(".", "\.") # (get-module pester -listavailable).path | foreach-object {" $_"} # if(-not (get-module pester -listavailable | where-object version -match $pestercheckregex)){ # write-output "installing pester $pesterrequiredversion ..." # install-module -name pester -requiredversion $pesterrequiredversion -force -scope currentuser -allowclobber # } else { # write-output "pester 5.5 already installed" # } # load correct one # import-module pester -minimumversion $pesterrequiredversion -maximumversion "$pesterrequiredversion.999999" # sure write-output "" write-output "using pester version: $((get-module pester).version)" write-output "" write-output "module test:" $modulepath = resolve-path ( join-path "$psscriptroot" "..") $modulename = (split-path $modulepath -leaf) + '.psm1' write-output " modulepath: $modulepath" write-output " modulename: $modulename" # preload module first time import-module -name (join-path $modulepath $modulename ) -force # -------------------------------------------------------------- # section: pester configuration # -------------------------------------------------------------- # start default configuration $pesterconfig = [pesterconfiguration]::default # create testresults.xml output $pesterconfig.testresult.enabled = $true $pesterconfig.testresult.outputformat = "nunitxml" $pesterconfig.testresult.outputpath = join-path $modulepath "testresults_unit.xml" # create code coverage output $pesterconfig.codecoverage.enabled = $true $pesterconfig.codecoverage.outputformat = "jacoco" $pesterconfig.codecoverage.outputpath = join-path $modulepath "coverage.xml" $pesterconfig.codecoverage.path = "$modulepath/[p]*/*.ps1" # public, private # filters (the fullname filter powershell "like" statement, regex) # important: fullname tag filters work combined "or", "and" $pesterconfig.filter.fullname = "*.unittests.*" $pesterconfig.filter.tag = 'unittest' $pesterconfig.filter.excludetag = "draft" # run parameters $pesterconfig.run.path = join-path $modulepath "public" $pesterconfig.run.exit = $false $pesterconfig.run.passthru = $false # -------------------------------------------------------------- # section: run tests # -------------------------------------------------------------- write-output "run tests..." invoke-pester -configuration $pesterconfig</file><file name="src\powershell\modules\Networking\public\Remove-NetworkingSqlServerFirewallRule.ps1">&lt;# .synopsis wrapper around az.sql functions remove-azsqlserverfirewallrule .description function add functionalities removing firewall rule sql server specific resource group * create list existing firewall rules * remove firewall rule exists -&gt; avoid throwing error .example ps&gt; remove-networkingsqlserverfirewallrule -servername "myserver" resourcegroupname "my-rg" -firewallrulename "rule0" #&gt; function remove-networkingsqlserverfirewallrule { [cmdletbinding()] param ( [parameter( mandatory = $true, helpmessage="name resource group sql server located")] [validatenotnullorempty()] [string] $resourcegroupname, [parameter( mandatory = $true, helpmessage="name sql server (xxx), full host server (xxx.database.windows.net)")] [validatenotnullorempty()] [string] $servername, [parameter( mandatory = $true, helpmessage="reference name to-be removed firewall rule")] [validatenotnullorempty()] [string] $firewallrulename ) ## start region: list script name parameters $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" #region print parameters $inputparameters = @( "resourcegroupname" "servername" "firewallrulename" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } #endregion # get list existing firewall rules server $existingrules = @( get-azsqlserverfirewallrule ` -resourcegroupname $resourcegroupname ` -servername $servername ` -whatif:$false # completeness, necessary add parameter ) # throw warning continue firewall rules found (-not $existingrules) { write-information "found existing firewall rules" # rule exist, throw warning continue write-warning "no rule found name '$firewallrulename'" } # firewall rules exist server else { write-information "found $($existingrules.count) existing firewall rule(s)" $existingrules.firewallrulename | write-verbose # filter: get list existing fireall rules name $firewallrulename $existingrule = $existingrules | where-object {$_.firewallrulename -eq $firewallrulename} # rule exists if( $existingrule) { # rule exists, remove surpress output $null = remove-azsqlserverfirewallrule ` -resourcegroupname $resourcegroupname ` -servername $servername ` -firewallrulename $firewallrulename ` -force } # rule exist else { # rule exist, throw warning continue write-warning "no rule found name '$firewallrulename'" } } }</file><file name="src\powershell\modules\Networking\public\Remove-NetworkingSqlServerFirewallRule.Tests.ps1">beforeall { # load function test . $pscommandpath.replace('.tests.ps1', '.ps1') # set path files test assignments $script:testfilepath = join-path $psscriptroot "../.test_config" # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "remove-networkingsqlserverfirewallrule" { # general context expected pester module context "unittests" { # group: multiple mocked firewall rules context "multiple existing rules" { # test result multiple existing firewall rules beforeall { # mock function az.sql module mock remove-azsqlserverfirewallrule {} # mock function az.sql module # function get-azsqlserverfirewallrule gets called, return 2 objects defined mock get-azsqlserverfirewallrule { $rule1 = new-object -typename "microsoft.azure.commands.sql.firewallrule.model.azuresqlserverfirewallrulemodel" $rule1.firewallrulename = "testrule1" $rule1.startipaddress = "startip" $rule1.endipaddress = "endip" $rule2 = new-object -typename "microsoft.azure.commands.sql.firewallrule.model.azuresqlserverfirewallrulemodel" $rule2.firewallrulename = "testrule2" $rule2.startipaddress = "startip" $rule2.endipaddress = "endip" return @( $rule1, $rule2 ) } } "test existing rule" { # firewallrulename exists, function remove-azsqlserverfirewallrule called exactly remove-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule1" ` -informationaction $pesterinformationaction -invoke remove-azsqlserverfirewallrule -times 1 -exactly } "test non-existing rule" { # firewallrulename exist, function remove-azsqlserverfirewallrule called remove-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "another rule" ` -informationaction $pesterinformationaction -invoke remove-azsqlserverfirewallrule -times 0 -exactly } } # group: add one firewall rule mock context "single existing rule" { # test result 1 existing firewall rule beforeall { # mock function az.sql module mock remove-azsqlserverfirewallrule {} # mock function az.sql module # function get-azsqlserverfirewallrule gets called, return object defined mock get-azsqlserverfirewallrule { $rule1 = new-object -typename "microsoft.azure.commands.sql.firewallrule.model.azuresqlserverfirewallrulemodel" $rule1.firewallrulename = "testrule1" $rule1.startipaddress = "startip" $rule1.endipaddress = "endip" return $rule1 } } "test existing rule" { # firewallrulename exists, function remove-azsqlserverfirewallrule called exactly remove-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule1" ` -informationaction $pesterinformationaction -invoke remove-azsqlserverfirewallrule -times 1 -exactly } "test non-existing rule" { # firewallrulename exist, function remove-azsqlserverfirewallrule called remove-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "another rule" ` -informationaction $pesterinformationaction -invoke remove-azsqlserverfirewallrule -times 0 -exactly } } # group: existing firewall rules context "no existing rules" { # test result existing firewall rules beforeall { # mock function az.sql module mock remove-azsqlserverfirewallrule {} # mock function az.sql module # function get-azsqlserverfirewallrule gets called, return empty object-list mock get-azsqlserverfirewallrule { return $null } } "test non-existing rule" { remove-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "another rule" ` -informationaction $pesterinformationaction -invoke remove-azsqlserverfirewallrule -times 0 -exactly } } } }</file><file name="src\powershell\modules\Networking\public\Set-NetworkingSqlServerFirewallRule.ps1">&lt;# .synopsis wrapper around new-azsqlserverfirewallrule set-azsqlserverfirewallrule simplify "create exists" .description function add functionalities creating firewall rule sql server specific resource group * create list existing firewall rules * create requested firewall rule exist already * update requested firewall rule already exist .example ps&gt; set-networkingsqlserverfirewallrule -servername "myserver" resourcegroupname "my-rg" -rulename "rule0" -startipaddress "10.0.0.0" -endipaddress "10.0.255.255" #&gt; function set-networkingsqlserverfirewallrule { [cmdletbinding(supportsshouldprocess)] param ( [parameter( mandatory = $true, helpmessage='name resource group sql server located')] [validatenotnullorempty()] [string] $resourcegroupname, [parameter( mandatory = $true, helpmessage="name sql server (xxx), full host server (xxx.database.windows.net)")] [validatenotnullorempty()] [string] $servername, [parameter( mandatory = $true, helpmessage="reference name to-be created firewall rule")] [validatenotnullorempty()] [string] $firewallrulename, [parameter( mandatory = $true, helpmessage="start ip-adress range needs added (e.g. xxx.0.0.0)")] [validatenotnullorempty()] [string] $startipaddress, [parameter( mandatory = $true, helpmessage="end ip-adress range needs added (e.g. xxx.0.255.255)")] [validatenotnullorempty()] [string] $endipaddress ) ## start region: list script name parameters $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" #region print parameters $inputparameters = @( "resourcegroupname" "servername" "firewallrulename" "startipaddress" "endipaddress" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "starting script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } #endregion # get list existing firewall rules sql server $existingrules = @( get-azsqlserverfirewallrule ` -resourcegroupname $resourcegroupname ` -servername $servername ` -whatif:$false # completeness, necessary add parameter ) # create empty array firewall rules exist (-not $existingrules) { write-information "found existing firewall rules" $existingrule = @() } # create list existing firewall rules existing ones # filter list firewallrulename needs added else { write-information "found $($existingrules.count) existing firewall rule(s)" $existingrules.firewallrulename | write-verbose $existingrule = $existingrules | where-object {$_.firewallrulename -eq $firewallrulename} } # existing firewall rule name: update if( $existingrule) { # rule exists, update different if( ( $existingrule.startipaddress -eq $startipaddress ) -and ( $existingrule.endipaddress -eq $endipaddress) ) { write-information "rule already exists definition" } else { # called: process initiated. # satefy precaution if($pscmdlet.shouldprocess("$servername", "update existing firewallrule")) { write-information "updating existing rule ..." $null = set-azsqlserverfirewallrule ` -resourcegroupname $resourcegroupname ` -servername $servername ` -firewallrulename $firewallrulename ` -startipaddress $startipaddress ` -endipaddress $endipaddress } } } # existing firewall rule name: create else { # rule exist, create # called: process initiated. # satefy precaution if($pscmdlet.shouldprocess("$servername", "create new firewallrule")) { write-information "creating new rule..." $null = new-azsqlserverfirewallrule ` -resourcegroupname $resourcegroupname ` -servername $servername ` -firewallrulename $firewallrulename ` -startipaddress $startipaddress ` -endipaddress $endipaddress } } }</file><file name="src\powershell\modules\Networking\public\Set-NetworkingSqlServerFirewallRule.Tests.ps1">beforeall { # import function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } $script:debugpreference = $true set-strictmode -version latest } describe "set-networkingsqlserverfirewallrule" { # general context expected pester module context "unittests" { # group: multiple mocked firewall rules context "multiple existing rules" { # test result multiple existing firewall rules beforeall { # mock functions az.sql module mock new-azsqlserverfirewallrule {} mock set-azsqlserverfirewallrule {} # mock function az.sql module # function get-azsqlserverfirewallrule gets called, return 2 objects defined mock get-azsqlserverfirewallrule { $rule1 = new-object -typename "microsoft.azure.commands.sql.firewallrule.model.azuresqlserverfirewallrulemodel" $rule1.firewallrulename = "testrule1" $rule1.startipaddress = "startip" $rule1.endipaddress = "endip" $rule2 = new-object -typename "microsoft.azure.commands.sql.firewallrule.model.azuresqlserverfirewallrulemodel" $rule2.firewallrulename = "testrule2" $rule2.startipaddress = "startip" $rule2.endipaddress = "endip" return @( $rule1, $rule2 ) } } "test existing rule" { # firewallrulename exists, function set-azsqlserverfirewallrule new-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule1" ` -startipaddress "startip" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 0 -exactly -invoke set-azsqlserverfirewallrule -times 0 -exactly } "test new rule" { # firewallrulename exist, function new-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "another rule" ` -startipaddress "startip" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 1 -exactly -invoke set-azsqlserverfirewallrule -times 0 -exactly } "test updated rule" { # firewallrulename exists, function set-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule1" ` -startipaddress "something else" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 0 -exactly -invoke set-azsqlserverfirewallrule -times 1 -exactly } } # group: add one firewall rule mock context "single existing rule" { # test result 1 existing firewall rule beforeall { # mock functions az.sql module mock new-azsqlserverfirewallrule {} mock set-azsqlserverfirewallrule {} # mock function az.sql module # function get-azsqlserverfirewallrule gets called, return object defined mock get-azsqlserverfirewallrule { $rule = new-object -typename "microsoft.azure.commands.sql.firewallrule.model.azuresqlserverfirewallrulemodel" $rule.firewallrulename = "testrule1" $rule.startipaddress = "startip" $rule.endipaddress = "endip" return $rule } } "test existing rule" { # firewallrulename exists, function set-azsqlserverfirewallrule new-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule1" ` -startipaddress "startip" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 0 -exactly -invoke set-azsqlserverfirewallrule -times 0 -exactly } "test new rule" { # firewallrulename exist, function new-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "another rule" ` -startipaddress "startip" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 1 -exactly -invoke set-azsqlserverfirewallrule -times 0 -exactly } "test updated rule" { # firewallrulename exists, function set-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule1" ` -startipaddress "something else" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 0 -exactly -invoke set-azsqlserverfirewallrule -times 1 -exactly } } # group: existing rules context "no existing rules" { # test result existing firewall rules beforeall { # mock functions az.sql module mock new-azsqlserverfirewallrule {} mock set-azsqlserverfirewallrule {} # mock function az.sql module # function get-azsqlserverfirewallrule gets called, return empty object-list mock get-azsqlserverfirewallrule { return $null } } "test new rule" { # rule exist, new-azsqlserverfirewallrule called set-networkingsqlserverfirewallrule ` -resourcegroupname "dummyresourcegroup" ` -servername "dummyserver" ` -firewallrulename "testrule" ` -startipaddress "startip" ` -endipaddress "endip" ` -informationaction $pesterinformationaction -invoke new-azsqlserverfirewallrule -times 1 -exactly -invoke set-azsqlserverfirewallrule -times 0 -exactly } } } }</file><file name="src\powershell\modules\TokenModule\.build\build.ps1">&lt;# .synopsis module contains functions tokenization detokenization .description tokenization practice adding meaningless string-values larger string (text, json...). values called tokens, start end specific (set of) character(s). detokenization practice replacing tokens defined correct value depending certain parameters/variables/conditions... make practice tokenization little easier, module contains set functions called easily create replace tokens. #&gt; [cmdletbinding()] param ( [parameter(helpmessage = "name module")] [string] $modulename = "tokenmodule", [parameter(helpmessage = "version number major.minor.path format")] [string] $buildversion, [parameter(helpmessage = "base directory module folder")] [string] $workingdir, [parameter(helpmessage = "directory localfeed psrepository")] [string] $localfeedpath, [parameter(helpmessage = "set switch enable cleaning module folder packaging")] [switch] $cleanfolder ) #local settings set-strictmode -version "latest" #requires -version 7.2 # --------------------------------------------------------------------------------------------------------- # write information script user $scriptname = $script:myinvocation.mycommand.path write-output "*** starting script: $scriptname ***" # start region: print variables $inputparameters = @( "modulename" "buildversion" "workingdir" "localfeedpath" ) # $inputparameters = $myinvocation.mycommand.parameters.keys dynamic, includes standard ones (debug, verbose, *action, ...) # print list input parameters $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "start script: $($myinvocation.mycommand.name) parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "psversion: $($psversiontable.psversion.tostring())" write-information "----------------------------------------------------------------------" #endregion script info # --------------------------------------------------------------------------------------------------------- # --------------------------------------------------------------------------------------------------------- write-output "" write-output "package providers:" get-packageprovider write-output "----------------------------------------------------------------------" #unload module currently loaded $currentmodules = get-module -name $modulename -erroraction "ignore" if( $currentmodules) { write-output "unloading currently loaded '$($modulename)' modules ..." $currentmodules.version | foreach-object { write-information " $_"} $currentmodules | remove-module | out-null } else { write-output "no currently loaded '$($modulename)' modules (ok)" } #uninstall package currently installed $currentpackages = get-package -name $modulename -erroraction "ignore" if( $currentpackages) { write-output "uninstalling currently installed packages ..." $currentpackages.version | foreach-object { write-information " $_"} $currentpackages | uninstall-package | out-null } else { write-output "no currently installed '$($modulename)' packages (ok)" } #build check path module folder $modulepath = join-path $workingdir $modulename write-information "" write-information "using modulepath: $modulepath" ( -not (test-path $modulepath -pathtype "container") ) { throw "module $($modulename) subfolder '$($workingdir)'" } else { write-information " modulepath: ok, folder exists" $modulepath = (resolve-path $modulepath).path write-information " resolved to: $($modulepath)" } #build check path module manifest $psdfile = $modulename + ".psd1" write-information "using psdfile: $psdfile" $manifestpath = join-path $modulepath $psdfile write-information "using manifestpath: $manifestpath" (-not (test-path $manifestpath) ) { throw "manifest exist at: $manifestpath" } #region prepare localfeed psrepository write-output "preparing localfeed psrepository ..." #check resolve localfeedpath (-not (test-path $localfeedpath)) { new-item -path $localfeedpath -itemtype "directory" | out-null } $localfeedpathresolved = ($localfeedpath | resolve-path -erroraction ignore) ( -not $localfeedpathresolved ) { throw "could resolve localfeedpath: '$localfeedpath'" } else { $localfeedpathresolved = $localfeedpathresolved.path write-information " resolved localfeedpath to: $localfeedpathresolved" } #check create "localfeed" psrepository $localfeed = get-psrepository -name localfeed -erroraction "ignore" ( $localfeed ) { write-output " found localfeed psrepository" # $localfeed # $localfeed.scriptsourcelocation # $localfeed.scriptpublishlocation # write-output "(members)" # $localfeed | get-member } else { write-output " localfeed psrepository found" } ( $localfeed -and ($localfeed.sourcelocation -ne $localfeedpathresolved) ) { write-warning " localfeed repository wrong path ($($localfeed.sourcelocation)), removing ..." #write-output "unregister existing localfeed repository ..." unregister-psrepository -name "localfeed" $localfeed = $null } ( -not $localfeed) { write-output " registering 'localfeed' psrepository path: $localfeedpathresolved ..." register-psrepository -name localfeed -sourcelocation $localfeedpathresolved -publishlocation $localfeedpathresolved -installationpolicy trusted } else { write-output " psrepository 'localfeed' already present path: $localfeedpathresolved" } # remove existing package current version (main avoid conflict (re)building version 0.0.0 locally) $existingpackages = get-childitem -path $localfeedpathresolved -filter "${modulename}.${buildversion}.nupkg" if( -not $existingpackages) { write-information " deleting existing package files : none found [ok]" } else { write-information " deleting existing package files ..." $existingpackages.fullname | foreach-object { write-information " $_"} $existingpackages | remove-item | out-null } #endregion #region create manifest write-output "" write-output "prepare module manifest..." # remove files required nuget package ( $cleanfolder) { write-information " remove unneeded files nuget package..." get-childitem -path $modulepath -include "*.tests.ps1" -recurse | remove-item get-childitem -path $modulepath -filter ".build" -directory | remove-item -recurse get-childitem -path $modulepath -filter ".test_config" -directory | remove-item -recurse get-childitem -path $modulepath -filter "coverage.xml" | remove-item get-childitem -path $modulepath -filter "testresults_unit.xml" | remove-item } else { write-information " (...skipping cleanfolder section...)" } # update build version manifest write-information " updating version number manifest..." $manifestcontent = get-content -path $manifestpath -raw $manifestcontent = $manifestcontent -replace '&lt;moduleversion&gt;', $buildversion # find public functions add cmdlets $publicfuncfolderpath = join-path $modulepath public $itemparams = @{ path = $publicfuncfolderpath file = $true recurse = $true filter = '*.ps1' exclude = '*.tests.ps1' } ((test-path -path $publicfuncfolderpath) -and ($publicfunctionnames = get-childitem @itemparams | select-object -expandproperty basename)) { $funcstrings = "'$($publicfunctionnames -join "',$([system.environment]::newline) '")'" $funcstrings = $([system.environment]::newline) + " " + $funcstrings + $([system.environment]::newline) } else { $funcstrings = $null } ## add public functions functionstoexport attribute $manifestcontent = $manifestcontent -replace "'&lt;functionstoexport&gt;'", $funcstrings write-information " " write-information " functions export:" $funcstrings | foreach-object { write-output " $_" } ## save updated manifest $manifestcontent | set-content -path "$manifestpath" write-information " updated manifest: $($manifestpath)" #endregion # create nuget package write-output "" write-output "create publish nuget package localfeed repository..." publish-module -path $modulepath -repository localfeed -nugetapikey 'dummy' -force # list available packages write-output "" write-output "list available packages localfeed repository..." #find-package -source "localfeed" | select-object version, providername, source, fullpath find-package -source "localfeed" | foreach-object {write-output " $($_.version) -- $($_.providername)"} # install package write-output "" write-output "install module localfeed..." install-package -name $modulename -source "localfeed" -providername "powershellget" -force | foreach-object {write-output " $($_.version)"} # review installed modules $modules = get-module -name $modulename -listavailable | select-object name, version, repositorysourcelocation, path write-output "" write-output "available module versions:" $modules | foreach-object {write-output " $($_.version)"} # check "current" version write-output "" write-output "check current version available..." $currentmodule = $modules | where-object {$_.version.tostring() -eq $buildversion } ( -not $currentmodule) { throw "module version '$buildversion' found available modules" } else { write-information " found available module '$($modulename)' version '$($buildversion)' (ok)" } # import module write-output "" write-output "import module ..." import-module -name $modulename get-module -name $modulename | foreach-object {write-output " $($_.version)"}</file><file name="src\powershell\modules\TokenModule\.build\unit_tests.ps1">[cmdletbinding()] param ( ) # -------------------------------------------------------------- # section: prerequisites # -------------------------------------------------------------- #requires -modules az.keyvault #requires -modules az.accounts #requires -modules az.resources #requires -modules az.synapse #requires -modules @{ modulename="pester"; moduleversion="5.5.0" } # install pester needed # note joachim: actually done upfront, simple use 'requires' statement script # write-output "available pester versions:" # $pesterrequiredversion = "5.5" # $pestercheckregex = "^" + $pesterrequiredversion.replace(".", "\.") # (get-module pester -listavailable).path | foreach-object {" $_"} # if(-not (get-module pester -listavailable | where-object version -match $pestercheckregex)){ # write-output "installing pester $pesterrequiredversion ..." # install-module -name pester -requiredversion $pesterrequiredversion -force -scope currentuser -allowclobber # } else { # write-output "pester 5.5 already installed" # } # load correct one #import-module pester -minimumversion $pesterrequiredversion -maximumversion "$pesterrequiredversion.999999" # sure write-output "" write-output "using pester version: $((get-module pester).version)" write-output "" write-output "module test:" $modulepath = resolve-path ( join-path "$psscriptroot" "..") $modulename = (split-path $modulepath -leaf) + '.psm1' write-output " modulepath: $modulepath" write-output " modulename: $modulename" # preload module first time import-module -name (join-path $modulepath $modulename ) -force # -------------------------------------------------------------- # section: pester configuration # -------------------------------------------------------------- # start default configuration $pesterconfig = [pesterconfiguration]::default # create testresults.xml output $pesterconfig.testresult.enabled = $true $pesterconfig.testresult.outputformat = "nunitxml" $pesterconfig.testresult.outputpath = join-path $modulepath "testresults_unit.xml" # create code coverage output $pesterconfig.codecoverage.enabled = $true $pesterconfig.codecoverage.outputformat = "jacoco" $pesterconfig.codecoverage.outputpath = join-path $modulepath "coverage.xml" $pesterconfig.codecoverage.path = "$modulepath/[p]*/*.ps1" # public, private # filters (the fullname filter powershell "like" statement, regex) # important: fullname tag filters work combined "or", "and" $pesterconfig.filter.fullname = "*.unittests.*" $pesterconfig.filter.tag = 'unittest' $pesterconfig.filter.excludetag = "draft" # run parameters $pesterconfig.run.path = join-path $modulepath "public" $pesterconfig.run.exit = $false $pesterconfig.run.passthru = $false # -------------------------------------------------------------- # section: run tests # -------------------------------------------------------------- write-output "run tests..." invoke-pester -configuration $pesterconfig</file><file name="src\powershell\modules\TokenModule\public\Get-DetokenizedString.ps1">&lt;# .synopsis replaces tokenized values using value variables name. .description variable set, function throws error removes token (see second example below). function first looks session variables, none found, look environment variables limitations version: * throw (non-terminating) error variable found keep token todo: * support parameter define behaviour missing variable (error/warning/none, stop/keep/drop token) * allow custom token identifiers (i.e. something else []) .parameter stringvalue string tokens need replaced .parameter starttoken opening string defining start token, default: "[" .parameter endtoken closing string defining start token, default: "]" .example ps&gt;$tokenizedtest = "this [modifier] text." ps&gt;$modifier = "very short" ps&gt;$result = get-detokenizedstring $tokenizedtest ps&gt;write-output $result short text. .example ps&gt;$tokenizedtest = "this [modifier] text [missing] variable." ps&gt;$result = get-detokenizedstring $tokenizedtest replacetokenswithvariables: could find replacement variable literal: [missing] ps&gt;write-output $result short text variable. #&gt; function get-detokenizedstring { [cmdletbinding()] param ( [parameter(mandatory=$true, position=0)] [string]$stringvalue, [parameter(mandatory=$false)] [string]$starttoken="[", [parameter(mandatory=$false)] [string]$endtoken="]", [parameter(mandatory=$false)] [validateset('error','warning')] [string]$missingvariableaction="error" ) # search pattern: '&lt;starttoken&gt;&lt;unlimitedsetofcharacters&gt;&lt;endtoken&gt;' $regex = [system.text.regularexpressions.regex]::escape($starttoken) + '(\w+)' + [system.text.regularexpressions.regex]::escape($endtoken) write-debug "regex: $regex" write-debug "stringvalue: $stringvalue" $regexmatches = [regex]::matches($stringvalue,$regex) foreach ( $match $regexmatches) { write-debug " match: $match" $toreplace = $match.groups[0].value write-debug (" look variable: " + '$' + $match.groups[1].value) # look session variable lookup name need, look env: variables $replacevar = (get-childitem variable: | where-object {$_.name -eq $match.groups[1].value}) $replacevar = $replacevar ?? (get-childitem env: | where-object {$_.name -eq $match.groups[1].value}) ( -not $replacevar -or ($null -eq $replacevar.value)) { switch ( $missingvariableaction) { "error" { write-error "could find replacement variable literal: $toreplace" } "warning" { write-warning "could find replacement variable literal: $toreplace" } } } else { $stringvalue = $stringvalue.replace($toreplace,$replacevar.value) write-verbose " replaced '$toreplace' '$($replacevar.value)'" } } return $stringvalue }</file><file name="src\powershell\modules\TokenModule\public\Get-DetokenizedString.Tests.ps1">beforeall { # import function test . $pscommandpath.replace('.tests.ps1', '.ps1') # informationaction preference tests ( test-path env:pester_information_action ) { $script:pesterinformationaction = $env:pester_information_action } else { $script:pesterinformationaction = $informationpreference } set-strictmode -version latest } describe "get-detokenizedstring" { $tokens = @( @{starttoken = '[' ; endtoken = ']'}, @{starttoken = '#{'; endtoken = '}#'} ) context "unittests" { context "&lt;starttoken&gt;&lt;endtoken&gt;" -foreach $tokens { context "basic tests" { beforeall { $script:test_var = "basic" } "word replacement" { $teststring = "this ${starttoken}test_var${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic test" } "part word replacement" { $teststring = "this a${starttoken}test_var${endtoken}test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this abasictest" } "double variable" { $teststring = "this ${starttoken}test_var${endtoken} ${starttoken}test_var${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic basic test" } "case insensitve" -testcases $tokens { $teststring = "this ${starttoken}test_var${endtoken} test" $test_var = "basic" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic test" $teststring = "this ${starttoken}test_var${endtoken} test" $test_var = "basic" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic test" #just get rid unused variable warnings $null = $test_var } "double token" { $teststring = "this ${starttoken}${starttoken}test_var${endtoken}${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this ${starttoken}basic${endtoken} test" } "multiple variables test" { $test_var1 = "very" $test_var2 = "basic" $teststring = "this ${starttoken}test_var1${endtoken} ${starttoken}test_var2${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic test" #just get rid unused variable warnings $null = $test_var1 + $test_var2 } "empty variable test" { $test_var1 = "very" $test_var2 = "" $teststring = "this ${starttoken}test_var1${endtoken} ${starttoken}test_var2${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this test" #just get rid unused variable warnings $null = $test_var1 + $test_var2 } "missing variable test" { $test_var1 = "very" $test_var2 = $null $teststring = "this ${starttoken}test_var1${endtoken} ${starttoken}test_var2${endtoken} test" #should throw terminating error { get-detokenizedstring -stringvalue $teststring -erroraction silentlycontinue -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction } | -not -throw #should raise non-terminating error get-detokenizedstring -stringvalue $teststring -erroraction silentlycontinue -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction -errorvariable err $err.count | -be 1 $err[0].exception.message | -be "could find replacement variable literal: ${starttoken}test_var2${endtoken}" #should keep token $result = get-detokenizedstring -stringvalue $teststring -erroraction silentlycontinue -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this ${starttoken}test_var2${endtoken} test" #just get rid unused variable warnings $null = $test_var1 + $test_var2 } "environment variables test" { $env:test_var1 = "very" $test_var2 = "basic" $teststring = "this ${starttoken}test_var1${endtoken} ${starttoken}test_var2${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic test" #just get rid unused variable warnings $null = $env:test_var1 + $test_var2 $env:test_var1 = $null } "session environment variables overlap test" { $env:test_var1 = "very" $test_var1 = "basic" $teststring = "this ${starttoken}test_var1${endtoken} test" $result = get-detokenizedstring -stringvalue $teststring -starttoken $starttoken -endtoken $endtoken -informationaction $pesterinformationaction $result | -be "this basic test" #just get rid unused variable warnings $null = $test_var1 + $env:test_var1 $env:test_var1 = $null } } } } }</file><file name="src\sql\metadb\functions\meta.fn_add_json_property.sql">create function [meta].[fn_add_json_property] ( @json [nvarchar](max),@property_path [nvarchar](100),@property_value [nvarchar](max) ) returns nvarchar(max) begin /* helper function, wrapping around json_modify() build json values. expects @json input existing single json object, null/empty string. * replace null empty string empty object {}, add update property * allow adding null properties * allow implicit path ("$.") future enhancement could automatically build missing elements path @property_value. example, path '$.object.property' could automaticlaly add 'object' exist, creating 'property'. note: function cannot throw exceptions. errors detected input parameters, string error message returned like "error: valid json". return value valid json. detected (trying to) store json type variable, checking varchar value isjson(). example usage: select meta.fn_add_json_property('{"property": "value"}','foo','bar'); returns: { "property": "value", "foo": "bar" } -- capture errors "json" type variable, case exception declare @result json; select @result = meta.fn_add_json_property('invalid','foo','bar'); result: msg 13609, level 16, state 9, line 2 json text properly formatted. unexpected character 'e' found position 0. -- capture errors isjson() check declare @result nvarchar(max); select @result = meta.fn_add_json_property('invalid','foo','bar'); select isjson(@result) valid_json; result: 0 credits trick add null properties from: https://stackoverflow.com/questions/59038499/add-new-key-with-value-null-to-existing-json-object first json_modify adds key empty string value, second one updates it, set null */ -- check incoming json declare @startjson json @json null trim(@json) = '' set @startjson = '{}'; else begin (isjson(@json) = 1) --cant throw exceptions functin we'll return something invalid return 'error: provided value valid json'; (isjson(@json, object) = 1) return 'error: provided value valid json object'; set @startjson = @json; end -- check @property_name value --do nothing property_name null @property_path null return @json; --assume root property property_name valid path @property_path like '$.%' set @property_path = '$.' + @property_path; --check property_value json ? declare @isjson bit = isjson(@property_value) --first modify adds property empty string value set @json = json_modify( -- inner json_modify adds key empty string value isnull(nullif(@json,''),'{}'), 'lax ' + @property_path, '' ) --second modify updates property real value, including null --if @property_value string contains json, consider isjson(@property_value) = 1 set @json = json_modify ( @json, 'strict ' + @property_path, json_query(@property_value) ); --if json, add string else set @json = json_modify ( @json, 'strict ' + @property_path, @property_value ); return @json; end</file><file name="src\sql\metadb\roles\role_ais_integration.sql">-- ====================== -- role creation -- ====================== create role [role_ais_integration] authorization dbo go -- ====================== -- permissions -- ====================== grant exec meta.usp_ais_log_landing role_ais_integration -- "as dbo" avoids drop/recreate go grant exec meta.usp_ais_get_landing_configuration role_ais_integration -- "as dbo" avoids drop/recreate go grant select meta.ais_landing_configuration role_ais_integration -- "as dbo" avoids drop/recreate go grant select meta.ais_landing_log role_ais_integration -- "as dbo" avoids drop/recreate</file><file name="src\sql\metadb\roles\role_ais_integration_admin.sql">-- ====================== -- role creation -- ====================== create role [role_ais_integration_admin] authorization dbo go -- ====================== -- permissions -- ====================== grant insert,update,delete meta.ais_landing_configuration role_ais_integration_admin dbo -- "as dbo" avoids drop/recreate go grant insert,update,delete meta.ais_landing_log role_ais_integration_admin dbo -- "as dbo" avoids drop/recreate go</file><file name="src\sql\metadb\roles\role_dap_core_engineer.sql">-- ====================== -- role creation -- ====================== create role [role_dap_core_engineer] authorization dbo go -- ====================== -- permissions -- ====================== -- impersonate grant impersonate user::[dbo] [role_dap_core_engineer] go -- execute grant execute [role_dap_core_engineer] go -- create table grant create table [role_dap_core_engineer] go -- alter grant alter [role_dap_core_engineer] go -- view definition grant view definition [role_dap_core_engineer] go -- update, insert, delete grant update, insert, delete [role_dap_core_engineer] go -- ====================== -- add roles to... -- ====================== -- add role db_datareader alter role db_datareader add member [role_dap_core_engineer] go</file><file name="src\sql\metadb\roles\role_dap_ingestion_engineer.sql">-- ====================== -- role creation -- ====================== create role [role_dap_ingestion_engineer] authorization dbo go -- ====================== -- permissions -- ====================== -- view definition grant view definition [role_dap_ingestion_engineer] go -- ====================== -- add roles to... -- ====================== -- add role db_datareader alter role db_datareader add member [role_dap_ingestion_engineer] go</file><file name="src\sql\metadb\roles\role_syn_integration.sql">/* role synapse managed identity (xxx-syn-core xxx-syn-interf) run meta stored procedures needed. */ create role [role_syn_integration] authorization dbo go -- dev info: "as dbo" avoids drop/recreate deploying dacpac grant exec schema::meta role_syn_integration dbo go</file><file name="src\sql\metadb\schemas\deploy.sql">-- create deploy-schema sql create schema [deploy];</file><file name="src\sql\metadb\schemas\meta.sql">-- create meta-schema sql create schema [meta];</file><file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_plan_configuration.sql">create procedure [deploy].[usp_set_plan_configuration] @plan_name varchar(100), @plan_description varchar(500), @enabled bit begin -- 1: quality checks: make sure necessary values null @plan_name null ltrim(rtrim(@plan_name)) = '' throw 50000, 'plan_name cannot null empty', 1; @plan_description null ltrim(rtrim(@plan_description)) = '' throw 50000, 'plan_description cannot null empty', 1; @enabled null throw 50000, 'enabled cannot null', 1; -- 2. merge data meta.plan_configuration table input_data ( select ltrim(rtrim(@plan_name)) plan_name, ltrim(rtrim(@plan_description)) plan_description, @enabled [enabled] ) merge meta.plan_configuration using input_data ( t.plan_name = s.plan_name ) matched ( t.enabled &lt;&gt; s.enabled t.plan_description &lt;&gt; s.plan_description ) update set t.enabled = s.enabled, t.plan_description = s.plan_description matched target insert (plan_name, plan_description, [enabled]) values (s.plan_name, s.plan_description, s.enabled) output $action [action], isnull(inserted.plan_name, deleted.plan_name) plan_name, isnull(inserted.plan_description, deleted.plan_description) plan_description, isnull(inserted.enabled, deleted.enabled) [enabled] ; end go</file><file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_plan_task_configuration.sql">create procedure [deploy].[usp_set_plan_task_configuration] @plan_name varchar(100), @plantask_data nvarchar(max) begin -- 1. quality checks: make sure necessary values null @plan_name null ltrim(rtrim(@plan_name)) = '' throw 50000, 'plan_name cannot null empty', 1; -- 2. merge data meta.plan_task_configuration table -- 2.1. create empty table expected schema declare @input_data table ( [plan_name] varchar(100) null, [task_group] varchar(100) null, [task_name] varchar(100) null, [task_sequence] int null, [enabled] bit null, primary key ( [task_group], [task_name] ) ); -- 2.2. insert data empty table -- dev-info: json-object (@checks_data) passed stored procedure -- individual keys-values passed sql table columns openjson -- note approach slightly different from, example, -- stored procedure deploy.usp_set_plan_configuration json-object insert @input_data select @plan_name, nullif(ltrim(rtrim(task_group)),''), nullif(ltrim(rtrim(task_name)),''), task_sequence, [enabled] openjson ( @plantask_data) ( task_group varchar(100) '$.task_group', task_name varchar(100) '$.task_name', task_sequence int '$.task_sequence', [enabled] bit '$.enabled' ); -- 2.3. merge input_data table meta.plan_task_configuration table merge meta.plan_task_configuration using @input_data ( t.plan_name = s.plan_name t.task_group = s.task_group t.task_name = s.task_name ) matched ( t.task_sequence &lt;&gt; s.task_sequence --not null t.enabled &lt;&gt; s.enabled --not null ) update set t.task_sequence = s.task_sequence, t.enabled = s.enabled matched target insert ( plan_name, task_sequence, task_name, task_group, [enabled]) values (s.plan_name, s.task_sequence, s.task_name, s.task_group, s.enabled) matched source t.plan_name = @plan_name delete output $action action, isnull(inserted.plan_name, deleted.plan_name) plan_name, isnull(inserted.task_group, deleted.task_group) task_group, isnull(inserted.task_name, deleted.task_name) task_name, isnull(inserted.task_name, deleted.task_name) is_enabled ; end go</file><file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_source_check_configuration.sql">create procedure [deploy].[usp_set_source_check_configuration] @file_layout varchar(100), -- name dataset @checks_data nvarchar(max) -- array json objects datachecks begin -- 1. quality checks: make sure necessary values null @file_layout null ltrim(rtrim(@file_layout)) = '' throw 50406, 'file_layout cannot null empty', 406 -- 406: acceptable -- 2. merge data meta.source_check_configuration table -- 2.1. create empty table expected schema declare @input_data table ( check_name varchar(50) null, file_layout varchar(100) null, [enabled] bit null, config_params varchar(max) null primary key (check_name, file_layout) ); -- 2.2. insert data empty table -- dev-info: json-object (@checks_data) passed stored procedure -- individual keys-values passed sql table columns openjson -- note approach slightly different from, example, -- stored procedure deploy.usp_set_plan_configuration json-object insert @input_data select nullif(ltrim(rtrim(check_name)),''), @file_layout, [enabled], config_params openjson (@checks_data) ( check_name varchar(50) '$.name', [enabled] bit '$.enabled', config_params varchar(max) '$.config_params' ); -- 2.3. merge input_data table meta.source_check_configuration table merge meta.source_check_configuration using @input_data on( t.file_layout = s.file_layout t.check_name = s.check_name t.enabled = s.enabled t.config_params = s.config_params ) matched and( (isnull( nullif(t.config_params, s.config_params), nullif(s.config_params, t.config_params) ) null) ) update set t.check_name = s.check_name, t.enabled = s.enabled, t.config_params = s.config_params matched target insert( check_name, file_layout, [enabled], config_params ) values (s.check_name, s.file_layout, s.enabled, s.config_params) matched source t.file_layout = @file_layout delete -- table returned show done output $action action, -- delete insert? isnull(inserted.check_name, deleted.check_name) check_name, isnull(inserted.file_layout, deleted.file_layout) file_layout, isnull(inserted.enabled, deleted.enabled) [enabled], isnull(inserted.config_params, deleted.config_params) config_params; end go</file><file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_source_column_configuration.sql">create procedure [deploy].[usp_set_source_column_configuration] @file_layout varchar(100), @column_data nvarchar(max) begin @file_layout null ltrim(rtrim(@file_layout)) = '' throw 50000, 'file_layout cannot null empty', 1; -- 2. merge data meta.source_column_configuration table -- 2.1. create empty table expected schema declare @input_data table ( file_layout varchar(100) null, column_sequence int null primary key, source_column_name varchar(100) null, sink_column_name varchar(100) null , dimension nvarchar(5) null, data_type nvarchar(20) null, column_info nvarchar(max) null ); -- 2.2. insert data empty table -- dev-info: json-object (@checks_data) passed stored procedure -- individual keys-values passed sql table columns openjson -- note approach slightly different from, example, -- stored procedure deploy.usp_set_plan_configuration json-object insert @input_data select @file_layout, column_sequence, nullif(ltrim(rtrim(source_column_name)),''), nullif(ltrim(rtrim(sink_column_name)),''), nullif(ltrim(rtrim(dimension)),''), nullif(ltrim(rtrim(data_type)),''), nullif(ltrim(rtrim(column_info)),'') openjson ( @column_data) ( column_sequence int '$.column_sequence' , source_column_name varchar(100) '$.source_column_name', sink_column_name varchar(100) '$.sink_column_name', dimension varchar(100) '$.dimension', data_type varchar(20) '$.data_type', column_info varchar(max) '$.column_info' ); -- 2.3. merge input_data table meta.source_column_configuration table merge meta.source_column_configuration using @input_data ( t.file_layout = s.file_layout t.column_order = s.column_sequence ) matched ( -- looks complicated works nullable columns: true null different one null, null (isnull( nullif(t.source_name, s.source_column_name), nullif(s.source_column_name, t.source_name) ) null) (isnull( nullif(t.sink_name, s.sink_column_name), nullif(s.sink_column_name, t.sink_name) ) null) (isnull( nullif(t.dimension, s.dimension), nullif(s.dimension, t.dimension) ) null) (isnull( nullif(t.data_type, s.data_type), nullif(s.data_type, t.data_type) ) null) (isnull( nullif(t.column_info, s.column_info), nullif(s.column_info, t.column_info) ) null) ) update set t.source_name = s.source_column_name, t.sink_name = s.sink_column_name, t.column_order = s.column_sequence, t.dimension = s.dimension, t.data_type = s.data_type, t.column_info = s.column_info matched target insert ( file_layout, column_order, source_name, sink_name, dimension, data_type, column_info) values ( s.file_layout, s.column_sequence, s.source_column_name, s.sink_column_name, s.dimension, s.data_type, s.column_info) matched source t.file_layout = @file_layout delete output $action action, isnull(inserted.file_layout, deleted.file_layout) file_layout, isnull(inserted.column_order, deleted.column_order) column_order, isnull(inserted.source_name, deleted.source_name) source_name, isnull(inserted.sink_name, deleted.sink_name) sink_name, isnull(inserted.dimension, deleted.dimension) dimension, isnull(inserted.data_type, deleted.data_type) data_type, isnull(inserted.column_info, deleted.column_info) column_info ; end go</file><file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_source_configuration.sql">create procedure [deploy].[usp_set_source_configuration] @file_layout varchar(100), @file_pattern varchar(100), @file_kind varchar(10), @file_extension varchar(10), @column_delimiter varchar(5), @row_delimiter varchar(5), @escape_character varchar(5), @quote_character varchar(5), @header bit, @encoding varchar(10), @skip_first_lines integer, @source_conditions nvarchar(max) begin -- 1. quality checks: make sure necessary values null @file_layout null ltrim(rtrim(@file_layout)) = '' throw 50000, 'file_layout cannot null empty', 1; @file_pattern null ltrim(rtrim(@file_pattern)) = '' throw 50000, 'file_pattern cannot null empty', 1; --prevent error adf: quote_character specified, escape_character also required isnull(ltrim(rtrim(@quote_character)),'') &lt;&gt; '' isnull(ltrim(rtrim(@escape_character)),'') = '' throw 50000, 'if quote_character specified, escape_character cannot null empty', 1; -- 2. merge data meta.source_configuration table input_data ( select ltrim(rtrim(@file_layout)) file_layout, ltrim(rtrim(@file_pattern)) file_pattern, ltrim(rtrim(@file_kind)) file_kind, ltrim(rtrim(@file_extension)) file_extension, ltrim(rtrim(@column_delimiter)) column_delimiter, ltrim(rtrim(@row_delimiter)) row_delimiter, nullif(ltrim(rtrim(@escape_character)),'') escape_character, nullif(ltrim(rtrim(@quote_character)),'') quote_character, case @header = 1 'true' else 'false' end header, nullif(ltrim(rtrim(@encoding)),'') [encoding], nullif(ltrim(rtrim(@skip_first_lines)),'') skip_first_lines, nullif(ltrim(rtrim(@source_conditions)),'') source_conditions ) merge meta.source_configuration using input_data ( t.file_layout = s.file_layout ) matched ( -- looks complicated works nullable columns: true null different one null, null -- (isnull(nullif(x,y),nullif(y,x)) null) reacts " distinct " (to refactored " distinct " ) (isnull( nullif(t.file_pattern, s.file_pattern), nullif(s.file_pattern, t.file_pattern) ) null) (isnull( nullif(t.file_kind, s.file_kind), nullif(s.file_kind, t.file_kind) ) null) (isnull( nullif(t.file_extension, s.file_extension), nullif(s.file_extension, t.file_extension) ) null) (isnull( nullif(t.column_delimiter, s.column_delimiter), nullif(s.column_delimiter, t.column_delimiter) ) null) (isnull( nullif(t.row_delimiter, s.row_delimiter), nullif(s.row_delimiter, t.row_delimiter) ) null) (isnull( nullif(t.escape_character, s.escape_character), nullif(s.escape_character, t.escape_character) ) null) (isnull( nullif(t.quote_character, s.quote_character), nullif(s.quote_character, t.quote_character) ) null) (isnull( nullif(t.header, s.header), nullif(s.header, t.header) ) null) (isnull( nullif(t.[encoding], s.[encoding]), nullif(s.[encoding], t.[encoding]) ) null) (isnull( nullif(t.skip_first_lines, s.skip_first_lines), nullif(s.skip_first_lines, t.skip_first_lines) ) null) (t.source_conditions distinct s.source_conditions) ) update set t.file_pattern = s.file_pattern, t.file_kind = s.file_kind, t.file_extension = s.file_extension, t.column_delimiter = s.column_delimiter, t.row_delimiter = s.row_delimiter, t.escape_character = s.escape_character, t.quote_character = s.quote_character, t.header = s.header, t.[encoding] = s.[encoding], t.skip_first_lines = s.skip_first_lines, t.source_conditions = s.source_conditions matched target insert ( file_layout, file_pattern, file_kind, file_extension, column_delimiter, row_delimiter, escape_character, quote_character, header, [encoding], skip_first_lines, source_conditions) values (s.file_layout, s.file_pattern, s.file_kind, s.file_extension, s.column_delimiter, s.row_delimiter, s.escape_character, s.quote_character, s.header, s.[encoding], s.skip_first_lines, s.source_conditions) output $action action, isnull(inserted.file_layout, deleted.file_layout) file_layout ; end go</file><file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_task_configuration.sql">create procedure [deploy].[usp_set_task_configuration] @task_name nvarchar(max), @task_category varchar(20) = 'ingest', -- backwards compatibility @task_type nvarchar(max), @worker_name nvarchar(max), @task_description nvarchar(max), @table_name nvarchar(max), @target_options nvarchar(max), @file_layout nvarchar(max), @source_folder nvarchar(max), @container_name nvarchar(max), @preprocess_pattern nvarchar(100) = null, -- backwards compatibility @preprocess_worker_options nvarchar(max) = null, -- backwards compatibility @enabled bit begin --#region 1. quality checks: make sure necessary values null @task_name null ltrim(rtrim(@task_name)) = '' throw 50000, 'task_name cannot null empty', 1; set @task_category = upper(trim(@task_category)); @task_category null @task_category = '' throw 50000, 'task_category cannot null empty', 1; @task_category ('ingest', 'preprocess') throw 50000, 'task_category must "ingest" "preprocess"', 1; @task_type null ltrim(rtrim(@task_type)) = '' throw 50000, 'task_type cannot null empty', 1; @worker_name null ltrim(rtrim(@worker_name)) = '' throw 50000, 'worker_name cannot null empty', 1; @task_description null ltrim(rtrim(@task_description)) = '' throw 50000, 'task_description cannot null empty', 1; @container_name null ltrim(rtrim(@container_name)) = '' throw 50000, 'container_name cannot null empty', 1; @enabled null throw 50000, 'enabled cannot null', 1; @task_category = 'ingest' begin @file_layout null ltrim(rtrim(@file_layout)) = '' throw 50000, 'file_layout cannot null empty ingest tasks', 1; end @task_category = 'preprocess' begin set @file_layout = nullif(trim(@file_layout), ''); @preprocess_pattern null throw 50000, 'preprocess_pattern cannot null preprocess tasks', 1; end --#endregion --#region 2. merge data meta.task_configuration table ;with input_data ( select ltrim(rtrim(@task_name)) task_name, @task_category task_category, ltrim(rtrim(@task_type)) task_type, ltrim(rtrim(@worker_name)) worker_name, ltrim(rtrim(@task_description)) task_description, ltrim(rtrim(@table_name)) table_name, nullif(ltrim(rtrim(@target_options)), '') target_options, ltrim(rtrim(@file_layout)) file_layout, ltrim(rtrim(@source_folder)) source_folder, ltrim(rtrim(@container_name)) container_name, @preprocess_pattern preprocess_pattern, -- trimming, could spaces valid part pattern @preprocess_worker_options preprocess_worker_options, @enabled [enabled] ) merge meta.task_configuration using input_data ( t.task_name = s.task_name ) matched ( -- looks complicated works nullable columns: true null different one null, null (isnull( nullif(t.enabled, s.enabled), nullif(s.enabled, t.enabled) ) null) (isnull( nullif(t.task_description, s.task_description), nullif(s.task_description, t.task_description) ) null) (isnull( nullif(t.task_type, s.task_type), nullif(s.task_type, t.task_type) ) null) (isnull( nullif(t.worker_name, s.worker_name), nullif(s.worker_name, t.worker_name) ) null) (isnull( nullif(t.table_name, s.table_name), nullif(s.table_name, t.table_name) ) null) (isnull( nullif(t.target_options, s.target_options), nullif(s.target_options, t.target_options) ) null) (isnull( nullif(t.file_layout, s.file_layout), nullif(s.file_layout, t.file_layout) ) null) (isnull( nullif(t.source_folder, s.source_folder), nullif(s.source_folder, t.source_folder) ) null) (isnull( nullif(t.container_name, s.container_name), nullif(s.container_name, t.container_name) ) null) (t.preprocess_pattern distinct s.preprocess_pattern) (t.preprocess_worker_options distinct s.preprocess_worker_options) ) update set t.enabled = s.enabled, t.task_category = s.task_category, t.task_name = s.task_name, t.task_type = s.task_type, t.worker_name = s.worker_name, t.task_description = s.task_description, t.table_name = s.table_name, t.target_options = s.target_options, t.file_layout = s.file_layout, t.source_folder = s.source_folder, t.container_name = s.container_name, t.preprocess_pattern = s.preprocess_pattern, t.preprocess_worker_options = s.preprocess_worker_options matched target insert (task_name, task_category, task_description, task_type, worker_name, table_name, target_options, file_layout, source_folder, container_name, preprocess_pattern, preprocess_worker_options, [enabled]) values (s.task_name, s.task_category, s.task_description, s.task_type, s.worker_name, s.table_name, s.target_options, s.file_layout, s.source_folder, s.container_name , s.preprocess_pattern, s.preprocess_worker_options, s.enabled) output $action [action], isnull(inserted.task_name, deleted.task_name) task_name, isnull(inserted.task_category, deleted.task_category) task_category, isnull(inserted.enabled, deleted.enabled) [enabled], isnull(inserted.task_type, deleted.task_type) task_type, isnull(inserted.worker_name, deleted.worker_name) worker_name, isnull(inserted.task_description, deleted.task_description) task_description, isnull(inserted.table_name, deleted.table_name) table_name, isnull(inserted.target_options, deleted.target_options) target_options, isnull(inserted.file_layout, deleted.file_layout) file_layout, isnull(inserted.source_folder, deleted.source_folder) source_folder, isnull(inserted.container_name, deleted.container_name) container_name, isnull(inserted.preprocess_pattern, deleted.preprocess_pattern) preprocess_pattern, isnull(inserted.preprocess_worker_options, deleted.preprocess_worker_options) preprocess_worker_options ; --#endregion end go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_ais_get_landing_configuration.sql">/* "locking" : want avoid landing automation starts processing certain dataset previous one still completed. lock configurations returned caller specified number hours. next usp_ais_get_landing_configuration calls return configurations lock either released logging result landing activity (using meta.usp_ais_log_landing) lock expired. ensure backwards compatibility allow engineers also run stored procedure without locking effects (as might debugging testing), lock_hours parameter defaults 0. */ create procedure meta.usp_ais_get_landing_configuration -- mandatory: timestamp used drop folders @drop_timestamp datetime, -- optional filters @source_system varchar(50) = null, @source_environment varchar(20) = null, @interface_name varchar(50) = null, @correlation_id varchar(50) = null, -- lock_hours set, procedure lock return configuration item given number hours -- (based @drop_timestamp) @lock_hours int = 0 begin -- set debug variable development debugging declare @debug bit = 0; declare @error_message nvarchar(max); -- format drop timestamp eg 20241231_172059 declare @drop_ts_string char(15) = format( @drop_timestamp, 'yyyymmdd_hhmmss'); -- get current environment declare @env varchar(20) = (select value string_split(db_name(), '-', 1) ordinal = 1) @env null begin set @error_message = concat( 'could determine current environment database name [', db_name(), ']'); throw 50000, @error_message, 1; end -- create lock period @lock_hours null @lock_hours &lt; 0 begin set @error_message = concat( 'invalid @lock_hours argument given: ', @lock_hours, '. null positive'); throw 50000, @error_message, 1; end else begin declare @lock_until datetime2 = dateadd(hour, @lock_hours, @drop_timestamp); @debug=1 begin print 'setting lock...' print ' lock_hours = ' + convert(varchar, @lock_hours) print ' drop timestamp = ' + convert(varchar, @drop_timestamp, 121) print ' lock_until = ' + convert(varchar, @lock_until, 121) end update meta.ais_landing_configuration set t_locked_until = @lock_until is_enabled = 1 ( @source_system null @source_system = source_system ) ( @source_environment null @source_environment = source_environment ) ( @interface_name null @interface_name = interface_name ) ( t_locked_until null t_locked_until &lt; @drop_timestamp ) end -- return enabled configurations select source_system ,source_environment ,interface_name ,source_path ,@env + 'dapstdala1' target_storage ,@env + '-dap-rg-dala' target_rg ,'landing' target_container ,concat(source_system, '/', source_environment, '/', interface_name, '/' , @drop_ts_string , '/' )as target_path -- add container well? ,isnull(t_last_file_ts, '1900-01-01') files_from_ts ,@drop_timestamp drop_ts ,t_locked_until locked_until meta.ais_landing_configuration is_enabled = 1 ( @source_system null @source_system = source_system ) ( @source_environment null @source_environment = source_environment ) ( @interface_name null @interface_name = interface_name ) -- lock_hours set, limit ones marked ( @lock_hours = 0 t_locked_until = @lock_until ) end go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_ais_log_landing.sql">/* log information (successfully) landed files. purpose threefold: * log file landed * update "last_file_ts" watermark ais_landing_configuration table * logging end landing process interface, releases "lock" (if any) configuration record. use json parameter fields time dynamically easily change information passed (without always change column lists calling caller logic) note: * @json_parameters, validating proper json, converted json datatype logging, logic still classic openjson() etc varchar(max). could probably simplified time now... parameters: @log_type: success error indicator, accepted values 'success' 'error', default 'success' @drop_timestamp: drop timestamp used identifier link landing log execution "run" delivery processes. @source_system, @source_environment, @interface_name triple key identifies dataset landed, unique record configuration table combination three fields @json_parameters: json object following properties: - last_file_ts | datetime | optional | (max) utc timestamp file(s) landed. provided, max data files_dropped list used - error_message | string | optional | error message description - target_folder | string | mandatory | file path file(s) landed, folder part eg 'igt/dev/pdm/20230502_091030/' - pipeline_guid | string | optional | guid pipeline execution, useful debugging example: { "last_file_ts" : "2024-06-07 12:34:56", "target_folder" : "igt/dev/pdm/20230502_091030/"" "pipeline_guid" : "xxxxxx-xxxx-xxxx-xxxx-xxxxx" } @files_dropped : array json objects information files dropped execution. maximum "filedate" provided information used update "last_file_ts" watermark value ais_landing_configuration table. object following properties: - filename | string | mandatory | file path file(s) landed, folder part eg 'igt/dev/pdm/20230502_091030/' - filedate | datetime | optional | utc timestamp file example: [ { "filename": "myfile.txt" }, { "filename": "anotherfile.txt" } ] @correlation_id : optional value used link landing logs across "applications", left null */ create procedure meta.usp_ais_log_landing --mandatory @drop_timestamp datetime2, @source_system varchar(50), @source_environment varchar(20), @interface_name varchar(50), @json_parameters nvarchar(max), --optional @log_type varchar(20) = 'success', @files_dropped nvarchar(max) = null, @correlation_id varchar(50) = null begin declare @error_message nvarchar(max), @proc_name sysname = object_schema_name(@@procid) + '.' + object_name(@@procid), @proc_start datetime2 = (select sysutcdatetime() time zone 'utc' time zone 'central europe standard time'), @log_properties_base nvarchar(max) = null, @log_properties nvarchar(max) = null, @num_files int = null ; -- generate correlation_id provided @correlation_id null set @correlation_id = newid() -- generate set log_properties we'll allways use set @log_properties_base = meta.fn_add_json_property(@log_properties_base, 'correlation_id', @correlation_id); -- log start proc set @log_properties = meta.fn_add_json_property(@log_properties_base, 'proc_start', convert(varchar(50), @proc_start, 121)); set @log_properties = meta.fn_add_json_property(@log_properties , 'proc_executed_by', (select suser_sname()) ); set @log_properties = meta.fn_add_json_property(@log_properties , 'drop_timestamp', convert(varchar(50), @drop_timestamp, 121)); set @log_properties = meta.fn_add_json_property(@log_properties , 'source_system', @source_system); set @log_properties = meta.fn_add_json_property(@log_properties , 'source_environment', @source_environment); set @log_properties = meta.fn_add_json_property(@log_properties , 'interface_name', @interface_name); set @log_properties = meta.fn_add_json_property(@log_properties , 'log_type', @log_type); set @log_properties = meta.fn_add_json_property(@log_properties , 'json_parameters', @json_parameters); set @log_properties = meta.fn_add_json_property(@log_properties , 'files_dropped', @files_dropped); exec meta.usp_proc_log @log_type = 'info' , @proc_name = @proc_name, @statement_name = 'proc_start', @message = 'procedure started', @extra_parameters = @log_properties, @query_text = null, @correlation_id = @correlation_id -- check combination source_system + source_environment + interface_name exists configuration table exists ( select * meta.ais_landing_configuration @source_system = source_system @source_environment = source_environment @interface_name = interface_name ) begin set @error_message = concat ( 'there configuration record source_system [', @source_system, '], source_environment [', @source_environment, '], interface_name [', @interface_name, ']'); exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 1 end -- @json_parameters provided, needs valid json @json_parameters null not((select isjson( @json_parameters, object )) = 1) begin set @error_message = 'provided json_parameters value valid json object'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 500000,@error_message, 1 end; --convert json datatype, gets rid whitespaces etc declare @json_parameters_json json = @json_parameters -- check log_type valid set @log_type = upper(trim(@log_type)); @log_type null @log_type ('fail', 'success') begin set @error_message = 'provided log_type value valid, accepted values: fail success'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 500000,@error_message, 1 end; -- check @files_dropped parameter either valid array null @files_dropped null begin isjson(@files_dropped, array) = 0 begin set @error_message = 'provided @files_dropped parameter valid json array'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 1; end end -- release lock configuration record update meta.ais_landing_configuration set t_locked_until = null @source_system = source_system @source_environment = source_environment @interface_name = interface_name -- log action proc_log set @log_properties = meta.fn_add_json_property(@log_properties_base, 'source_system', @source_system); set @log_properties = meta.fn_add_json_property(@log_properties , 'source_environment', @source_environment); set @log_properties = meta.fn_add_json_property(@log_properties , 'interface_name', @interface_name); exec meta.usp_proc_log @log_type = 'info' , @proc_name = @proc_name, @statement_name = 'release_lock', @message = 'released configuration lock', @extra_parameters = @log_properties, @query_text = null, @correlation_id = @correlation_id; -- log_type 'fail', log end exit -- (not processing needed) @log_type = 'fail' begin -- create landig_log entry insert meta.ais_landing_log ( log_type ,log_ts ,source_system ,source_environment ,interface_name ,last_file_ts ,drop_ts ,json_parameters ,files_dropped ) select @log_type ,(select sysutcdatetime() time zone 'utc' time zone 'central europe standard time') log_ts ,@source_system source_system ,@source_environment source_environment ,@interface_name interface_name ,null last_file_ts ,@drop_timestamp drop_ts ,convert(varchar(max),@json_parameters_json) ,@files_dropped files_dropped ; -- log action proc_log set @log_properties = meta.fn_add_json_property(@log_properties_base, 'source_system', @source_system); set @log_properties = meta.fn_add_json_property(@log_properties , 'source_environment', @source_environment); set @log_properties = meta.fn_add_json_property(@log_properties , 'interface_name', @interface_name); set @log_properties = meta.fn_add_json_property(@log_properties , 'num_files', @num_files); exec meta.usp_proc_log @log_type = 'info' , @proc_name = @proc_name, @statement_name = 'success', @message = 'logged landing error successfully', @extra_parameters = @log_properties, @query_text = null, @correlation_id = @correlation_id; return; end -- extract required fields @json_parameters declare @json_parameters_table table ( last_file_ts datetime2 null, target_folder varchar(500) null, pipeline_guid varchar(100) null ); insert @json_parameters_table select try_convert(datetime2, last_file_ts) last_file_ts, nullif(ltrim(rtrim(target_folder)),'') target_folder, nullif(ltrim(rtrim(pipeline_guid)), '') pipeline_guid openjson ( @json_parameters) ( last_file_ts varchar(100) 'lax $.last_file_ts', target_folder varchar(500) 'lax $.target_folder', pipeline_guid varchar(100) 'lax $.pipeline_guid' ); -- evaluate present valid (select count(*) @json_parameters_table) = 0 begin set @error_message = 'no data provided json parameters'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 1; end (select count(*) @json_parameters_table) &gt; 1 begin set @error_message = 'invalid data provided json parameters'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 1; end (select target_folder @json_parameters_table) null begin set @error_message = 'target_folder property json parameters missing invalid'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 1; end (select pipeline_guid @json_parameters_table) null begin set @error_message = 'pipeline_guid property json parameters missing invalid'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 1; end -- store json parameters variables need columns declare @last_file_ts datetime2, @pipeline_guid varchar(100) select @last_file_ts = last_file_ts, @pipeline_guid = pipeline_guid @json_parameters_table; -- extract required fields @files_dropped declare @files_dropped_table table ( filename varchar(500) null, filedate datetime2 null ); insert @files_dropped_table select nullif(ltrim(rtrim(filename)),'') filename, try_convert(datetime2, filedate) filedate openjson ( @files_dropped) ( filename varchar(100) 'lax $.filename', filedate varchar(500) 'lax $.filedate' ); -- determine number files dropped @files_dropped null begin select @num_files = count(*) @files_dropped_table; end -- last_file_ts provided, entries @files_dropped must (valid) filedates @last_file_ts null @num_files &gt; 0 begin exists ( select * @files_dropped_table filedate null) begin set @error_message = 'the entries @files_dropped must "filedate" property unless last_file_ts provided'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 2; end select @last_file_ts = max(filedate) @files_dropped_table end -- files dropped, save @drop_timestamp "last_drop_ts" declare @last_drop_ts datetime2; @num_files &gt; 0 set @last_drop_ts = @drop_timestamp; -- insert landing_log entry insert meta.ais_landing_log ( log_type ,log_ts ,source_system ,source_environment ,interface_name ,last_file_ts ,drop_ts ,json_parameters ,files_dropped ) select @log_type ,(select sysutcdatetime() time zone 'utc' time zone 'central europe standard time') log_ts ,@source_system source_system ,@source_environment source_environment ,@interface_name interface_name ,@last_file_ts last_file_ts ,@drop_timestamp drop_ts ,convert(varchar(max),@json_parameters_json) ,@files_dropped files_dropped; --update configuration item update meta.ais_landing_configuration set t_last_file_ts = case @num_files &gt; 0 @last_file_ts else t_last_file_ts end ,t_last_drop_ts = case @num_files &gt; 0 @last_drop_ts else t_last_drop_ts end ,t_last_sync_ts = @drop_timestamp @source_system = source_system @source_environment = source_environment @interface_name = interface_name @@rowcount = 0 begin set @error_message = 'no record updated configuration table'; exec meta.usp_proc_log @log_type = 'error', @proc_name = @proc_name, @statement_name = 'input_validation', @message = @error_message, @extra_parameters = @log_properties_base, @query_text = null, @correlation_id = @correlation_id; throw 50000, @error_message, 3; end -- log action proc_log set @log_properties = meta.fn_add_json_property(@log_properties_base, 'source_system', @source_system); set @log_properties = meta.fn_add_json_property(@log_properties , 'source_environment', @source_environment); set @log_properties = meta.fn_add_json_property(@log_properties , 'interface_name', @interface_name); set @log_properties = meta.fn_add_json_property(@log_properties , 'num_files', @num_files); set @log_properties = meta.fn_add_json_property(@log_properties , 't_last_file_ts', @last_file_ts); set @log_properties = meta.fn_add_json_property(@log_properties , 't_last_drop_ts', @last_drop_ts); set @log_properties = meta.fn_add_json_property(@log_properties , 't_last_sync_ts', @drop_timestamp); exec meta.usp_proc_log @log_type = 'info' , @proc_name = @proc_name, @statement_name = 'success', @message = 'configuration table updated successfully', @extra_parameters = @log_properties, @query_text = null, @correlation_id = @correlation_id; end go --exec tsqlt.run 't_usp_ais_log_landing'</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_end_plan.sql">/* description: end plan current status depending given success flag. success_flag = 1: current status = 'success' success_flag = 0: current status = 'failed' checks - mandatory parameters cannot null - ids need exist logging tables - current_status needs 'in progress' put 'success' */ create procedure [meta].[usp_end_plan] --mandatory @plan_id int, -- id plan @success_flag bit, -- 1 success, 0 failure @run_id int, -- id run --optional @pipeline_id varchar(40) = null, -- id synapse pipeline @comment varchar(max) = null -- optional comment begin -- step 1 : validate input parameters -- check mandatory parameter: cannot empty null ( @plan_id null) begin raiserror('plan_id parameter mandatory', 11, 102 ); return -1; end ( @run_id null) begin raiserror('run_id parameter mandatory', 11, 102 ); return 102; end ( @success_flag null ) begin raiserror('success_flag parameter mandatory', 11, 103 ); return -1; end -- step 2 : check parameters exist logging tables (-3) exists ( select run_id meta.log_runs run_id = @run_id ) begin raiserror ('the run run_id ''%i'' could located log_runs table', 11, 301, @run_id ); return 301; end exists ( select plan_id meta.log_plans plan_id = @plan_id ) begin raiserror('the plan plan_id ''%i'' could located log_plans table', 11, 302, @plan_id ); return 302; end exists ( select plan_id, run_id meta.log_run_plans plan_id = @plan_id run_id = @run_id) begin raiserror('the plan plan_id ''%i'' could located log_run_plans table run_id ''%i''', 11, 303, @plan_id, @run_id ); return 303; end -- step 3: validate current status plan = 'in progress' success_flag = 1 declare @current_status nvarchar(20); select @current_status = isnull(current_status, 'null') meta.log_plans plan_id = @plan_id; (@success_flag = 1 @current_status &lt;&gt; 'in progress') begin -- "success" log happen previous status "in progress" raiserror('the plan plan_id ''%i'' invalid current_status ''%s''', 16, 302, @plan_id, @current_status ); return -3; end (@success_flag = 1) begin -- count number unsuccessful tasks declare @unsuccessful_tasks int; set @unsuccessful_tasks = (select count(*) meta.log_tasks plan_id = @plan_id coalesce(current_status, 'incomplete') &lt;&gt; 'succeeded') -- plan successful tasks successful @unsuccessful_tasks &lt;&gt; 0 begin raiserror('the plan plan_id ''%i'' still ''%i'' unsuccessful tasks', 16, 302, @plan_id, @unsuccessful_tasks ); return -3; end end -- step 4 : update meta.log_plans table begin try declare @result nvarchar(20); (@success_flag = 1) set @result = 'succeeded'; else set @result = 'failed'; update [meta].[log_plans] set current_status = @result, -- azure sql database runs utc, want cest end_ts = convert(datetimeoffset, getdate()) time zone 'central europe standard time', --replace comment success, append failure comment = case @success_flag=1 @comment else isnull(comment + char(13),'') + @comment end plan_id = @plan_id ; (@success_flag = 0) begin -- set 'in progress' tasks 'failed' update meta.log_tasks set current_status = 'failed' current_status = 'in progress' plan_id = @plan_id -- set remaining (unsuccessful) tasks 'cancelled' update meta.log_tasks set current_status = 'cancelled' current_status &lt;&gt; 'succeeded' current_status &lt;&gt; 'failed' plan_id = @plan_id end -- ok return 0 end try begin catch --rethrow exception throw --return error code return -9 ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_end_run.sql">/* description: end run current status depending given success flag. success_flag = 1: current status = 'success' success_flag = 0: current status = 'failed' checks - mandatory parameters cannot null - ids need exist logging tables - current_status needs 'in progress' put 'success' */ create procedure [meta].[usp_end_run] --mandatory @run_id int, -- id run @success_flag bit, -- 1 success, 0 failure -- optional @pipeline_id nvarchar(40) = null, -- synapse pipeline id @comment nvarchar(max) = null -- optional comment begin -- step 1 : validate input parameters -- check mandatory parameter: cannot empty null ( @run_id null) begin raiserror('run_id parameter mandatory', 11, 102 ); return -1; end ( @success_flag null ) begin raiserror('success_flag parameter mandatory', 11, 103 ); return -1; end -- step 2 : check parameters exist logging tables (-3) exists ( select run_id meta.log_runs run_id = @run_id ) begin raiserror ('the run run_id ''%i'' could located log_tasks table', 11, 301, @run_id ); return 301; end -- step 3: validate current status run = 'in progress' success_flag = 1 declare @current_status nvarchar(20); select @current_status = isnull(current_status, 'null') meta.log_runs run_id = @run_id; @success_flag = 1 @current_status &lt;&gt; 'in progress' begin -- "success" log happen previous status "in progress" raiserror('the run run_id ''%i'' invalid current_status ''%s''', 16, 302, @run_id, @current_status ); return -3; end -- step 4 : update meta.log_runs table begin try declare @result nvarchar(20); @success_flag = 1 set @result = 'succeeded'; else set @result = 'failed'; update [meta].[log_runs] set current_status = @result, -- azure sql database runs utc, want cest end_ts = convert(datetimeoffset, getdate()) time zone 'central europe standard time', --replace comment success, append failure comment = case @success_flag=1 @comment else isnull(comment + char(13),'') + @comment end run_id = @run_id ; -- ok return 0 end try begin catch --rethrow exception throw --return error code return -9 ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_end_task.sql">/* description: end task current status depending given success flag. success_flag = 1: current status = 'success' success_flag = 0: current status = 'failed' checks - mandatory parameters cannot null - ids need exist logging tables - current_status needs 'in progress' put 'success' */ create procedure [meta].[usp_end_task] --mandatory @task_id int, -- id task end @plan_id int, -- id plan executing task @success_flag bit, -- 1 success, 0 failure --optional @pipeline_id varchar(40) = null, -- id synapse pipeline @comment varchar(max) = null, -- optional comment @max_values varchar(50) = null -- ?? --@recordcount_ins int = null, -- number records inserted table --@recordcount_upd int = null, -- number records updated table --@recordcount_del int = null, -- number records deleted table --@recordcount_rej int = null, -- number records rejected table --@recordcount_stg int = null -- number records moved raw begin ---- step 1 : validate mandatory input parameters ---- check mandatory parameter: cannot empty null ( @task_id null) begin raiserror('task_id parameter mandatory', 11, 101); return 101; end ( @plan_id null) begin raiserror('plan_id parameter mandatory', 11, 102 ); return 102; end ( @success_flag null ) begin raiserror('success_flag parameter mandatory', 11, 103 ); return 103; end -- step 2 : check parameters exist logging tables (-3) exists ( select task_id meta.log_tasks task_id = @task_id ) begin raiserror ('the task task_id ''%i'' could located log_tasks table', 11, 301, @task_id ); return 301; end exists ( select plan_id meta.log_plans plan_id = @plan_id ) begin raiserror('the task plan_id ''%i'' could located log_plans table', 11, 302, @plan_id ); return 302; end exists ( select plan_id, task_id meta.log_plan_tasks plan_id = @plan_id task_id = @task_id) begin raiserror('the task task_id ''%i'' could located log_plan_tasks table plan_id ''%i''', 11, 303, @task_id, @plan_id ); return 303; end -- step 3: validate current status run = 'in progress' success_flag = 1 declare @current_status nvarchar(20), @prev_pipeline_id varchar(50); select @current_status = isnull(current_status, 'null'), @prev_pipeline_id = isnull(pipeline_id, 'null') meta.log_tasks task_id = @task_id ; @success_flag = 1 @current_status &lt;&gt; 'in progress' begin -- "success" log happen previous status "in progress" raiserror('the task task_id ''%i'' invalid current_status ''%s''', 16, 302, @task_id, @current_status ); return -3; end /* @success_flag = 0 @prev_pipeline_id &lt;&gt; @pipeline_id @current_status &lt;&gt; 'in progress' begin -- "fail" log different execution guid ok previous status "in progress" raiserror('the task task_id ''%i'' invalid current_status ''%s''', 16, 303, @task_id, @current_status ); return -3; end */ -- step 4 : update meta.log_tasks table begin try declare @result nvarchar(20); @success_flag = 1 set @result = 'succeeded'; else set @result = 'failed'; update [meta].[log_tasks] set -- record_count_ins = @recordcount_ins, -- record_count_upd = @recordcount_upd, -- record_count_del = @recordcount_del, -- record_count_rej = @recordcount_rej, -- record_count_stg = @recordcount_stg, last_plan_id = @plan_id, pipeline_id = isnull(@pipeline_id, @prev_pipeline_id), current_status = @result, -- azure sql database runs utc, want cest end_ts = convert(datetimeoffset, getdate()) time zone 'central europe standard time', --replace comment success, append failure comment = case @success_flag=1 @comment else isnull(comment + char(13),'') + @comment end, max_value = @max_values task_id = @task_id ; -- ok return 0 end try begin catch --rethrow exception throw --return error code return -9 ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_plans.sql">/* ======================================================= author: joachim baert, simon plancke create date: 2024-08-28 alter date: 2024-09-03 description: return table pending plans @run_id @plan_name ======================================================= :param @run_id [int]: id run initiating plan :param @plan_name [varchar]: name plan run initiating ======================================================= */ create procedure [meta].[usp_get_plans] -- mandatory @run_id int, @plan_name nvarchar(100) begin declare @errormessage nvarchar(max) -- step 1: validate input parameters -- check mandatory parameter: cannot empty null (@run_id null) begin set @errormessage = 'run_id parameter mandatory'; throw 50000, @errormessage, 100; end; (@plan_name null) begin set @errormessage = 'plan_name parameter mandatory'; throw 50000, @errormessage, 100; end; --step 2: check ids exist logging tables (@run_id (select run_id meta.log_runs)) begin set @errormessage = 'run_id exist log_runs table'; throw 50000, @errormessage, 100; end; (@run_id (select run_id meta.log_run_plans)) begin set @errormessage = 'run_id exist log_run_plans table'; throw 50000, @errormessage, 100; end; -- step 3: check plan name exists logging tables (@plan_name (select plan_name meta.log_run_plans run_id= @run_id)) begin set @errormessage = 'plan_name exist log_run_plans table'; throw 50000, @errormessage, 100; end; -- select plans start @run_id @plan_name select lp.plan_id, lp.plan_name meta.log_plans lp inner join meta.log_run_plans lrp (lrp.plan_id = lp.plan_id) 1=1 lrp.run_id = @run_id lp.current_status ('pending') lp.plan_name = @plan_name end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_plan_metadata.sql">/* ======================================================= author: joachim baert, simon plancke create date: 2024-08-28 alter date: 2024-09-03 description: return list task objects run plan task_group ======================================================= :param @run_id [int]: id run initiating plan :param @plan_id [int]: id plan run :param @task_group [varchar]: name task group run :param @task_type [varchar]: type tasks need returned (synapse_pipeline vs spark_notebook) :param @results_as_select [bit]: boolean value, indicating whether return value needed (default = 1) :return @num_tasks [int]: total number tasks execute :return @tasks [varchar]: list json objects, containing tasks executed. ======================================================= detailed description stored procedure return 2 things: list json objects, object resembles task needs run total number tasks need run plan task_type == 'spark_notebook' task_type == 'synapse_pipeline' -&gt; call usp: meta.usp_get_task_metadata -&gt; filter tasks task_type task_group return value @tasks &lt;else case added new task_type options introduced require different metadata&gt; else -&gt; return total number tasks execute ======================================================= */ create procedure [meta].[usp_get_plan_metadata] --mandatory @run_id int, @plan_id int, @task_group nvarchar(10), @task_type nvarchar(20), --optional @results_as_select bit = 1, --output @num_tasks int = null output, @tasks nvarchar(max) = null output begin declare @errormsg nvarchar(max) -- step 1 validate input parameters -- check mandatory parameter: cannot empty null ( @run_id null) begin; throw 50000, 'run_id parameter mandatory', 100 ; end ( @plan_id null) begin; throw 50000, 'plan_id parameter mandatory', 101 ; end ( @task_group null ) begin; throw 50000, 'task_group parameter mandatory', 102 ; end ( @task_type null ) begin; throw 50000, 'task_type parameter mandatory', 103 ; end --step 2: check ids exist logging tables ( @run_id (select run_id meta.log_runs) ) begin; raiserror ('the task run_id ''%i'' could located log_runs table', 11, 301, @run_id ); return 301; end ( @plan_id (select plan_id meta.log_plans) ) begin; raiserror('the task plan_id ''%i'' could located log_plans table', 11, 302, @plan_id ); return 302; end exists ( select plan_id, run_id meta.log_run_plans plan_id = @plan_id run_id = @run_id) begin raiserror('the task plan_id ''%i'' could located log_run_plans table run_id ''%i''', 11, 303, @plan_id, @run_id ); return 303; end -- if: case foreseen -- dev-info: would need, separate case could made different task_types, returning different metadata @task_type ('spark_notebook', 'synapse_pipeline') begin -- execute stored procedure 'meta.usp_get_tasks' return list tasks @tasks exec meta.usp_get_tasks @run_id = @run_id, @plan_id = @plan_id, @task_group = @task_group, @task_type = @task_type, @tasks = @tasks output, @num_tasks = @num_tasks output -- return tasks @results_as_select = 1 select @tasks tasks, @num_tasks num_tasks return end -- else: return number tasks pending plan, task_group, task_type else begin set @num_tasks = ( select count(*) meta.log_plan_tasks b inner join meta.log_tasks j ( j.task_id = b.task_id) 1=1 b.task_group = @task_group b.plan_id = @plan_id b.task_type = @task_type j.current_status ('pending') ) -- return number tasks @results_as_select = 1 select @num_tasks num_tasks end end go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_preprocessing_info.sql">/* ======================================================= author: simon plancke create date: 2024-12-11 alter date: - description: return plan_name task_filter determine tasks run ======================================================= :param @trigger_file_path [varchar]: path file triggered synapse blob trigger :param @trigger_file_name [varhcar]: name file triggered synapse blob trigger :return @plan_name [varchar]: name plan run :return @task_filter [varchar]: sql-regex filter determine plans log ======================================================= */ create procedure [meta].[usp_get_preprocessing_info] -- mandatory @trigger_file_path nvarchar(max), -- optional @trigger_file_name nvarchar(100) = null, -- result @task_filter varchar(100) = null output, @plan_name varchar(100) = null output begin declare @error_message nvarchar(max); declare @preprocessing_count int; -- check valid parameters (@trigger_file_path null) begin set @error_message = 'trigger_file_path mandatory cannot null' ; throw 50000, @error_message, 1; end begin try -- set plan_name set @plan_name = 'preprocessing' -- get number tasks match trigger_file_path enabled task_config plan_task_config set @preprocessing_count = ( select count(*) meta.plan_task_configuration 1=1 plan_name = @plan_name [enabled] = 1 task_name ( select task_name meta.task_configuration 1=1 @trigger_file_path like concat('%', preprocess_pattern, '%') task_category = 'preprocess' [enabled] = 1 ) ) -- little many tasks, throw error (@preprocessing_count &lt;&gt; 1) begin set @error_message = concat('only allowed match one task, matched ', @preprocessing_count, ' tasks') ; throw 50000, @error_message, 1; end -- onyl one task, return name task set @task_filter = ( select task_name meta.plan_task_configuration plantaskconfig 1=1 plan_name = @plan_name [enabled] = 1 task_name ( select task_name meta.task_configuration 1=1 @trigger_file_path like preprocess_pattern task_category = 'preprocess' [enabled] = 1 ) ) -- return output values select @task_filter task_filter, @plan_name plan_name end try begin catch --rethrow exception throw --return error code return -9 ; end catch end go;</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_tasks.sql">/* ======================================================= author: joachim baert, simon plancke create date: 2024-08-28 alter date: 2024-09-03 description: return pending tasks plan_id, task_group, task_type list json objects ======================================================= :param @run_id [int]: id run initiating plan :param @plan_id [int]: id plan run :param @task_group [varchar]: name task group run :param @task_type [varchar]: type tasks need returned (synapse_pipeline vs spark_notebook) :param @results_as_select [bit]: boolean value, indicating whether return value needed (default = 1) :return @num_tasks [int]: total number tasks execute :return @tasks [varchar]: list json objects, containing tasks executed. ======================================================= */ create procedure [meta].[usp_get_tasks] --mandatory @run_id int, @plan_id int, @task_group nvarchar(10), @task_type nvarchar(20), --optional @results_as_select bit = 1, --result @num_tasks int = null output, @tasks nvarchar(max) = null output begin declare @errormsg nvarchar(max) -- step 1 validate input parameters -- check mandatory parameter: cannot empty null ( @run_id null) begin; throw 50000, 'run_id parameter mandatory', 100 ; end ( @plan_id null) begin; throw 50000, 'plan_id parameter mandatory', 101 ; end ( @task_group null ) begin; throw 50000, 'task_group parameter mandatory', 102 ; end ( @task_type null ) begin; throw 50000, 'task_type parameter mandatory', 103 ; end -- step 2: check ids exist ( @run_id (select run_id meta.log_runs) ) begin; set @errormsg = concat('run_id parameter "', @run_id, '" exist log_runs table'); throw 50000, @errormsg, 200; end ( @plan_id (select plan_id meta.log_plans) ) begin; set @errormsg = concat('plan_id parameter "', @plan_id, '" exist log_plans table'); throw 50000, @errormsg, 201; end --joachim: i've removed check valid scenario plan started tasks -- ( @plan_id (select plan_id meta.log_plan_tasks plan_id = @plan_id) ) -- begin; -- set @errormsg = concat('plan_id parameter "', @plan_id, '" exist [log_plan_tasks] table'); -- throw 50000, @errormsg, 201; -- end -- step 3: return values @results_as_select = 1 -- return list tasks total number tasks execute begin set @tasks = ( select lpt.*, lt.current_status meta.log_plan_tasks lpt inner join meta.log_tasks lt ( lt.task_id = lpt.task_id) 1=1 lpt.task_group = @task_group lpt.plan_id = @plan_id lpt.task_type = @task_type -- lpt.original_plan_id = @plan_id lt.current_status ('pending') order task_id json path ); set @num_tasks = ( select count(*) meta.log_plan_tasks lpt inner join meta.log_tasks lt ( lt.task_id = lpt.task_id) 1=1 lpt.task_group = @task_group lpt.plan_id = @plan_id lpt.task_type = @task_type --and lpt.original_plan_id = @plan_id lt.current_status ('pending') ) end else -- return total number tasks execute begin set @num_tasks = ( select count(*) meta.log_plan_tasks lpt inner join meta.log_tasks lt ( lt.task_id = lpt.task_id) 1=1 lpt.task_group = @task_group lpt.plan_id = @plan_id --and lpt.original_plan_id = @plan_id lt.current_status ('pending') ) ; end -- ok return 0 end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_task_last_loaded_file.sql">/* description: retrieve last successfully processed extended_filename given task. returns empty dataset there's previous sucessfull execution checks - mandatory parameters cannot null - ids need exist logging tables - current_status needs 'succeeded' */ create procedure [meta].[usp_get_task_last_loaded_file] @task_name varchar(50) begin declare @error_message nvarchar(max); @task_name null begin set @error_message = '@task_name mandatory cannot null' ; throw 50000, @error_message, 1; end (@task_name (select task_name meta.task_configuration task_name = @task_name)) begin set @error_message = concat('task_name "',@task_name,'" found task_configuration table' ); throw 50000, @error_message, 100; end; --select complete possible filename used select [filename] = extended_filename meta.log_files task_id ( select max(task_id) meta.log_tasks x x.current_status = 'succeeded' task_name = @task_name ) end;</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_task_metadata.sql">/* description get metadata needed execute task checks - mandatory parameters empty - ids exist logging tables */ create proc [meta].[usp_get_task_metadata] @task_id int begin declare @error_message nvarchar(max); @task_id null begin set @error_message = '@task_id mandatory cannot null' ; throw 50000, @error_message, 1; end (@task_id (select task_id meta.log_tasks task_id= @task_id)) begin set @error_message = 'task_id exist log_tasks table' ; throw 50000, @error_message, 100; end; -------------------------------------------------------------------------------------------- -- retrieve worker_name task_name task_configuration -------------------------------------------------------------------------------------------- declare @worker_name varchar(100), @task_name varchar(100); select @worker_name = tc.worker_name, @task_name = tc.task_name meta.task_configuration tc inner join meta.log_tasks lt (lt.task_name = tc.task_name) lt.task_id = @task_id -------------------------------------------------------------------------------------------- -- task exist, configured adf_worker_name type, -------------------------------------------------------------------------------------------- @worker_name null begin set @error_message = concat( 'could locate worker_name information task_id ', @task_id); throw 50000, @error_message, 1 ; end -------------------------------------------------------------------------------------------- -- create json answer worker_name: ingestionworker &amp; filworker -------------------------------------------------------------------------------------------- @worker_name ('ingestionworker', 'filworker') begin -- ================== -- declare -- ================== ---- declare file_layout easy look-up (similar keys) declare @file_layout nvarchar(100) ---- json-objects declare @task_config nvarchar(max) declare @source_config nvarchar(max) declare @source_column_config nvarchar(max) declare @source_check_config nvarchar(max) -- ================== -- set -- ================== ---- set file_layout used easy look-up set @file_layout = (select file_layout meta.task_configuration task_name = @task_name) ---- set json-objects returned set @task_config = ( select task_name, task_type, worker_name, file_layout, source_folder, container_name, table_name, target_options, [enabled] meta.task_configuration file_layout = @file_layout json path ) set @source_config = ( select file_pattern , file_extension , file_kind , column_delimiter , row_delimiter , escape_character , quote_character , header , skip_first_lines , source_conditions meta.source_configuration file_layout = @file_layout json path ) set @source_column_config =( select source_name, sink_name, column_order, dimension, data_type, column_info meta.source_column_configuration file_layout = @file_layout json path ) set @source_check_config = ( select check_name, config_params meta.source_check_configuration file_layout = @file_layout [enabled] = 1 json path ) -- ================== -- result -- ================== select @task_config task_config, @source_config source_config, @source_column_config source_column_config, @source_check_config source_check_config return end -------------------------------------------------------------------------------------------- -- create json answer worker_name: dummyworker -------------------------------------------------------------------------------------------- -- empty test worker_name used testing execution -- framework, need configuration else @worker_name = 'dummyworker' begin return end -------------------------------------------------------------------------------------------- -- create json answer worker_name: pl_unzip_worker -------------------------------------------------------------------------------------------- else @worker_name = 'pl_unzip_worker' begin -- ================== -- declare -- ================== ---- declare file_layout easy look-up (similar keys) declare @file_layout_unzip nvarchar(100) ---- json-objects declare @task_config_unzip nvarchar(max) declare @source_config_unzip nvarchar(max) declare @source_check_config_unzip nvarchar(max) -- ================== -- set -- ================== ---- set file_layout used easy look-up set @file_layout_unzip = (select file_layout meta.task_configuration task_name = @task_name) ---- set json-objects returned set @task_config_unzip = ( select task_name, task_type, worker_name, file_layout, source_folder, container_name, table_name, [enabled] meta.task_configuration file_layout = @file_layout_unzip json path ) set @source_config_unzip = ( select file_pattern, file_extension, column_delimiter, row_delimiter, escape_character, quote_character, header meta.source_configuration file_layout = @file_layout_unzip json path ) set @source_check_config_unzip = ( select check_name, config_params meta.source_check_configuration file_layout = @file_layout_unzip [enabled] = 1 json path ) -- ================== -- result -- ================== select @task_config_unzip task_config, @source_config_unzip source_config, @source_check_config_unzip source_check_config return end -------------------------------------------------------------------------------------------- -- task configured adf_worker_name, target_name supported -------------------------------------------------------------------------------------------- set @error_message = concat( 'unsupported target_name: ''', @worker_name, ''''); throw 50000, @error_message, 1 ; end go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_new_file.sql">/* ======================================================= description: prepare logging tables ingest new file ======================================================= :param @task_id [int]: id task run :param @plan_id [int]: id plan run :param @filename [varchar]: name file run :param @extended_filename [varchar]: extended name file run : filename + '_' + timestamp_folder\n" :param @info_message [varchar]: path file located landing zone optional :param @no_select [bit]: boolean indication whether stored procedure return final select statement :return @file_id [data type]: [description return value (as recordset)] ======================================================= detailed description stored procedure log new file meta.log_file file new : create new record else extended_filename already exists job: update existing record setting info field null ======================================================= */ create procedure [meta].[usp_new_file] ( -- mandatory @task_id int, @plan_id int, @filename nvarchar(max), @extended_filename nvarchar(max), @info_message nvarchar(max), -- optional @no_select bit = 0 -- set boolean 1 avoid returning output resultset ) begin declare @errormsg nvarchar(max), @file_id int -- step 1: make sure mandatory parameters provided ( @filename null) begin set @errormsg = 'filename parameter mandatory'; throw 50000, @errormsg, 101; end ( @task_id null) begin set @errormsg = 'task_id parameter mandatory'; throw 50000, @errormsg, 101; end ( @plan_id null) begin set @errormsg = 'plan_id parameter mandatory'; throw 50000, @errormsg, 101; end ( @extended_filename null) begin set @errormsg = 'extended_filename parameter mandatory'; throw 50000, @errormsg, 101; end ( @info_message null) begin set @errormsg = 'info_message parameter mandatory'; throw 50000, @errormsg, 101; end -- step 2: make sure file already tracked -- check extended_filename already tracked progres, throw error ( @extended_filename (select [extended_filename] meta.log_files archive_status ='succeeded') ) begin set @errormsg = concat ( 'extended_filename ''', @extended_filename, ''' already exists log_files table recorded progress'); throw 50000, @errormsg, 201; end -- step 3: check ids log tables exists ( select task_id meta.log_tasks task_id = @task_id ) begin raiserror ('the task task_id ''%i'' could located log_tasks table', 11, 301, @task_id ); return 301; end exists ( select plan_id meta.log_plans plan_id = @plan_id ) begin raiserror('the task plan_id ''%i'' could located log_plans table', 11, 302, @plan_id ); return 302; end exists ( select plan_id, task_id meta.log_plan_tasks plan_id = @plan_id task_id = @task_id) begin raiserror('the task task_id ''%i'' could located log_plan_tasks table plan_id ''%i''', 11, 303, @task_id, @plan_id ); return 303; end -- step 4: start logging features -- check extended_filename already exists job (if yes, reuse entry) declare @existing_file_id int = ( select [file_id] meta.log_files extended_filename = @extended_filename (coalesce(archive_status, 'not ingested') &lt;&gt; 'succeeded') ); -- file new, existing_file_id null -&gt; create new record @existing_file_id null begin -- create new record insert meta.log_files (task_id, plan_id, [filename], extended_filename, registration_ts,landing_ts,landing_info,landing_status) values (@task_id, @plan_id, @filename, @extended_filename, getdate(), getdate(),@info_message,'succeeded'); set @file_id = scope_identity(); end -- file new, existing_file_id cannot null -&gt; update existing record else begin -- update existing record set @file_id = @existing_file_id; update meta.log_files set upload_ts = getdate(), raw_info = null, silver_info = null, archive_info = null, raw_status = null, silver_status = null, archive_status = null, raw_ts = null, silver_ts = null, archive_ts = null -- record_count_init = null, -- record_count_stg = null, -- record_count_ins = null, -- record_count_del = null, -- record_count_upd = null, -- record_count_rej = null, -- copied_message = null, [file_id] = @file_id end -- return result recordset (@no_select = 1) begin select @file_id [file_id], @task_id task_id, @plan_id plan_id, @filename [filename], @extended_filename extended_filename; end end go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_new_run.sql">/* ======================================================= author: joachim baert, simon plancke create date: 2024-08-28 alter date: 2024-09-03 description: prepare logging tables start new run ingestion framework ======================================================= :param @pipeline_id [varchar]: id synapse pipeline started run :param @new_plan [bit]: boolean indication whether run logging new plan rerunning old (failed) plan :param @plan_name [varchar]: name plan run :param @debug [bit]: boolean indication whether stored procedure run debug-mode :param @no_select [bit]: boolean indication whether stored procedure return final select statement :param @task_filter [varchar]: (regex-like) string value filter tasks plan, allowing task-subset executions :return @run_id [data type]: [description return value] ======================================================= detailed description stored procedure log new run meta.log_runs parameter new_plan = 1 log new plan meta.log_plans log new row meta.log_run_plans task plan: log new tasks meta.log_tasks task: log new row meta.log_plan_tasks else parameter new_plan = 0: get plan_id last plan name @plan_name current_status = 'succeeded': nothing synapse pipeline get list 'pending' plans empty else current_status = 'failed' 'cancelled' log new entry meta.log_run_plans new run id previous plan id reset current_status 'pending' failed/cancelled plan tasks ======================================================= */ create procedure [meta].[usp_new_run] @pipeline_id nvarchar(40), @new_plan bit = 1, @plan_name nvarchar(100), @run_id int = null output, @task_filter nvarchar(100) = null, @debug bit = 0, @no_select bit = 1 begin -- check mandatory parameters -- check mandatory parameter: 'pipeline_id' cannot empty null ( @pipeline_id null @pipeline_id = '') begin raiserror('pipeline_id parameter mandatory', 11, 101 ); return -1; end -- check mandatory parameter: 'plan_name' cannot empty null ( @plan_name null @plan_name = '') begin raiserror('plan_name parameter mandatory', 11, 102 ); return -1; end -- check metadata configuration -- plan_name needs exist meta.plan_configuration table ( @plan_name (select plan_name meta.plan_configuration) ) begin raiserror('plan_name parameter "%s" exist plan_configuration table', 11, 201, @plan_name); return -2; end -- plan_name needs [enabled] allow execution ( @plan_name (select plan_name meta.plan_configuration [enabled] = 1) ) begin raiserror('plan_name parameter "%s" enabled plan_configuration table', 11,202, @plan_name ); return -2; end begin try -- azure sql database runs utc, want cest declare @registration_ts datetime = convert(datetimeoffset, getdate()) time zone 'central europe standard time'; begin tran; -- add new run meta.log_runs status 'pending' insert meta.log_runs( pipeline_id, new_plan, plan_name, current_status, registration_ts ) select @pipeline_id, @new_plan, @plan_name, 'pending', @registration_ts -- get id run inserted meta.log_runs set @run_id = scope_identity(); -- new_plan = true -&gt; add new rows logging tables @new_plan = 1 begin -- insert new plan table meta.log_plans status pending insert meta.log_plans( pipeline_id, plan_name, registration_ts, current_status, run_id ) select @pipeline_id, @plan_name, @registration_ts, 'pending', @run_id -- get id plan inserted meta.log_plans declare @plan_id int = scope_identity(); -- insert new run-plan instance meta.log_run_plans insert meta.log_run_plans values (@run_id, @plan_id, @plan_name, @run_id) -- insert set new tasks meta.log_tasks status 'pending' -- insert enabled tasks plan initialized declare @task_filter_string nvarchar(100); set @task_filter_string = case @task_filter null '%' else @task_filter end; insert meta.log_tasks ( task_name, plan_id, current_status, registration_ts ) select plan_task.task_name, @plan_id plan_id, 'pending' current_status, @registration_ts registration_ts meta.plan_task_configuration plan_task inner join meta.task_configuration task plan_task.task_name = task.task_name plan_task.plan_name = @plan_name task.[enabled] = 1 plan_task.enabled=1 task.task_name like @task_filter_string order plan_task.task_sequence ; -- insert set new plan-task instances meta.log_plan_tasks -- select relevant metadata meta.configuration tables plan_task_data ( select distinct plan_task.task_name, plan_task.task_sequence, plan_task.task_group, task.task_type meta.plan_task_configuration plan_task inner join meta.task_configuration task ( task.task_name = plan_task.task_name ) plan_name = ( select plan_name meta.log_plans plan_id = @plan_id ) ) insert meta.log_plan_tasks ( plan_id, plan_name, task_id, task_group, task_type, task_name, worker_name, task_sequence, original_plan_id ) select @plan_id, plan_name, plan_task_log_data.task_id, plan_task_log_data.task_group, plan_task_log_data.task_type, plan_task_log_data.task_name, plan_task_log_data.worker_name, plan_task_data.task_sequence, plan_task_log_data.plan_id original_plan_id ( -- first select configuration data relevant logging select task.task_id, task.plan_id, task.task_name, task.current_status, [plan].plan_name, plan_task.task_group, task_config.task_type, task_config.worker_name meta.log_tasks task inner join meta.log_plans [plan] ( task.plan_id = [plan].plan_id ) inner join meta.plan_task_configuration plan_task ( [plan].plan_name = plan_task.plan_name task.task_name = plan_task.task_name ) inner join meta.task_configuration task_config ( plan_task.task_name = task_config.task_name ) inner join meta.plan_configuration plan_config ( plan_task.plan_name = plan_config.plan_name ) ) plan_task_log_data -- filter tasks within plan inner join plan_task_data (plan_task_data.task_name = plan_task_log_data.task_name) 1=1 -- filter tasks initialized current plan_id plan_task_log_data.plan_id = @plan_id -- filter tasks status 'pending' current_status ('pending') order plan_task_data.task_sequence asc, plan_task_log_data.task_name desc, plan_task_log_data.task_id asc ; end @new_plan = 0 begin -- create temporary table create table #latest_plan ( run_id int, plan_id int, plan_name varchar(50), original_run_id int ); -- get last plan-instance failed @plan_name latest_plan_cte ( select top 1 @run_id run_id, plan_id, plan_name, run_id original_run_id meta.log_plans 1=1 run_id &lt; @run_id current_status ('failed') plan_name = @plan_name order original_run_id desc ) -- add instance temporary table insert #latest_plan select * latest_plan_cte; -- add new run-plan meta.log_run_plans insert meta.log_run_plans( run_id, plan_id, plan_name, original_run_id ) select * #latest_plan; -- reset status plan 'pending' update meta.log_plans set current_status = 'pending', start_ts = null, end_ts = null plan_id = (select plan_id #latest_plan); -- reset status failed/cancelled tasks 'pending' update meta.log_tasks set current_status = 'pending', start_ts = null, end_ts = null 1=1 plan_id = (select plan_id #latest_plan) current_status ('failed', 'cancelled') end; --successful transaction commit tran; --output new run info (@no_select = 0) begin select * meta.log_runs run_id = @run_id; end return 0 -- successful execution end try begin catch rollback tran; declare @msg varchar(max) = 'scheduling plan failed -- message: ' + error_message(); raiserror( @msg , 11, 1 ); return -9 ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_proc_log.sql">create procedure meta.usp_proc_log --mandatory @proc_name sysname, --optional @log_type varchar(20) = 'info', @statement_name varchar(100) = null, @message nvarchar(max) = null, @extra_parameters nvarchar(max) = null, @query_text nvarchar(max) = null, @correlation_id varchar(50) = null begin declare @error_message nvarchar(max); -- check log_type set @log_type = trim(@log_type); @log_type null @log_type = '' begin set @error_message = '@log_type property cannot null empty'; throw 50000, @error_message, 1; end set @log_type = lower(@log_type); @log_type ( 'info', 'debug', 'error', 'warning', 'query') begin set @error_message = concat('@log_type property [', isnull(@log_type, 'null'), '] valid, accepted values are: ''info'',''debug'',''error'',''warning'',''query'' '); throw 50000, @error_message, 1; end -- check proc_name set @proc_name = trim(@proc_name); @proc_name null @proc_name = '' begin set @error_message = '@proc_name property cannot null empty'; throw 50000, @error_message, 1; end -- timestamp local time declare @timestamp datetime2; set @timestamp = (select sysutcdatetime() time zone 'utc' time zone 'central europe standard time') insert meta.proc_log ( [log_ts], [log_type], [proc_name], [statement_name], [message], [extra_parameters], [query_text], correlation_id ) select @timestamp log_ts ,@log_type log_type ,@proc_name proc_name ,@statement_name statement_name ,@message message ,@extra_parameters extra_parameters ,@query_text query_text ,@correlation_id end go /* exec meta.usp_proc_log @log_type = 'warning' , @proc_name = 'x', @statement_name = null, @message = null, @extra_parameters = null, @query_text = null select * meta.proc_log */</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_start_plan.sql">/* description set status plan "in progress" checks - mandatory parameters cannot null - ids need exist logging tables - current status cannot 'in progress', 'aborted' 'succeeded' - compare original_run_id run_id */ create procedure [meta].[usp_start_plan] -- mandatory @plan_id int, @run_id int, -- optional @pipeline_id varchar(40) = null begin -- step 1 validate input parameters -- check mandatory parameter: cannot empty null ( @plan_id null) begin raiserror('plan_id parameter mandatory', 16, 102 ); return -1; end ( @run_id null) begin raiserror('run_id parameter mandatory', 16, 102 ); return -1; end -- step 2: make sure file already tracked exists ( select run_id meta.log_runs run_id = @run_id ) begin raiserror ('the run run_id ''%i'' could located log_runs table', 11, 301, @run_id ); return 301; end exists ( select plan_id meta.log_plans plan_id = @plan_id ) begin raiserror('the plan plan_id ''%i'' could located log_plans table', 11, 302, @plan_id ); return 302; end exists ( select plan_id, run_id meta.log_run_plans plan_id = @plan_id run_id = @run_id) begin raiserror('the plan plan_id ''%i'' could located log_run_plans table run_id ''%i''', 11, 303, @plan_id, @run_id ); return 303; end --step 3: plan set 'in progress' current_status pending declare @current_status nvarchar(15) = ( select current_status [meta].[log_plans] plan_id = @plan_id); @current_status ( 'aborted', 'in progress', 'succeeded') begin raiserror('cannot start plan, current_status "%s"', 16, 3, @current_status ); return -3; end -- step 4: run initially tried initiating plan cannot higher id one trying execute plan -- declare @original_run_id int = ( select run_id meta.log_plans plan_id = @plan_id ); -- ( @run_id &lt; @original_run_id ) -- begin -- raiserror('run_id value "%i" cannot lower original_run_id value "%i" log_run table', 16, 203, @run_id, @original_run_id ); -- return 203; -- end -- step 5: set plan 'in progress' begin try declare @plan_name nvarchar(100); select @plan_name = plan_name meta.log_plans plan_id = @plan_id; --update log_plans update [meta].[log_plans] -- azure sql db runs utc, want cest set start_ts = convert(datetimeoffset, getdate()) time zone 'central europe standard time', current_status = 'in progress', end_ts = null, comment = null plan_id = @plan_id ; -- ok return 0 end try begin catch declare @msg varchar(max) = 'logging plan failed update step -- message: ' + error_message() raiserror( @msg , 16, 9 ); return -9 ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_start_run.sql">/* description set status run "in progress" checks - mandatory parameters cannot null - ids need exist logging tables - current status cannot 'in progress', 'aborted' 'succeeded' - */ create procedure [meta].[usp_start_run] -- mandatory @run_id int, -- optional @pipeline_id nvarchar(40) = null begin -- step 1 validate input parameters -- check mandatory parameter: cannot empty null ( @run_id null) begin raiserror('run_id parameter mandatory', 16, 102 ); return -1; end -- step 2: make sure ids exist logging tables ( @run_id (select run_id meta.log_runs) ) begin raiserror('run_id parameter exist log_runs table', 16, 202 ); return -2; end -- step 3: run set 'in progress' current_status pending declare @current_status nvarchar(15) = ( select current_status [meta].[log_runs] run_id = @run_id); @current_status ( 'aborted', 'in progress', 'succeeded') begin raiserror('cannot start plan, current_status "%s"', 16, 3, @current_status ); return -3; end -- step 4: set run 'in progress' begin try --update log_plans update [meta].[log_runs] -- azure sql db runs utc, want cest set start_ts = convert(datetimeoffset, getdate()) time zone 'central europe standard time', current_status = 'in progress', end_ts = null, comment = null run_id = @run_id ; -- ok return 0 end try begin catch declare @msg varchar(max) = 'logging run failed update step -- message: ' + error_message() raiserror( @msg , 16, 9 ); return -9 ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_start_task.sql">/* description set status task "in progress" checks - mandatory parameters cannot null - ids need exist logging tables - current status cannot 'in progress', 'aborted' 'succeeded' - compare original_run_id run_id */ create procedure [meta].[usp_start_task] --mandatory @task_id int, @plan_id int, -- optional @pipeline_id varchar(40) = null begin -- step 1: validate input parameters -- check mandatory parameter: cannot empty null (10*) ( @task_id null) begin raiserror('task_id parameter mandatory', 16, 101 ); return 101; end ( @plan_id null) begin raiserror('plan_id parameter mandatory', 16, 102 ); return 102; end -- step 2: ids need exist logging tables ( @task_id (select task_id meta.log_tasks) ) begin raiserror('task_id parameter exist log_tasks table', 16, 201 ); return 201; end ( @plan_id (select plan_id meta.log_plans) ) begin raiserror('plan_id parameter exist log_plans table', 16, 202 ); return 202; end --step 3 current status cannot 'in progress', 'aborted' 'succeeded' declare @current_status nvarchar(15) = ( select current_status [meta].[log_tasks] task_id = @task_id ); @current_status ( 'aborted', 'in progress', 'succeeded') begin raiserror('cannot start task, current_status "%s"', 16, 301, @current_status ); return 301; end -- step 4 compare original_plan_id plan_id declare @original_plan_id int = ( select plan_id meta.log_tasks task_id = @task_id ); ( @plan_id &lt; @original_plan_id ) begin raiserror('plan_id value "%i" cannot lower original_plan_id value "%i" log_tasks table', 16, 401, @plan_id, @original_plan_id ); return 401; end begin try -- set current status 'in progress' -- get name task task_configuration table declare @task_name nvarchar(100); select @task_name = b.task_name meta.log_tasks inner join meta.task_configuration b (a.task_name = b.task_name) task_id = @task_id; --update log_tasks update [meta].[log_tasks] -- azure sql db runs utc, want cest set start_ts = convert(datetimeoffset, getdate()) time zone 'central europe standard time', current_status = 'in progress', end_ts = null, attempts = isnull(attempts,0) + 1, last_plan_id = @plan_id, pipeline_id = @pipeline_id, comment = null task_id = @task_id ; -- ok return 0 end try begin catch declare @msg varchar(max) = 'task logging failed update step -- message: ' + error_message() raiserror( @msg , 16, 900 ); ; end catch end; go</file><file name="src\sql\metadb\stored_procedures\meta\meta.usp_update_file_activity.sql">/* ======================================================= description: update metadata file_record ======================================================= :param @extended_filename [varchar]: extended name file run : filename + '_' + timestamp_folder\n" :param @activity [varchar]: field indicate location file loaded : accepted values: 'raw', 'silver', 'archive' :param @success [bit]: boolean indication whether loading successful :param @info_message [varchar]: path file located landing zone ======================================================= detailed description stored procedure update file meta.log_file @activity = 'raw' : update values raw fields track file loaded raw layer @activity = 'silver' update values silver fields track file loaded silver layer else @activity = 'archive' update values archive fields track file loaded archive layer ======================================================= */ --dev-note: future success parameter used indicate successful unsuccessful file loading. --might relevant add error throw specific file error occurs, set status failed. create procedure [meta].[usp_update_file_activity] ( -- mandatory @extended_filename nvarchar(150), @activity varchar(50), -- accepted values: 'raw', 'silver', 'archive' @success bit, @info_message varchar(max) -- optional -- @source_folder nvarchar(max) = null, -- @sink_container nvarchar(max) = null, -- @sink_folder nvarchar(max) = null, -- @record_count_init int = null, -- @record_count_stg int = null, -- @record_count_ins int = null, -- @record_count_del int = null, -- @record_count_upd int = null, -- @record_count_rej int = null, -- @message nvarchar(max) = null ) begin declare @errormsg nvarchar(max); -- step 1: mandatory parameters null ( @activity null) begin set @errormsg = 'activity parameter mandatory'; throw 50000, @errormsg, 101; end ( @extended_filename null) begin set @errormsg = 'extended_filename parameter mandatory'; throw 50000, @errormsg, 101; end ( @success null) begin set @errormsg = 'success parameter mandatory'; throw 50000, @errormsg, 101; end ( @info_message null) begin set @errormsg = 'info_message parameter mandatory'; throw 50000, @errormsg, 101; end -- step 2: validation values --make sure file already raw trying load silver layer ( @activity = 'silver' 'succeeded' &lt;&gt;(select raw_status meta.log_files @extended_filename=[extended_filename])) begin set @errormsg = concat ( 'extended_filename ''', @extended_filename, ''' trying loaded silver layer without raw layer'); throw 50000, @errormsg, 201; end --make sure file already tracked ( @extended_filename (select [extended_filename] meta.log_files) ) begin set @errormsg = concat ( 'extended_filename ''', @extended_filename, ''' exist log_files table'); throw 50000, @errormsg, 201; end -- step 3: validate activity ('raw', 'silver', 'archive') ( @activity ( 'raw', 'silver', 'archive' ) ) begin set @errormsg = concat ( 'activity parameter provided ''', @activity, ''' valid, accepted values ''raw'', ''silver'' ''archive'''); throw 50000, @errormsg, 201; end -- step 4: start activity update begin try declare @result nvarchar(20); @success = 1 set @result = 'succeeded'; else set @result = 'failed'; ( @activity = 'raw') begin update meta.log_files set raw_info = @info_message, raw_status = @result, raw_ts = getdate() extended_filename = @extended_filename end else ( @activity = 'silver') begin update meta.log_files set silver_info = @info_message, silver_status = @result, silver_ts = getdate() extended_filename = @extended_filename end else ( @activity = 'archive') begin update meta.log_files set archive_info = @info_message, archive_status = @result, archive_ts = getdate() extended_filename = @extended_filename end ; end try begin catch --rethrow exception throw --return error code return -9 ; end catch end; go</file><file name="src\sql\metadb\tables\meta\meta.ais_landing_configuration.sql">create table meta.ais_landing_configuration ( source_system varchar(50) null -- eg "igt" ,source_environment varchar(20) null -- eg "prod" ,interface_name varchar(50) null -- eg "pdm" ,source_path varchar(200) null -- eg "pdm\archive" "igt\archive\xxx" ,is_enabled bit null -- set disabled receive anything ,t_last_file_ts datetime2 null -- timestamp last loaded file (next drop contain files newer timestamp) ,t_last_drop_ts datetime2 null -- timestamp last ais interface run effetively landed files ,t_last_sync_ts datetime2 null -- timestamp last ais interface run, whether landed files ,t_locked_until datetime2 null -- cet timestamp. null, interface locked timestamp (eg. drop) ); go alter table meta.ais_landing_configuration add constraint pk__meta__ais_landing_configuration primary key clustered ( source_system, source_environment, interface_name)</file><file name="src\sql\metadb\tables\meta\meta.ais_landing_log.sql">create table meta.ais_landing_log ( log_id int null identity(1,1) ,log_ts datetime2 null -- timestamp log entry created (cest local time) ,log_type varchar(20) null ,source_system varchar(50) null -- eg "igt" ,source_environment varchar(20) null -- eg "dev" ,interface_name varchar(50) null -- eg "pdm" ,last_file_ts datetime2 null ,drop_ts datetime2 null ,json_parameters nvarchar(max) null ,files_dropped nvarchar(max) null ) go alter table meta.ais_landing_log add constraint pk__meta__ais_landing_log primary key clustered ( log_id )</file><file name="src\sql\metadb\tables\meta\meta.log_files.sql">-- drop create logging table files -- primary key: (file_id) --drop table exists [meta].[log_files] create table [meta].[log_files] ( [file_id] int null identity(1,1), -- id file; auto-generated sql creating new instance [task_id] int null, -- id task processing file [plan_id] int null, -- id plan processing file [registration_ts] datetime null, -- timestamp file first registered [filename] nvarchar(150) null, -- name file landing [extended_filename] nvarchar(200) null, -- filename_folder: add name folder file found -&gt; expect timestamp [upload_ts] datetime null, -- timestamp file uploaded landing [landing_ts] datetime null, -- timestamp last ingestion landing folder [landing_status] nvarchar(150) null, -- status file:succeeded, failed, null [landing_info] nvarchar(max) null, -- location file landing folder [raw_ts] datetime null, -- timestamp last ingestion raw folder [raw_status] nvarchar(150) null, -- status file:succeeded, failed, null [raw_info] nvarchar(max) null, -- location file raw folder [silver_ts] datetime null, -- timestamp last ingestion silver folder [silver_status] nvarchar(150) null, -- status file:succeeded, failed, null [silver_info] nvarchar(max) null, -- location file silver folder [archive_ts] datetime null, -- timestamp last ingestion archive folder [archive_status] nvarchar(150) null, -- status file:succeeded, failed, null [archive_info] nvarchar(max) null, -- location file archive folder -- [record_count_init] int null, -- total number records found file -- [record_count_stg] int null, -- records staged (not including rejected) -- [record_count_ins] int null, -- records inserted final table -- [record_count_del] int null, -- records deleted final table -- [record_count_upd] int null, -- records updated final table (type 1 2) -- [record_count_rej] int null -- records rejected staging ) go alter table [meta].[log_files] add constraint [pk__meta__log_files] primary key clustered ( [file_id] asc ) (data_compression = page); go alter table [meta].[log_files] add constraint [uc__meta__log_files__extended_filename] unique ( [extended_filename] ) go</file><file name="src\sql\metadb\tables\meta\meta.log_plans.sql">-- drop create logging table plans -- primary key: (plan_id) -- drop table exists meta.log_plans create table meta.log_plans ( -- batch configuration [plan_id] int null identity(1,1), -- id plan -&gt; auto-completed sql [pipeline_id] varchar(40) null, -- id synapse pipeline initiated plan [plan_name] nvarchar(100) null, -- name plan [registration_ts] datetime null, -- timestamp plan first registrated [run_id] int null, -- id run first initiated plan -- status tracking [start_ts] datetime null, -- timestamp plan started [end_ts] datetime null, -- timestamp plan ended [current_status] nvarchar(15) null, -- status plan: pending, progress, succeeded, failed, aborted [last_pipeline_id] int null, -- id run last tried execute plan [comment] nvarchar(max) null -- optional comment ) ; go alter table meta.log_plans add constraint meta__pk__log_plans primary key clustered (plan_id) ( data_compression = page ) ; go alter table meta.log_plans add constraint ck__meta__log_plans__current_status check (([current_status]='aborted' [current_status]='in progress' [current_status]='cancelled' [current_status]='succeeded' [current_status]='failed' [current_status]='pending')) go</file><file name="src\sql\metadb\tables\meta\meta.log_plan_tasks.sql">-- drop create logging table plan_tasks -- primary key: (plan_id, task_id) -- drop table exists [meta].[log_plan_tasks] create table [meta].[log_plan_tasks] ( [plan_id] int null, -- id plan [plan_name] nvarchar(100) null, -- name plan [task_id] int null, -- id task [task_name] nvarchar(100) null, -- name task [task_group] nvarchar(100) null, -- task group task [task_type] nvarchar(100) null, -- executor task: synapse_pipeline spark_notebook [worker_name] nvarchar(100) null, -- name worker execute task [task_sequence] int null, -- index task plan [original_plan_id] int null -- id plan first tried execute task ) ; go alter table meta.log_plan_tasks add constraint pk__meta__log_plan_tasks primary key clustered ( plan_id, task_id ) (data_compression = page); go</file><file name="src\sql\metadb\tables\meta\meta.log_runs.sql">-- drop create logging table pipeline runs -- primary key: (run_id) -- drop table exists meta.log_runs create table meta.log_runs ( -- batch configuration [run_id] int null identity(1,1), -- id run -&gt; auto-generated sql [pipeline_id] nvarchar(40) null, -- id synapse pipeline initiated run [new_plan] bit null, -- boolean: starting new plan? [plan_name] varchar(100) null, -- name plan executed [current_status] nvarchar(100) null, -- current status run: pending, progress, success, failed... [registration_ts] datetime null, -- timestamp run first registered [start_ts] datetime null, -- timestamp run started [end_ts] datetime null, -- timestamp run ended [comment] nvarchar(max) null -- optional comment ) ; go alter table meta.log_runs add constraint meta__pk__log_runs primary key clustered (run_id) ( data_compression = page ) ; go</file><file name="src\sql\metadb\tables\meta\meta.log_run_plans.sql">-- drop create logging table files -- primary key: (plan_id, run_id) -- drop table exists meta.log_run_plans create table meta.log_run_plans ( [run_id] int null, -- id run [plan_id] int null, -- id plan [plan_name] nvarchar(100) null, -- name plan [original_run_id] int null, -- id run originally initiated plan ) ; go alter table meta.log_run_plans add constraint pk__meta__log_run_plans primary key clustered ( plan_id, run_id ) (data_compression = page); go</file><file name="src\sql\metadb\tables\meta\meta.log_tasks.sql">-- drop create logging table tasks -- primary key: (task_id) -- drop table exists meta.log_tasks create table meta.log_tasks ( -- job configuration [task_id] int null identity(1,1), -- id task -&gt; auto-generated sql [task_name] nvarchar(100) null, -- name task [registration_ts] datetime null, -- timestamp task first registered [plan_id] int null, -- id first plan tried executing task -- status tracking [start_ts] datetime null, -- timestamp task started [end_ts] datetime null, -- timestamp task ended [current_status] nvarchar(15) null, -- current status task: pending, progress, success, failure... [comment] nvarchar(max) null, -- optional comment [max_value] nvarchar(50) null, -- ?? -- record_count_stg int null, -- records staged (not including rejected) -- record_count_ins int null, -- records inserted final table -- record_count_del int null, -- records deleted final table -- record_count_upd int null, -- records updated final table (type 1 2) -- record_count_rej int null, -- records rejected staging [attempts] int null, -- number attempts made execute task [last_plan_id] int null, -- last plan id tried executing task [pipeline_id] varchar(40) null, -- ssis/df run guid [archive_flg] nvarchar(1) null -- set data archived (applicable landing data only) ) ; go alter table meta.log_tasks add constraint pk__meta__log_tasks primary key (task_id) (data_compression = page); go alter table meta.log_tasks add constraint ck__meta__log_tasks__current_status check (([current_status]='aborted' [current_status]='in progress' [current_status]='cancelled' [current_status]='succeeded' [current_status]='failed' [current_status]='pending')); go</file><file name="src\sql\metadb\tables\meta\meta.plan_configuration.sql">-- drop create configuration table plans -- primary key: (plan_name) -- drop table exists meta.plan_configuration create table meta.plan_configuration ( [plan_name] nvarchar(100) null, -- name plan [plan_description] nvarchar(max) null, -- description plan [enabled] bit null -- boolean: plan enabled? ); go alter table meta.plan_configuration add constraint pk__meta__plan_configuration primary key (plan_name) (data_compression = page); go</file><file name="src\sql\metadb\tables\meta\meta.plan_task_configuration.sql">-- drop create configuration table plan_task combintations -- primary key: (plan_name, task_group, task_name) -- drop table exists meta.plan_task_configuration create table meta.plan_task_configuration ( [plan_name] nvarchar(100) null, -- name plan [task_name] nvarchar(100) null, -- name task [task_sequence] nvarchar(100) null, -- execution sequence task plan [task_group] nvarchar(100) null, -- task group task [enabled] bit null -- boolean: plan_task combination enabled? ); go alter table meta.plan_task_configuration add constraint pk_meta__plan_task_configuration primary key ( [plan_name] asc, [task_group] asc, [task_name] asc) (data_compression = page) go</file><file name="src\sql\metadb\tables\meta\meta.proc_log.sql">create table meta.proc_log ( log_id int null identity(1,1) ,log_ts datetime2 null -- timestamp log entry created cest local time ,correlation_id varchar(50) null ,[log_type] varchar(20) null -- info, debug, error, warning, query ,[proc_name] sysname null -- name stored procedure ,[statement_name] varchar(100) null -- name statement location proc ,[message] nvarchar(max) null -- freetext message ,[extra_parameters] nvarchar(max) null -- json ,[query_text] nvarchar(max) null -- placeholder store dynamically generated queries ); go alter table meta.proc_log add constraint pk__meta__proc_log primary key clustered ( log_id )</file><file name="src\sql\metadb\tables\meta\meta.source_check_configuration.sql">-- drop create configuration table assign right checks task -- primary key: (error_name) -- drop table exists meta.source_check_configuration -- do: write test see database exists / created create table meta.source_check_configuration ( [check_name] nvarchar(100) null, -- name check [file_layout] nvarchar(100) null, -- layout file: used identify sources paired checks [enabled] bit null, -- boolean: task_check combination enabled? [config_params] nvarchar(max) null -- column flat json-objects stored strings save needed parameters regarding configuration checks ); go -- create pk alter table meta.source_check_configuration add constraint pk__meta__task_check_configuration primary key (check_name, file_layout) (data_compression = page); -- compress data save space go</file><file name="src\sql\metadb\tables\meta\meta.source_column_configuration.sql">-- drop create configuration table data source columns -- primary key: (file_layout, column_order) -- drop table exists meta.source_column_configuration create table meta.source_column_configuration ( [file_layout] varchar(100) null, -- layout file: used identify sources paired tasks [source_name] varchar(100) null, -- column names source files [sink_name] varchar(100) null, -- column names delta table table [column_order] int null, -- order column source file (necessary headers) [dimension] varchar(5) null, -- dimension column: business key, slowly changing dimension, ... [data_type] varchar(20) null, -- expected data type values column [column_info] varchar(max) null -- additional information needed columns ); go alter table meta.source_column_configuration add constraint pk__meta__source_column_configuration primary key (file_layout, column_order) (data_compression = page); go</file><file name="src\sql\metadb\tables\meta\meta.source_configuration.sql">-- drop create configuration table data sources -- primary key: (file_layout) -- drop table exists meta.source_configuration create table meta.source_configuration ( [file_layout] nvarchar(100) null, -- layout file: used identify sources paired tasks [file_pattern] nvarchar(100) null, -- expected file_name pattern: name file look like (eg. qualifio_*) [file_kind] nvarchar(10) null, -- kind file: csv, json, parquet, zip, etc. (see enum powershell/modules/dataconfig/schemas/ingestionmetadata.schema.json) [file_extension] nvarchar(10) null, -- expected extension file: csv, json, ... [column_delimiter] nvarchar(5) null, -- column delimiter file content [row_delimiter] nvarchar(5) null, -- row delimiter file content [escape_character] nvarchar(5) null, -- escape character file content [quote_character] nvarchar(5) null, -- quote characted file content [header] nvarchar(5) null, -- boolean: expect files headers? [encoding] nvarchar(10) null, -- encoding file: utf-8, ansi... [skip_first_lines] integer null, -- integer indicating line start ingestion [source_conditions] nvarchar(max) null -- optional precond values: loadorder sequence, etc ) go alter table meta.source_configuration add constraint pk__meta__source_configuration primary key (file_layout) (data_compression = page); go</file><file name="src\sql\metadb\tables\meta\meta.task_configuration.sql">-- drop create configuration table tasks -- primary key: (task_name) create table meta.task_configuration ( [task_name] nvarchar(100) null, -- name task [task_category] varchar(20) null, -- category task: ingest preprocess [task_description] nvarchar(max) null, -- description task [task_type] nvarchar(100) null, -- executor task: synapse_pipeline, spark_notebook [worker_name] nvarchar(100) null, -- worker execute task [file_layout] nvarchar(100) null, -- ingestion tasks only: layout file: used identify sources paired sources [source_folder] nvarchar(100) null, -- location expect find file [container_name] nvarchar(50) null, -- data lake container expect find file [table_name] nvarchar(100) null, -- name table file needs ingested [target_options] nvarchar(max) null, -- potential additional values needed target location (partitioning columns, etc.) [preprocess_pattern] nvarchar(100) null, -- preprocessing tasks only: filter pattern [preprocess_worker_options] nvarchar(max) null, -- preprocessing tasks only: worker-specific parameters options (json) [enabled] bit null -- boolean: task enabled? ); go alter table meta.task_configuration add constraint pk__meta__task_configuration primary key (task_name) (data_compression = page); go</file><file name="src\sql\metadb_test\schemas\test.sql">create schema [test] authorization dbo;</file><file name="src\sql\metadb_test\stored_procedures\test.usp_reset_database.sql">/* reset database deleting non-configuration data - truncate 'lnd', 'stg', 'err', 'whs', 'deploy' tables - truncate deploy err tables - truncate batch/job/file tracking triple safety guard (helps to) prevent(s) accidential execution: - work dev/test databases (name like "gm6dwh-sndxx-sqldb-meta" equal "gm6dwh-int-sqldb-meta" "gm6dwh-tst-sqldb-meta") - need provide database explicitly parameter (must match database) - default mode "preview", set @sure parameter 1 execute truncates */ create procedure [test].[usp_reset_database] @database_name sysname, @sure bit = 0 begin --------------------------------------------- -- safeguard : dev/test databases --------------------------------------------- declare @database sysname = db_name(); ( @database like 'dev-dap-sqldb-core-meta' @database like 'int-dap-sqldb-core-meta') begin; throw 50000, 'error, use usp_reset_database allowed database', 1; end --------------------------------------------- -- safeguard : compare current db parameter --------------------------------------------- ( @database = @database_name) begin; throw 50000, 'error, provided database_name value match current database', 2; end --------------------------------------------- -- preview mode --------------------------------------------- ( @sure = 1 ) begin print '------------------------------------------------------' print 'preview mode - print executing statements ' print ' (set @sure parameter 1 avoid preview mode)' print '------------------------------------------------------' end --------------------------------------------- -- specific tables meta schemas --------------------------------------------- print 'truncate table meta.log_runs'; @sure = 1 truncate table meta.log_runs; print 'truncate table meta.log_plans'; @sure = 1 truncate table meta.log_plans; print 'truncate table meta.log_tasks'; @sure = 1 truncate table meta.log_tasks; print 'truncate table meta.log_files'; @sure = 1 truncate table meta.log_files; print 'truncate table meta.log_run_plans'; @sure = 1 truncate table meta.log_run_plans; print 'truncate table meta.log_plan_tasks'; @sure = 1 truncate table meta.log_plan_tasks; end</file><file name="src\sql\metadb_test\stored_procedures\test.usp_run_tsqlt_tests.sql">/* run test classes classified given type test classes run type determined evaluating test class name valid types classes run: unit : t_usp_* , t_vw_* integration : ti_* design : t_design_* consistency : tc_* */ create procedure test.usp_run_tsqlt_tests @test_type varchar(100), @debug int = 0 begin ---------------------------------------------------------------------------------------------------------------- -- validation ---------------------------------------------------------------------------------------------------------------- --validate test type declare @errormsg nvarchar(max) = concat( 'invalid value testtype parameter (', isnull( '''' + @test_type + '''', 'null'), '). valid values @test_type : unit, integration, design, consistency.'); isnull( @test_type, '') ( 'unit', 'integration', 'design', 'consistency' ) begin; throw 50000, @errormsg, 101; end ---------------------------------------------------------------------------------------------------------------- -- determine testclasses execcute ---------------------------------------------------------------------------------------------------------------- --table hold class names (schema names) declare @testclasses table ( testclass sysname ); --unit tests @test_type = 'unit' begin set nocount insert @testclasses select schemaname testclass ( select schema_name(major_id) schemaname sys.extended_properties [name] = 'tsqlt.testclass' class_desc = 'schema' ) schemaname like 't[_]usp[_]%' schemaname like 't[_]vw[_]%' schemaname like 't[_]fn[_]%' set nocount end --integration else @test_type = 'integration' begin set nocount insert @testclasses select schemaname testclass ( select schema_name(major_id) schemaname sys.extended_properties [name] = 'tsqlt.testclass' class_desc = 'schema' ) schemaname like 'ti[_]%' set nocount end -- design else @test_type = 'design' begin set nocount insert @testclasses select schemaname testclass ( select schema_name(major_id) schemaname sys.extended_properties [name] = 'tsqlt.testclass' class_desc = 'schema' ) schemaname like 't[_]design[_]%' set nocount end -- consistency else @test_type = 'consistency' begin set nocount insert @testclasses select schemaname testclass ( select schema_name(major_id) schemaname sys.extended_properties [name] = 'tsqlt.testclass' class_desc = 'schema' ) schemaname like 'tc[_]%' set nocount end else throw 50000, '@test_type parameter match valid types', 102 ; ---------------------------------------------------------------------------------------------------------------- -- test execution ---------------------------------------------------------------------------------------------------------------- set nocount on; --temporary table hold test results declare @results table ( [id] [int] null, [class] [nvarchar](max) null, [testcase] [nvarchar](max) null, [name] [nvarchar](max) null, [tranname] [nvarchar](max) null, [result] [nvarchar](max) null, [msg] [nvarchar](max) null, [teststarttime] [datetime] null, [testendtime] [datetime] null ); --loop run tests declare @testclass sysname = '' @testclass null begin select @testclass = min(testclass) @testclasses testclass &gt; @testclass; @testclass null break; begin try @debug &gt; 0 print 'tsqlt.run ''' + @testclass + '''...'; exec tsqlt.run @testclass , 'tsqlt.nulltestresultformatter'; end try begin catch --ignore continue end catch insert @results select * tsqlt.testresult; end --restore results runs truncate table tsqlt.testresult; begin try set identity_insert tsqlt.testresult on; insert tsqlt.testresult ( [id] ,[class], [testcase], [tranname], [result], [msg], [teststarttime], [testendtime] ) select [id] ,[class], [testcase], [tranname], [result], [msg], [teststarttime], [testendtime] @results; set identity_insert tsqlt.testresult off; end try begin catch set identity_insert tsqlt.testresult off; throw; end catch --results xml --exec tsqlt.xmlresultformatter; -- output console output exec tsqlt.defaultresultformatter; -- list classes errors declare @classeswitherrors nvarchar(max) = null select distinct @classeswitherrors = isnull(@classeswitherrors + ',','') + class tsqlt.testresult ( result = 'success') --return execution summary dataset select count(id) numtests, sum(case result = 'success' 1 else 0 end) numsuccess, sum(case result = 'success' 0 else 1 end) numfailed, @classeswitherrors classeswitherrors tsqlt.testresult return 0; end go --exec test.usp_run_tsqlt_tests @test_type = 'unit'</file><file name="src\sql\metadb_test\t_design_tests\class_t_design_rules.sql">create schema [t_design_rules] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_design_rules'</file><file name="src\sql\metadb_test\t_design_tests\t_design_rules -- schema name convention.sql">create procedure [t_design_rules].[test schema name convention] begin /* schema names must (all) lowercase */ declare @names varchar(250) = null; testdata ( select schema_name information_schema.schemata 1=1 schema_owner = 'meta' schema_name &lt;&gt; 'tsqlt' schema_name like 't[_]%' schema_name like 't_[_]%' lower(schema_name) &lt;&gt; schema_name collate latin1_general_cs_as ) select @names = isnull(@names + ', ' , '') + schema_name testdata; declare @error_msg varchar(250) = 'the following schemas full lowercase per design rule: ' + @names; exec tsqlt.assertequals @expected = null, @actual = @names, @message = @error_msg; end</file><file name="src\sql\metadb_test\t_design_tests\t_design_rules -- table name convention.sql">create procedure [t_design_rules].[test table name convention] begin /* table names must (all) lowercase */ declare @names varchar(250) = null; declare @num_fail int = 0; testdata ( select distinct tbl.table_schema + '.' + tbl.table_name table_name information_schema.tables tbl 1=1 --exclude test schemas: tbl.table_schema ('test','tsqlt') tbl.table_type = 'base table' tbl.table_name &lt;&gt; lower(tbl.table_name) collate latin1_general_cs_as --exclude ssdt table: table_name &lt;&gt; '__refactorlog' ) select @names = isnull(@names + ', ' , '') + table_name , @num_fail = @num_fail + 1 testdata; declare @error_msg varchar(250) = 'the following tables full lowercase per design rule: ' + @names; exec tsqlt.assertequals @expected = 0, @actual = @num_fail, @message = @error_msg; end go</file><file name="src\sql\metadb_test\t_unit_tests\t_fn_add_json_property\class.sql">create schema [t_fn_add_json_property] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_fn_add_json_property'</file><file name="src\sql\metadb_test\t_unit_tests\t_fn_add_json_property\test_normal_execution.sql">/*if object_id('[t_fn_add_json_property].[test_normal_execution]') null drop proc [t_fn_add_json_property].[test_normal_execution] go */ create procedure [t_fn_add_json_property].[test_normal_execution] begin ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @startjson json; declare @expectedvalue json; declare @actualvalue json; -- tsqlt asserts use variant datatype, json supported, we'll convert varchar assertions: declare @expectedvaluestring varchar(1000); declare @actualvaluestring varchar(1000); ---------------------------------------------------------------------------------------------------------------- -- act &amp; assert ---------------------------------------------------------------------------------------------------------------- -- add property null string set @expectedvalue = '{"foo": "bar"}'; set @actualvalue = (select meta.fn_add_json_property(null, 'foo', 'bar')); set @expectedvaluestring = convert(varchar(1000),@expectedvalue) set @actualvaluestring = convert(varchar(1000),@actualvalue) exec tsqlt.assertequals @expected = @expectedvaluestring, @actual = @actualvaluestring , @message = 'test: adding property null string failed.' -- add property empty string set @expectedvalue = '{"foo": "bar"}'; set @actualvalue = (select meta.fn_add_json_property('', 'foo', 'bar')); set @expectedvaluestring = convert(varchar(1000),@expectedvalue) set @actualvaluestring = convert(varchar(1000),@actualvalue) exec tsqlt.assertequals @expected = @expectedvaluestring, @actual = @actualvaluestring , @message = 'test: adding property empty string failed.' -- add property json string explicit path set @startjson = '{"test": {}}' set @expectedvalue = '{"test": {"foo": "bar"}}'; set @actualvalue = (select meta.fn_add_json_property(convert(nvarchar(max),@startjson), '$.test.foo', 'bar')); set @expectedvaluestring = convert(varchar(1000),@expectedvalue) set @actualvaluestring = convert(varchar(1000),@actualvalue) exec tsqlt.assertequals @expected = @expectedvaluestring, @actual = @actualvaluestring , @message = 'test: adding property explicit path failed.' end go --exec tsqlt.run '[t_fn_add_json_property].[test_normal_execution]'</file><file name="src\sql\metadb_test\t_unit_tests\t_fn_add_json_property\test_should_fail_when_invalid_json.sql">/* object_id('[t_fn_add_json_property].[test_should_fail_when_invalid_json]') null drop proc [t_fn_add_json_property].[test_should_fail_when_invalid_json] go */ create procedure [t_fn_add_json_property].[test_should_fail_when_invalid_json] begin ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @expectedvaluestring varchar(1000); declare @actualvaluestring varchar(1000); ---------------------------------------------------------------------------------------------------------------- -- act &amp; assert ---------------------------------------------------------------------------------------------------------------- -- add property invalid start string set @expectedvaluestring = 'error%not valid json%'; set @actualvaluestring = (select meta.fn_add_json_property('invalid', 'foo', 'bar')); exec tsqlt.assertlike @expectedpattern = @expectedvaluestring, @actual = @actualvaluestring , @message = 'test: adding property invalid json failed.' end go --exec tsqlt.run '[t_fn_add_json_property].[test_should_fail_when_invalid_json]'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\class_t_usp_ais_get_landing_configuration.sql">create schema [t_usp_ais_get_landing_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_ais_get_landing_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\setup_t_usp_ais_get_landing_configuration.sql">create procedure [t_usp_ais_get_landing_configuration].[setup] begin exec tsqlt.faketable 'meta.ais_landing_configuration' exec tsqlt.faketable 'meta.ais_landing_log', @identity=1 exec tsqlt.faketable 'meta.proc_log', @identity=1 insert meta.ais_landing_configuration (source_system, source_environment, interface_name, source_path, is_enabled, t_last_file_ts, t_locked_until) values ('igt' , 'acc' , 'pdm' , 'pdm/archive' , 1 , null , null), ('igt' , 'prod', 'pdm' , 'pdm/archive' , 1 , null , null), ('igt' , 'dev' , 'something_else', 'blah' , 1 , '2024-06-01 13:14:15', '2024-01-01'), ('disabled', 'blah', 'foo' , 'bar' , 0 , null , '2024-01-01') ; end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\test_locking.sql">/* object_id('[t_usp_ais_get_landing_configuration].[test_normal_execution]') null drop procedure [t_usp_ais_get_landing_configuration].[test_normal_execution] go */ create procedure [t_usp_ais_get_landing_configuration].[test_locking] begin /* test: we'll three calls proc, lock_hours = 1 start 1 disabled 3 enabled records, two lock one old, expired lock * first time, expect 3 records returned, "locked_until" set @drop_timestamp + 1 hour * second time simulate minutes later, expect results (everything still locked) * third time simulate next day, expect 3 results (locks expired) */ -- enabled @debug test development declare @debug bit = 0; ---------------------------------------------------------------------------------------------------------------- -- assemble (1) ---------------------------------------------------------------------------------------------------------------- declare @drop_ts datetime = '2024-06-25 12:11:56.933' create table t_usp_ais_get_landing_configuration.actual ( source_system varchar(100), source_environment varchar(100), interface_name varchar(100), source_path varchar(200), target_storage varchar(100), target_rg varchar(100), target_container varchar(100), target_path varchar(500), files_from_ts datetime2, drop_ts datetime2, locked_until datetime2 ); ---------------------------------------------------------------------------------------------------------------- -- act (1) ---------------------------------------------------------------------------------------------------------------- -- get configuration lock_hours = 1 insert t_usp_ais_get_landing_configuration.actual exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts, @lock_hours = 1 @debug=1 select 'actual' actual, * t_usp_ais_get_landing_configuration.actual ---------------------------------------------------------------------------------------------------------------- -- assert (1) ---------------------------------------------------------------------------------------------------------------- -- original setup data contains 3 enabled records, 2 lock (null) 1 expired lock -- expect three returned, updated lock declare @expectedvalue varchar(50); declare @actualvalue varchar(50); -- check recordcount (the fake config table contains disabled record ignored) set @expectedvalue = 3 set @actualvalue = (select count(*) t_usp_ais_get_landing_configuration.actual) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' -- check lock set set @expectedvalue = 3 declare @lock_until datetime2 = dateadd(hour, 1, @drop_ts); @debug=1 begin select 'config' configuration, *, @lock_until expected_lock meta.ais_landing_configuration end set @actualvalue = (select count(*) meta.ais_landing_configuration t_locked_until = @lock_until); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' ---------------------------------------------------------------------------------------------------------------- -- assemble (2) ---------------------------------------------------------------------------------------------------------------- -- minutes later set @drop_ts = '2024-06-25 12:15:00' -- clear results table truncate table t_usp_ais_get_landing_configuration.actual ---------------------------------------------------------------------------------------------------------------- -- act (2) ---------------------------------------------------------------------------------------------------------------- -- get configuration lock_hours = 1 insert t_usp_ais_get_landing_configuration.actual exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts, @lock_hours = 1 @debug=1 select 'actual' actual, * t_usp_ais_get_landing_configuration.actual ---------------------------------------------------------------------------------------------------------------- -- assert (2) ---------------------------------------------------------------------------------------------------------------- -- ran procedure again, enabled records "locked", expected results set @expectedvalue = 0 set @actualvalue = (select count(*) t_usp_ais_get_landing_configuration.actual) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' ---------------------------------------------------------------------------------------------------------------- -- assemble (3) ---------------------------------------------------------------------------------------------------------------- -- hours later set @drop_ts = '2024-06-26 00:00:000' -- clear results table truncate table t_usp_ais_get_landing_configuration.actual ---------------------------------------------------------------------------------------------------------------- -- act (3) ---------------------------------------------------------------------------------------------------------------- -- get configuration lock_hours = 1 insert t_usp_ais_get_landing_configuration.actual exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts, @lock_hours = 1 @debug=1 select 'actual' actual, * t_usp_ais_get_landing_configuration.actual ---------------------------------------------------------------------------------------------------------------- -- assert (3) ---------------------------------------------------------------------------------------------------------------- -- ran procedure "few hours later", past previous locks expect 3 recors updated locks set @expectedvalue = 3 set @actualvalue = (select count(*) t_usp_ais_get_landing_configuration.actual) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' -- check lock set set @expectedvalue = 3 set @lock_until = dateadd(hour, 1, @drop_ts); @debug=1 begin select 'config' configuration, *, @lock_until expected_lock meta.ais_landing_configuration end set @actualvalue = (select count(*) meta.ais_landing_configuration t_locked_until = @lock_until); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' end go /* exec tsqlt.run 't_usp_ais_get_landing_configuration.test_normal_execution' */</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\test_normal_execution.sql">/* object_id('[t_usp_ais_get_landing_configuration].[test_normal_execution]') null drop procedure [t_usp_ais_get_landing_configuration].[test_normal_execution] go */ create procedure [t_usp_ais_get_landing_configuration].[test_normal_execution] begin ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @drop_ts datetime = '2024-06-25 12:11:56.933' declare @drop_ts_string varchar(50) = '20240625_121156' declare @env varchar(50) = (select left( db_name(), 3)); create table t_usp_ais_get_landing_configuration.expected ( source_system varchar(100), source_environment varchar(100), interface_name varchar(100), source_path varchar(200), target_storage varchar(100), target_rg varchar(100), target_container varchar(100), target_path varchar(500), files_from_ts datetime2, drop_ts datetime2, locked_until datetime2 ) create table t_usp_ais_get_landing_configuration.actual ( source_system varchar(100), source_environment varchar(100), interface_name varchar(100), source_path varchar(200), target_storage varchar(100), target_rg varchar(100), target_container varchar(100), target_path varchar(500), files_from_ts datetime2, drop_ts datetime2, locked_until datetime2 ) insert t_usp_ais_get_landing_configuration.expected ( source_system, source_environment, interface_name, source_path, target_storage, target_rg, target_container, target_path, files_from_ts, drop_ts, locked_until ) select source_system ,source_environment ,interface_name ,source_path ,@env + 'dapstdala1' target_storage ,@env + '-dap-rg-dala' target_rg ,'landing' target_container ,source_system + '/' + source_environment + '/' + interface_name + '/' + @drop_ts_string + '/' target_path ,isnull(t_last_file_ts,'1900-01-01') files_from_ts ,@drop_ts drop_ts ,@drop_ts locked_until meta.ais_landing_configuration is_enabled = 1 --select * t_usp_ais_get_landing_configuration.expected ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- insert t_usp_ais_get_landing_configuration.actual exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts --select * t_usp_ais_get_landing_configuration.actual ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(50); declare @actualvalue varchar(50); -- check recordcount (the fake config table contains disabled record ignored) set @expectedvalue = 3 set @actualvalue = (select count(*) t_usp_ais_get_landing_configuration.actual) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' -- check results exec tsqlt.assertequalstable @expected = 't_usp_ais_get_landing_configuration.expected', @actual = 't_usp_ais_get_landing_configuration.actual' , @message = 'expected results table check failed.' end go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\class_t_usp_ais_log_landing.sql">create schema [t_usp_ais_log_landing] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_ais_log_landing'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\setup_t_usp_ais_log_landing.sql">create procedure [t_usp_ais_log_landing].[setup] begin exec tsqlt.faketable 'meta.ais_landing_configuration' exec tsqlt.faketable 'meta.ais_landing_log', @identity=1 exec tsqlt.faketable 'meta.proc_log', @identity=1 insert meta.ais_landing_configuration (source_system, source_environment, interface_name, source_path, is_enabled, t_last_file_ts) values ('igt', 'acc', 'pdm', 'pdm/archive' , 1, null), ('igt', 'prod', 'pdm', 'pdm/archive', 1, null), ('igt', 'dev', 'something_else','blah', 1, '2024-06-01 13:14:15'), ('disabled', 'blah', 'foo','bar', 0, null) ; end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_bad_log_type.sql">/* object_id('[t_usp_ais_log_landing].[test_bad_logtype]') null drop procedure [t_usp_ais_log_landing].[test_bad_logtype] go */ create procedure [t_usp_ais_log_landing].[test_bad_logtype] begin /* test validates log_type validation works * throw error parameter validation * update last_file_ts last_drop_ts config table * update last_sync_ts config table */ -- set 1 test development output additional information declare @test_debug bit = 0; ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @drop_ts varchar(50) = '2024-06-25 12:11:56.9330000', @drop_ts_string varchar(50) = '20240625_121156', @correlation_id varchar(20) = 'unit_test', @source_system varchar(50) = 'igt', @source_environment varchar(50) = 'prod', @interface_name varchar(50) = 'pdm', @error_message varchar(100) = 'error message unit test' ; update meta.ais_landing_configuration set t_last_file_ts = '2000-01-01', t_last_drop_ts = '2000-01-01', t_last_sync_ts = '2000-01-01' source_system = @source_system source_environment = @source_environment interface_name = @interface_name; @test_debug = 1 begin select 'before', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; end ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- declare @error_message_raised nvarchar(max); declare @json_parameters nvarchar(max) = '{ "error_message": "' + @error_message + '"}' -- run bad log_type_value begin try exec meta.usp_ais_log_landing @log_type = 'bad', @drop_timestamp = @drop_ts, @source_system = @source_system, @source_environment = @source_environment, @interface_name = @interface_name, @json_parameters = @json_parameters, @files_dropped = null, @correlation_id = @correlation_id end try begin catch set @error_message_raised = error_message() end catch ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(50), @actualvalue varchar(50), @expecteddate datetime2, @actualdate datetime2; -- verify exception thrown exec tsqlt.assertlike @expectedpattern = '%provided log_type value valid%', @actual = @error_message_raised -- verify configuration change set @expecteddate = convert(date,'2000-01-01'); set @actualdate = (select t_last_file_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name); exec tsqlt.assertequals @expected = @expecteddate, @actual = @actualdate set @actualdate = (select t_last_drop_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name); exec tsqlt.assertequals @expected = @expecteddate, @actual = @actualdate set @actualdate = (select t_last_sync_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name); exec tsqlt.assertequals @expected = @expecteddate, @actual = @actualdate -- verify ais_landing_log made set @expectedvalue = 0; set @actualvalue = (select count(1) meta.ais_landing_log); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue; @test_debug = 1 begin select 'after', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; select * meta.ais_landing_log; select * meta.proc_log; end end go /* object_id('t_usp_ais_log_landing.[test_bad_logtype]') null throw 50000, 'test procedure doesn''t exist', 1; exec tsqlt.run 't_usp_ais_log_landing.[test_bad_logtype]'; */</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_fail.sql">/* object_id('[t_usp_ais_log_landing].[test_normal_execution_fail]') null drop procedure [t_usp_ais_log_landing].[test_normal_execution_fail] go */ create procedure [t_usp_ais_log_landing].[test_normal_execution_fail] begin /* test validates logging failed landing works correctly * log event * update last_file_ts last_drop_ts config table * update last_sync_ts config table */ -- set 1 test development output additional information declare @test_debug bit = 0; ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @drop_ts varchar(50) = '2024-06-25 12:11:56.9330000', @drop_ts_string varchar(50) = '20240625_121156', @correlation_id varchar(20) = 'unit_test', @source_system varchar(50) = 'igt', @source_environment varchar(50) = 'prod', @interface_name varchar(50) = 'pdm', @error_message varchar(100) = 'error message unit test' ; update meta.ais_landing_configuration set t_last_file_ts = '2000-01-01', t_last_drop_ts = '2000-01-01', t_last_sync_ts = '2000-01-01' source_system = @source_system source_environment = @source_environment interface_name = @interface_name; @test_debug = 1 begin select 'before', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; end ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- declare @json_parameters nvarchar(max) = '{ "error_message": "' + @error_message + '"}' -- run 'fail' log_type exec meta.usp_ais_log_landing @log_type = 'fail', @drop_timestamp = @drop_ts, @source_system = @source_system, @source_environment = @source_environment, @interface_name = @interface_name, @json_parameters = @json_parameters, @files_dropped = null, @correlation_id = @correlation_id ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- @test_debug = 1 begin select 'after', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; select * meta.ais_landing_log; select * meta.proc_log; end declare @expectedvalue varchar(50), @actualvalue varchar(50), @expecteddate datetime2, @actualdate datetime2; -- verify configuration change set @expecteddate = convert(date,'2000-01-01'); set @actualdate = (select t_last_file_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name); exec tsqlt.assertequals @expected = @expecteddate, @actual = @actualdate set @actualdate = (select t_last_drop_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name); exec tsqlt.assertequals @expected = @expecteddate, @actual = @actualdate set @actualdate = (select t_last_sync_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name); exec tsqlt.assertequals @expected = @expecteddate, @actual = @actualdate -- verify ais_landing_log made set @expectedvalue = 1; set @actualvalue = (select count(1) meta.ais_landing_log ); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue; set @actualvalue = (select count(1) meta.ais_landing_log log_type = 'fail'); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue; end go /* object_id('t_usp_ais_log_landing.[test_normal_execution_fail]') null throw 50000, 'test procedure doesn''t exist', 1; exec tsqlt.run 't_usp_ais_log_landing.[test_normal_execution_fail]'; */</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_filedates.sql">/* object_id('[t_usp_ais_log_landing].[test_normal_execution_filedates]') null drop procedure [t_usp_ais_log_landing].[test_normal_execution_filedates] go */ create procedure [t_usp_ais_log_landing].[test_normal_execution_filedates] begin -- set 1 test development output additional information declare @test_debug bit = 0 ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @drop_ts varchar(50) = '2024-06-25 12:11:56.9330000', @drop_ts_string varchar(50) = '20240625_121156', @correlation_id varchar(20) = 'unit_test', @source_system varchar(50) = 'igt', @source_environment varchar(50) = 'prod', @interface_name varchar(50) = 'pdm' --,@last_file_ts varchar(50) = '2020-10-02 07:00:00.0000000' ; @test_debug = 1 begin select 'before', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; end ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- declare @json_parameters nvarchar(max) = '{ "target_folder": "xxx/yyy/20240502_123456/", "pipeline_guid": "123-456-789-zzzz" }'; declare @files_dropped nvarchar(max) = '[ { "filename": "pdm_001.zip", "filedate": "2020-10-02 07:00:00.0000000" }, { "filename": "pdm_002.zip", "filedate": "2020-10-03 09:00:00.0000000" } ]' exec meta.usp_ais_log_landing @drop_timestamp = @drop_ts, @source_system = @source_system, @source_environment = @source_environment, @interface_name = @interface_name, @json_parameters = @json_parameters, @files_dropped = @files_dropped, @correlation_id = @correlation_id @test_debug = 1 begin select 'after', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; select 'after', * meta.proc_log select 'after', * meta.ais_landing_log end ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(50); declare @actualvalue varchar(50); -- check landing log set @expectedvalue = 1 set @actualvalue = (select count(*) meta.ais_landing_log) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'landing_log recordcount check failed.' -- check config table declare @expected_last_file_ts varchar(50) = '2020-10-03 09:00:00.0000000', @actual_last_file_ts varchar(50), @actual_drop_ts varchar(50), @actual_sync_ts varchar(50) ; select @actual_last_file_ts = t_last_file_ts, @actual_drop_ts = t_last_drop_ts, @actual_sync_ts = t_last_sync_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; exec tsqlt.assertequals @expected = @expected_last_file_ts, @actual = @actual_last_file_ts , @message = 'found unexpected t_last_file_ts configuration table' exec tsqlt.assertequals @expected = @drop_ts, @actual = @actual_drop_ts , @message = 'found unexpected t_last_drop_ts configuration table' exec tsqlt.assertequals @expected = @drop_ts, @actual = @actual_sync_ts , @message = 'found unexpected t_last_sync_ts configuration table' end go --exec tsqlt.run 't_usp_ais_log_landing.[test_normal_execution_filedates]'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_last_file_ts.sql">/* drop procedure [t_usp_ais_log_landing].[test_normal_execution_last_file_ts] go */ create procedure [t_usp_ais_log_landing].[test_normal_execution_last_file_ts] begin -- set 1 test development output additional information declare @test_debug bit = 0 ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @drop_ts varchar(50) = '2024-06-25 12:11:56.9330000', @drop_ts_string varchar(50) = '20240625_121156', @correlation_id varchar(20) = 'unit_test', @source_system varchar(50) = 'igt', @source_environment varchar(50) = 'prod', @interface_name varchar(50) = 'pdm', @last_file_ts varchar(50) = '2020-10-02 07:00:00.0000000' ; @test_debug = 1 begin select 'before', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; end ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- declare @json_parameters nvarchar(max) = '{ "last_file_ts": "' + @last_file_ts + '", "target_folder": "xxx/yyy/20240502_123456/", "pipeline_guid": "123-456-789-zzzz" }'; declare @files_dropped nvarchar(max) = '[ { "filename": "pdm_xxxxxx.zip", "foo": "bar" } ]' exec meta.usp_ais_log_landing @drop_timestamp = @drop_ts, @source_system = @source_system, @source_environment = @source_environment, @interface_name = @interface_name, @json_parameters = @json_parameters, @files_dropped = @files_dropped, @correlation_id = @correlation_id @test_debug = 1 begin select 'after', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; select 'proc_log', * meta.proc_log select 'ais_landing_log', * meta.ais_landing_log end ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(50); declare @actualvalue varchar(50); -- check landing log set @expectedvalue = 1 set @actualvalue = (select count(*) meta.ais_landing_log) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'landing_log recordcount check failed.' -- check config table declare @expected_last_file_ts varchar(50) = @last_file_ts, @actual_last_file_ts varchar(50), @actual_drop_ts varchar(50), @actual_sync_ts varchar(50) ; select @actual_last_file_ts = t_last_file_ts, @actual_drop_ts = t_last_drop_ts, @actual_sync_ts = t_last_sync_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; exec tsqlt.assertequals @expected = @expected_last_file_ts, @actual = @actual_last_file_ts , @message = 'found unexpected t_last_file_ts configuration table' exec tsqlt.assertequals @expected = @drop_ts, @actual = @actual_drop_ts , @message = 'found unexpected t_last_drop_ts configuration table' exec tsqlt.assertequals @expected = @drop_ts, @actual = @actual_sync_ts , @message = 'found unexpected t_last_sync_ts configuration table' end go --exec tsqlt.run 't_usp_ais_log_landing.[test_normal_execution_last_file_ts]'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_no_files.sql">/* object_id('[t_usp_ais_log_landing].[test_normal_execution_no_files]') null drop procedure [t_usp_ais_log_landing].[test_normal_execution_no_files] go */ create procedure [t_usp_ais_log_landing].[test_normal_execution_no_files] begin /* test validates logging "no files landed time event" behaves correctly. * log event * update last_file_ts last_drop_ts config table * update last_sync_ts config table test verifies behaviour already loaded least (so last_file_ts last_drop_ts) */ -- set 1 test development output additional information declare @test_debug bit = 0; ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(50); declare @actualvalue varchar(50); declare @drop_ts varchar(50) = '2024-06-25 12:11:56.9330000', @drop_ts_string varchar(50) = '20240625_121156', @correlation_id varchar(20) = 'unit_test', @source_system varchar(50) = 'igt', @source_environment varchar(50) = 'prod', @interface_name varchar(50) = 'pdm', @last_file_ts varchar(50) = '2024-01-01 00:00:00.0000000', @last_drop_ts varchar(50) = '2024-02-02 00:00:00.0000000' ; -- test expects one record config table set @expectedvalue = 1; set @actualvalue = ( select count(*) meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name ); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue, @message = 'test prerequisite check failed.' -- set test record normal situation (i.e. first time) update meta.ais_landing_configuration set t_last_file_ts = @last_file_ts , t_last_drop_ts = @last_drop_ts , t_last_sync_ts = @last_drop_ts source_system = @source_system source_environment = @source_environment interface_name = @interface_name @test_debug = 1 begin select 'before', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name end ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- -- logging files delivered declare @json_parameters nvarchar(max) = '{ "target_folder": "xxx/yyy/20240502_123456/", "pipeline_guid": "123-456-789-zzzz" }'; declare @files_dropped nvarchar(max) = '[]' exec meta.usp_ais_log_landing @drop_timestamp = @drop_ts, @source_system = @source_system, @source_environment = @source_environment, @interface_name = @interface_name, @json_parameters = @json_parameters, @files_dropped = @files_dropped, @correlation_id = @correlation_id @test_debug = 1 begin select 'after', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name select 'after', * meta.ais_landing_log select 'after', * meta.proc_log end ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- --check log - expect 1 record set @expectedvalue = 1; set @actualvalue = (select count(*) meta.ais_landing_log) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' -- check config table declare @expected_last_file_ts varchar(50) = @last_file_ts, --should remain unchanged @expected_drop_ts varchar(50) = @last_drop_ts, --should remain unchanged @expected_sync_ts varchar(50) = @drop_ts, --should updated @actual_last_file_ts varchar(50), @actual_drop_ts varchar(50), @actual_sync_ts varchar(50) ; select @actual_last_file_ts = t_last_file_ts, @actual_drop_ts = t_last_drop_ts, @actual_sync_ts = t_last_sync_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; exec tsqlt.assertequals @expected = @expected_last_file_ts, @actual = @actual_last_file_ts , @message = 'found unexpected t_last_file_ts configuration table'; exec tsqlt.assertequals @expected = @expected_drop_ts , @actual = @actual_drop_ts , @message = 'found unexpected t_last_drop_ts configuration table'; exec tsqlt.assertequals @expected = @expected_sync_ts , @actual = @actual_sync_ts , @message = 'found unexpected t_last_sync_ts configuration table'; end go /* object_id('t_usp_ais_log_landing.[test_normal_execution_no_files]') null throw 50000, 'test procedure doesn''t exist', 1; exec tsqlt.run 't_usp_ais_log_landing.[test_normal_execution_no_files]'; */</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_no_files_first_time.sql">/* object_id('[t_usp_ais_log_landing].[test_normal_execution_no_files_first_time]') null drop procedure [t_usp_ais_log_landing].[test_normal_execution_no_files_first_time] go */ create procedure [t_usp_ais_log_landing].[test_normal_execution_no_files_first_time] begin /* test validates logging "no files landed time event" behaves correctly. * log event * update last_file_ts last_drop_ts config table * update last_sync_ts config table test verifies behaviour interface wes never loaded (so last_file_ts, last_drop_ts, last_sync_ts) */ -- set 1 test development output additional information declare @test_debug bit = 0; ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(50); declare @actualvalue varchar(50); declare @drop_ts varchar(50) = '2024-06-25 12:11:56.9330000', @drop_ts_string varchar(50) = '20240625_121156', @correlation_id varchar(20) = 'unit_test', @source_system varchar(50) = 'igt', @source_environment varchar(50) = 'prod', @interface_name varchar(50) = 'pdm', @last_file_ts varchar(50) = null, @last_drop_ts varchar(50) = null ; -- test expects one record config table set @expectedvalue = 1; set @actualvalue = ( select count(*) meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name ); exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue, @message = 'test prerequisite check failed.' -- set test record "first time" situation update meta.ais_landing_configuration set t_last_file_ts = @last_file_ts , t_last_drop_ts = @last_drop_ts , t_last_sync_ts = @last_drop_ts source_system = @source_system source_environment = @source_environment interface_name = @interface_name @test_debug = 1 begin select 'before', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name end ---------------------------------------------------------------------------------------------------------------- -- act ---------------------------------------------------------------------------------------------------------------- -- logging files delivered declare @json_parameters nvarchar(max) = '{ "target_folder": "xxx/yyy/20240502_123456/", "pipeline_guid": "123-456-789-zzzz" }'; declare @files_dropped nvarchar(max) = '[]' exec meta.usp_ais_log_landing @drop_timestamp = @drop_ts, @source_system = @source_system, @source_environment = @source_environment, @interface_name = @interface_name, @json_parameters = @json_parameters, @files_dropped = @files_dropped, @correlation_id = @correlation_id @test_debug = 1 begin select 'after', * meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name select 'after', * meta.ais_landing_log select 'after', * meta.proc_log end ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- --check log - expect 1 record set @expectedvalue = 1; set @actualvalue = (select count(*) meta.ais_landing_log) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue , @message = 'recordcount check failed.' -- check config table declare @expected_last_file_ts varchar(50) = @last_file_ts, --should remain unchanged @expected_drop_ts varchar(50) = @last_drop_ts, --should remain unchanged @expected_sync_ts varchar(50) = @drop_ts, --should updated @actual_last_file_ts varchar(50), @actual_drop_ts varchar(50), @actual_sync_ts varchar(50) ; select @actual_last_file_ts = t_last_file_ts, @actual_drop_ts = t_last_drop_ts, @actual_sync_ts = t_last_sync_ts meta.ais_landing_configuration source_system = @source_system source_environment = @source_environment interface_name = @interface_name; exec tsqlt.assertequals @expected = @expected_last_file_ts, @actual = @actual_last_file_ts , @message = 'found unexpected t_last_file_ts configuration table'; exec tsqlt.assertequals @expected = @expected_drop_ts , @actual = @actual_drop_ts , @message = 'found unexpected t_last_drop_ts configuration table'; exec tsqlt.assertequals @expected = @expected_sync_ts , @actual = @actual_sync_ts , @message = 'found unexpected t_last_sync_ts configuration table'; end go /* object_id('t_usp_ais_log_landing.[test_normal_execution_no_files_first_time]') null throw 50000, 'test procedure doesn''t exist', 1; exec tsqlt.run 't_usp_ais_log_landing.[test_normal_execution_no_files_first_time]'; */</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\class_t_usp_end_plan.sql">create schema [t_usp_end_plan] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_end_plan'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\fail on pending tasks for successful plan.sql">create procedure [t_usp_end_plan].[fail pending tasks successful plan] @debug int = 0 begin -- declare set id params declare @plan_id int; declare @run_id int; -- test 2: success flag = 0 set @plan_id = 5 set @run_id = 5 execute tsqlt.expectexception @expectedmessagepattern = 'the plan plan_id 5 still 3 unsuccessful tasks' ; execute [meta].[usp_end_plan] @run_id = @run_id, @plan_id = @plan_id, @success_flag = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\failed plan should cancel tasks.sql">create procedure [t_usp_end_plan].[failed plan cancel tasks] @debug int = 0 begin -- declare set id params declare @plan_id int; declare @run_id int; -- test 2: success flag = 0 set @plan_id = 3 set @run_id = 3 create table [t_usp_end_plan].expected_log_tasks ( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(15) ) create table [t_usp_end_plan].actual_log_tasks ( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(15) ) create table [t_usp_end_plan].expected_log_plans ( [plan_id] int, [plan_name] nvarchar(100), [run_id] int, [current_status] nvarchar(15) ) create table [t_usp_end_plan].actual_log_plans ( [plan_id] int, [plan_name] nvarchar(100), [run_id] int, [current_status] nvarchar(15) ) insert [t_usp_end_plan].expected_log_plans(plan_id, plan_name, run_id, current_status) values (@plan_id, 'unit_test_end_plan_success', @run_id, 'failed') insert [t_usp_end_plan].expected_log_tasks(task_id, task_name, plan_id, current_status) values (1, 'progress_task', 3, 'cancelled'), (2, 'pending_task', 3, 'cancelled'), (3, 'successful_task', 3, 'succeeded'), (4, 'empty_task', 3, 'cancelled') exec meta.usp_end_plan @plan_id = @plan_id, @run_id = @run_id, @success_flag = 0 insert [t_usp_end_plan].actual_log_plans ( plan_id, plan_name, run_id, current_status ) select lp.plan_id, lp.plan_name, lrp.run_id, lp.current_status meta.log_plans lp inner join meta.log_run_plans lrp (lp.plan_id = lrp.plan_id) lp.plan_id = @plan_id insert [t_usp_end_plan].actual_log_tasks ( task_id, task_name, plan_id, current_status ) select task_id, task_name, plan_id, current_status meta.log_tasks plan_id = @plan_id -- assert: check expect = actual exec tsqlt.assertequalstable @expected = '[t_usp_end_plan].expected_log_plans', @actual = '[t_usp_end_plan].actual_log_plans' , @message = 'plan-log test failed' exec tsqlt.assertequalstable @expected = '[t_usp_end_plan].expected_log_tasks', @actual = '[t_usp_end_plan].actual_log_tasks' , @message = 'task-log test failed' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\setup_t_usp_end_plan.sql">create procedure [t_usp_end_plan].[setup] begin exec tsqlt.faketable 'meta.log_runs' --, @identity=1 exec tsqlt.faketable 'meta.log_plans' --, @identity=1 exec tsqlt.faketable 'meta.log_tasks' -- @identity=1 exec tsqlt.faketable 'meta.log_run_plans' --, @identity = 1 -- exec tsqlt.faketable 'meta.log_plan_tasks', -- @identity=1 --log_runs insert meta.log_runs ( run_id, pipeline_id, new_plan, plan_name, current_status, registration_ts ) values ( 1, 'xxx', 1, 'unit_test_end_plan_pending', 'pending', getdate()), ( 2, 'xxx', 1, 'unit_test_end_plan_success', 'in progress', getdate()), ( 3, 'xxx', 1, 'unit_test_end_plan_fail', 'in progress', getdate()), ( 4, 'xxx', 1, 'unit_test_end_plan_failed_plan', 'failed', getdate()), ( 5, 'xxx', 1, 'unit_test_end_plan_success_2', 'in progress', getdate()) --log_plans insert meta.log_plans ( plan_id, pipeline_id, plan_name, registration_ts, current_status ) values ( 1, 'xxx', 'unit_test_end_plan_pending', getdate(), 'pending'), ( 2, 'xxx', 'unit_test_end_plan_success', getdate(), 'in progress'), ( 3, 'xxx', 'unit_test_end_plan_fail', getdate(), 'in progress'), ( 4, 'xxx', 'unit_test_end_plan_failed_plan', getdate(), 'failed'), ( 5, 'xxx', 'unit_test_end_plan_success_2', getdate(), 'in progress') --log_run_plans insert meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id ) values (1, 1, 'unit_test_end_plan_pending', 1), (2, 2, 'unit_test_end_plan_success', 2), (3, 3, 'unit_test_end_plan_fail', 3), (4, 4, 'unit_test_end_plan_failed_plan', 4), (5, 5, 'unit_test_end_plan_success_2', 5) insert meta.log_tasks(task_id, task_name, registration_ts, plan_id, current_status) values (1, 'progress_task', getdate(), 3,'in progress'), (2, 'pending_task', getdate(), 3,'pending'), (3, 'successful_task', getdate(), 3,'succeeded'), (4, 'empty_task', getdate(), 3, null), (5, 'progress_task', getdate(), 5,'in progress'), (6, 'pending_task', getdate(), 5,'pending'), (7, 'successful_task', getdate(), 5,'succeeded'), (8, 'empty_task', getdate(), 5, null) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\test should fail on bad params.sql">create procedure [t_usp_end_plan].[test fail bad params] @debug int = 1 begin -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%run_id%mandatory%' ; execute [meta].[usp_end_plan] @run_id = null, @plan_id = 1, @success_flag = 1 -- fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%plan_id%mandatory%' ; execute [meta].[usp_end_plan] @run_id = 1, @plan_id = null, @success_flag = 1 -- fail: success_flag execute tsqlt.expectexception @expectedmessagepattern = '%success_flag%mandatory%' ; execute [meta].[usp_end_plan] @run_id = 1, @plan_id = 1, @success_flag = null -- fail: invalid run_id execute tsqlt.expectexception @expectedmessagepattern = '%log_runs%' ; execute [meta].[usp_end_plan] @run_id = 0, @plan_id = 1, @success_flag = 1 -- fail: invalid plan_id execute tsqlt.expectexception @expectedmessagepattern = '%log_plans%' ; execute [meta].[usp_end_plan] @run_id = 1, @plan_id = 0, @success_flag = 1 -- fail: current_status != 'in progress' declare @run_id int, @plan_id int set @plan_id = 1 --(select plan_id meta.log_plans plan_name = 'unit_test_end_plan_pending') set @run_id = 1 --(select run_id meta.log_runs plan_name = 'unit_test_end_plan_pending') execute tsqlt.expectexception @expectedmessagepattern = '%invalid%current_status%' ; execute [meta].[usp_end_plan] @run_id = @plan_id, @plan_id = @plan_id, @success_flag = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\test should fail on failed plan.sql">create procedure [t_usp_end_plan].[test fail failed plan] @debug int = 0 begin -- declare set id params declare @plan_id int; declare @run_id int; set @plan_id = 4--(select plan_id meta.log_plans plan_name = 'unit_test_end_plan_failed_plan') set @run_id = 4 --(select run_id meta.log_runs plan_name = 'unit_test_end_plan_failed_plan') exec tsqlt.expectexception @expectedmessagepattern = '%plan%current%status%failed%' ; exec [meta].[usp_end_plan] @run_id = @run_id, @plan_id = @plan_id, @success_flag = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\test should succeed.sql">create procedure [t_usp_end_plan].[test succeed] @debug int = 0 begin -- declare set id params declare @plan_id int; declare @run_id int; set @plan_id = 2 --(select plan_id meta.log_plans plan_name = 'unit_test_end_plan_success') set @run_id = 2 --(select run_id meta.log_runs plan_name = 'unit_test_end_plan_success') -- prepare expected actual tables create table [t_usp_end_plan].expected ( [plan_id] int, [plan_name] nvarchar(100), [run_id] int, [current_status] nvarchar(15) ) create table [t_usp_end_plan].actual ( [plan_id] int, [plan_name] nvarchar(100), [run_id] int, [current_status] nvarchar(15) ) -- test 1: success flag = 1 insert [t_usp_end_plan].expected( plan_id, plan_name, run_id, current_status ) values (@plan_id, 'unit_test_end_plan_success', @run_id, 'succeeded') exec meta.usp_end_plan @plan_id = @plan_id, @run_id = @run_id, @success_flag = 1 insert [t_usp_end_plan].actual( plan_id, plan_name, run_id, current_status ) select lp.plan_id, lp.plan_name, lrp.run_id, lp.current_status meta.log_plans lp inner join meta.log_run_plans lrp (lp.plan_id = lrp.plan_id) lp.plan_id = @plan_id -- assert: check expect = actual exec tsqlt.assertequalstable @expected = '[t_usp_end_plan].expected', @actual = '[t_usp_end_plan].actual' , @message = 'test 1 failed' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_run\class_t_usp_end_run.sql">create schema [t_usp_end_run] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_end_run'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_run\setup_t_usp_end_run.sql">create procedure [t_usp_end_run].[setup] begin exec tsqlt.faketable 'meta.log_runs', @identity=1 --exec tsqlt.faketable 'meta.log_plans', @identity=1 --exec tsqlt.faketable 'meta.log_tasks', @identity=1 --exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 --exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 --log_runs insert meta.log_runs ( pipeline_id, new_plan, plan_name, current_status, registration_ts ) values ( 'xxx', 1, 'unit_test', 'pending', getdate()) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_run\test should fail on bad params.sql">create procedure [t_usp_end_run].[test fail bad params] @debug int = 1 begin -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%run_id%mandatory%' ; execute [meta].[usp_end_run] @run_id = null, @success_flag = 1 -- fail: success_flag execute tsqlt.expectexception @expectedmessagepattern = '%success_flag%mandatory%' ; execute [meta].[usp_end_run] @run_id = 1, @success_flag = null -- fail: invalid run_id execute tsqlt.expectexception @expectedmessagepattern = '%log_runs%' ; execute [meta].[usp_end_run] @run_id = 0, @success_flag = 1 -- fail: current_status != 'in progress' declare @run_id int; set @run_id = (select top 1 run_id meta.log_runs current_status = 'pending') execute tsqlt.expectexception @expectedmessagepattern = '%invalid%current_status%' ; execute [meta].[usp_end_run] @run_id = @run_id, @success_flag = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_task\class_t_usp_end_task.sql">create schema [t_usp_end_task] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_end_task'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_task\setup_t_usp_end_task.sql">create procedure [t_usp_end_task].[setup] begin --exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 exec tsqlt.faketable 'meta.log_tasks', @identity=1 --exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 --log_plans insert meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status ) values ( 'xxx', 'unit_test', getdate(), 'pending') declare @plan_id int = scope_identity() --log_tasks insert meta.log_tasks ( task_name, registration_ts, plan_id, current_status) values ( 'task', getdate(), @plan_id, 'pending') --declare @task_id int = scope_identity() --log_plan_tasks insert meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name, task_group, task_type, worker_name, task_sequence, original_plan_id ) select plan_id, 'unit_test', task_id, task_name, 'unit_test', left(task_name,charindex(' ', task_name)), 'xxx', task_id, plan_id meta.log_tasks end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_end_task\test should fail on bad params.sql">create procedure [t_usp_end_task].[test fail bad params] @debug int = 0 begin -- fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%plan_id%mandatory%' ; execute [meta].[usp_end_task] @plan_id = null, @task_id = 1, @success_flag = 1 -- fail: task_id execute tsqlt.expectexception @expectedmessagepattern = '%task_id%mandatory%' ; execute [meta].[usp_end_task] @plan_id = 1, @task_id = null, @success_flag = 1 -- fail: success_flag execute tsqlt.expectexception @expectedmessagepattern = '%success_flag%mandatory%' ; execute [meta].[usp_end_task] @plan_id = 1, @task_id = 1, @success_flag = null -- fail: invalid plan_id execute tsqlt.expectexception @expectedmessagepattern = '%log_plans%' ; execute [meta].[usp_end_task] @plan_id = 0, @task_id = 1, @success_flag = 1 -- fail: invalid plan_id execute tsqlt.expectexception @expectedmessagepattern = '%log_tasks%' ; execute [meta].[usp_end_task] @plan_id = 1, @task_id = 0, @success_flag = 1 declare @task_id int, @plan_id int set @task_id = (select top 1 task_id meta.log_tasks current_status = 'pending') set @plan_id = (select plan_id meta.log_plan_tasks task_id = @task_id) execute tsqlt.expectexception @expectedmessagepattern = '%invalid%current_status%' ; execute [meta].[usp_end_task] @plan_id = @plan_id, @task_id = @plan_id, @success_flag = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plans\class_t_usp_get_plans.sql">create schema [t_usp_get_plans] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_get_plans'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plans\setup_t_usp_get_plans.sql">create procedure [t_usp_get_plans].[setup] begin exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 -- exec tsqlt.faketable 'meta.log_tasks', @identity=1 exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 -- exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 exec tsqlt.faketable 'meta.plan_configuration' --log_runs insert meta.log_runs ( pipeline_id, new_plan, plan_name, current_status, registration_ts ) values ( 'xxx', 1, 'unit_test', 'pending', getdate()) declare @run_id int = scope_identity() --log_plans insert meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status ) values ( 'xxx', 'unit_test', getdate(), 'pending'), ( 'xxx', 'unit_test', getdate(), 'in progress'), ( 'xxx', 'unit_test', getdate(), 'failed') --log_plan_tasks insert meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id) select @run_id, plan_id, plan_name, @run_id meta.log_plans -- plan_configuration insert meta.plan_configuration( plan_name) values ('unit_test') end;</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plans\test should fail on bad params.sql">create procedure [t_usp_get_plans].[test fail bad params] @debug int = 0 begin -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%run_id%mandatory%' ; execute meta.usp_get_plans @run_id = null, @plan_name = 'unit_test' -- fail: plan_name execute tsqlt.expectexception @expectedmessagepattern = '%plan_name%mandatory%' ; execute meta.usp_get_plans @run_id = 1, @plan_name = null -- fail: invalid run_id execute tsqlt.expectexception @expectedmessagepattern = '%log_runs%' ; execute meta.usp_get_plans @run_id = 0, @plan_name = 'unit_test' -- fail: invalid plan_name execute tsqlt.expectexception @expectedmessagepattern = '%plan_configuration%' ; execute meta.usp_get_plans @run_id = 1, @plan_name = 'unit_test' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plan_metadata\class_t_usp_get_plan_metadata.sql">create schema [t_usp_get_plan_metadata] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_get_plan_metadata'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plan_metadata\setup_t_usp_get_plan_metadata.sql">create procedure [t_usp_get_plan_metadata].[setup] begin exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 exec tsqlt.faketable 'meta.log_tasks', @identity=1 exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 --log_runs insert meta.log_runs ( pipeline_id, new_plan, plan_name, current_status, registration_ts ) values ( 'xxx', 1, 'unit_test', 'pending', getdate()) declare @run_id int = scope_identity() --log_plans insert meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status ) values ( 'xxx', 'unit_test', getdate(), 'pending') declare @plan_id int = scope_identity() --log_tasks insert meta.log_tasks ( task_name, registration_ts, plan_id, current_status) values ( 'task', getdate(), @plan_id, 'pending') --log_plan_tasks insert meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name, task_group, task_type, worker_name, task_sequence, original_plan_id) select plan_id, 'unit_test', task_id, task_name, 'unit_test', left(task_name,charindex(' ', task_name)), 'xxx', task_id, plan_id meta.log_tasks end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plan_metadata\test should fail on bad params.sql">create procedure [t_usp_get_plan_metadata].[test fail bad params] @debug int = 0 begin -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%run_id%mandatory%' ; execute [meta].[usp_get_plan_metadata] @run_id = null, @plan_id = 1, @task_group = 'dummy', @task_type = 'dummy' -- fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%plan_id%mandatory%' ; execute [meta].[usp_get_plan_metadata] @run_id = 1, @plan_id = null, @task_group = 'dummy', @task_type = 'dummy' -- fail: task_group execute tsqlt.expectexception @expectedmessagepattern = '%task_group%mandatory%' ; execute [meta].[usp_get_plan_metadata] @run_id = 1, @plan_id = 1, @task_group = null, @task_type = 'dummy' -- fail: task_type execute tsqlt.expectexception @expectedmessagepattern = '%task_type%mandatory%' ; execute [meta].[usp_get_plan_metadata] @run_id = 1, @plan_id = 1, @task_group = 'dummy', @task_type = null -- fail: invalid run_id execute tsqlt.expectexception @expectedmessagepattern = '%log_runs%' ; execute [meta].[usp_get_plan_metadata] @run_id = 0, @plan_id = 1, @task_group = 'dummy', @task_type = 'dummy' -- fail: invalid plan_id execute tsqlt.expectexception @expectedmessagepattern = '%log_plans%' ; execute [meta].[usp_get_plan_metadata] @run_id = 1, @plan_id = 0, @task_group = 'dummy', @task_type = 'dummy' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\class_t_usp_get_preprocessing_info.sql">create schema [t_usp_get_preprocessing_info] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_get_preprocessing_info'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\setup_t_usp_get_preprocessing_info.sql">create procedure [t_usp_get_preprocessing_info].[setup] begin -- mock relevant configuration tables exec tsqlt.faketable 'meta.plan_task_configuration' exec tsqlt.faketable 'meta.task_configuration' -- add fake data mocked tables -- task_configuration insert meta.task_configuration ( task_name, task_category, preprocess_pattern, [enabled]) values ( 'unittest_task_success', 'preprocess', 'parent/%/source_name1/', 1), ( 'unittest_task_disable', 'preprocess', 'parent/%/source_name2/', 0), ( 'unittest_task_fail', 'preprocess', 'parent/%/source_name3/', 1), ( 'unittest_task_fail2', 'preprocess', 'parent/%/source_name3/', 1), ( 'unittest_task_unparented','preprocess', 'parent/%/source_name4/', 1) -- plan_task_configuration insert meta.plan_task_configuration ( plan_name, task_name, [enabled] ) values ( 'preprocessing', 'unittest_task_success', 1 ), ( 'preprocessing', 'unittest_task_fail', 1), ( 'preprocessing', 'unittest_task_fail2', 1), ( 'preprocessing', 'unittest_task_disable', 0 ) end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\test functionalities of the stored proc.sql">create procedure [t_usp_get_preprocessing_info].[test functionalities stored proc] @debug int = 0 begin -- declare variables declare @expected nvarchar(4000); declare @actual nvarchar(4000); declare @if_error int; -- failure tasks retured enabled begin try set @if_error = 0 exec meta.usp_get_preprocessing_info @trigger_file_path = 'parent/20241212_121212/source_name2/', @trigger_file_name = 'filename' end try -- validate error matches pattern begin catch set @expected = '%match%one%0%tasks%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: zero matches' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: zero matches' -- failure many source_path matches begin try set @if_error = 0 exec meta.usp_get_preprocessing_info @trigger_file_path = 'parent/20241212_121212/source_name3/', @trigger_file_name = 'filename' end try -- validate error matches pattern begin catch set @expected = '%match%one%task%2%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: multiple matches' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: multiple matches' -- failure task exists plan begin try set @if_error = 0 exec meta.usp_get_preprocessing_info @trigger_file_path = 'parent/20241212_121212/source_name4/', @trigger_file_name = 'filename' end try -- validate error matches pattern begin catch set @expected = '%match%one%task%0%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: plan' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: plan' -- failure task match w source_path begin try set @if_error = 0 exec meta.usp_get_preprocessing_info @trigger_file_path = 'parent/20241212_121212/source_name5/', @trigger_file_name = 'filename' end try -- validate error matches pattern begin catch set @expected = '%match%one%task%0%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: match' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: match' -- success match enabled create table #actual( [task_filter] varchar(100), [plan_name] varchar(100) ) create table #expected( [task_filter] varchar(100), [plan_name] varchar(100) ) insert #actual (task_filter, plan_name) exec meta.usp_get_preprocessing_info @trigger_file_path = 'parent/20241212_121212/source_name1/', @trigger_file_name = 'filename' insert #expected (task_filter, plan_name) values ('unittest_task_success', 'preprocessing') exec tsqlt.assertequalstable @expected = #expected, @actual = #actual , @message = 'proc return expected task_filter' end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\test should fail on bad params.sql">create procedure [t_usp_get_preprocessing_info].[test fail bad params] @debug int = 0 begin -- declare variables declare @expected nvarchar(4000); declare @actual nvarchar(4000); declare @if_error int; -- failure trigger_file_path = null begin try set @if_error = 0 exec meta.usp_get_preprocessing_info @trigger_file_path = null, @trigger_file_name = 'filename' end try -- validate error matched pattern: trigger_file_path%mandatory% begin catch set @expected = '%trigger_file_path%mandatory%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @trigger_file_path = null' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @trigger_file_path = null' end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_tasks\class_t_usp_get_tasks.sql">create schema [t_usp_get_tasks] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_get_tasks'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_tasks\setup_t_usp_get_tasks.sql">create procedure [t_usp_get_tasks].[setup] begin exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 exec tsqlt.faketable 'meta.log_tasks', @identity=1 exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 declare @plan_name varchar(100) = 'unit_test' --log_runs insert meta.log_runs ( pipeline_id, new_plan, plan_name, registration_ts, current_status ) values ( 'xxx', 1, @plan_name, getdate(), 'pending') declare @run_id int = scope_identity() --log_plans insert meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status ) values ( 'xxx', @plan_name, getdate(), 'pending') declare @plan_id int = scope_identity() --log_tasks insert meta.log_tasks ( task_name, registration_ts, plan_id, current_status) values ( 'pipeline task 1 succeeded', getdate(), @plan_id, 'succeeded'), ( 'pipeline task 2 failed', getdate(), @plan_id, 'failed'), ( 'pipeline task 3 pending', getdate(), @plan_id, 'pending'), ( 'spark task 1 succeeded', getdate(), @plan_id, 'succeeded'), ( 'spark task 2 failed', getdate(), @plan_id, 'failed'), ( 'spark task 3 pending', getdate(), @plan_id, 'pending'), ( 'stored_proc task 1 succeeded', getdate(), @plan_id, 'succeeded'), ( 'stored_proc task 2 failed', getdate(), @plan_id, 'failed'), ( 'stored_proc task 3 pending', getdate(), @plan_id, 'pending') --log_run_plans insert meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id) select @run_id, @plan_id, @plan_name, @run_id meta.log_plans --log_plan_tasks insert meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name, task_group, task_type, worker_name, task_sequence, original_plan_id) select plan_id, @plan_name, task_id, task_name, 'unit_test', left(task_name,charindex(' ', task_name)), 'xxx', task_id, plan_id meta.log_tasks end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_tasks\test should fail on bad params.sql">create procedure [t_usp_get_tasks].[test fail bad params] @debug int = 0 begin -- declare variables declare @expected nvarchar(4000); declare @actual nvarchar(4000); declare @if_error int; -- failure run_id = null begin try set @if_error = 0 exec meta.usp_get_tasks @run_id = null, @plan_id = 1, @task_group = 'test', @task_type = 'dummy' end try -- validate error matched pattern: run_id%mandatory% begin catch set @expected = 'run_id%mandatory%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @run_id = null' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @run_id = null' -- failure plan_id = null begin try set @if_error = 0 exec meta.usp_get_tasks @run_id = 1, @plan_id = null, @task_group = 'test', @task_type = 'dummy' end try -- validate error matched pattern: plan_id%mandatory% begin catch set @expected = 'plan_id%mandatory%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @plan_id = null' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @plan_id = null' -- failure task_group = null begin try set @if_error = 0 exec meta.usp_get_tasks @run_id = 1, @plan_id = 1, @task_group = null, @task_type = 'dummy' end try -- validate error matched pattern: task_group%mandatory% begin catch set @expected = 'task_group%mandatory%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @task_group = null' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @task_group = null' -- failure task_type = null begin try set @if_error = 0 exec meta.usp_get_tasks @run_id = 1, @plan_id = 1, @task_group = 'test', @task_type = null end try -- validate error matched pattern: task_group%mandatory% begin catch set @expected = 'task_type%mandatory%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @task_type = null' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @task_group = null' -- failure run_id meta.log_runs begin try set @if_error = 0 exec meta.usp_get_tasks @run_id = 0, @plan_id = 1, @task_group = 'test', @task_type = 'dummy' end try -- validate error matched pattern: %log_runs% begin catch set @expected = '%log_runs%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @run_id log_runs' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @run_id log_runs' -- failure plan_id meta.log_plans begin try set @if_error = 0 exec meta.usp_get_tasks @run_id = 1, @plan_id = 0, @task_group = 'test', @task_type = 'dummy' end try -- validate error matched pattern: task_group%mandatory% begin catch set @expected = '%log_plans%' set @actual = error_message() set @if_error = 1 exec tsqlt.assertlike @expectedpattern = @expected, @actual = @actual, @message = 'proc fail expected: @plan_id log_plans' end catch -- validate error actually occured exec tsqlt.assertequals @expected = 1, @actual = @if_error, @message = 'proc fail expected: @plan_id log_plans' end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\class_t_usp_get_task_last_loaded_file.sql">create schema [t_usp_get_task_last_loaded_file] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_get_task_last_loaded_file'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\setup_t_usp_get_task_last_loaded_file_.sql">create procedure [t_usp_get_task_last_loaded_file].[setup] begin exec tsqlt.faketable 'meta.log_tasks', @identity=1 --used exec tsqlt.faketable 'meta.log_files', @identity=1 --used exec tsqlt.faketable 'meta.task_configuration', @identity=1 --used exec tsqlt.faketable 'meta.log_run_plans', @identity=1 declare @plan_name varchar(100) = 'unit_test' --log_runs insert meta.log_runs ( pipeline_id, new_plan, plan_name, registration_ts, current_status ) values ( 'xxx', 1, @plan_name, getdate(), 'pending') declare @run_id int = scope_identity() --log_plans insert meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status, run_id ) values ( 'xxx', @plan_name, getdate(), 'pending', @run_id) declare @plan_id int = scope_identity() --log_tasks insert meta.log_tasks ( task_name, registration_ts, plan_id, current_status) values ( 'pipeline task 1 name' , getdate(), @plan_id, 'succeeded' ), ( 'pipeline task 2 with_name' , getdate(), @plan_id, 'failed' ) --log_run_plans insert meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id) select @run_id, @plan_id, @plan_name, @run_id meta.log_plans --log_files insert meta.log_files( task_id, plan_id, filename, extended_filename) select task_id, plan_id, filename='task_filename_20241217_123456.csv', extended_filename='task_filename_20241217_123456_extended_filename' meta.log_tasks -- task_configuration insert meta.task_configuration (task_name, task_description, task_type, worker_name, container_name, enabled) select task_name, task_description= 'unit_test', task_type= 'spark_notebook', worker_name='ingestionworker', container_name='landing', enabled=1 meta.log_tasks end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\test pos load filename.sql">create procedure [t_usp_get_task_last_loaded_file].[test pos load filename] @debug int = 0 begin declare @result table( [filename] varchar(max) ) declare @task_name varchar(max) = 'pipeline task 1 name' insert @result exec meta.usp_get_task_last_loaded_file @task_name declare @filename varchar(max) = null select @filename = [filename] @result exec [tsqlt].[assertequalsstring] @expected = 'task_filename_20241217_123456_extended_filename', @actual = @filename, @message = 'expected filename found' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\test should fail missing mandatory params.sql">create procedure [t_usp_get_task_last_loaded_file].[test fail missing mandatory params] @debug int = 0 begin -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%task_name%mandatory%' ; execute meta.usp_get_task_last_loaded_file @task_name = null end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\test should fail task not exists.sql">create procedure [t_usp_get_task_last_loaded_file].[test fail task exists] @debug int = 0 begin -- fail: task exist execute tsqlt.expectexception @expectedmessagepattern = '%task_name % found task_configuration table%' ; execute meta.usp_get_task_last_loaded_file @task_name = 'task_does_not_exists' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\class_t_usp_get_task_metadata.sql">-- creates schema oop: add much test classes want -- create schema also used create tables, views set grant, deny, revoke perms create schema [t_usp_get_task_metadata] -- set name database-level principal schema authorization dbo go -- create new test class, functions schema. schema already exists, one dropped new created. execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_get_task_metadata'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\setup_t_usp_get_task_metadata.sql">-- create stored procedure (don't use alter: gives errors) create procedure [t_usp_get_task_metadata].[setup] begin -- ///////////////////// -- table creation -- ///////////////////// ---- first, tables created name actual tables (!) ---- set @identity = 1 preserve identity properties (auto increments), else leave exec tsqlt.faketable 'meta.log_tasks' exec tsqlt.faketable 'meta.task_configuration' exec tsqlt.faketable 'meta.source_configuration' exec tsqlt.faketable 'meta.source_column_configuration' exec tsqlt.faketable 'meta.plan_task_configuration' exec tsqlt.faketable 'meta.source_check_configuration' --exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 --exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 --exec tsqlt.faketable 'meta.plan_configuration' --exec tsqlt.faketable 'meta.log_runs', @identity=1 --exec tsqlt.faketable 'meta.log_plans', @identity=1 -- ///////////////////// -- table insertion -- ///////////////////// ---- log_tasks insert meta.log_tasks ( task_id, task_name, registration_ts ) values ( 1,'unit_test', getdate() ), ( 2, 'unit_test_withworker_unsupported', getdate() ), ( 3, 'ingestionworker', getdate() ) -- returns last identity value inserted identity column scope -- declare @task_id int = scope_identity() ---- task_configuration insert meta.task_configuration( task_name, file_layout, worker_name ) values ( 'unit_test', 'unit_test_layout_normal', 'ingestionworker' ), ( 'unit_test_withworker_unsupported', 'unit_test_layout_unsupported', 'worker_unsupported' ), ( 'ingestionworker', 'unit_test_layout', 'ingestionworker' ) ---- source_configuration insert meta.source_configuration ( file_layout, file_kind ) values ( 'unit_test_layout', 'kind' ) ---- source_column_configuration insert meta.source_column_configuration ( file_layout ) values ( 'unit_test_layout' ) ---- source_check_configuration insert meta.source_check_configuration (check_name, file_layout, [enabled]) values ('data_type', 'unit_test_layout', 1) end;</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\test json return statements.sql">-- create stored procedure (don't use alter: gives errors) create procedure [t_usp_get_task_metadata].[test json return statements] @debug int = 0 begin -- ///////////////////// -- declarations -- ///////////////////// create table [t_usp_get_task_metadata].[actual] ( taskconfig nvarchar(max), sourceconfig nvarchar(max), sourcecolumnconfig nvarchar(max), sourcecheckconfig nvarchar(max) ) -- declare @expectedresults table( -- taskconfig nvarchar(max), -- sourceconfig nvarchar(max), -- sourcecolumnconfig nvarchar(max), -- sourcecheckconfig nvarchar(max) -- ) declare @actualjson nvarchar(max) -- declare @expectedjson nvarchar(max) -- ///////////////////// -- prepare objects -- ///////////////////// -- insert @expectedresults(taskconfig, sourceconfig, sourcecolumnconfig, sourcecheckconfig) -- values('[{"task_name":"ingestionworker","worker_name":"ingestionworker","file_layout":"unit_test_layout"}]', '[{}]', '[{}]', '[{"check_name":"data_type"}]') insert [t_usp_get_task_metadata].[actual] exec meta.usp_get_task_metadata @task_id = 3; set @actualjson = ( select * [t_usp_get_task_metadata].[actual] json path ) -- set @expectedjson = ( -- select * @expectedresults -- json path -- ) -- ///////////////////// -- check -- ///////////////////// -- expected: [{"taskconfig":"[{\"task_name\":\"ingestionworker\",\"worker_name\":\"ingestionworker\",\"file_layout\":\"unit_test_layout\"}]","sourceconfig":"[{}]","sourcecolumnconfig":"[{}]","sourcecheckconfig":"[{\"check_name\":\"data_type\"}]"}] -- check actual results contains certain variables certain values exec tsqlt.assertlike @expectedpattern = '[[]%]', @actual = @actualjson, @message='job 1' -- check square brackets beginning &amp; end exec tsqlt.assertlike @expectedpattern = '%taskconfig%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%task_name%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%ingestionworker%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%worker_name%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%file_layout%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%file_kind%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%unit_test_layout%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%sourceconfig%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%sourcecolumnconfig%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%sourcecheckconfig%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%check_name%', @actual = @actualjson, @message='job 1' exec tsqlt.assertlike @expectedpattern = '%data_type%', @actual = @actualjson, @message='job 1' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\test retrieve task and worker name.sql">-- create stored procedure (don't use alter: gives errors) create procedure [t_usp_get_task_metadata].[test retrieve task worker name] @debug int = 0 begin -- ///////////////////// -- prep -- ///////////////////// ---- worker name -- null &amp;&amp; found &amp;&amp; configured | expect error -- test check happens worker configured sql execute tsqlt.expectexception @expectedmessagepattern = '%unsupported%' ; execute meta.usp_get_task_metadata @task_id = 2 -- null &amp;&amp; found &amp;&amp; configured | expect error -- test check happens worker configured sql execute tsqlt.expectnoexception ; execute meta.usp_get_task_metadata @task_id = 3 ---- task id -- null &amp;&amp; found | expect error -- test check happens task_id present meta.log_tasks execute tsqlt.expectexception @expectedmessagepattern = '%could%not%locate%' ; execute meta.usp_get_task_metadata @task_id = 4 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\test should fail on bad params.sql">-- create stored procedure (don't use alter: gives errors) create procedure [t_usp_get_task_metadata].[test fail bad params] @debug int = 0 begin -- ///////////////////// -- parameters check -- ///////////////////// ---- task_id -- null | expect error execute tsqlt.expectexception @expectedmessagepattern = '%task_id%mandatory%' ; execute meta.usp_get_task_metadata @task_id = null -- null &amp;&amp; exists | expect error ---- task_id supposed exist log_tasks execute tsqlt.expectexception @expectedmessagepattern = '%log_tasks%' ; execute meta.usp_get_task_metadata @task_id = 0 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\class_t_usp_new_file.sql">create schema [t_usp_new_file] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_new_file'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\setup_t_usp_new_file.sql">create procedure [t_usp_new_file].[setup] begin -- exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 exec tsqlt.faketable 'meta.log_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 exec tsqlt.faketable 'meta.log_files', @identity = 1 -- exec tsqlt.faketable 'meta.plan_configuration' -- exec tsqlt.faketable 'meta.task_configuration' -- exec tsqlt.faketable 'meta.source_configuration' -- exec tsqlt.faketable 'meta.source_column_configuration' -- exec tsqlt.faketable 'meta.plan_task_configuration' insert meta.log_tasks (task_name) values ('unit_test') insert meta.log_plans (plan_name) values ('unit_test') insert meta.log_plan_tasks (plan_id, plan_name, task_id, task_name) values (1, 'unit_test', 1, 'unit_test') insert meta.log_files( [filename], [extended_filename],[landing_status],[raw_status],[silver_status], [archive_status],task_id) values ( 'testfile.txt', 'testfile_ts.txt', 'succeeded', 'succeeded', 'succeeded', 'succeeded',1), ( 'testfilefailed.txt', 'testfilefailed_ts.txt', 'succeeded', 'succeeded', 'succeeded',null,1) end;</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\test should fail on bad params.sql">create procedure [t_usp_new_file].[test fail bad params] @debug int = 0 begin --fail: task_id execute tsqlt.expectexception @expectedmessagepattern = '%task_id%mandatory%' ; execute meta.usp_new_file @task_id = null, @plan_id = 1, @filename = '', @extended_filename = '', @info_message = '', @no_select = 1 --fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%plan_id%mandatory%' ; execute meta.usp_new_file @task_id = 1, @plan_id = null, @filename = '', @extended_filename = '', @info_message = '', @no_select = 1 --fail: filename execute tsqlt.expectexception @expectedmessagepattern = '%filename%mandatory%' ; execute meta.usp_new_file @task_id = 1, @plan_id = 1, @filename = null, @extended_filename = '', @info_message = '', @no_select = 1 --fail: extended_filename execute tsqlt.expectexception @expectedmessagepattern = '%extended_filename%mandatory%' ; execute meta.usp_new_file @task_id = 1, @plan_id = 1, @filename = '', @extended_filename = null, @info_message = '', @no_select = 1 --fail: info_message execute tsqlt.expectexception @expectedmessagepattern = '%info_message%mandatory%' ; execute meta.usp_new_file @task_id = 1, @plan_id = 1, @filename = '', @extended_filename = '', @info_message = null, @no_select = 1 --fail: invalid task_id execute tsqlt.expectexception @expectedmessagepattern = '%info_message%mandatory%' ; execute meta.usp_new_file @task_id = 0, @plan_id = 1, @filename = '', @extended_filename = '', @info_message = '', @no_select = 1 --fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%info_message%mandatory%' ; execute meta.usp_new_file @task_id = 1, @plan_id = 0, @filename = '', @extended_filename = '', @info_message = '', @no_select = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\test should fail on existing file.sql">create procedure [t_usp_new_file].[test fail existing file] @debug int = 0 begin -- fail: extended filename already exists log_files table archive_status succeeded execute tsqlt.expectexception @expectedmessagepattern = '%exists%log_files%' ; execute meta.usp_new_file @task_id = 1, @plan_id = 1, @filename = 'testfile.txt', @extended_filename = 'testfile_ts.txt', @info_message = '', @no_select = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\test should rerun after a file has failed.sql">create procedure [t_usp_new_file].[test rerun file failed] @debug int = 0 begin -- succeed: extended filename already exists log_files table archive_status null create table #expected_log_runs( [raw_status] nvarchar(100), [silver_status] nvarchar(100), [archive_status] nvarchar(100), [task_id] nvarchar(100), [extended_filename] nvarchar(100) ) create table #actual_log_runs( [raw_status] nvarchar(100), [silver_status] nvarchar(100), [archive_status] nvarchar(100), [task_id] nvarchar(100), [extended_filename] nvarchar(100) ) insert #expected_log_runs(raw_status,silver_status,archive_status,task_id,extended_filename) values (null, null, null,1,'testfilefailed_ts.txt') ; execute meta.usp_new_file @task_id = 1,--that'll work @plan_id = 1, @filename = 'testfilefailed.txt', @extended_filename = 'testfilefailed_ts.txt', @info_message = '', @no_select = 1 insert #actual_log_runs(raw_status,silver_status,archive_status,task_id,extended_filename) select raw_status,silver_status,archive_status,task_id,extended_filename meta.log_files extended_filename='testfilefailed_ts.txt'; execute tsqlt.assertequalstable @expected = #expected_log_runs, @actual = #actual_log_runs, @message = 'it expected row status reset ' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\t_usp_new_file.test should add a new row.sql">create procedure [t_usp_new_file].[test add new row] begin declare @count int; execute meta.usp_new_file @task_id = 1,--that'll work @plan_id = 1, @filename = 'testnewfile.txt', @extended_filename = 'testnewfile_ts.txt', @info_message = '', @no_select = 1 set @count= (select count(*) meta.log_files extended_filename = 'testnewfile_ts.txt'); execute tsqlt.assertequals @expected =1, @actual = @count, @message = 'expected one row add int meta.log_files' end;</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\class_t_usp_new_run.sql">create schema [t_usp_new_run] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_new_run'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\setup_t_usp_new_run.sql">create procedure [t_usp_new_run].[setup] @debug int = 0 begin -- mock instances logging tables -- identity = 1 -&gt; reset identity = 1 exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 exec tsqlt.faketable 'meta.log_tasks', @identity=1 exec tsqlt.faketable 'meta.log_run_plans', @identity=1 exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 -- mock instances configuration tables exec tsqlt.faketable 'meta.plan_configuration' exec tsqlt.faketable 'meta.task_configuration' exec tsqlt.faketable 'meta.plan_task_configuration' -- insert fake data configuration tables -- insert plan enabled one disabled insert meta.plan_configuration( plan_name, [enabled] ) values ( 'unit_test_enabled', 1 ), ( 'unit_test_notenabled', 0), ( 'unit_test_filtered', 1) -- insert task enabled one disabled insert meta.task_configuration( task_name, [enabled] ) values ( 'unit_test_enabled', 1 ), ( 'unit_test_notenabled', 0), ( 'unit_test_filtered', 1) -- insert plan-task pairs plans tasks insert meta.plan_task_configuration( plan_name, task_name,enabled) values ( 'unit_test_enabled', 'unit_test_enabled',1 ), ( 'unit_test_enabled', 'unit_test_notenabled',1), ( 'unit_test_notenabled', 'unit_test_enabled',1 ), ( 'unit_test_notenabled', 'unit_test_notenabled',1), ( 'unit_test_filtered', 'unit_test_filtered',1 ), ( 'unit_test_filtered', 'unit_test_enabled',1), ( 'unit_test_filtered', 'unit_test_notenabled',1) -- insert fake data logging tables -- insert 4 different rows meta.log_runs insert meta.log_runs(plan_name, current_status) values ('unit_test_enabled', 'failed'), ('unit_test_enabled', 'failed'), ('unit_test_enabled', 'cancelled'), ('unit_test_enabled', 'pending') -- insert 4 different rows meta.log_plans insert meta.log_plans(plan_name, run_id, current_status) values ('unit_test_enabled', 1, 'failed'), ('unit_test_enabled', 2, 'failed'), ('unit_test_disabled', 3,'failed'), ('unit_test_enabled', 4, 'pending') -- insert 4 different rows meta.log_tasks insert meta.log_tasks(task_name, plan_id, current_status) values ('unit_test_enabled', 1, 'failed'), ('unit_test_enabled', 2, 'failed'), ('unit_test_disabled', 3, 'failed'), ('unit_test_enabled', 4, 'pending') -- insert 4 different rows meta.log_plan_task insert meta.log_plan_tasks(plan_id, plan_name, task_id, task_name) values (1, 'unit_test_enabled', 1, 'unit_test_enabled'), (2, 'unit_test_enabled', 2, 'unit_test_enabled'), (3, 'unit_test_disabled', 3, 'unit_test_disabled'), (4, 'unit_test_enabled', 4, 'unit_test_enabled') -- insert 4 different rows meta.log_run_plans insert meta.log_run_plans(run_id, plan_id, plan_name, original_run_id) values (1, 1, 'unit_test_enabled', 1), (2, 2, 'unit_test_enabled', 2), (3, 3, 'unit_test_disabled', 3), (4, 4, 'unit_test_enabled', 4) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test should fail on bad params.sql">create procedure [t_usp_new_run].[test fail bad params] @debug int = 0 begin -- fail: pipeline_id execute tsqlt.expectexception @expectedmessagepattern = '%pipeline_id%mandatory%' ; execute meta.usp_new_run @pipeline_id = null, @new_plan = 1, @plan_name = 'unit_test', @no_select = 1 -- fail: plan_name execute tsqlt.expectexception @expectedmessagepattern = '%plan_name%mandatory%' ; execute meta.usp_new_run @pipeline_id = 'xxx', @new_plan = 1, @plan_name = null, @no_select = 1 -- fail: enabled plan_name execute tsqlt.expectexception @expectedmessagepattern = '%exist%plan_configuration%' ; execute meta.usp_new_run @pipeline_id = 'xxx', @new_plan = 1, @plan_name = 'random', @no_select = 1 -- fail: enabled plan_name execute tsqlt.expectexception @expectedmessagepattern = '%not%enabled%plan_configuration%' ; execute meta.usp_new_run @pipeline_id = 'xxx', @new_plan = 1, @plan_name = 'unit_test_notenabled', @no_select = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test should fail on non-existing plan_name.sql">create procedure [t_usp_new_run].[test fail non-existing plan_name] @debug int = 0 begin -- fail: enabled plan_name execute tsqlt.expectexception @expectedmessagepattern = '%exist%plan_configuration%' ; execute meta.usp_new_run @pipeline_id = 'xxx', @new_plan = 1, @plan_name = 'random', @no_select = 1 -- fail: enabled plan_name execute tsqlt.expectexception @expectedmessagepattern = '%not%enabled%plan_configuration%' ; execute meta.usp_new_run @pipeline_id = 'xxx', @new_plan = 1, @plan_name = 'unit_test_notenabled', @no_select = 1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test with filter should log specific tasks.sql">/* ======================================================= author: simon plancke create date: 2024-12-13 description: test meta.usp_new_run parameter new_plan = 1 task_filter null ======================================================= parameter task_filter null: * expect specific tasks added log tasks pending ======================================================= */ create procedure [t_usp_new_run].[test filter log specific tasks] @debug int = 0 begin --region declare parameters stored procedure declare @plan_name nvarchar(100) = 'unit_test_filtered' declare @new_plan bit = 1 declare @task_filter nvarchar(100) = 'unit_test_filtered' --endregion --region create temporary tables assertions: expected actual -- create set temporary tables (based actual logging tables metadb) -- use relevant column names (ids, names, status) -- log_runs create table #expected_log_runs( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100) ) create table #actual_log_runs( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100) ) -- log_plans create table #expected_log_plans( [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100), run_id int ); create table #actual_log_plans( [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100), run_id int ); -- log_tasks create table #expected_log_tasks( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(100) ); create table #actual_log_tasks( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(100) ); -- log_run_plans create table #expected_log_run_plans( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [original_run_id] int ); create table #actual_log_run_plans( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [original_run_id] int ); -- log_plan_tasks create table #expected_log_plan_tasks( [plan_id] int, [plan_name] nvarchar(100), [task_id] int, [task_name] nvarchar(100) ); create table #actual_log_plan_tasks( [plan_id] int, [plan_name] nvarchar(100), [task_id] int, [task_name] nvarchar(100) ); --endregion --region insert expected values expected temporary tables -- copy existing tables setup expected tables -- table, add new row shows expected values executing stored procedure meta.usp_new_run -- meta.log_runs insert #expected_log_runs(run_id, plan_name, current_status) select run_id, plan_name, current_status meta.log_runs -- expect new row pending run insert #expected_log_runs(run_id, plan_name, current_status) values (5, @plan_name, 'pending') -- meta.log_plans insert #expected_log_plans(plan_id, plan_name, current_status) select plan_id, plan_name, current_status meta.log_plans -- expect new row pending plan insert #expected_log_plans(plan_id, plan_name, current_status) values (5, @plan_name, 'pending') -- meta.log_tasks insert #expected_log_tasks(task_id, task_name, plan_id, current_status) select task_id, task_name, plan_id, current_status meta.log_tasks -- expect 1 new pending task (so tasks plan logged (unit_test_enabled unit_test_disabled)) insert #expected_log_tasks(task_id, task_name, plan_id, current_status) values (5, @task_filter, 5, 'pending') -- meta.log_run_plans insert #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id) select run_id, plan_id, plan_name, original_run_id meta.log_run_plans -- expect new row log_run_plans insert #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id) values (5, 5, @plan_name, 5) -- meta.log_plan_tasks insert #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name) select plan_id, plan_name, task_id, task_name meta.log_plan_tasks -- expect new row log_plan_tasks insert #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name) values (5, @plan_name, 5, @task_filter) --endregion --region execute stored procedure get results actual temporary tables -- execute stored procedure task_filter -- expect one task added log_tasks table -- tasks 'unit_test_enabled' 'unit_test_disabled' skipped comply filter execute meta.usp_new_run @plan_name = @plan_name, @new_plan = @new_plan, @pipeline_id = "filtered_ingest", @task_filter = @task_filter -- get results faketables actual temporary tables insert #actual_log_runs(run_id, plan_name, current_status) select run_id, plan_name, current_status meta.log_runs insert #actual_log_plans(plan_id, plan_name, current_status) select plan_id, plan_name, current_status meta.log_plans insert #actual_log_tasks(task_id, task_name, plan_id, current_status) select task_id, task_name, plan_id, current_status meta.log_tasks insert #actual_log_run_plans(run_id, plan_id, plan_name, original_run_id) select run_id, plan_id, plan_name, original_run_id meta.log_run_plans insert #actual_log_plan_tasks(plan_id, plan_name, task_id, task_name) select plan_id, plan_name, task_id, task_name meta.log_plan_tasks --endregion --region evaluate expected tables actual tables -- check expectations match actuals -- words, actual tables expected tables execute tsqlt.assertequalstable @expected = '#expected_log_runs', @actual = '#actual_log_runs' , @message = 'expected log_runs table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_plans', @actual = '#actual_log_plans' , @message = 'expected log_plans table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_tasks', @actual = '#actual_log_tasks' , @message = 'expected log_tasks table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_run_plans', @actual = '#actual_log_run_plans', @message = 'expected log_run_plans table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_plan_tasks', @actual = '#actual_log_plan_tasks', @message = 'expected log_plan_tasks table check failed.' --endregion end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test_new_plan_should_add_logging_items.sql">/* ======================================================= author: simon plancke create date: 2024-09-03 description: test meta.usp_new_run parameter new_plan = 1 ======================================================= parameter new_plan == 1: * expect new row added logging tables, current_status = 'pending' ======================================================= */ create procedure [t_usp_new_run].[test_new_plan_should_add_logging_items] @debug int = 0 begin -- declare parameters stored procedure declare @plan_name nvarchar(100) = 'unit_test_enabled' declare @new_plan bit = 1 -- create temporary tables assertions: expected actual -- log_runs create table #expected_log_runs( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100) ) create table #actual_log_runs( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100) ) -- log_plans create table #expected_log_plans( [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100), run_id int ); create table #actual_log_plans( [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100), run_id int ); -- log_tasks create table #expected_log_tasks( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(100) ); create table #actual_log_tasks( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(100) ); -- log_run_plans create table #expected_log_run_plans( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [original_run_id] int ); create table #actual_log_run_plans( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [original_run_id] int ); -- log_plan_tasks create table #expected_log_plan_tasks( [plan_id] int, [plan_name] nvarchar(100), [task_id] int, [task_name] nvarchar(100) ); create table #actual_log_plan_tasks( [plan_id] int, [plan_name] nvarchar(100), [task_id] int, [task_name] nvarchar(100) ); -- insert expected values expected tables -- meta.log_runs insert #expected_log_runs(run_id, plan_name, current_status) select run_id, plan_name, current_status meta.log_runs insert #expected_log_runs(run_id, plan_name, current_status) values (5, @plan_name, 'pending') -- meta.log_plans insert #expected_log_plans(plan_id, plan_name, current_status, run_id) select plan_id, plan_name, current_status, run_id meta.log_plans insert #expected_log_plans(plan_id, plan_name, current_status, run_id) values (5, @plan_name, 'pending', 5) -- meta.log_tasks insert #expected_log_tasks(task_id, task_name, plan_id, current_status) select task_id, task_name, plan_id, current_status meta.log_tasks insert #expected_log_tasks(task_id, task_name, plan_id, current_status) values (5, 'unit_test_enabled', 5, 'pending') -- meta.log_run_plans insert #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id) select run_id, plan_id, plan_name, original_run_id meta.log_run_plans insert #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id) values (5, 5, @plan_name, 5) -- meta.log_plan_tasks insert #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name) select plan_id, plan_name, task_id, task_name meta.log_plan_tasks insert #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name) values (5, @plan_name, 5, 'unit_test_enabled') execute meta.usp_new_run @plan_name = @plan_name, @new_plan = @new_plan, @pipeline_id = "successful_unittest" insert #actual_log_runs(run_id, plan_name, current_status) select run_id, plan_name, current_status meta.log_runs insert #actual_log_plans(plan_id, plan_name, current_status, run_id) select plan_id, plan_name, current_status, run_id meta.log_plans insert #actual_log_tasks(task_id, task_name, plan_id, current_status) select task_id, task_name, plan_id, current_status meta.log_tasks insert #actual_log_run_plans(run_id, plan_id, plan_name, original_run_id) select run_id, plan_id, plan_name, original_run_id meta.log_run_plans insert #actual_log_plan_tasks(plan_id, plan_name, task_id, task_name) select plan_id, plan_name, task_id, task_name meta.log_plan_tasks execute tsqlt.assertequalstable @expected = '#expected_log_runs', @actual = '#actual_log_runs' , @message = 'expected log_runs table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_plans', @actual = '#actual_log_plans' , @message = 'expected log_plans table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_tasks', @actual = '#actual_log_tasks' , @message = 'expected log_tasks table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_run_plans', @actual = '#actual_log_run_plans', @message = 'expected log_run_plans table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_plan_tasks', @actual = '#actual_log_plan_tasks', @message = 'expected log_plan_tasks table check failed.' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test_rerun_plan_should_reset_failure.sql">/* ======================================================= author: simon plancke create date: 2024-09-03 description: test meta.usp_new_run parameter new_plan = 1 ======================================================= parameter new_plan == 0: * expect new row added meta.log_runs meta.log_run_plans * expect last failed plan @plan_name reset 'pending' * expect failed tasks plan reset 'pending' ======================================================= */ create procedure [t_usp_new_run].[test_rerun_plan_should_reset_failure] @debug int = 0 begin -- declare parameters stored procedure declare @plan_name nvarchar(100) = 'unit_test_enabled' declare @new_plan bit = 0 -- create temporary tables assertions: expected actual -- log_runs create table #expected_log_runs( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100) ) create table #actual_log_runs( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100) ) -- log_plans create table #expected_log_plans( [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100), run_id int ); create table #actual_log_plans( [plan_id] int, [plan_name] nvarchar(100), [current_status] nvarchar(100), run_id int ); -- log_tasks create table #expected_log_tasks( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(100) ); create table #actual_log_tasks( [task_id] int, [task_name] nvarchar(100), [plan_id] int, [current_status] nvarchar(100) ); -- log_run_plans create table #expected_log_run_plans( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [original_run_id] int ); create table #actual_log_run_plans( [run_id] int, [plan_id] int, [plan_name] nvarchar(100), [original_run_id] int ); -- log_plan_tasks create table #expected_log_plan_tasks( [plan_id] int, [plan_name] nvarchar(100), [task_id] int, [task_name] nvarchar(100) ); create table #actual_log_plan_tasks( [plan_id] int, [plan_name] nvarchar(100), [task_id] int, [task_name] nvarchar(100) ); -- insert expected values expected tables -- meta.log_runs insert #expected_log_runs(run_id, plan_name, current_status) select run_id, plan_name, current_status meta.log_runs insert #expected_log_runs(run_id, plan_name, current_status) values (5, @plan_name, 'pending') -- meta.log_plans insert #expected_log_plans(plan_id, plan_name, current_status) select plan_id, plan_name, current_status meta.log_plans update #expected_log_plans set current_status = 'pending' plan_id = 2 -- meta.log_tasks insert #expected_log_tasks(task_id, task_name, plan_id, current_status) select task_id, task_name, plan_id, current_status meta.log_tasks update #expected_log_tasks set current_status = 'pending' plan_id = 2 -- meta.log_run_plans insert #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id) select run_id, plan_id, plan_name, original_run_id meta.log_run_plans insert #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id) values (5, 2, @plan_name, 2) -- meta.log_plan_tasks insert #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name) select plan_id, plan_name, task_id, task_name meta.log_plan_tasks execute meta.usp_new_run @plan_name = @plan_name, @new_plan = @new_plan, @pipeline_id = "successful_unittest" insert #actual_log_runs(run_id, plan_name, current_status) select run_id, plan_name, current_status meta.log_runs insert #actual_log_plans(plan_id, plan_name, current_status) select plan_id, plan_name, current_status meta.log_plans insert #actual_log_tasks(task_id, task_name, plan_id, current_status) select task_id, task_name, plan_id, current_status meta.log_tasks insert #actual_log_run_plans(run_id, plan_id, plan_name, original_run_id) select run_id, plan_id, plan_name, original_run_id meta.log_run_plans insert #actual_log_plan_tasks(plan_id, plan_name, task_id, task_name) select plan_id, plan_name, task_id, task_name meta.log_plan_tasks execute tsqlt.assertequalstable @expected = '#expected_log_runs', @actual = '#actual_log_runs' , @message = 'expected log_runs table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_plans', @actual = '#actual_log_plans' , @message = 'expected log_plans table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_tasks', @actual = '#actual_log_tasks' , @message = 'expected log_tasks table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_run_plans', @actual = '#actual_log_run_plans', @message = 'expected log_run_plans table check failed.' execute tsqlt.assertequalstable @expected = '#expected_log_plan_tasks', @actual = '#actual_log_plan_tasks', @message = 'expected log_plan_tasks table check failed.' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\class.sql">create schema [t_usp_proc_log] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_proc_log'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\setup.sql">create procedure [t_usp_proc_log].[setup] begin exec tsqlt.faketable 'meta.proc_log', @identity=1 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\test_normal_execution.sql">/* object_id('[t_usp_proc_log].[test_normal_execution]') null drop proc [t_usp_proc_log].[test_normal_execution] go */ create procedure [t_usp_proc_log].[test_normal_execution] begin ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @expectedvalue varchar(1000); declare @actualvalue varchar(1000); ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- exec meta.usp_proc_log @proc_name = 'unit_test', @log_type = 'warning', @statement_name = 'statement_name', @message = 'foo', @extra_parameters = '{"foo":"bar"}' ---------------------------------------------------------------------------------------------------------------- -- assert ---------------------------------------------------------------------------------------------------------------- -- expect 1 log record set @expectedvalue = 1 set @actualvalue = (select count(*) meta.proc_log) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue, @message = 'rowcount expected.' -- check extra_parameters set @expectedvalue = '{"foo":"bar"}' set @actualvalue = (select extra_parameters meta.proc_log) exec tsqlt.assertequals @expected = @expectedvalue, @actual = @actualvalue, @message = 'extra_parameters expected.' end go --exec tsqlt.run '[t_usp_proc_log].[test_normal_execution]'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\test_should_fail_when_invalid_json.sql">/* object_id('[t_usp_proc_log].[test_should_fail_when_invalid_json]') null drop proc [t_usp_proc_log].[test_should_fail_when_invalid_json] go */ create procedure [t_usp_proc_log].[test_should_fail_when_invalid_json] begin ---------------------------------------------------------------------------------------------------------------- -- assemble ---------------------------------------------------------------------------------------------------------------- declare @expectedvaluestring varchar(1000); declare @actualvaluestring varchar(1000); ---------------------------------------------------------------------------------------------------------------- -- act &amp; assert ---------------------------------------------------------------------------------------------------------------- -- add property invalid start string set @expectedvaluestring = 'error%not valid json%'; set @actualvaluestring = (select meta.fn_add_json_property('invalid', 'foo', 'bar')); exec tsqlt.assertlike @expectedpattern = @expectedvaluestring, @actual = @actualvaluestring , @message = 'test: adding property invalid json failed.' end go --exec tsqlt.run '[t_usp_proc_log].[test_should_fail_when_invalid_json]'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_configuration\class_t_usp_plan_configuration.sql">create schema [t_usp_set_plan_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_set_plan_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_configuration\setup_t_usp_set_plan_configuration.sql">create procedure [t_usp_set_plan_configuration].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.plan_configuration' insert meta.plan_configuration ( plan_name, [enabled]) values ( 'unit_test_deploy_enabled', 1 ), ( 'unit_test_deploy_disabled', 0 ) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_configuration\test should fail on bad params.sql">create procedure [t_usp_set_plan_configuration].[test fail bad params] @debug int = 0 begin -- fail: plan_name execute tsqlt.expectexception @expectedmessagepattern = '%plan_name%cannot%null%empty%' execute deploy.usp_set_plan_configuration @plan_name = null, @plan_description = 'unittest', @enabled = 1 -- fail: plan_description execute tsqlt.expectexception @expectedmessagepattern = '%plan_desc%cannot%null%empty%' execute deploy.usp_set_plan_configuration @plan_name = 'unittest', @plan_description = null, @enabled = 1 -- fail: enabled flag execute tsqlt.expectexception @expectedmessagepattern = '%enabled%cannot%null%' execute deploy.usp_set_plan_configuration @plan_name = 'unittest', @plan_description = 'unittest', @enabled = null end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_task_configuration\class_t_usp_set_plan_task_configuration.sql">create schema [t_usp_set_plan_task_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_set_plan_task_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_task_configuration\setup_t_usp_set_plan_task_configuration.sql">create procedure [t_usp_set_plan_task_configuration].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.plan_task_configuration' insert meta.plan_task_configuration( plan_name, task_name, [enabled]) values ( 'unit_test_deploy_enabled', 'unit_test_deploy_enabled', 1), ( 'unit_test_deploy_disabled', 'unit_test_deploy_disabled', 0) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_task_configuration\test should fail on bad params.sql">create procedure [t_usp_set_plan_task_configuration].[test fail bad params] @debug int = 0 begin -- fail: plan_name execute tsqlt.expectexception @expectedmessagepattern = '%plan_name%cannot%null%empty%' execute deploy.usp_set_plan_task_configuration @plan_name = null, @plantask_data = 'unittest' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_check_configuration\class_t_usp_set_source_check_configuration.sql">create schema [t_usp_set_source_check_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_set_source_check_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_check_configuration\setup_t_usp_set_source_check_configuration.sql">create procedure [t_usp_set_source_check_configuration].[setup] begin exec tsqlt.faketable 'meta.source_check_configuration' insert meta.source_check_configuration (check_name, file_layout, [enabled]) values ('unit_test_deploy_enabled', 'unit_test_deploy_enabled', 1), ('unit_test_deploy_disabled', 'unit_test_deploy_disabled', 0) end; go</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_check_configuration\test should fail on bad params.sql">create procedure [t_usp_set_source_check_configuration].[test fail bad params] @debug int = 0 begin -- fail: file_layout execute tsqlt.expectexception @expectedmessagepattern = '%file_layout%cannot%null%empty%' execute deploy.usp_set_source_check_configuration @file_layout = null, @checks_data = 'unittest_mock' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\class_t_usp_set_source_column_configuration.sql">create schema [t_usp_set_source_column_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_set_source_column_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\setup_t_usp_set_source_column_configuration.sql">create procedure [t_usp_set_source_column_configuration].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.source_column_configuration' -- insert meta.source_column_configuration ( file_layout ) -- values ( 'unit_test_deploy_enabled') end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\test should fail on bad params.sql">create procedure [t_usp_set_source_column_configuration].[test fail bad params] @debug int = 0 begin -- fail: file_layout execute tsqlt.expectexception @expectedmessagepattern = '%file_layout%cannot%null%empty%' execute deploy.usp_set_source_column_configuration @file_layout = null, @column_data = 'unittest_mock' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\test_should_ingest_new_source_code.sql">/* ======================================================= author: simon plancke create date: 2024-09-10 description: add new row meta.source_column_configuration ======================================================= */ create procedure [t_usp_set_source_column_configuration].[test_should_ingest_new_source_code] @debug int = 0 begin create table #expected_source_column_config( [file_layout] varchar(100), [source_name] varchar(100), [sink_name] varchar(100), [column_order] int, [dimension] varchar(5), [data_type] varchar(20), [column_info] varchar(max) ) create table #actual_source_column_config( [file_layout] varchar(100), [source_name] varchar(100), [sink_name] varchar(100), [column_order] int, [dimension] varchar(5), [data_type] varchar(20), [column_info] varchar(max) ) insert #expected_source_column_config(file_layout, source_name, sink_name, column_order, dimension, data_type, column_info) values ('test_source_column', 'test_column_info', 'test_column_info', 1, 'pk', 'test_datatype_1', '{locale: fr-fr}'), ('test_source_column', 'test_no_column_info', 'test_no_column_info', 2, 'scd2', 'test_datatype_2', null) exec [deploy].[usp_set_source_column_configuration] @file_layout = 'test_source_column', @column_data = '[ { "column_sequence": 1, "source_column_name": "test_column_info", "sink_column_name": "test_column_info", "dimension": "pk", "data_type": "test_datatype_1", "column_info": "{locale: fr-fr}" }, { "column_sequence": 2, "source_column_name": "test_no_column_info", "sink_column_name": "test_no_column_info", "dimension": "scd2", "data_type": "test_datatype_2" } ]' insert #actual_source_column_config(file_layout, source_name, sink_name, column_order, dimension, data_type, column_info) select * meta.source_column_configuration execute tsqlt.assertequalstable @expected = '#expected_source_column_config', @actual = '#actual_source_column_config' , @message = 'expected source_column_config table check failed.' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_configuration\class_t_usp_set_source_configuration.sql">create schema [t_usp_set_source_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_set_source_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_configuration\setup_t_usp_set_source_configuration.sql">create procedure [t_usp_set_source_configuration].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.source_configuration' insert meta.source_configuration ( file_layout, file_pattern, quote_character, escape_character) values ( 'unit_test_deploy_enabled', 'unit_test_deploy_enabled', 'm', 'm' ) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_configuration\test should fail on bad params.sql">create procedure [t_usp_set_source_configuration].[test fail bad params] @debug int = 0 begin -- fail: file_layout execute tsqlt.expectexception @expectedmessagepattern = '%file_layout%cannot%null%empty%' execute deploy.usp_set_source_configuration @file_layout = null, @file_pattern = 'unittest', @file_extension = 'unittest', @file_kind = null, @column_delimiter = null, @row_delimiter = null, @escape_character = 'mock', @quote_character = 'mock', @header = null, @encoding = null, @skip_first_lines = null, @source_conditions = null -- fail: file_pattern execute tsqlt.expectexception @expectedmessagepattern = '%file_pattern%cannot%null%empty%' execute deploy.usp_set_source_configuration @file_layout = 'unittest', @file_pattern = null, @file_kind = 'unittest', @file_extension = null, @column_delimiter = null, @row_delimiter = null, @escape_character = 'mock', @quote_character = 'mock', @header = null, @encoding = null, @skip_first_lines = null, @source_conditions = null -- fail: quote_character specified, escape_character cannot null empty execute tsqlt.expectexception @expectedmessagepattern = '%quote_character%specified%escape_character%cannot%null%empty%' execute deploy.usp_set_source_configuration @file_layout = 'unittest', @file_pattern = 'unittest', @file_kind = 'unittest', @file_kind = null, @file_extension = null, @column_delimiter = null, @row_delimiter = null, @escape_character = null, @quote_character = 'mock', @header = null, @encoding = null, @skip_first_lines = null, @source_conditions = null end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_task_configuration\class_t_usp_set_task_configuration.sql">create schema [t_usp_set_task_configuration] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_set_task_configuration'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_task_configuration\setup_t_usp_set_task_configuration.sql">create procedure [t_usp_set_task_configuration].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.task_configuration' insert meta.task_configuration ( task_name, task_type, worker_name, task_description, file_layout, container_name, [enabled]) values ( 'unit_test_deploy_enabled', 'mock_type', 'mock_worker', 'mock_description', 'mock_file', 'mock_container', 1 ), ( 'unit_test_deploy_disabled', 'mock_type2', 'mock_worker2', 'mock_description2', 'mock_file2', 'mock_container2', 0 ) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_set_task_configuration\test should fail on bad params.sql">create procedure [t_usp_set_task_configuration].[test fail bad params] @debug int = 0 begin -- fail: task_name execute tsqlt.expectexception @expectedmessagepattern = '%task_name%cannot%null%empty%' execute deploy.usp_set_task_configuration @task_name = null, @task_type = 'mock', @worker_name = 'mock', @task_description = 'mock', @table_name = null, @target_options = null, @file_layout = 'mock', @source_folder = null, @container_name = 'mock', @enabled = 1 -- fail: task_type execute tsqlt.expectexception @expectedmessagepattern = '%task_type%cannot%null%empty%' execute deploy.usp_set_task_configuration @task_name = 'mock', @task_type = null, @worker_name = 'mock', @task_description = 'mock', @table_name = null, @target_options = null, @file_layout = 'mock', @source_folder = null, @container_name = 'mock', @enabled = 1 -- fail: worker_name execute tsqlt.expectexception @expectedmessagepattern = '%worker_name%cannot%null%empty%' execute deploy.usp_set_task_configuration @task_name = 'mock', @task_type = 'mock', @worker_name = null, @task_description = 'mock', @table_name = null, @target_options = null, @file_layout = 'mock', @source_folder = null, @container_name = 'mock', @enabled = 1 -- fail: task_description execute tsqlt.expectexception @expectedmessagepattern = '%task_description%cannot%null%empty%' execute deploy.usp_set_task_configuration @task_name = 'mock', @task_type = 'mock', @worker_name = 'mock', @task_description = null, @table_name = null, @target_options = null, @file_layout = 'mock', @source_folder = null, @container_name = 'mock', @enabled = 1 -- fail: file_layout execute tsqlt.expectexception @expectedmessagepattern = '%file_layout%cannot%null%empty%' execute deploy.usp_set_task_configuration @task_name = 'mock', @task_type = 'mock', @worker_name = 'mock', @task_description = 'mock', @table_name = null, @target_options = null, @file_layout = null, @source_folder = null, @container_name = 'mock', @enabled = 1 -- fail: container_name execute tsqlt.expectexception @expectedmessagepattern = '%container_name%cannot%null%empty%' execute deploy.usp_set_task_configuration @task_name = 'mock', @task_type = 'mock', @worker_name = 'mock', @task_description = 'mock', @table_name = null, @target_options = null, @file_layout = 'mock', @source_folder = null, @container_name = null, @enabled = 1 -- fail: enabled flag execute tsqlt.expectexception @expectedmessagepattern = '%enabled%cannot%null%' execute deploy.usp_set_task_configuration @task_name = 'mock', @task_type = 'mock', @worker_name = 'mock', @task_description = 'mock', @table_name = null, @target_options = null, @file_layout = 'mock', @source_folder = null, @container_name = 'mock', @enabled = null end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_plan\class_t_usp_start_plan.sql">create schema [t_usp_start_plan] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_start_plan'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_plan\setup_t_usp_start_plan.sql">create procedure [t_usp_start_plan].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 -- exec tsqlt.faketable 'meta.log_tasks', @identity=1 exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 -- exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_files', @identity = 1 -- exec tsqlt.faketable 'meta.plan_configuration' -- exec tsqlt.faketable 'meta.task_configuration' -- exec tsqlt.faketable 'meta.source_configuration' -- exec tsqlt.faketable 'meta.source_column_configuration' -- exec tsqlt.faketable 'meta.plan_task_configuration' insert meta.log_runs ( pipeline_id, new_plan, plan_name, current_status ) values ( 'xxx', 1, 'unit_test', 'pending' ) declare @run_id int = scope_identity() insert meta.log_plans ( pipeline_id, plan_name, registration_ts, run_id, current_status ) values ( 'xxx', 'unit_test', getdate(), @run_id, 'in progress') declare @plan_id int = scope_identity() insert meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id ) values ( @run_id, @plan_id, 'unit_test', @run_id ) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_plan\test should fail on bad params.sql">create procedure [t_usp_start_plan].[test fail bad params] @debug int = 0 begin -- fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%run_id%mandatory%' execute meta.usp_start_plan @plan_id = 1, @run_id = null -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%plan_id%mandatory%' execute meta.usp_start_plan @plan_id = null, @run_id = 1 -- fail: invalid run_id execute tsqlt.expectexception @expectedmessagepattern = '%log_plans%' execute meta.usp_start_plan @plan_id = 0, @run_id = 1 -- fail: invalid plan_id execute tsqlt.expectexception @expectedmessagepattern = '%log_runs%' execute meta.usp_start_plan @plan_id = 1, @run_id = 0 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_run\class_t_usp_start_run.sql">create schema [t_usp_start_run] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_start_run'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_run\setup_t_usp_start_run.sql">create procedure [t_usp_start_run].[setup] @debug int = 0 begin exec tsqlt.faketable 'meta.log_runs', @identity=1 --exec tsqlt.faketable 'meta.log_plans', @identity=1 -- exec tsqlt.faketable 'meta.log_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 --exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_files', @identity = 1 -- exec tsqlt.faketable 'meta.plan_configuration' -- exec tsqlt.faketable 'meta.task_configuration' -- exec tsqlt.faketable 'meta.source_configuration' -- exec tsqlt.faketable 'meta.source_column_configuration' -- exec tsqlt.faketable 'meta.plan_task_configuration' insert meta.log_runs ( pipeline_id, new_plan, plan_name, current_status ) values ( 'xxx', 1, 'unit_test', 'in progress' ) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_run\test should fail on bad params.sql">create procedure [t_usp_start_run].[test fail bad params] @debug int = 0 begin -- fail: run_id execute tsqlt.expectexception @expectedmessagepattern = '%run_id%mandatory%' execute meta.usp_start_run @run_id = null -- fail: invalid run_id execute tsqlt.expectexception @expectedmessagepattern = '%log_runs%' execute meta.usp_start_run @run_id = 0 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_task\class_t_usp_start_task.sql">create schema [t_usp_start_task] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_start_task'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_task\setup_t_usp_start_task.sql">create procedure [t_usp_start_task].[setup] @debug int = 0 begin --exec tsqlt.faketable 'meta.log_runs', @identity=1 exec tsqlt.faketable 'meta.log_plans', @identity=1 exec tsqlt.faketable 'meta.log_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_files', @identity = 1 -- exec tsqlt.faketable 'meta.plan_configuration' -- exec tsqlt.faketable 'meta.task_configuration' -- exec tsqlt.faketable 'meta.source_configuration' -- exec tsqlt.faketable 'meta.source_column_configuration' -- exec tsqlt.faketable 'meta.plan_task_configuration' insert meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status ) values ( 'xxx', 'unit_test', getdate(), 'in progress') declare @plan_id int = scope_identity() insert meta.log_tasks ( task_name, plan_id, current_status ) values ( 'unit_test', @plan_id, 'pending' ) declare @task_id int = scope_identity() insert meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name ) values ( @plan_id, 'unit_test', @task_id, 'unit_test' ) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_start_task\test should fail on bad params.sql">create procedure [t_usp_start_task].[test fail bad params] @debug int = 0 begin -- fail: task_id execute tsqlt.expectexception @expectedmessagepattern = 'plan_id%mandatory%' execute meta.usp_start_task @task_id = 1, @plan_id = null -- fail: plan_id execute tsqlt.expectexception @expectedmessagepattern = '%task_id%mandatory%' execute meta.usp_start_task @task_id = null, @plan_id = 1 -- fail: invalid plan_id execute tsqlt.expectexception @expectedmessagepattern = '%log_tasks%' execute meta.usp_start_task @task_id = 0, @plan_id = 1 -- fail: invalid task_id execute tsqlt.expectexception @expectedmessagepattern = '%log_plans%' execute meta.usp_start_task @task_id = 1, @plan_id = 0 end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\class_t_usp_update_file_activity.sql">create schema [t_usp_update_file_activity] authorization dbo go execute sp_addextendedproperty @name = 'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = 't_usp_update_file_activity'</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\setup_t_usp_update_file_activity.sql">create procedure [t_usp_update_file_activity].[setup] @debug int = 0 begin -- exec tsqlt.faketable 'meta.log_runs', @identity=1 -- exec tsqlt.faketable 'meta.log_plans', @identity=1 -- exec tsqlt.faketable 'meta.log_tasks', @identity=1 -- exec tsqlt.faketable 'meta.log_run_plans', @identity = 1 -- exec tsqlt.faketable 'meta.log_plan_tasks', @identity=1 exec tsqlt.faketable 'meta.log_files' -- exec tsqlt.faketable 'meta.plan_configuration' -- exec tsqlt.faketable 'meta.task_configuration' -- exec tsqlt.faketable 'meta.source_configuration' -- exec tsqlt.faketable 'meta.source_column_configuration' -- exec tsqlt.faketable 'meta.plan_task_configuration' insert meta.log_files ([file_id], [filename], [extended_filename], [archive_status] ) values (1, 'unit_test.txt', 'unit_test_ts.txt', 'succeeded'), (2, 'raw_test.txt', 'raw_test_ts.txt', null), (3, 'silver_test.txt', 'silver_test_ts.txt', null), (4, 'archive_test.txt', 'archive_test_ts.txt', null) end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\test should fail on bad params.sql">create procedure [t_usp_update_file_activity].[test fail bad params] @debug int = 0 begin -- fail: extended_filename execute tsqlt.expectexception @expectedmessagepattern = '%extended_filename%mandatory%' execute meta.usp_update_file_activity @extended_filename = null, @activity = 'raw', @success = 1, @info_message = '' -- fail: activity execute tsqlt.expectexception @expectedmessagepattern = '%activity%mandatory%' execute meta.usp_update_file_activity @extended_filename = 'unit_test_ts.txt', @activity = null, @success = 1, @info_message = '' -- fail: success_bit execute tsqlt.expectexception @expectedmessagepattern = '%success%mandatory%' execute meta.usp_update_file_activity @extended_filename = 'unit_test_ts.txt', @activity = 'raw', @success = null, @info_message = '' -- fail: info_message execute tsqlt.expectexception @expectedmessagepattern = '%info_message%mandatory%' execute meta.usp_update_file_activity @extended_filename = 'unit_test_ts.txt', @activity = 'raw', @success = 1, @info_message = null -- fail: info_message execute tsqlt.expectexception @expectedmessagepattern = '%info_message%mandatory%' execute meta.usp_update_file_activity @extended_filename = 'invalid_unit_test_ts.txt', @activity = 'raw', @success = 1, @info_message = '' -- fail: invalid activity execute tsqlt.expectexception @expectedmessagepattern = '%info_message%mandatory%' execute meta.usp_update_file_activity @extended_filename = 'invalid_unit_test_ts.txt', @activity = 'test', @success = 1, @info_message = '' end</file><file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\update status for failed files.sql">create procedure [t_usp_update_file_activity].[update status failed files] @debug int = 0 begin -- prepare expected actual tables create table [t_usp_end_plan].expected_log_files ( [file_id] int, [filename] varchar(20), [extended_filename] varchar(20), [raw_status] varchar(10), [raw_info] varchar(50), [silver_status] varchar(10), [silver_info] varchar(50), [archive_status] varchar(10), [archive_info] varchar(50) ) create table [t_usp_end_plan].actual_log_files ( [file_id] int, [filename] varchar(20), [extended_filename] varchar(20), [raw_status] varchar(10), [raw_info] varchar(50), [silver_status] varchar(10), [silver_info] varchar(50), [archive_status] varchar(10), [archive_info] varchar(50) ) insert [t_usp_end_plan].expected_log_files([file_id], [filename], [extended_filename], [raw_status], [raw_info], [silver_status], [silver_info], [archive_status], [archive_info]) values (2, 'raw_test.txt', 'raw_test_ts.txt', 'failed', 'raw file ingestion failed', null, null, null, null), (3, 'silver_test.txt', 'silver_test_ts.txt', null, null, 'failed', 'silver file ingestion failed', null, null), (4, 'archive_test.txt', 'archive_test_ts.txt', null, null, null, null, 'failed', 'archive file ingestion failed') execute meta.usp_update_file_activity @extended_filename = 'raw_test_ts.txt', @activity = 'raw', @success = 0, @info_message = 'raw file ingestion failed' execute meta.usp_update_file_activity @extended_filename = 'silver_test_ts.txt', @activity = 'silver', @success = 0, @info_message = 'silver file ingestion failed' execute meta.usp_update_file_activity @extended_filename = 'archive_test_ts.txt', @activity = 'archive', @success = 0, @info_message = 'archive file ingestion failed' insert [t_usp_end_plan].actual_log_files([file_id], [filename], [extended_filename], [raw_status], [raw_info], [silver_status], [silver_info], [archive_status], [archive_info]) select [file_id], [filename], [extended_filename], [raw_status], [raw_info], [silver_status], [silver_info], [archive_status], [archive_info] meta.log_files exec tsqlt.assertequalstable @expected = '[t_usp_end_plan].expected_log_files', @actual = '[t_usp_end_plan].actual_log_files' , @message = 'file-log test failed' end</file><file name="src\sql\_build\AzureV12_master.dacpac.readme.md">`azurev12_master.dacpac` file copied `c:\users\joachim.baert\.vscode\extensions\ms-mssql.sql-database-projects-vscode-0.19.0\builddirectory\systemdacpacs\azurev12\master.dacpac` local machine entirely clear could located build agent. project references updated "system database" reference "dacpac" reference reflect that. obviously, referencing "system database" let build process figure get master.dacpac from, would generic works well...</file><file name="src\sql\_build\db_build.ps1">[cmdletbinding()] param( [string] $projectpath = "$env:reporoot/src/sql/metadb_test/metadb_test.sqlproj" ) # see https://learn.microsoft.com/en-us/sql/azure-data-studio/extensions/sql-database-project-extension-sdk-style-projects?view=sql-server-2016 #$systemdacpacslocation = "/home/vscode/.vscode-server/extensions/ms-mssql.sql-database-projects-vscode-0.19.0/builddirectory" #dotnet build $psscriptroot/../dwhdemo/dwhdemo.sqlproj /p:netcorebuild=true /p:systemdacpacslocation=$systemdacpacslocation # build error: https://github.com/microsoft/azuredatastudio/issues/20240 # dotnet nuget add source https://api.nuget.org/v3/index.json -n nuget.org # dotnet nuget list source dotnet build $projectpath /p:netcorebuild=true ( $lastexitcode -ne 0 ) { write-error "dotnet build failed." }</file><file name="src\sql\_build\db_deploy.ps1">[cmdletbinding()] param ( [parameter()] [validateset("script","publish")] [string] $action = "publish", [parameter()] [string] $sqlpackageexepath = $env:agent_sqlpackage_path, [parameter()] [string] $dacpacpath = "$($env:reporoot)\src\sql\metadb_test\bin\debug\metadb_test.dacpac", #defaults name folder &amp; name dacpac "_script.sql" suffix [parameter()] [string] $outputscriptpath = $null, [parameter()] [string] $publishprofilepath = $null, [parameter()] [string] $targetserver = $env:sql_server, [parameter()] [string] $targetdatabase = $env:sql_database, [parameter()] [string] $accesstoken = ((get-azaccesstoken -resourceurl "https://database.windows.net" -warningaction "ignore" -assecurestring -debug:$false).token | convertfrom-securestring -asplaintext) ) # dotnet tool install -g microsoft.sqlpackage # download sqlpackage windows: https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-download?view=sql-server-ver16#windows-net-6 #region local script settings $erroractionpreference = "stop" set-strictmode -version "latest" #endregion #region script info $inputparameters = $myinvocation.mycommand.parameters.keys | where-object { ($_ -notin [system.management.automation.pscmdlet]::commonparameters ) -and ($_ -notin [system.management.automation.pscmdlet]::optionalcommonparameters ) } $maxparameterlength = ( $inputparameters | measure-object -maximum -property length).maximum write-information "" write-information "----------------------------------------------------------------------" write-information "starting script: $($myinvocation.mycommand.name)" write-information " path: $(split-path $myinvocation.mycommand.source)" write-information " parameters:" foreach ($param $inputparameters) { write-information (" | " + $param.padright( $maxparameterlength ) + ": " + (get-variable -name $param).value ) } write-information "----------------------------------------------------------------------" #endregion script info # create deploy script ( $action -eq "script") { # outputscriptpath specified, create default ( -not $outputscriptpath ) { $outputscriptpath = join-path ( split-path $dacpacpath ) ((split-path $dacpacpath -leafbase) + "_script.sql") write-output "calculated outputscriptpath as: $outputscriptpath" } # run sqlpackage script mode . $sqlpackageexepath ` /action:script ` /sourcefile:"$dacpacpath" /outputpath:"$outputscriptpath" ` /targetdatabasename:"$targetdatabase" /targetservername:"$targetserver" /accesstoken:"$accesstoken" $sqlpackageexitcode = $lastexitcode # remove-variable -name "clearpwd" ( $sqlpackageexitcode -ne 0) { write-error "sqlpackage returned failure, exitcode: $sqlpackageexitcode" } } # deploy elseif ( $action -eq "publish") { #$clearpwd = convertfrom-securestring -securestring $targetpassword -asplaintext . $sqlpackageexepath ` /action:publish ` /sourcefile:"$dacpacpath" ` /targetdatabasename:"$targetdatabase" /targetservername:"$targetserver" /accesstoken:"$accesstoken" $sqlpackageexitcode = $lastexitcode # remove-variable -name "clearpwd" ( $sqlpackageexitcode -ne 0) { write-error "sqlpackage returned failure, exitcode: $sqlpackageexitcode" } } # unsupported action else { throw "unsupported action: '$action'" } #region end script write-information "" write-information "--- end script: $($myinvocation.mycommand.name) ---" write-information "" #endregion</file><file name="src\sql\_test\drop_tsqlt_testclasses.sql">/* script drops test classes (so tsqlt framework itself), useful remove obsolete test artifacts redeploying. */ print 'dropping tsqlt test classes...' declare @testclass sysname = ''; @testclass null begin select @testclass = min( [name]) tsqlt.testclasses @testclass null begin print ' dropping testclass: ' + @testclass exec tsqlt.dropclass @classname = @testclass end end</file><file name="src\sql\_test\run_testclasses.ps1">#requires -module sqlserver [cmdletbinding()] param ( [parameter(mandatory)] [string] $targetserver, [parameter(mandatory)] [string] $targetdatabase, [parameter(mandatory=$false)] [string] $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net -debug:$false).token, [parameter(mandatory)] [string[]] $testsets = @( "design", "unit") ) foreach ($testset $testsets) { # $query = " # declare @cmd nvarchar(max); # set @cmd='alter database ' + quotename(db_name()) + ' set trustworthy on;'; # exec(@cmd); # " # invoke-sqlcmd -serverinstance $targetserver -database $targetdatabase -accesstoken $accesstoken -query $query -includesqlusererrors write-output "" write-output "-----------------------------------" write-output "execute tsqlt testset: $testset ..." write-output "-----------------------------------" $query = "exec test.usp_run_tsqlt_tests @test_type = '$testset', @debug = 1" #add -verbose see sql "print" output write-output $query invoke-sqlcmd -serverinstance $targetserver -database $targetdatabase -accesstoken $accesstoken -query $query -includesqlusererrors }</file><file name="src\sql\_test\tSQLt_1.0.5873.27393.sql">/* copyright 2011 tsqlt licensed apache license, version 2.0 (the "license"); may use file except compliance license. may obtain copy license http://www.apache.org/licenses/license-2.0 unless required applicable law agreed writing, software distributed license distributed "as is" basis, without warranties conditions kind, either express implied. see license specific language governing permissions limitations license. */ declare @msg nvarchar(max);select @msg = 'installed '+convert(nvarchar,getdate(),121);raiserror(@msg,0,1); go type_id('tsqlt.private') null drop type tsqlt.private; type_id('tsqltprivate') null drop type tsqltprivate; go object_id('tsqlt.dropclass') null exec tsqlt.dropclass tsqlt; go exists (select 1 sys.assemblies name = 'tsqltclr') drop assembly tsqltclr; go create schema tsqlt; go set quoted_identifier on; go go create procedure tsqlt.dropclass @classname nvarchar(max) begin declare @cmd nvarchar(max); objectinfo(name, type) ( select quotename(schema_name(o.schema_id))+'.'+quotename(o.name) , o.type sys.objects o.schema_id = schema_id(@classname) ), typeinfo(name) ( select quotename(schema_name(t.schema_id))+'.'+quotename(t.name) sys.types t.schema_id = schema_id(@classname) ), xmlschemainfo(name) ( select quotename(schema_name(xsc.schema_id))+'.'+quotename(xsc.name) sys.xml_schema_collections xsc xsc.schema_id = schema_id(@classname) ), dropstatements(no,cmd) ( select 10, 'drop ' + case type 'p' 'procedure' 'pc' 'procedure' 'u' 'table' 'if' 'function' 'tf' 'function' 'fn' 'function' 'v' 'view' end + ' ' + name + ';' objectinfo union select 20, 'drop type ' + name + ';' typeinfo union select 30, 'drop xml schema collection ' + name + ';' xmlschemainfo union select 10000,'drop schema ' + quotename(name) +';' sys.schemas schema_id = schema_id(parsename(@classname,1)) ), statementblob(xml)as ( select cmd [text()] dropstatements order xml path(''), type ) select @cmd = xml.value('/', 'nvarchar(max)') statementblob; exec(@cmd); end; go go create function tsqlt.private_bin2hex(@vb varbinary(max)) returns table return select x.s bare, '0x'+x.s prefix (select lower(cast('' xml).value('xs:hexbinary(sql:variable("@vb") )','varchar(max)')))x(s); go go create table tsqlt.private_newtestclasslist ( classname nvarchar(450) primary key clustered ); go go create procedure tsqlt.private_resetnewtestclasslist begin set nocount on; delete tsqlt.private_newtestclasslist; end; go go go create view tsqlt.private_systypes select * sys.types t; go if(cast(serverproperty('productversion') varchar(max)) like '9.%') begin exec('alter view tsqlt.private_systypes select *,0 is_table_type sys.types t;'); end; go go go create function tsqlt.private_getfulltypename(@typeid int, @length int, @precision int, @scale int, @collationname nvarchar(max)) returns table return select x.schemaname + '.' + x.name + x.suffix + x.collation typename, x.schemaname, x.name, x.suffix, x.is_table_type istabletype from( select quotename(schema_name(t.schema_id)) schemaname, quotename(t.name) name, case t.max_length = -1 '' @length = -1 '(max)' t.name like 'n%char' '(' + cast(@length / 2 nvarchar) + ')' t.name like '%char' t.name like '%binary' '(' + cast(@length nvarchar) + ')' t.name ('decimal', 'numeric') '(' + cast(@precision nvarchar) + ',' + cast(@scale nvarchar) + ')' else '' end suffix, case @collationname null t.is_user_defined = 1 '' else ' collate ' + @collationname end collation, t.is_table_type tsqlt.private_systypes t.user_type_id = @typeid )x; go create procedure tsqlt.private_disallowoverwritingnontestschema @classname nvarchar(max) begin schema_id(@classname) null tsqlt.private_istestclass(@classname) = 0 begin raiserror('attempted execute tsqlt.newtestclass ''%s'' existing schema test class', 16, 10, @classname); end end; go create function tsqlt.private_quoteclassnamefornewtestclass(@classname nvarchar(max)) returns nvarchar(max) begin return case @classname like '[[]%]' @classname else quotename(@classname) end; end; go create procedure tsqlt.private_markschemaastestclass @quotedclassname nvarchar(max) begin set nocount on; declare @unquotedclassname nvarchar(max); select @unquotedclassname = name sys.schemas quotename(name) = @quotedclassname; exec sp_addextendedproperty @name = n'tsqlt.testclass', @value = 1, @level0type = 'schema', @level0name = @unquotedclassname; insert tsqlt.private_newtestclasslist(classname) select @unquotedclassname exists ( select * tsqlt.private_newtestclasslist ntc with(updlock,rowlock,holdlock) ntc.classname = @unquotedclassname ); end; go create procedure tsqlt.newtestclass @classname nvarchar(max) begin begin try exec tsqlt.private_disallowoverwritingnontestschema @classname; exec tsqlt.dropclass @classname = @classname; declare @quotedclassname nvarchar(max); select @quotedclassname = tsqlt.private_quoteclassnamefornewtestclass(@classname); exec ('create schema ' + @quotedclassname); exec tsqlt.private_markschemaastestclass @quotedclassname; end try begin catch declare @errmsg nvarchar(max);set @errmsg = error_message() + ' (error originated ' + error_procedure() + ')'; declare @errsvr int;set @errsvr = error_severity(); raiserror(@errmsg, @errsvr, 10); end catch; end; go create procedure tsqlt.fail @message0 nvarchar(max) = '', @message1 nvarchar(max) = '', @message2 nvarchar(max) = '', @message3 nvarchar(max) = '', @message4 nvarchar(max) = '', @message5 nvarchar(max) = '', @message6 nvarchar(max) = '', @message7 nvarchar(max) = '', @message8 nvarchar(max) = '', @message9 nvarchar(max) = '' begin declare @warningmessage nvarchar(max); set @warningmessage = ''; xact_state() = -1 begin set @warningmessage = char(13)+char(10)+'warning: uncommitable transaction detected!'; declare @tranname nvarchar(max); select @tranname = tranname tsqlt.testresult id = (select max(id) tsqlt.testresult); declare @trancount int; set @trancount = @@trancount; rollback; while(@trancount&gt;0) begin begin tran; set @trancount = @trancount -1; end; save tran @tranname; end; insert tsqlt.testmessage(msg) select coalesce(@message0, '!null!') + coalesce(@message1, '!null!') + coalesce(@message2, '!null!') + coalesce(@message3, '!null!') + coalesce(@message4, '!null!') + coalesce(@message5, '!null!') + coalesce(@message6, '!null!') + coalesce(@message7, '!null!') + coalesce(@message8, '!null!') + coalesce(@message9, '!null!') + @warningmessage; raiserror('tsqlt.failure',16,10); end; go go create table tsqlt.testresult( id int identity(1,1) primary key clustered, class nvarchar(max) null, testcase nvarchar(max) null, name (quotename(class) + '.' + quotename(testcase)), tranname nvarchar(max) null, result nvarchar(max) null, msg nvarchar(max) null, teststarttime datetime null constraint [df:testresult(teststarttime)] default getdate(), testendtime datetime null ); go create table tsqlt.testmessage( msg nvarchar(max) ); go create table tsqlt.run_lastexecution( testname nvarchar(max), sessionid int, logintime datetime ); go create table tsqlt.private_expectexception(i int); go create procedure tsqlt.private_print @message nvarchar(max), @severity int = 0 begin declare @spos int;set @spos = 1; declare @epos int; declare @len int; set @len = len(@message); declare @submsg nvarchar(max); declare @cmd nvarchar(max); declare @cleanedmessage nvarchar(max); set @cleanedmessage = replace(@message,'%','%%'); (@spos &lt;= @len) begin set @epos = charindex(char(13)+char(10),@cleanedmessage+char(13)+char(10),@spos); set @submsg = substring(@cleanedmessage, @spos, @epos - @spos); set @cmd = n'raiserror(@msg,@severity,10) nowait;'; exec sp_executesql @cmd, n'@msg nvarchar(max),@severity int', @submsg, @severity; select @spos = @epos + 2, @severity = 0; --print first line high severity end return 0; end; go create procedure tsqlt.private_printxml @message xml begin select @message xml path('');--required together ":xml on" sqlcmd statement allow 1mb returned return 0; end; go create procedure tsqlt.getnewtranname @tranname char(32) output begin select @tranname = left('tsqlttran'+replace(cast(newid() nvarchar(60)),'-',''),32); end; go create procedure tsqlt.settestresultformatter @formatter nvarchar(4000) begin exists (select 1 sys.extended_properties [name] = n'tsqlt.resultsformatter') begin exec sp_dropextendedproperty @name = n'tsqlt.resultsformatter', @level0type = 'schema', @level0name = 'tsqlt', @level1type = 'procedure', @level1name = 'private_outputtestresults'; end; exec sp_addextendedproperty @name = n'tsqlt.resultsformatter', @value = @formatter, @level0type = 'schema', @level0name = 'tsqlt', @level1type = 'procedure', @level1name = 'private_outputtestresults'; end; go create function tsqlt.gettestresultformatter() returns nvarchar(max) begin declare @formattername nvarchar(max); select @formattername = cast(value nvarchar(max)) sys.extended_properties name = n'tsqlt.resultsformatter' major_id = object_id('tsqlt.private_outputtestresults'); select @formattername = coalesce(@formattername, 'tsqlt.defaultresultformatter'); return @formattername; end; go create procedure tsqlt.private_outputtestresults @testresultformatter nvarchar(max) = null begin declare @formatter nvarchar(max); select @formatter = coalesce(@testresultformatter, tsqlt.gettestresultformatter()); exec (@formatter); end go ---------------------------------------------------------------------- create function tsqlt.private_getlasttestnameifnotprovided(@testname nvarchar(max)) returns nvarchar(max) begin if(ltrim(isnull(@testname,'')) = '') begin select @testname = testname tsqlt.run_lastexecution le join sys.dm_exec_sessions es le.sessionid = es.session_id le.logintime = es.login_time es.session_id = @@spid; end return @testname; end go create procedure tsqlt.private_savetestnameforsession @testname nvarchar(max) begin delete tsqlt.run_lastexecution sessionid = @@spid; insert tsqlt.run_lastexecution(testname, sessionid, logintime) select testname = @testname, session_id, login_time sys.dm_exec_sessions session_id = @@spid; end go ---------------------------------------------------------------------- create view tsqlt.testclasses select s.name name, s.schema_id schemaid sys.extended_properties ep join sys.schemas ep.major_id = s.schema_id ep.name = n'tsqlt.testclass'; go create view tsqlt.tests select classes.schemaid, classes.name testclassname, procs.object_id objectid, procs.name name tsqlt.testclasses classes join sys.procedures procs classes.schemaid = procs.schema_id lower(procs.name) like 'test%'; go create function tsqlt.testcasesummary() returns table return a(cnt, successcnt, failcnt, errorcnt) ( select count(1), isnull(sum(case result = 'success' 1 else 0 end), 0), isnull(sum(case result = 'failure' 1 else 0 end), 0), isnull(sum(case result = 'error' 1 else 0 end), 0) tsqlt.testresult ) select 'test case summary: ' + cast(cnt nvarchar) + ' test case(s) executed, '+ cast(successcnt nvarchar) + ' succeeded, '+ cast(failcnt nvarchar) + ' failed, '+ cast(errorcnt nvarchar) + ' errored.' msg,* a; go create procedure tsqlt.private_validateprocedurecanbeusedwithspyprocedure @procedurename nvarchar(max) begin exists(select 1 sys.procedures object_id = object_id(@procedurename)) begin raiserror('cannot use spyprocedure %s procedure exist', 16, 10, @procedurename) nowait; end; (1020 &lt; (select count(*) sys.parameters object_id = object_id(@procedurename))) begin raiserror('cannot use spyprocedure procedure %s contains 1020 parameters', 16, 10, @procedurename) nowait; end; end; go create procedure tsqlt.assertequals @expected sql_variant, @actual sql_variant, @message nvarchar(max) = '' begin ((@expected = @actual) (@actual null @expected null)) return 0; declare @msg nvarchar(max); select @msg = 'expected: &lt;' + isnull(cast(@expected nvarchar(max)), 'null') + '&gt; was: &lt;' + isnull(cast(@actual nvarchar(max)), 'null') + '&gt;'; if((coalesce(@message,'') &lt;&gt; '') (@message like '% ')) set @message = @message + ' '; exec tsqlt.fail @message, @msg; end; go /*******************************************************************************************/ /*******************************************************************************************/ /*******************************************************************************************/ create function tsqlt.private_getcleanschemaname(@schemaname nvarchar(max), @objectname nvarchar(max)) returns nvarchar(max) begin return (select schema_name(schema_id) sys.objects object_id = case isnull(@schemaname,'') ('','[]') object_id(@objectname) else object_id(@schemaname + '.' + @objectname) end); end; go create function [tsqlt].[private_getcleanobjectname](@objectname nvarchar(max)) returns nvarchar(max) begin return (select object_name(object_id(@objectname))); end; go create function tsqlt.private_resolvefaketablenamesforbackwardcompatibility (@tablename nvarchar(max), @schemaname nvarchar(max)) returns table return select quotename(object_schema_name(object_id)) cleanschemaname, quotename(object_name(object_id)) cleantablename (select case @schemaname null object_id(@tablename) else coalesce(object_id(@schemaname + '.' + @tablename),object_id(@tablename + '.' + @schemaname)) end object_id ) ids; go /*******************************************************************************************/ /*******************************************************************************************/ /*******************************************************************************************/ create function tsqlt.private_getoriginaltablename(@schemaname nvarchar(max), @tablename nvarchar(max)) --delete!!! returns nvarchar(max) begin return (select cast(value nvarchar(4000)) sys.extended_properties class_desc = 'object_or_column' major_id = object_id(@schemaname + '.' + @tablename) minor_id = 0 name = 'tsqlt.faketable_orgtablename'); end; go create function tsqlt.private_getoriginaltableinfo(@tableobjectid int) returns table return select cast(value nvarchar(4000)) orgtablename, object_id(quotename(object_schema_name(@tableobjectid)) + '.' + quotename(cast(value nvarchar(4000)))) orgtableobjectid sys.extended_properties class_desc = 'object_or_column' major_id = @tableobjectid minor_id = 0 name = 'tsqlt.faketable_orgtablename'; go create function [tsqlt].[f_num]( @n int ) returns table return c0(c) (select 1 union select 1), c1(c) (select 1 c0 cross join c0 b), c2(c) (select 1 c1 cross join c1 b), c3(c) (select 1 c2 cross join c2 b), c4(c) (select 1 c3 cross join c3 b), c5(c) (select 1 c4 cross join c4 b), c6(c) (select 1 c5 cross join c5 b) select top(case @n&gt;0 @n else 0 end) row_number() (order c) c6; go create procedure [tsqlt].[private_setfakeviewon_singleview] @viewname nvarchar(max) begin declare @cmd nvarchar(max), @schemaname nvarchar(max), @triggername nvarchar(max); select @schemaname = object_schema_name(objid), @viewname = object_name(objid), @triggername = object_name(objid) + '_setfakeviewon' (select object_id(@viewname) objid) x; set @cmd = 'create trigger $$schema_name$$.$$trigger_name$$ $$schema_name$$.$$view_name$$ instead insert begin raiserror(''test system invalid state. setfakeviewoff must called setfakeviewon called. call setfakeviewoff creating test case procedures.'', 16, 10) nowait; return; end; '; set @cmd = replace(@cmd, '$$schema_name$$', quotename(@schemaname)); set @cmd = replace(@cmd, '$$view_name$$', quotename(@viewname)); set @cmd = replace(@cmd, '$$trigger_name$$', quotename(@triggername)); exec(@cmd); exec sp_addextendedproperty @name = n'setfakeviewontrigger', @value = 1, @level0type = 'schema', @level0name = @schemaname, @level1type = 'view', @level1name = @viewname, @level2type = 'trigger', @level2name = @triggername; return 0; end; go create procedure [tsqlt].[setfakeviewon] @schemaname nvarchar(max) begin declare @viewname nvarchar(max); declare viewnames cursor local fast_forward select quotename(object_schema_name(object_id)) + '.' + quotename([name]) viewname sys.views schema_id = schema_id(@schemaname); open viewnames; fetch next viewnames @viewname; @@fetch_status = 0 begin exec tsqlt.private_setfakeviewon_singleview @viewname; fetch next viewnames @viewname; end; close viewnames; deallocate viewnames; end; go create procedure [tsqlt].[private_setfakeviewoff_singleview] @viewname nvarchar(max) begin declare @cmd nvarchar(max), @schemaname nvarchar(max), @triggername nvarchar(max); select @schemaname = quotename(object_schema_name(objid)), @triggername = quotename(object_name(objid) + '_setfakeviewon') (select object_id(@viewname) objid) x; set @cmd = 'drop trigger %schema_name%.%trigger_name%;'; set @cmd = replace(@cmd, '%schema_name%', @schemaname); set @cmd = replace(@cmd, '%trigger_name%', @triggername); exec(@cmd); end; go create procedure [tsqlt].[setfakeviewoff] @schemaname nvarchar(max) begin declare @viewname nvarchar(max); declare viewnames cursor local fast_forward select quotename(object_schema_name(t.parent_id)) + '.' + quotename(object_name(t.parent_id)) viewname sys.extended_properties ep join sys.triggers ep.major_id = t.object_id ep.name = n'setfakeviewontrigger' open viewnames; fetch next viewnames @viewname; @@fetch_status = 0 begin exec tsqlt.private_setfakeviewoff_singleview @viewname; fetch next viewnames @viewname; end; close viewnames; deallocate viewnames; end; go create function tsqlt.private_getquotedfullname(@objectid int) returns nvarchar(517) begin declare @quotedname nvarchar(517); select @quotedname = quotename(object_schema_name(@objectid)) + '.' + quotename(object_name(@objectid)); return @quotedname; end; go create function tsqlt.private_getschemaid(@schemaname nvarchar(max)) returns int begin return ( select top(1) schema_id sys.schemas @schemaname (name, quotename(name), quotename(name, '"')) order case name = @schemaname 0 else 1 end ); end; go create function tsqlt.private_istestclass(@testclassname nvarchar(max)) returns bit begin return case exists( select 1 tsqlt.testclasses schemaid = tsqlt.private_getschemaid(@testclassname) ) 1 else 0 end; end; go create function tsqlt.private_resolveschemaname(@name nvarchar(max)) returns table return ids(schemaid) (select tsqlt.private_getschemaid(@name) ), idswithnames(schemaid, quotedschemaname) (select schemaid, quotename(schema_name(schemaid)) ids ) select schemaid, quotedschemaname, case exists(select 1 tsqlt.testclasses testclasses.schemaid = idswithnames.schemaid) 1 else 0 end istestclass, case schemaid null 1 else 0 end isschema idswithnames; go create function tsqlt.private_resolveobjectname(@name nvarchar(max)) returns table return ids(schemaid, objectid) (select schema_id(object_schema_name(object_id(@name))), object_id(@name) ), idswithnames(schemaid, objectid, quotedschemaname, quotedobjectname) (select schemaid, objectid, quotename(schema_name(schemaid)) quotedschemaname, quotename(object_name(objectid)) quotedobjectname ids ) select schemaid, objectid, quotedschemaname, quotedobjectname, quotedschemaname + '.' + quotedobjectname quotedfullname, case lower(quotedobjectname) like '[[]test%]' objectid = object_id(quotedschemaname + '.' + quotedobjectname,'p') 1 else 0 end istestcase idswithnames; go create function tsqlt.private_resolvename(@name nvarchar(max)) returns table return resolvednames(ord, schemaid, objectid, quotedschemaname, quotedobjectname, quotedfullname, istestclass, istestcase, isschema) (select 1, schemaid, null, quotedschemaname, null, quotedschemaname, istestclass, 0, 1 tsqlt.private_resolveschemaname(@name) union select 2, schemaid, objectid, quotedschemaname, quotedobjectname, quotedfullname, 0, istestcase, 0 tsqlt.private_resolveobjectname(@name) union select 3, null, null, null, null, null, 0, 0, 0 ) select top(1) schemaid, objectid, quotedschemaname, quotedobjectname, quotedfullname, istestclass, istestcase, isschema resolvednames schemaid null ord = 3 order ord go create procedure tsqlt.uninstall begin drop type tsqlt.private; exec tsqlt.dropclass 'tsqlt'; drop assembly tsqltclr; end; go go go create function tsqlt.private_getexternalaccesskeybytes() returns table return select 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 externalaccesskeybytes, 0x7722217d36028e4c externalaccesskeythumbprint; go go go create procedure tsqlt.removeexternalaccesskey begin if(not exists(select * sys.fn_my_permissions(null,'server') fmp fmp.permission_name = 'control server')) begin raiserror('only principals control server permission execute procedure.',16,10); return -1; end; declare @master_sys_sp_executesql nvarchar(max); set @master_sys_sp_executesql = 'master.sys.sp_executesql'; suser_id('tsqltexternalaccesskey') null drop login tsqltexternalaccesskey; exec @master_sys_sp_executesql n'if asymkey_id(''tsqltexternalaccesskey'') null drop asymmetric key tsqltexternalaccesskey;'; exec @master_sys_sp_executesql n'if exists(select * sys.assemblies name = ''tsqltexternalaccesskey'') drop assembly tsqltexternalaccesskey;'; end; go go go create procedure tsqlt.installexternalaccesskey begin if(not exists(select * sys.fn_my_permissions(null,'server') fmp fmp.permission_name = 'control server')) begin raiserror('only principals control server permission execute procedure.',16,10); return -1; end; declare @cmd nvarchar(max); declare @cmd2 nvarchar(max); declare @master_sys_sp_executesql nvarchar(max); set @master_sys_sp_executesql = 'master.sys.sp_executesql'; set @cmd = 'if exists(select * sys.assemblies name = ''tsqltexternalaccesskey'') drop assembly tsqltexternalaccesskey;'; exec @master_sys_sp_executesql @cmd; set @cmd2 = 'select @cmd = ''drop assembly ''+quotename(a.name)+'';'''+ ' master.sys.assemblies a'+ ' a.clr_name like ''tsqltexternalaccesskey, %'';'; exec sys.sp_executesql @cmd2,n'@cmd nvarchar(max) output',@cmd out; exec @master_sys_sp_executesql @cmd; select @cmd = 'create assembly tsqltexternalaccesskey authorization dbo ' + bh.prefix + ' permission_set = safe;' tsqlt.private_getexternalaccesskeybytes() pgeakb cross apply tsqlt.private_bin2hex(pgeakb.externalaccesskeybytes) bh; exec @master_sys_sp_executesql @cmd; suser_id('tsqltexternalaccesskey') null drop login tsqltexternalaccesskey; set @cmd = n'if asymkey_id(''tsqltexternalaccesskey'') null drop asymmetric key tsqltexternalaccesskey;'; exec @master_sys_sp_executesql @cmd; set @cmd2 = 'select @cmd = isnull(''drop login ''+quotename(sp.name)+'';'','''')+''drop asymmetric key '' + quotename(ak.name) + '';'''+ ' master.sys.asymmetric_keys ak'+ ' join tsqlt.private_getexternalaccesskeybytes() pgeakb'+ ' ak.thumbprint = pgeakb.externalaccesskeythumbprint'+ ' left join master.sys.server_principals sp'+ ' ak.sid = sp.sid;'; exec sys.sp_executesql @cmd2,n'@cmd nvarchar(max) output',@cmd out; exec @master_sys_sp_executesql @cmd; set @cmd = 'create asymmetric key tsqltexternalaccesskey assembly tsqltexternalaccesskey;'; exec @master_sys_sp_executesql @cmd; set @cmd = 'create login tsqltexternalaccesskey asymmetric key tsqltexternalaccesskey;'; exec @master_sys_sp_executesql @cmd; set @cmd = 'drop assembly tsqltexternalaccesskey;'; exec @master_sys_sp_executesql @cmd; set @cmd = 'grant external access assembly tsqltexternalaccesskey;'; exec @master_sys_sp_executesql @cmd; end; go go go create procedure tsqlt.enableexternalaccess @try bit = 0, @enable bit = 1 begin begin try @enable = 1 begin exec('alter assembly tsqltclr permission_set = external_access;'); end else begin exec('alter assembly tsqltclr permission_set = safe;'); end end try begin catch if(@try = 0) begin declare @message nvarchar(4000); set @message = 'the attempt ' + case @enable = 1 'enable' else 'disable' end + ' tsqlt features requiring external_access failed' + ': '+error_message(); raiserror(@message,16,10); end; return -1; end catch; return 0; end; go go create table tsqlt.private_configurations ( name nvarchar(100) primary key clustered, value sql_variant ); go go create procedure tsqlt.private_setconfiguration @name nvarchar(100), @value sql_variant begin if(exists(select 1 tsqlt.private_configurations with(rowlock,updlock) name = @name)) begin update tsqlt.private_configurations set value = @value name = @name; end; else begin insert tsqlt.private_configurations(name,value) values(@name,@value); end; end; go go go create function tsqlt.private_getconfiguration( @name nvarchar(100) ) returns table return select pc.name, pc.value tsqlt.private_configurations pc pc.name = @name; go go go create procedure tsqlt.setverbose @verbose bit = 1 begin exec tsqlt.private_setconfiguration @name = 'verbose', @value = @verbose; end; go go create table tsqlt.captureoutputlog ( id int identity(1,1) primary key clustered, outputtext nvarchar(max) ); go create procedure tsqlt.logcapturedoutput @text nvarchar(max) begin insert tsqlt.captureoutputlog (outputtext) values (@text); end; go go create assembly [tsqltclr] authorization [dbo] 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 permission_set = safe; go go go create procedure tsqlt.resultsetfilter @resultsetno int, @command nvarchar(max) external name tsqltclr.[tsqltclr.storedprocedures].resultsetfilter; go create procedure tsqlt.assertresultsetshavesamemetadata @expectedcommand nvarchar(max), @actualcommand nvarchar(max) external name tsqltclr.[tsqltclr.storedprocedures].assertresultsetshavesamemetadata; go create type tsqlt.[private] external name tsqltclr.[tsqltclr.tsqltprivate]; go create procedure tsqlt.newconnection @command nvarchar(max) external name tsqltclr.[tsqltclr.storedprocedures].newconnection; go create procedure tsqlt.captureoutput @command nvarchar(max) external name tsqltclr.[tsqltclr.storedprocedures].captureoutput; go create procedure tsqlt.suppressoutput @command nvarchar(max) external name tsqltclr.[tsqltclr.storedprocedures].suppressoutput; go go go create procedure tsqlt.tabletotext @txt nvarchar(max) output, @tablename nvarchar(max), @orderby nvarchar(max) = null, @printonlycolumnnamealiaslist nvarchar(max) = null begin set @txt = tsqlt.private::tabletostring(@tablename, @orderby, @printonlycolumnnamealiaslist); end; go go create table tsqlt.private_renamedobjectlog ( id int identity(1,1) constraint pk__private_renamedobjectlog__id primary key clustered, objectid int null, originalname nvarchar(max) null ); go create procedure tsqlt.private_markobjectbeforerename @schemaname nvarchar(max), @originalname nvarchar(max) begin insert tsqlt.private_renamedobjectlog (objectid, originalname) values (object_id(@schemaname + '.' + @originalname), @originalname); end; go create procedure tsqlt.private_renameobjecttouniquename @schemaname nvarchar(max), @objectname nvarchar(max), @newname nvarchar(max) = null output begin set @newname=tsqlt.private::createuniqueobjectname(); declare @renamecmd nvarchar(max); set @renamecmd = 'exec sp_rename ''' + @schemaname + '.' + @objectname + ''', ''' + @newname + ''';'; exec tsqlt.private_markobjectbeforerename @schemaname, @objectname; exec tsqlt.suppressoutput @renamecmd; end; go create procedure tsqlt.private_renameobjecttouniquenameusingobjectid @objectid int, @newname nvarchar(max) = null output begin declare @schemaname nvarchar(max); declare @objectname nvarchar(max); select @schemaname = quotename(object_schema_name(@objectid)), @objectname = quotename(object_name(@objectid)); exec tsqlt.private_renameobjecttouniquename @schemaname,@objectname, @newname output; end; go go create procedure tsqlt.removeobject @objectname nvarchar(max), @newname nvarchar(max) = null output, @ifexists int = 0 begin declare @objectid int; select @objectid = object_id(@objectname); if(@objectid null) begin if(@ifexists = 1) return; raiserror('%s exist!',16,10,@objectname); end; exec tsqlt.private_renameobjecttouniquenameusingobjectid @objectid, @newname = @newname output; end; go go go create procedure tsqlt.removeobjectifexists @objectname nvarchar(max), @newname nvarchar(max) = null output begin exec tsqlt.removeobject @objectname = @objectname, @newname = @newname out, @ifexists = 1; end; go go go create procedure tsqlt.private_cleantestresult begin delete tsqlt.testresult; end; go go go create procedure tsqlt.private_init begin exec tsqlt.private_cleantestresult; declare @enable bit; set @enable = 1; declare @version_match bit;set @version_match = 0; begin try exec sys.sp_executesql n'select @r = case i.version = i.clrversion 1 else 0 end tsqlt.info() i;',n'@r bit output',@version_match out; end try begin catch raiserror('cannot access clr. assembly might invalid state. try running exec tsqlt.enableexternalaccess @enable = 0; reinstalling tsqlt.',16,10); return; end catch; if(@version_match = 0) begin raiserror('tsqlt invalid state. please reinstall tsqlt.',16,10); return; end; if((select sqledition tsqlt.info()) &lt;&gt; 'sql azure') begin exec tsqlt.enableexternalaccess @enable = @enable, @try = 1; end; end; go go create procedure tsqlt.private_getsetupprocedurename @testclassid int = null, @setupprocname nvarchar(max) output begin select @setupprocname = tsqlt.private_getquotedfullname(object_id) sys.procedures schema_id = @testclassid lower(name) = 'setup'; end; go create procedure tsqlt.private_runtest @testname nvarchar(max), @setup nvarchar(max) = null begin declare @msg nvarchar(max); set @msg = ''; declare @msg2 nvarchar(max); set @msg2 = ''; declare @cmd nvarchar(max); set @cmd = ''; declare @testclassname nvarchar(max); set @testclassname = ''; declare @testprocname nvarchar(max); set @testprocname = ''; declare @result nvarchar(max); set @result = 'success'; declare @tranname char(32); exec tsqlt.getnewtranname @tranname out; declare @testresultid int; declare @preexectrancount int; declare @verbosemsg nvarchar(max); declare @verbose bit; set @verbose = isnull((select cast(value bit) tsqlt.private_getconfiguration('verbose')),0); truncate table tsqlt.captureoutputlog; create table #expectexception(expectexception int,expectedmessage nvarchar(max), expectedseverity int, expectedstate int, expectedmessagepattern nvarchar(max), expectederrornumber int, failmessage nvarchar(max)); exists (select 1 sys.extended_properties name = n'setfakeviewontrigger') begin raiserror('test system invalid state. setfakeviewoff must called setfakeviewon called. call setfakeviewoff creating test case procedures.', 16, 10) nowait; return -1; end; select @cmd = 'exec ' + @testname; select @testclassname = object_schema_name(object_id(@testname)), --tsqlt.private_getcleanschemaname('', @testname), @testprocname = tsqlt.private_getcleanobjectname(@testname); insert tsqlt.testresult(class, testcase, tranname, result) select @testclassname, @testprocname, @tranname, 'a severe error happened test execution. test finish.' option(maxdop 1); select @testresultid = scope_identity(); if(@verbose = 1) begin set @verbosemsg = 'tsqlt.run '''+@testname+'''; --starting'; exec tsqlt.private_print @message =@verbosemsg, @severity = 0; end; begin tran; save tran @tranname; set @preexectrancount = @@trancount; truncate table tsqlt.testmessage; declare @tmpmsg nvarchar(max); declare @testendtime datetime; set @testendtime = null; begin try (@setup null) exec @setup; exec (@cmd); set @testendtime = getdate(); if(exists(select 1 #expectexception expectexception = 1)) begin set @tmpmsg = coalesce((select failmessage #expectexception)+' ','')+'expected error raised.'; exec tsqlt.fail @tmpmsg; end end try begin catch set @testendtime = isnull(@testendtime,getdate()); error_message() like '%tsqlt.failure%' begin select @msg = msg tsqlt.testmessage; set @result = 'failure'; end else begin declare @errorinfo nvarchar(max); select @errorinfo = coalesce(error_message(), '&lt;error_message() null&gt;') + '[' +coalesce(ltrim(str(error_severity())), '&lt;error_severity() null&gt;') + ','+coalesce(ltrim(str(error_state())), '&lt;error_state() null&gt;') + ']' + '{' + coalesce(error_procedure(), '&lt;error_procedure() null&gt;') + ',' + coalesce(cast(error_line() nvarchar), '&lt;error_line() null&gt;') + '}'; if(exists(select 1 #expectexception)) begin declare @expectexception int; declare @expectedmessage nvarchar(max); declare @expectedmessagepattern nvarchar(max); declare @expectedseverity int; declare @expectedstate int; declare @expectederrornumber int; declare @failmessage nvarchar(max); select @expectexception = expectexception, @expectedmessage = expectedmessage, @expectedseverity = expectedseverity, @expectedstate = expectedstate, @expectedmessagepattern = expectedmessagepattern, @expectederrornumber = expectederrornumber, @failmessage = failmessage #expectexception; if(@expectexception = 1) begin set @result = 'success'; set @tmpmsg = coalesce(@failmessage+' ','')+'exception match expectation!'; if(error_message() &lt;&gt; @expectedmessage) begin set @tmpmsg = @tmpmsg +char(13)+char(10)+ 'expected message: &lt;'+@expectedmessage+'&gt;'+char(13)+char(10)+ 'actual message : &lt;'+error_message()+'&gt;'; set @result = 'failure'; end if(error_message() like @expectedmessagepattern) begin set @tmpmsg = @tmpmsg +char(13)+char(10)+ 'expected message like &lt;'+@expectedmessagepattern+'&gt;'+char(13)+char(10)+ 'actual message : &lt;'+error_message()+'&gt;'; set @result = 'failure'; end if(error_number() &lt;&gt; @expectederrornumber) begin set @tmpmsg = @tmpmsg +char(13)+char(10)+ 'expected error number: '+cast(@expectederrornumber nvarchar(max))+char(13)+char(10)+ 'actual error number : '+cast(error_number() nvarchar(max)); set @result = 'failure'; end if(error_severity() &lt;&gt; @expectedseverity) begin set @tmpmsg = @tmpmsg +char(13)+char(10)+ 'expected severity: '+cast(@expectedseverity nvarchar(max))+char(13)+char(10)+ 'actual severity : '+cast(error_severity() nvarchar(max)); set @result = 'failure'; end if(error_state() &lt;&gt; @expectedstate) begin set @tmpmsg = @tmpmsg +char(13)+char(10)+ 'expected state: '+cast(@expectedstate nvarchar(max))+char(13)+char(10)+ 'actual state : '+cast(error_state() nvarchar(max)); set @result = 'failure'; end if(@result = 'failure') begin set @msg = @tmpmsg; end end else begin set @result = 'failure'; set @msg = coalesce(@failmessage+' ','')+ 'expected error raised. instead error encountered:'+ char(13)+char(10)+ @errorinfo; end end else begin set @result = 'error'; set @msg = @errorinfo; end end; end catch begin try rollback tran @tranname; end try begin catch declare @postexectrancount int; set @postexectrancount = @preexectrancount - @@trancount; (@@trancount &gt; 0) rollback; begin tran; if( @result &lt;&gt; 'success' @postexectrancount &lt;&gt; 0 ) begin select @msg = coalesce(@msg, '&lt;null&gt;') + ' (there also rollback error --&gt; ' + coalesce(error_message(), '&lt;error_message() null&gt;') + '{' + coalesce(error_procedure(), '&lt;error_procedure() null&gt;') + ',' + coalesce(cast(error_line() nvarchar), '&lt;error_line() null&gt;') + '})'; set @result = 'error'; end end catch if(@result &lt;&gt; 'success') begin set @msg2 = @testname + ' failed: (' + @result + ') ' + @msg; exec tsqlt.private_print @message = @msg2, @severity = 0; end exists(select 1 tsqlt.testresult id = @testresultid) begin update tsqlt.testresult set result = @result, msg = @msg, testendtime = @testendtime id = @testresultid; end else begin insert tsqlt.testresult(class, testcase, tranname, result, msg) select @testclassname, @testprocname, '?', 'error', 'testresult entry missing; original outcome: ' + @result + ', ' + @msg; end commit; if(@verbose = 1) begin set @verbosemsg = 'tsqlt.run '''+@testname+'''; --finished'; exec tsqlt.private_print @message =@verbosemsg, @severity = 0; end; end; go create procedure tsqlt.private_runtestclass @testclassname nvarchar(max) begin declare @testcasename nvarchar(max); declare @testclassid int; set @testclassid = tsqlt.private_getschemaid(@testclassname); declare @setupprocname nvarchar(max); exec tsqlt.private_getsetupprocedurename @testclassid, @setupprocname output; declare testcases cursor local fast_forward select tsqlt.private_getquotedfullname(object_id) sys.procedures schema_id = @testclassid lower(name) like 'test%'; open testcases; fetch next testcases @testcasename; @@fetch_status = 0 begin exec tsqlt.private_runtest @testcasename, @setupprocname; fetch next testcases @testcasename; end; close testcases; deallocate testcases; end; go create procedure tsqlt.private_run @testname nvarchar(max), @testresultformatter nvarchar(max) begin set nocount on; declare @fullname nvarchar(max); declare @testclassid int; declare @istestclass bit; declare @istestcase bit; declare @isschema bit; declare @setup nvarchar(max);set @setup = null; select @testname = tsqlt.private_getlasttestnameifnotprovided(@testname); exec tsqlt.private_savetestnameforsession @testname; select @testclassid = schemaid, @fullname = quotedfullname, @istestclass = istestclass, @isschema = isschema, @istestcase = istestcase tsqlt.private_resolvename(@testname); @isschema = 1 begin exec tsqlt.private_runtestclass @fullname; end @istestcase = 1 begin declare @setupprocname nvarchar(max); exec tsqlt.private_getsetupprocedurename @testclassid, @setupprocname output; exec tsqlt.private_runtest @fullname, @setupprocname; end; exec tsqlt.private_outputtestresults @testresultformatter; end; go create procedure tsqlt.private_runcursor @testresultformatter nvarchar(max), @getcursorcallback nvarchar(max) begin set nocount on; declare @testclassname nvarchar(max); declare @testprocname nvarchar(max); declare @testclasscursor cursor; exec @getcursorcallback @testclasscursor = @testclasscursor out; ---- while(1=1) begin fetch next @testclasscursor @testclassname; if(@@fetch_status&lt;&gt;0)break; exec tsqlt.private_runtestclass @testclassname; end; close @testclasscursor; deallocate @testclasscursor; exec tsqlt.private_outputtestresults @testresultformatter; end; go create procedure tsqlt.private_getcursorforrunall @testclasscursor cursor varying output begin set @testclasscursor = cursor local fast_forward select name tsqlt.testclasses; open @testclasscursor; end; go create procedure tsqlt.private_runall @testresultformatter nvarchar(max) begin exec tsqlt.private_runcursor @testresultformatter = @testresultformatter, @getcursorcallback = 'tsqlt.private_getcursorforrunall'; end; go create procedure tsqlt.private_getcursorforrunnew @testclasscursor cursor varying output begin set @testclasscursor = cursor local fast_forward select tc.name tsqlt.testclasses tc join tsqlt.private_newtestclasslist pntcl pntcl.classname = tc.name; open @testclasscursor; end; go create procedure tsqlt.private_runnew @testresultformatter nvarchar(max) begin exec tsqlt.private_runcursor @testresultformatter = @testresultformatter, @getcursorcallback = 'tsqlt.private_getcursorforrunnew'; end; go create procedure tsqlt.private_runmethodhandler @runmethod nvarchar(max), @testresultformatter nvarchar(max) = null, @testname nvarchar(max) = null begin select @testresultformatter = isnull(@testresultformatter,tsqlt.gettestresultformatter()); exec tsqlt.private_init; if(@@error = 0) begin if(exists(select * sys.parameters p p.object_id = object_id(@runmethod) name = '@testname')) begin exec @runmethod @testname = @testname, @testresultformatter = @testresultformatter; end; else begin exec @runmethod @testresultformatter = @testresultformatter; end; end; end; go -------------------------------------------------------------------------------- go create procedure tsqlt.runall begin exec tsqlt.private_runmethodhandler @runmethod = 'tsqlt.private_runall'; end; go create procedure tsqlt.runnew begin exec tsqlt.private_runmethodhandler @runmethod = 'tsqlt.private_runnew'; end; go create procedure tsqlt.runtest @testname nvarchar(max) begin raiserror('tsqlt.runtest retired. please use tsqlt.run instead.', 16, 10); end; go create procedure tsqlt.run @testname nvarchar(max) = null, @testresultformatter nvarchar(max) = null begin exec tsqlt.private_runmethodhandler @runmethod = 'tsqlt.private_run', @testresultformatter = @testresultformatter, @testname = @testname; end; go create procedure tsqlt.private_inputbuffer @inputbuffer nvarchar(max) output begin create table #inputbuffer(eventtype sysname, parameters smallint, eventinfo nvarchar(max)); insert #inputbuffer exec('dbcc inputbuffer(@@spid) no_infomsgs;'); select @inputbuffer = i.eventinfo #inputbuffer i; end; go create procedure tsqlt.runc begin declare @testname nvarchar(max);set @testname = null; declare @inputbuffer nvarchar(max); exec tsqlt.private_inputbuffer @inputbuffer = @inputbuffer out; if(@inputbuffer like 'exec tsqlt.runc;--%') begin set @testname = ltrim(rtrim(stuff(@inputbuffer,1,18,''))); end; exec tsqlt.run @testname = @testname; end; go create procedure tsqlt.runwithxmlresults @testname nvarchar(max) = null begin exec tsqlt.run @testname = @testname, @testresultformatter = 'tsqlt.xmlresultformatter'; end; go create procedure tsqlt.runwithnullresults @testname nvarchar(max) = null begin exec tsqlt.run @testname = @testname, @testresultformatter = 'tsqlt.nulltestresultformatter'; end; go create procedure tsqlt.defaultresultformatter begin declare @msg1 nvarchar(max); declare @msg2 nvarchar(max); declare @msg3 nvarchar(max); declare @msg4 nvarchar(max); declare @issuccess int; declare @successcnt int; declare @severity int; select row_number() over(order result desc, name asc) no,name [test case name], right(space(7)+cast(datediff(millisecond,teststarttime,testendtime) varchar(7)),7) [dur(ms)], result #testresultoutput tsqlt.testresult; exec tsqlt.tabletotext @msg1 output, '#testresultoutput', 'no'; select @msg3 = msg, @issuccess = 1 - sign(failcnt + errorcnt), @successcnt = successcnt tsqlt.testcasesummary(); select @severity = 16*(1-@issuccess); select @msg2 = replicate('-',len(@msg3)), @msg4 = char(13)+char(10); exec tsqlt.private_print @msg4,0; exec tsqlt.private_print '+----------------------+',0; exec tsqlt.private_print '|test execution summary|',0; exec tsqlt.private_print '+----------------------+',0; exec tsqlt.private_print @msg4,0; exec tsqlt.private_print @msg1,0; exec tsqlt.private_print @msg2,0; exec tsqlt.private_print @msg3, @severity; exec tsqlt.private_print @msg2,0; end; go create procedure tsqlt.xmlresultformatter begin declare @xmloutput xml; select @xmloutput = ( select *--tag, parent, [testsuites!1!hide!hide], [testsuite!2!name], [testsuite!2!tests], [testsuite!2!errors], [testsuite!2!failures], [testsuite!2!timestamp], [testsuite!2!time], [testcase!3!classname], [testcase!3!name], [testcase!3!time], [failure!4!message] ( select 1 tag, null parent, 'root' [testsuites!1!hide!hide], null [testsuite!2!id], null [testsuite!2!name], null [testsuite!2!tests], null [testsuite!2!errors], null [testsuite!2!failures], null [testsuite!2!timestamp], null [testsuite!2!time], null [testsuite!2!hostname], null [testsuite!2!package], null [properties!3!hide!hide], null [testcase!4!classname], null [testcase!4!name], null [testcase!4!time], null [failure!5!message], null [failure!5!type], null [error!6!message], null [error!6!type], null [system-out!7!hide], null [system-err!8!hide] union select 2 tag, 1 parent, 'root', row_number()over(order class), class, count(1), sum(case result 'error' 1 else 0 end), sum(case result 'failure' 1 else 0 end), convert(varchar(19),min(testresult.teststarttime),126), cast(cast(datediff(millisecond,min(testresult.teststarttime),max(testresult.testendtime))/1000.0 numeric(20,3))as varchar(max)), cast(serverproperty('servername') nvarchar(max)), 'tsqlt', null, null, null, null, null, null, null, null, null, null tsqlt.testresult group class union select 3 tag, 2 parent, 'root', null, class, null, null, null, null, null, null, null, null, class, null, null, null, null, null, null, null, null tsqlt.testresult group class union select 4 tag, 2 parent, 'root', null, class, null, null, null, null, null, null, null, null, class, testcase, cast(cast(datediff(millisecond,testresult.teststarttime,testresult.testendtime)/1000.0 numeric(20,3))as varchar(max)), null, null, null, null, null, null tsqlt.testresult union select 5 tag, 4 parent, 'root', null, class, null, null, null, null, null, null, null, null, class, testcase, cast(cast(datediff(millisecond,testresult.teststarttime,testresult.testendtime)/1000.0 numeric(20,3))as varchar(max)), msg, 'tsqlt.fail', null, null, null, null tsqlt.testresult result ('failure') union select 6 tag, 4 parent, 'root', null, class, null, null, null, null, null, null, null, null, class, testcase, cast(cast(datediff(millisecond,testresult.teststarttime,testresult.testendtime)/1000.0 numeric(20,3))as varchar(max)), null, null, msg, 'sql error', null, null tsqlt.testresult result ( 'error') union select 7 tag, 2 parent, 'root', null, class, null, null, null, null, null, null, null, null, class, null, null, null, null, null, null, null, null tsqlt.testresult group class union select 8 tag, 2 parent, 'root', null, class, null, null, null, null, null, null, null, null, class, null, null, null, null, null, null, null, null tsqlt.testresult group class ) x order [testsuite!2!name],case tag (7,8) 1 else 0 end, [testcase!4!name], tag xml explicit ); exec tsqlt.private_printxml @xmloutput; end; go create procedure tsqlt.nulltestresultformatter begin return 0; end; go create procedure tsqlt.runtestclass @testclassname nvarchar(max) begin exec tsqlt.run @testclassname; end go --build- go create procedure tsqlt.expectexception @expectedmessage nvarchar(max) = null, @expectedseverity int = null, @expectedstate int = null, @message nvarchar(max) = null, @expectedmessagepattern nvarchar(max) = null, @expectederrornumber int = null begin if(exists(select 1 #expectexception expectexception = 1)) begin delete #expectexception; raiserror('each test contain one call tsqlt.expectexception.',16,10); end; insert #expectexception(expectexception, expectedmessage, expectedseverity, expectedstate, expectedmessagepattern, expectederrornumber, failmessage) values(1, @expectedmessage, @expectedseverity, @expectedstate, @expectedmessagepattern, @expectederrornumber, @message); end; go create procedure tsqlt.expectnoexception @message nvarchar(max) = null begin if(exists(select 1 #expectexception expectexception = 0)) begin delete #expectexception; raiserror('each test contain one call tsqlt.expectnoexception.',16,10); end; if(exists(select 1 #expectexception expectexception = 1)) begin delete #expectexception; raiserror('tsqlt.expectnoexception cannot follow tsqlt.expectexception inside single test.',16,10); end; insert #expectexception(expectexception, failmessage) values(0, @message); end; go go create function tsqlt.private_sqlversion() returns table return select cast(serverproperty('productversion')as nvarchar(128)) productversion, cast(serverproperty('edition')as nvarchar(128)) edition; go go create function tsqlt.info() returns table return select version = '1.0.5873.27393', clrversion = (select tsqlt.private::info()), clrsigningkey = (select tsqlt.private::signingkey()), v.sqlversion, v.sqlbuild, v.sqledition ( select cast(vi.major+'.'+vi.minor numeric(10,2)) sqlversion, cast(vi.build+'.'+vi.revision numeric(10,2)) sqlbuild, sqledition ( select parsename(psv.productversion,4) major, parsename(psv.productversion,3) minor, parsename(psv.productversion,2) build, parsename(psv.productversion,1) revision, edition sqledition tsqlt.private_sqlversion() psv )vi )v; go if((select sqlversion tsqlt.info())&gt;9) begin exec('create view tsqlt.private_sysindexes select * sys.indexes;'); end else begin exec('create view tsqlt.private_sysindexes select *,0 has_filter,'''' filter_definition sys.indexes;'); end; go go create function tsqlt.private_scriptindex ( @object_id int, @index_id int ) returns table return select i.index_id, i.name index_name, i.is_primary_key, i.is_unique, i.is_disabled, 'create ' + case i.is_unique = 1 'unique ' else '' end + case i.type 1 'clustered' 2 'nonclustered' 5 'clustered columnstore' 6 'nonclustered columnstore' else '{index type supported!}' end + ' index ' + quotename(i.name)+ ' ' + quotename(object_schema_name(@object_id)) + '.' + quotename(object_name(@object_id)) + case i.type (5) '('+ cl.column_list + ')' else '' end + case i.has_filter = 1 'where' + i.filter_definition else '' end + case i.is_hypothetical = 1 'with(statistics_only = -1)' else '' end + ';' create_cmd tsqlt.private_sysindexes cross apply ( select ( select case oic.rn &gt; 1 ',' else '' end + case oic.rn = 1 oic.is_included_column = 1 i.type (6) ')include(' else '' end + quotename(oic.name) + case oic.is_included_column = 0 case oic.is_descending_key = 1 'desc' else 'asc' end else '' end ( select c.name, ic.is_descending_key, ic.key_ordinal, ic.is_included_column, row_number()over(partition ic.is_included_column order ic.key_ordinal, ic.index_column_id) rn sys.index_columns ic join sys.columns c ic.column_id = c.column_id ic.object_id = c.object_id ic.object_id = i.object_id ic.index_id = i.index_id )oic order oic.is_included_column, oic.rn xml path(''),type ).value('.','nvarchar(max)') column_list )cl i.object_id = @object_id i.index_id = isnull(@index_id,i.index_id); go go go create procedure tsqlt.private_removeschemabinding @object_id int begin declare @cmd nvarchar(max); select @cmd = tsqlt.[private]::getalterstatementwithoutschemabinding(sm.definition) sys.sql_modules sm sm.object_id = @object_id; exec(@cmd); end; go go go create procedure tsqlt.private_removeschemaboundreferences @object_id int begin declare @cmd nvarchar(max); select @cmd = ( select 'exec tsqlt.private_removeschemaboundreferences @object_id = '+str(sed.referencing_id)+';'+ 'exec tsqlt.private_removeschemabinding @object_id = '+str(sed.referencing_id)+';' ( select distinct sedi.referencing_id,sedi.referenced_id sys.sql_expression_dependencies sedi sedi.is_schema_bound_reference = 1 ) sed sed.referenced_id = @object_id xml path(''),type ).value('.','nvarchar(max)'); exec(@cmd); end; go go go create function tsqlt.private_getforeignkeyparcolumns( @constraintobjectid int ) returns table return select stuff(( select ','+quotename(pci.name) sys.foreign_key_columns c join sys.columns pci pci.object_id = c.parent_object_id pci.column_id = c.parent_column_id @constraintobjectid = c.constraint_object_id xml path(''),type ).value('.','nvarchar(max)'),1,1,'') colnames go create function tsqlt.private_getforeignkeyrefcolumns( @constraintobjectid int ) returns table return select stuff(( select ','+quotename(rci.name) sys.foreign_key_columns c join sys.columns rci rci.object_id = c.referenced_object_id rci.column_id = c.referenced_column_id @constraintobjectid = c.constraint_object_id xml path(''),type ).value('.','nvarchar(max)'),1,1,'') colnames; go create function tsqlt.private_getforeignkeydefinition( @schemaname nvarchar(max), @parenttablename nvarchar(max), @foreignkeyname nvarchar(max), @nocascade bit ) returns table return select 'constraint ' + name + ' foreign key (' + parcols + ') references ' + refname + '(' + refcols + ')'+ case @nocascade = 1 '' else delete_referential_action_cmd + ' ' + update_referential_action_cmd end cmd, case reftableisfakedind = 1 'create unique index ' + tsqlt.private::createuniqueobjectname() + ' ' + refname + '(' + refcols + ');' else '' end creidxcmd (select quotename(schema_name(k.schema_id)) schemaname, quotename(k.name) name, quotename(object_name(k.parent_object_id)) parname, quotename(schema_name(reftab.schema_id)) + '.' + quotename(reftab.name) refname, parcol.colnames parcols, refcol.colnames refcols, 'on update '+ case k.update_referential_action 0 'no action' 1 'cascade' 2 'set null' 3 'set default' end update_referential_action_cmd, 'on delete '+ case k.delete_referential_action 0 'no action' 1 'cascade' 2 'set null' 3 'set default' end delete_referential_action_cmd, case e.name null 0 else 1 end reftableisfakedind sys.foreign_keys k cross apply tsqlt.private_getforeignkeyparcolumns(k.object_id) parcol cross apply tsqlt.private_getforeignkeyrefcolumns(k.object_id) refcol left join sys.extended_properties e e.name = 'tsqlt.faketable_orgtablename' e.value = object_name(k.referenced_object_id) join sys.tables reftab coalesce(e.major_id,k.referenced_object_id) = reftab.object_id k.parent_object_id = object_id(@schemaname + '.' + @parenttablename) k.object_id = object_id(@schemaname + '.' + @foreignkeyname) )x; go go go create function tsqlt.private_getquotedtablenameforconstraint(@constraintobjectid int) returns table return select quotename(schema_name(newtbl.schema_id)) + '.' + quotename(object_name(newtbl.object_id)) quotedtablename, schema_name(newtbl.schema_id) schemaname, object_name(newtbl.object_id) tablename, object_name(constraints.parent_object_id) orgtablename sys.objects constraints join sys.extended_properties p join sys.objects newtbl newtbl.object_id = p.major_id p.minor_id = 0 p.class_desc = 'object_or_column' p.name = 'tsqlt.faketable_orgtablename' object_name(constraints.parent_object_id) = cast(p.value nvarchar(4000)) constraints.schema_id = newtbl.schema_id constraints.object_id = @constraintobjectid; go create function tsqlt.private_findconstraint ( @tableobjectid int, @constraintname nvarchar(max) ) returns table return select top(1) constraints.object_id constraintobjectid, type_desc constrainttype sys.objects constraints cross join tsqlt.private_getoriginaltableinfo(@tableobjectid) orgtbl @constraintname (constraints.name, quotename(constraints.name)) constraints.parent_object_id = orgtbl.orgtableobjectid order len(constraints.name) asc; go create function tsqlt.private_resolveapplyconstraintparameters ( @a nvarchar(max), @b nvarchar(max), @c nvarchar(max) ) returns table return select constraintobjectid, constrainttype tsqlt.private_findconstraint(object_id(@a), @b) @c null union select * tsqlt.private_findconstraint(object_id(@a + '.' + @b), @c) union select * tsqlt.private_findconstraint(object_id(@c + '.' + @a), @b); go create procedure tsqlt.private_applycheckconstraint @constraintobjectid int begin declare @cmd nvarchar(max); select @cmd = 'constraint ' + quotename(name) + ' check' + definition sys.check_constraints object_id = @constraintobjectid; declare @quotedtablename nvarchar(max); select @quotedtablename = quotedtablename tsqlt.private_getquotedtablenameforconstraint(@constraintobjectid); exec tsqlt.private_renameobjecttouniquenameusingobjectid @constraintobjectid; select @cmd = 'alter table ' + @quotedtablename + ' add ' + @cmd sys.objects object_id = @constraintobjectid; exec (@cmd); end; go create procedure tsqlt.private_applyforeignkeyconstraint @constraintobjectid int, @nocascade bit begin declare @schemaname nvarchar(max); declare @orgtablename nvarchar(max); declare @tablename nvarchar(max); declare @constraintname nvarchar(max); declare @createfkcmd nvarchar(max); declare @altertablecmd nvarchar(max); declare @createindexcmd nvarchar(max); declare @finalcmd nvarchar(max); select @schemaname = schemaname, @orgtablename = orgtablename, @tablename = tablename, @constraintname = object_name(@constraintobjectid) tsqlt.private_getquotedtablenameforconstraint(@constraintobjectid); select @createfkcmd = cmd, @createindexcmd = creidxcmd tsqlt.private_getforeignkeydefinition(@schemaname, @orgtablename, @constraintname, @nocascade); select @altertablecmd = 'alter table ' + quotename(@schemaname) + '.' + quotename(@tablename) + ' add ' + @createfkcmd; select @finalcmd = @createindexcmd + @altertablecmd; exec tsqlt.private_renameobjecttouniquename @schemaname, @constraintname; exec (@finalcmd); end; go create procedure tsqlt.private_applyuniqueconstraint @constraintobjectid int begin declare @schemaname nvarchar(max); declare @orgtablename nvarchar(max); declare @tablename nvarchar(max); declare @constraintname nvarchar(max); declare @createconstraintcmd nvarchar(max); declare @altercolumnscmd nvarchar(max); select @schemaname = schemaname, @orgtablename = orgtablename, @tablename = tablename, @constraintname = object_name(@constraintobjectid) tsqlt.private_getquotedtablenameforconstraint(@constraintobjectid); select @altercolumnscmd = notnullcolumncmd, @createconstraintcmd = createconstraintcmd tsqlt.private_getuniqueconstraintdefinition(@constraintobjectid, quotename(@schemaname) + '.' + quotename(@tablename)); exec tsqlt.private_renameobjecttouniquename @schemaname, @constraintname; exec (@altercolumnscmd); exec (@createconstraintcmd); end; go create function tsqlt.private_getconstrainttype(@tableobjectid int, @constraintname nvarchar(max)) returns table return select object_id,type,type_desc sys.objects object_id = object_id(schema_name(schema_id)+'.'+@constraintname) parent_object_id = @tableobjectid; go create procedure tsqlt.applyconstraint @tablename nvarchar(max), @constraintname nvarchar(max), @schemaname nvarchar(max) = null, --parameter preserved backward compatibility. use. removed soon. @nocascade bit = 0 begin declare @constrainttype nvarchar(max); declare @constraintobjectid int; select @constrainttype = constrainttype, @constraintobjectid = constraintobjectid tsqlt.private_resolveapplyconstraintparameters (@tablename, @constraintname, @schemaname); @constrainttype = 'check_constraint' begin exec tsqlt.private_applycheckconstraint @constraintobjectid; return 0; end @constrainttype = 'foreign_key_constraint' begin exec tsqlt.private_applyforeignkeyconstraint @constraintobjectid, @nocascade; return 0; end; @constrainttype in('unique_constraint', 'primary_key_constraint') begin exec tsqlt.private_applyuniqueconstraint @constraintobjectid; return 0; end; raiserror ('applyconstraint could resolve object names, ''%s'', ''%s''. sure call applyconstraint pass two parameters, as: exec tsqlt.applyconstraint ''myschema.mytable'', ''myconstraint''', 16, 10, @tablename, @constraintname); return 0; end; go go create procedure tsqlt.private_validatefaketableparameters @schemaname nvarchar(max), @origtablename nvarchar(max), @origschemaname nvarchar(max) begin @schemaname null begin declare @fullname nvarchar(max); set @fullname = @origtablename + coalesce('.' + @origschemaname, ''); raiserror ('faketable could resolve object name, ''%s''. (when calling tsqlt.faketable, avoid use @schemaname parameter, deprecated.)', 16, 10, @fullname); end; end; go go create function tsqlt.private_getdatatypeorcomputedcolumndefinition(@usertypeid int, @maxlength int, @precision int, @scale int, @collationname nvarchar(max), @objectid int, @columnid int, @returndetails bit) returns table return select coalesce(cc.iscomputedcolumn, 0) iscomputedcolumn, coalesce(cc.computedcolumndefinition, gftn.typename) columndefinition (select @usertypeid, @maxlength, @precision, @scale, @collationname, @objectid, @columnid, @returndetails) v(usertypeid, maxlength, precision, scale, collationname, objectid, columnid, returndetails) cross apply tsqlt.private_getfulltypename(v.usertypeid, v.maxlength, v.precision, v.scale, v.collationname) gftn left join (select 1 iscomputedcolumn, ' '+ cci.definition + case cci.is_persisted = 1 ' persisted' else '' end computedcolumndefinition, cci.object_id, cci.column_id sys.computed_columns cci )cc cc.object_id = v.objectid cc.column_id = v.columnid v.returndetails = 1; go create function tsqlt.private_getidentitydefinition(@objectid int, @columnid int, @returndetails bit) returns table return select coalesce(isidentity, 0) isidentitycolumn, coalesce(identitydefinition, '') identitydefinition (select 1) x(x) left join (select 1 isidentity, ' identity(' + cast(seed_value nvarchar(max)) + ',' + cast(increment_value nvarchar(max)) + ')' identitydefinition, object_id, column_id sys.identity_columns ) id id.object_id = @objectid id.column_id = @columnid @returndetails = 1; go go create function tsqlt.private_getdefaultconstraintdefinition(@objectid int, @columnid int, @returndetails bit) returns table return select coalesce(isdefault, 0) isdefault, coalesce(defaultdefinition, '') defaultdefinition (select 1) x(x) left join (select 1 isdefault,' default '+ definition defaultdefinition,parent_object_id,parent_column_id sys.default_constraints )dc dc.parent_object_id = @objectid dc.parent_column_id = @columnid @returndetails = 1; go go create function tsqlt.private_getuniqueconstraintdefinition ( @constraintobjectid int, @quotedtablename nvarchar(max) ) returns table return select 'alter table '+ @quotedtablename + ' add constraint ' + quotename(object_name(@constraintobjectid)) + ' ' + case kc.type_desc = 'unique_constraint' 'unique' else 'primary key' end + '(' + stuff(( select ','+quotename(c.name) sys.index_columns ic join sys.columns c ic.object_id = c.object_id ic.column_id = c.column_id kc.unique_index_id = ic.index_id kc.parent_object_id = ic.object_id xml path(''),type ).value('.','nvarchar(max)'), 1, 1, '' ) + ');' createconstraintcmd, case kc.type_desc = 'unique_constraint' '' else ( select 'alter table ' + @quotedtablename + ' alter column ' + quotename(c.name)+ cc.columndefinition + ' null;' sys.index_columns ic join sys.columns c ic.object_id = c.object_id ic.column_id = c.column_id cross apply tsqlt.private_getdatatypeorcomputedcolumndefinition(c.user_type_id, c.max_length, c.precision, c.scale, c.collation_name, c.object_id, c.column_id, 0) cc kc.unique_index_id = ic.index_id kc.parent_object_id = ic.object_id xml path(''),type ).value('.','nvarchar(max)') end notnullcolumncmd sys.key_constraints kc kc.object_id = @constraintobjectid; go go create procedure tsqlt.private_createfakeoftable @schemaname nvarchar(max), @tablename nvarchar(max), @origtablefullname nvarchar(max), @identity bit, @computedcolumns bit, @defaults bit begin declare @cmd nvarchar(max); declare @cols nvarchar(max); select @cols = ( select ',' + quotename(name) + cc.columndefinition + dc.defaultdefinition + id.identitydefinition + case cc.iscomputedcolumn = 1 id.isidentitycolumn = 1 '' else ' null' end sys.columns c cross apply tsqlt.private_getdatatypeorcomputedcolumndefinition(c.user_type_id, c.max_length, c.precision, c.scale, c.collation_name, c.object_id, c.column_id, @computedcolumns) cc cross apply tsqlt.private_getdefaultconstraintdefinition(c.object_id, c.column_id, @defaults) dc cross apply tsqlt.private_getidentitydefinition(c.object_id, c.column_id, @identity) id object_id = object_id(@origtablefullname) order column_id xml path(''), type ).value('.', 'nvarchar(max)'); select @cmd = 'create table ' + @schemaname + '.' + @tablename + '(' + stuff(@cols,1,1,'') + ')'; exec (@cmd); end; go create procedure tsqlt.private_markfaketable @schemaname nvarchar(max), @tablename nvarchar(max), @newnameoforiginaltable nvarchar(4000) begin declare @unquotedschemaname nvarchar(max);set @unquotedschemaname = object_schema_name(object_id(@schemaname+'.'+@tablename)); declare @unquotedtablename nvarchar(max);set @unquotedtablename = object_name(object_id(@schemaname+'.'+@tablename)); exec sys.sp_addextendedproperty @name = n'tsqlt.faketable_orgtablename', @value = @newnameoforiginaltable, @level0type = n'schema', @level0name = @unquotedschemaname, @level1type = n'table', @level1name = @unquotedtablename; end; go create procedure tsqlt.faketable @tablename nvarchar(max), @schemaname nvarchar(max) = null, --parameter preserved backward compatibility. use. removed soon. @identity bit = null, @computedcolumns bit = null, @defaults bit = null begin declare @origschemaname nvarchar(max); declare @origtablename nvarchar(max); declare @newnameoforiginaltable nvarchar(4000); declare @origtablefullname nvarchar(max); set @origtablefullname = null; select @origschemaname = @schemaname, @origtablename = @tablename if(@origtablename (parsename(@origtablename,1),quotename(parsename(@origtablename,1))) @origschemaname null) begin raiserror('when @tablename multi-part identifier, @schemaname must null!',16,10); end select @schemaname = cleanschemaname, @tablename = cleantablename tsqlt.private_resolvefaketablenamesforbackwardcompatibility(@tablename, @schemaname); exec tsqlt.private_validatefaketableparameters @schemaname,@origtablename,@origschemaname; exec tsqlt.private_renameobjecttouniquename @schemaname, @tablename, @newnameoforiginaltable output; select @origtablefullname = s.base_object_name sys.synonyms s.object_id = object_id(@schemaname + '.' + @newnameoforiginaltable); if(@origtablefullname null) begin if(coalesce(object_id(@origtablefullname,'u'),object_id(@origtablefullname,'v')) null) begin raiserror('cannot fake synonym %s.%s pointing %s, table view!',16,10,@schemaname,@tablename,@origtablefullname); end; end; else begin set @origtablefullname = @schemaname + '.' + @newnameoforiginaltable; end; exec tsqlt.private_createfakeoftable @schemaname, @tablename, @origtablefullname, @identity, @computedcolumns, @defaults; exec tsqlt.private_markfaketable @schemaname, @tablename, @newnameoforiginaltable; end go create procedure tsqlt.private_createprocedurespy @procedureobjectid int, @originalprocedurename nvarchar(max), @logtablename nvarchar(max), @commandtoexecute nvarchar(max) = null begin declare @cmd nvarchar(max); declare @procparmlist nvarchar(max), @tablecollist nvarchar(max), @procparmtypelist nvarchar(max), @tablecoltypelist nvarchar(max); declare @seperator char(1), @procparmtypelistseparater char(1), @paramname sysname, @typename sysname, @isoutput bit, @iscursorref bit, @istabletype bit; select @seperator = '', @procparmtypelistseparater = '', @procparmlist = '', @tablecollist = '', @procparmtypelist = '', @tablecoltypelist = ''; declare parameters cursor select p.name, t.typename, p.is_output, p.is_cursor_ref, t.istabletype sys.parameters p cross apply tsqlt.private_getfulltypename(p.user_type_id,p.max_length,p.precision,p.scale,null) object_id = @procedureobjectid; open parameters; fetch next parameters @paramname, @typename, @isoutput, @iscursorref, @istabletype; (@@fetch_status = 0) begin @iscursorref = 0 begin select @procparmlist = @procparmlist + @seperator + case @istabletype = 1 '(select * '+@paramname+' xml path(''row''),type,root('''+stuff(@paramname,1,1,'')+'''))' else @paramname end, @tablecollist = @tablecollist + @seperator + '[' + stuff(@paramname,1,1,'') + ']', @procparmtypelist = @procparmtypelist + @procparmtypelistseparater + @paramname + ' ' + @typename + case @istabletype = 1 ' readonly' else ' = null ' end+ case @isoutput = 1 ' out' else '' end, @tablecoltypelist = @tablecoltypelist + ',[' + stuff(@paramname,1,1,'') + '] ' + case @istabletype = 1 'xml' @typename like '%nchar%' @typename like '%nvarchar%' 'nvarchar(max)' @typename like '%char%' 'varchar(max)' else @typename end + ' null'; select @seperator = ','; select @procparmtypelistseparater = ','; end else begin select @procparmtypelist = @procparmtypelistseparater + @paramname + ' cursor varying output'; select @procparmtypelistseparater = ','; end; fetch next parameters @paramname, @typename, @isoutput, @iscursorref, @istabletype; end; close parameters; deallocate parameters; declare @insertstmt nvarchar(max); select @insertstmt = 'insert ' + @logtablename + case @tablecollist = '' ' default values' else ' (' + @tablecollist + ') select ' + @procparmlist end + ';'; select @cmd = 'create table ' + @logtablename + ' (_id_ int identity(1,1) primary key clustered ' + @tablecoltypelist + ');'; exec(@cmd); select @cmd = 'create procedure ' + @originalprocedurename + ' ' + @procparmtypelist + ' begin ' + @insertstmt + isnull(@commandtoexecute, '') + ';' + ' end;'; exec(@cmd); return 0; end; go create procedure tsqlt.spyprocedure @procedurename nvarchar(max), @commandtoexecute nvarchar(max) = null begin declare @procedureobjectid int; select @procedureobjectid = object_id(@procedurename); exec tsqlt.private_validateprocedurecanbeusedwithspyprocedure @procedurename; declare @logtablename nvarchar(max); select @logtablename = quotename(object_schema_name(@procedureobjectid)) + '.' + quotename(object_name(@procedureobjectid)+'_spyprocedurelog'); exec tsqlt.private_renameobjecttouniquenameusingobjectid @procedureobjectid; exec tsqlt.private_createprocedurespy @procedureobjectid, @procedurename, @logtablename, @commandtoexecute; return 0; end; go go create function tsqlt.private_getcommaseparatedcolumnlist (@table nvarchar(max), @excludecolumn nvarchar(max)) returns nvarchar(max) begin return stuff(( select ',' + case system_type_id = type_id('timestamp') ';timestamp columns unsupported!;' else quotename(name) end sys.columns object_id = object_id(@table) name &lt;&gt; @excludecolumn order column_id xml path(''), type).value('.','nvarchar(max)') ,1, 1, ''); end; go go go create procedure tsqlt.private_createresulttableforcomparetables @resulttable nvarchar(max), @resultcolumn nvarchar(max), @basetable nvarchar(max) begin declare @cmd nvarchar(max); set @cmd = ' select ''='' ' + @resultcolumn + ', expected.* ' + @resulttable + ' tsqlt.private_nullcelltable n left join ' + @basetable + ' expected n.i &lt;&gt; n.i truncate table ' + @resulttable + ';' --need insert actual row prevent identity property transfering (identity_col can't nullable); exec(@cmd); end go go go create procedure tsqlt.private_validatethatalldatatypesintablearesupported @resulttable nvarchar(max), @columnlist nvarchar(max) begin begin try exec('declare @eatresult int; select @eatresult = count(1) ' + @resulttable + ' group ' + @columnlist + ';'); end try begin catch raiserror('the table contains datatype supported tsqlt.assertequalstable. please refer http://tsqlt.org/user-guide/assertions/assertequalstable/ list unsupported datatypes.',16,10); end catch end; go go go create procedure tsqlt.private_comparetablesfailifunequalrowsexists @unequalrowsexist int, @resulttable nvarchar(max), @resultcolumn nvarchar(max), @columnlist nvarchar(max), @failmsg nvarchar(max) begin @unequalrowsexist &gt; 0 begin declare @tabletotextresult nvarchar(max); declare @outputcolumnlist nvarchar(max); select @outputcolumnlist = '[_m_],' + @columnlist; exec tsqlt.tabletotext @tablename = @resulttable, @orderby = @resultcolumn, @printonlycolumnnamealiaslist = @outputcolumnlist, @txt = @tabletotextresult output; declare @message nvarchar(max); select @message = @failmsg + char(13) + char(10); exec tsqlt.fail @message, @tabletotextresult; end; end go go go create procedure tsqlt.private_comparetables @expected nvarchar(max), @actual nvarchar(max), @resulttable nvarchar(max), @columnlist nvarchar(max), @matchindicatorcolumnname nvarchar(max) begin declare @cmd nvarchar(max); declare @restoredrowindexcountercolname nvarchar(max); set @restoredrowindexcountercolname = @matchindicatorcolumnname + '_rr'; select @cmd = ' insert ' + @resulttable + ' (' + @matchindicatorcolumnname + ', ' + @columnlist + ') select case restoredrowindex.'+@restoredrowindexcountercolname+' &lt;= case [_{left}_]&lt;[_{right}_] [_{left}_] else [_{right}_] end ''='' restoredrowindex.'+@restoredrowindexcountercolname+' &lt;= [_{left}_] ''&lt;'' else ''&gt;'' end ' + @matchindicatorcolumnname + ', ' + @columnlist + ' from( select sum([_{left}_]) [_{left}_], sum([_{right}_]) [_{right}_], ' + @columnlist + ' ( select 1 [_{left}_], 0[_{right}_], ' + @columnlist + ' ' + @expected + ' union select 0[_{left}_], 1 [_{right}_], ' + @columnlist + ' ' + @actual + ' ) x group ' + @columnlist + ' ) collapsedrows cross apply ( select top(case [_{left}_]&gt;[_{right}_] [_{left}_] else [_{right}_] end) row_number() over(order by(select 1)) (select 1 ' + @actual + ' union select 1 ' + @expected + ') x(x) ) restoredrowindex(' + @restoredrowindexcountercolname + ');'; exec (@cmd); --maingroupquery set @cmd = 'set @r = case exists( select 1 ' + @resulttable + ' ' + @matchindicatorcolumnname + ' (''&lt;'', ''&gt;'')) 1 else 0 end'; declare @unequalrowsexist int; exec sp_executesql @cmd, n'@r int output',@unequalrowsexist output; return @unequalrowsexist; end; go go create table tsqlt.private_nullcelltable( int ); go insert tsqlt.private_nullcelltable (i) values (null); go create trigger tsqlt.private_nullcelltable_stopdeletes tsqlt.private_nullcelltable instead delete, insert, update begin return; end; go go create procedure tsqlt.assertobjectexists @objectname nvarchar(max), @message nvarchar(max) = '' begin declare @msg nvarchar(max); if(@objectname like '#%') begin object_id('tempdb..'+@objectname) null begin select @msg = '''' + coalesce(@objectname, 'null') + ''' exist'; exec tsqlt.fail @message, @msg; return 1; end; end else begin object_id(@objectname) null begin select @msg = '''' + coalesce(@objectname, 'null') + ''' exist'; exec tsqlt.fail @message, @msg; return 1; end; end; return 0; end; go create procedure tsqlt.assertobjectdoesnotexist @objectname nvarchar(max), @message nvarchar(max) = '' begin declare @msg nvarchar(max); object_id(@objectname) null or(@objectname like '#%' object_id('tempdb..'+@objectname) null) begin select @msg = '''' + @objectname + ''' exist!'; exec tsqlt.fail @message,@msg; end; end; go go create procedure tsqlt.assertequalsstring @expected nvarchar(max), @actual nvarchar(max), @message nvarchar(max) = '' begin ((@expected = @actual) (@actual null @expected null)) return 0; declare @msg nvarchar(max); select @msg = char(13)+char(10)+ 'expected: ' + isnull('&lt;'+@expected+'&gt;', 'null') + char(13)+char(10)+ 'but : ' + isnull('&lt;'+@actual+'&gt;', 'null'); exec tsqlt.fail @message, @msg; end; go go create procedure tsqlt.assertequalstable @expected nvarchar(max), @actual nvarchar(max), @message nvarchar(max) = null, @failmsg nvarchar(max) = 'unexpected/missing resultset rows!' begin exec tsqlt.assertobjectexists @expected; exec tsqlt.assertobjectexists @actual; declare @resulttable nvarchar(max); declare @resultcolumn nvarchar(max); declare @columnlist nvarchar(max); declare @unequalrowsexist int; declare @combinedmessage nvarchar(max); select @resulttable = tsqlt.private::createuniqueobjectname(); select @resultcolumn = 'rc_' + @resulttable; exec tsqlt.private_createresulttableforcomparetables @resulttable = @resulttable, @resultcolumn = @resultcolumn, @basetable = @expected; select @columnlist = tsqlt.private_getcommaseparatedcolumnlist(@resulttable, @resultcolumn); exec tsqlt.private_validatethatalldatatypesintablearesupported @resulttable, @columnlist; exec @unequalrowsexist = tsqlt.private_comparetables @expected = @expected, @actual = @actual, @resulttable = @resulttable, @columnlist = @columnlist, @matchindicatorcolumnname = @resultcolumn; set @combinedmessage = isnull(@message + char(13) + char(10),'') + @failmsg; exec tsqlt.private_comparetablesfailifunequalrowsexists @unequalrowsexist = @unequalrowsexist, @resulttable = @resulttable, @resultcolumn = @resultcolumn, @columnlist = @columnlist, @failmsg = @combinedmessage; end; go go create procedure tsqlt.stubrecord(@sntablename nvarchar(max), @bintobjid bigint) begin raiserror('warning, tsqlt.stubrecord currently supported. use risk!', 0, 1) nowait; declare @vcinsertstmt nvarchar(max), @vcinsertvalues nvarchar(max); declare @sncolumnname nvarchar(max); declare @sintdatatype smallint; declare @nvcfkcmd nvarchar(max); declare @vcfkval nvarchar(max); set @vcinsertstmt = 'insert ' + @sntablename + ' (' declare curcolumns cursor local fast_forward select syscolumns.name, syscolumns.xtype, cmd.cmd syscolumns left outer join dbo.sysconstraints syscolumns.id = sysconstraints.id syscolumns.colid = sysconstraints.colid sysconstraints.status = 1 -- primary key constraints left outer join (select fkeyid id,fkey colid,n'select @v=cast(min('+syscolumns.name+') nvarchar) '+sysobjects.name cmd sysforeignkeys join sysobjects sysobjects.id=sysforeignkeys.rkeyid join syscolumns sysobjects.id=syscolumns.id syscolumns.colid=rkey) cmd cmd.id=syscolumns.id cmd.colid=syscolumns.colid syscolumns.id = object_id(@sntablename) (syscolumns.isnullable = 0 ) order isnull(sysconstraints.status, 9999), -- order primary key constraints first syscolumns.colorder open curcolumns fetch next curcolumns @sncolumnname, @sintdatatype, @nvcfkcmd -- treat first column retrieved differently, commas need added -- objid column @@fetch_status = 0 begin set @vcinsertstmt = @vcinsertstmt + @sncolumnname select @vcinsertvalues = ')values(' + isnull(cast(@bintobjid nvarchar), 'null') fetch next curcolumns @sncolumnname, @sintdatatype, @nvcfkcmd end else begin -- columns retrieved, need insert first column select @vcinsertstmt = @vcinsertstmt + syscolumns.name syscolumns syscolumns.id = object_id(@sntablename) syscolumns.colorder = 1 select @vcinsertvalues = ')values(' + isnull(cast(@bintobjid nvarchar), 'null') end @@fetch_status = 0 begin set @vcinsertstmt = @vcinsertstmt + ',' + @sncolumnname set @vcfkval=',0' @nvcfkcmd null begin set @vcfkval=null exec sp_executesql @nvcfkcmd,n'@v nvarchar(max) output',@vcfkval output set @vcfkval=isnull(','''+@vcfkval+'''',',null') end set @vcinsertvalues = @vcinsertvalues + @vcfkval fetch next curcolumns @sncolumnname, @sintdatatype, @nvcfkcmd end close curcolumns deallocate curcolumns set @vcinsertstmt = @vcinsertstmt + @vcinsertvalues + ')' exists (select 1 syscolumns status = 128 id = object_id(@sntablename)) begin set @vcinsertstmt = 'set identity_insert ' + @sntablename + ' ' + char(10) + @vcinsertstmt + char(10) + 'set identity_insert ' + @sntablename + ' ' end exec (@vcinsertstmt) -- execute actual insert statement end go go go create procedure [tsqlt].[assertlike] @expectedpattern nvarchar(max), @actual nvarchar(max), @message nvarchar(max) = '' begin (len(@expectedpattern) &gt; 4000) begin raiserror ('@expectedpattern may exceed 4000 characters.', 16, 10); end; ((@actual like @expectedpattern) (@actual null @expectedpattern null)) begin return 0; end declare @msg nvarchar(max); select @msg = char(13) + char(10) + 'expected: &lt;' + isnull(@expectedpattern, 'null') + '&gt;' + char(13) + char(10) + ' was: &lt;' + isnull(@actual, 'null') + '&gt;'; exec tsqlt.fail @message, @msg; end; go go create procedure tsqlt.assertnotequals @expected sql_variant, @actual sql_variant, @message nvarchar(max) = '' begin (@expected = @actual) (@expected null @actual null) begin declare @msg nvarchar(max); set @msg = 'expected actual value ' + coalesce('equal &lt;' + tsqlt.private_sqlvariantformatter(@expected)+'&gt;', 'be null') + '.'; exec tsqlt.fail @message,@msg; end; return 0; end; go create function tsqlt.private_sqlvariantformatter(@value sql_variant) returns nvarchar(max) begin return case upper(cast(sql_variant_property(@value,'basetype')as sysname)) 'float' convert(nvarchar(max),@value,2) 'real' convert(nvarchar(max),@value,1) 'money' convert(nvarchar(max),@value,2) 'smallmoney' convert(nvarchar(max),@value,2) 'date' convert(nvarchar(max),@value,126) 'datetime' convert(nvarchar(max),@value,126) 'datetime2' convert(nvarchar(max),@value,126) 'datetimeoffset' convert(nvarchar(max),@value,126) 'smalldatetime' convert(nvarchar(max),@value,126) 'time' convert(nvarchar(max),@value,126) 'binary' convert(nvarchar(max),@value,1) 'varbinary' convert(nvarchar(max),@value,1) else cast(@value nvarchar(max)) end; end go create procedure tsqlt.assertemptytable @tablename nvarchar(max), @message nvarchar(max) = '' begin exec tsqlt.assertobjectexists @tablename; declare @fullname nvarchar(max); if(object_id(@tablename) null object_id('tempdb..'+@tablename) null) begin set @fullname = case left(@tablename,1) = '[' @tablename else quotename(@tablename)end; end; else begin set @fullname = tsqlt.private_getquotedfullname(object_id(@tablename)); end; declare @cmd nvarchar(max); declare @exists int; set @cmd = 'select @exists = case exists(select 1 '+@fullname+') 1 else 0 end;' exec sp_executesql @cmd,n'@exists int output', @exists output; if(@exists = 1) begin declare @tabletotext nvarchar(max); exec tsqlt.tabletotext @tablename = @fullname,@txt = @tabletotext output; declare @msg nvarchar(max); set @msg = @fullname + ' empty:' + char(13) + char(10)+ @tabletotext; exec tsqlt.fail @message,@msg; end end go create procedure tsqlt.applytrigger @tablename nvarchar(max), @triggername nvarchar(max) begin declare @orgtableobjectid int; select @orgtableobjectid = orgtableobjectid tsqlt.private_getoriginaltableinfo(object_id(@tablename)) orgtbl if(@orgtableobjectid null) begin raiserror('%s exist faked tsqlt.faketable.', 16, 10, @tablename); end; declare @fulltriggername nvarchar(max); declare @triggerobjectid int; select @fulltriggername = quotename(schema_name(schema_id))+'.'+quotename(name), @triggerobjectid = object_id sys.objects parsename(@triggername,1) = name parent_object_id = @orgtableobjectid; declare @triggercode nvarchar(max); select @triggercode = m.definition sys.sql_modules m.object_id = @triggerobjectid; (@triggercode null) begin raiserror('%s trigger %s', 16, 10, @triggername, @tablename); end; exec tsqlt.removeobject @fulltriggername; exec(@triggercode); end; go go create procedure tsqlt.private_validateobjectscompatiblewithfakefunction @functionname nvarchar(max), @fakefunctionname nvarchar(max), @functionobjectid int output, @fakefunctionobjectid int output, @isscalarfunction bit output begin set @functionobjectid = object_id(@functionname); set @fakefunctionobjectid = object_id(@fakefunctionname); if(@functionobjectid null) begin raiserror('%s exist!',16,10,@functionname); end; if(@fakefunctionobjectid null) begin raiserror('%s exist!',16,10,@fakefunctionname); end; declare @functiontype char(2); declare @fakefunctiontype char(2); select @functiontype = type sys.objects object_id = @functionobjectid; select @fakefunctiontype = type sys.objects object_id = @fakefunctionobjectid; if((@functiontype in('fn','fs') @fakefunctiontype in('fn','fs')) (@functiontype in('tf','if','ft') @fakefunctiontype in('tf','if','ft')) (@functiontype in('fn','fs','tf','if','ft')) ) begin raiserror('both parameters must contain name either scalar table valued functions!',16,10); end; set @isscalarfunction = case @functiontype in('fn','fs') 1 else 0 end; if(exists(select 1 sys.parameters p p.object_id in(@functionobjectid,@fakefunctionobjectid) group p.name, p.max_length, p.precision, p.scale, p.parameter_id count(1) &lt;&gt; 2 )) begin raiserror('parameters functions must match! (this includes return type scalar functions.)',16,10); end; end; go go go create procedure tsqlt.private_createfakefunction @functionname nvarchar(max), @fakefunctionname nvarchar(max), @functionobjectid int, @fakefunctionobjectid int, @isscalarfunction bit begin declare @returntype nvarchar(max); select @returntype = t.typename sys.parameters p cross apply tsqlt.private_getfulltypename(p.user_type_id,p.max_length,p.precision,p.scale,null) p.object_id = @functionobjectid p.parameter_id = 0; declare @parameterlist nvarchar(max); select @parameterlist = coalesce( stuff((select ','+p.name+' '+t.typename+case t.istabletype = 1 ' readonly' else '' end sys.parameters p cross apply tsqlt.private_getfulltypename(p.user_type_id,p.max_length,p.precision,p.scale,null) p.object_id = @functionobjectid p.parameter_id &gt; 0 order p.parameter_id xml path(''),type ).value('.','nvarchar(max)'),1,1,''),''); declare @parametercalllist nvarchar(max); select @parametercalllist = coalesce( stuff((select ','+p.name sys.parameters p cross apply tsqlt.private_getfulltypename(p.user_type_id,p.max_length,p.precision,p.scale,null) p.object_id = @functionobjectid p.parameter_id &gt; 0 order p.parameter_id xml path(''),type ).value('.','nvarchar(max)'),1,1,''),''); if(@isscalarfunction = 1) begin exec('create function '+@functionname+'('+@parameterlist+') returns '+@returntype+' begin return '+@fakefunctionname+'('+@parametercalllist+');end;'); end else begin exec('create function '+@functionname+'('+@parameterlist+') returns table return select * '+@fakefunctionname+'('+@parametercalllist+');'); end; end; go go go create procedure tsqlt.fakefunction @functionname nvarchar(max), @fakefunctionname nvarchar(max) begin declare @functionobjectid int; declare @fakefunctionobjectid int; declare @isscalarfunction bit; exec tsqlt.private_validateobjectscompatiblewithfakefunction @functionname = @functionname, @fakefunctionname = @fakefunctionname, @functionobjectid = @functionobjectid out, @fakefunctionobjectid = @fakefunctionobjectid out, @isscalarfunction = @isscalarfunction out; exec tsqlt.removeobject @objectname = @functionname; exec tsqlt.private_createfakefunction @functionname = @functionname, @fakefunctionname = @fakefunctionname, @functionobjectid = @functionobjectid, @fakefunctionobjectid = @fakefunctionobjectid, @isscalarfunction = @isscalarfunction; end; go go create procedure tsqlt.renameclass @schemaname nvarchar(max), @newschemaname nvarchar(max) begin declare @migrateobjectscommand nvarchar(max); select @newschemaname = parsename(@newschemaname, 1), @schemaname = parsename(@schemaname, 1); exec tsqlt.newtestclass @newschemaname; select @migrateobjectscommand = ( select cmd [text()] ( select 'alter schema ' + quotename(@newschemaname) + ' transfer ' + quotename(@schemaname) + '.' + quotename(name) + ';' cmd sys.objects schema_id = schema_id(@schemaname) type ('pk', 'f') union select 'alter schema ' + quotename(@newschemaname) + ' transfer xml schema collection::' + quotename(@schemaname) + '.' + quotename(name) + ';' cmd sys.xml_schema_collections schema_id = schema_id(@schemaname) union select 'alter schema ' + quotename(@newschemaname) + ' transfer type::' + quotename(@schemaname) + '.' + quotename(name) + ';' cmd sys.types schema_id = schema_id(@schemaname) ) cmds xml path(''), type).value('.', 'nvarchar(max)'); exec (@migrateobjectscommand); exec tsqlt.dropclass @schemaname; end; go go create table [tsqlt].[private_assertequalstableschema_actual] ( name nvarchar(256) null, [rank(column_id)] int null, system_type_id nvarchar(max) null, user_type_id nvarchar(max) null, max_length smallint null, precision tinyint null, scale tinyint null, collation_name nvarchar(256) null, is_nullable bit null, is_identity bit null ); go exec(' set nocount on; select top(0) * tsqlt.private_assertequalstableschema_expected tsqlt.private_assertequalstableschema_actual aetsa; '); go go go create procedure tsqlt.assertequalstableschema @expected nvarchar(max), @actual nvarchar(max), @message nvarchar(max) = null begin insert tsqlt.private_assertequalstableschema_expected([rank(column_id)],name,system_type_id,user_type_id,max_length,precision,scale,collation_name,is_nullable) select rank()over(order c.column_id), c.name, cast(c.system_type_id nvarchar(max))+quotename(ts.name) system_type_id, cast(c.user_type_id nvarchar(max))+case tu.system_type_id&lt;&gt; tu.user_type_id quotename(schema_name(tu.schema_id))+'.' else '' end + quotename(tu.name) user_type_id, c.max_length, c.precision, c.scale, c.collation_name, c.is_nullable sys.columns c join sys.types ts c.system_type_id = ts.user_type_id join sys.types tu c.user_type_id = tu.user_type_id c.object_id = object_id(@expected); insert tsqlt.private_assertequalstableschema_actual([rank(column_id)],name,system_type_id,user_type_id,max_length,precision,scale,collation_name,is_nullable) select rank()over(order c.column_id), c.name, cast(c.system_type_id nvarchar(max))+quotename(ts.name) system_type_id, cast(c.user_type_id nvarchar(max))+case tu.system_type_id&lt;&gt; tu.user_type_id quotename(schema_name(tu.schema_id))+'.' else '' end + quotename(tu.name) user_type_id, c.max_length, c.precision, c.scale, c.collation_name, c.is_nullable sys.columns c join sys.types ts c.system_type_id = ts.user_type_id join sys.types tu c.user_type_id = tu.user_type_id c.object_id = object_id(@actual); exec tsqlt.assertequalstable 'tsqlt.private_assertequalstableschema_expected','tsqlt.private_assertequalstableschema_actual',@message=@message,@failmsg='unexpected/missing column(s)'; end; go go go not(cast(serverproperty('productversion') varchar(max)) like '9.%') begin exec('create type tsqlt.assertstringtable table(value nvarchar(max));'); end; go go go not(cast(serverproperty('productversion') varchar(max)) like '9.%') begin exec(' create procedure tsqlt.assertstringin @expected tsqlt.assertstringtable readonly, @actual nvarchar(max), @message nvarchar(max) = '''' begin if(not exists(select 1 @expected value = @actual)) begin declare @expectedmessage nvarchar(max); select value #expectedset @expected; exec tsqlt.tabletotext @tablename = ''#expectedset'', @orderby = ''value'',@txt = @expectedmessage output; set @expectedmessage = isnull(''&lt;''+@actual+''&gt;'',''null'')+char(13)+char(10)+''is in''+char(13)+char(10)+@expectedmessage; exec tsqlt.fail @message, @expectedmessage; end; end; '); end; go go go create procedure tsqlt.reset begin exec tsqlt.private_resetnewtestclasslist; end; go go go set nocount on; declare @ver nvarchar(max); declare @match int; select @ver = '| tsqlt version: ' + i.version, @match = case i.version = i.clrversion 1 else 0 end tsqlt.info() i; set @ver = @ver+space(42-len(@ver))+'|'; raiserror('',0,1)with nowait; raiserror('+-----------------------------------------+',0,1)with nowait; raiserror('| |',0,1)with nowait; raiserror('| thank using tsqlt. |',0,1)with nowait; raiserror('| |',0,1)with nowait; raiserror(@ver,0,1)with nowait; if(@match = 0) begin raiserror('| |',0,1)with nowait; raiserror('| error: mismatching clr version. |',0,1)with nowait; raiserror('| please download new version tsqlt. |',0,1)with nowait; end raiserror('| |',0,1)with nowait; raiserror('+-----------------------------------------+',0,1)with nowait; go</file><file name="src\sql\_test\tsqlt_config_ci.json">{ "testclassses": [ { "name" : "design rules", "classfilter": "t_design_rules", "description": "verify design rules", "max_failures": "0" }, { "name": "procedure unit tests", "classfilter": "t_usp_%", "description" : "all stored procedure unit tests", "max_failures": "0" }, { "name": "function unit tests", "classfilter": "t_fn_%", "description" : "all stored procedure unit tests", "max_failures": "0" } ] }</file><file name="src\sql\_test\tsqlt_with_xml_output.ps1">&lt;# .synopsis script run tsqlt tests deployed targetserver export results xml file .description #&gt; [cmdletbinding()] param ( [parameter( mandatory = $false)] [string] $configfile = "${env:reporoot}/src/sql/_test/tsqlt_config_ci.json", # configfile - path json definition tests run [parameter( mandatory = $false)] [string] $outputpath = "${env:reporoot}/src/sql/_test/output", # outputpath - path folder store xml results [parameter( mandatory = $true)] [string] $targetserver, # name sql server access [parameter( mandatory = $true)] [string] $databasename, # name sql database access [parameter( mandatory = $false)] [string] $accesstoken = (get-azaccesstoken -resourceurl https://database.windows.net).token # accesstoken user trying accesss db ) # create outputpath exist already if(!(test-path $outputpath)) { new-item -itemtype directory -force -path $outputpath | out-null } # sql query template execute $sql_run_test_class = " begin try exec tsqlt.run '&lt;classname&gt;'; end try begin catch end catch; exec tsqlt.xmlresultformatter; select count(id) numtests, sum(case result = 'success' 1 else 0 end) numsuccess, sum(case result = 'success' 0 else 1 end) numfailed tsqlt.testresult " # read json configfile extract testclasses execute $configdata = get-content -raw -path $configfile | convertfrom-json $testclasses = $configdata.testclassses ( (-not $testclasses) -or ( $testclasses.count -eq 0) ) { write-error "no testclasses found configuration" return } else{ write-information "number testclasses run: $($testclasses.count)" } ## initialize parameters running test classes # boolean: tests executed successfully $testresult = $true # list error messages received $errormessages = [system.text.stringbuilder]::new() # total number test classes executed $numclasses = 0 try { # loop testclass definition json configuration foreach ( $testclass $testclasses ) { $testname = $testclass.name $classfilter = $testclass.classfilter $maxerrors= $testclass.max_failures write-information "" write-information "---------------------------------------" write-information "class : $testname" write-information "filter : $classfilter" write-information "max errors: $maxerrors" write-information "---------------------------------------" write-information "" $params = @{ serverinstance = $targetserver database = $databasename accesstoken = $accesstoken } #get class names matching filter $test_cases = invoke-sqlcmd -query "select name tsqlt.testclasses name like '$classfilter'" @params # list test cases execute write-information "list test cases: " $test_cases | foreach-object { write-information " $($_.name)"} # total number failed tests within class $numfailed = 0 #loop matched cases foreach( $test_case $test_cases ) { $classname = $test_case.name $query = ( $sql_run_test_class -replace "&lt;classname&gt;", $classname ) $params = @{ serverinstance = $targetserver database = $databasename accesstoken = $accesstoken } $sqlresult = invoke-sqlcmd -query $query @params -outputas dataset $numresulttables = $sqlresult.tables.count # get xml output (expected last-but-one result table) $xmlcolumn = ($sqlresult.tables[$numresulttables-2] | get-member -membertype property).name $xmldata = $sqlresult.tables[$numresulttables-2].$xmlcolumn # sanity check: xml output table? ( $xmldata -notlike "&lt;testsuites&gt;*" ) { write-error "xml test output could located" } else { # write xml data file $xmldata | out-file "${outputpath}\tsqlt_result.$databasename.$classname.xml" } # keep track classes tested $numclasses++ # keep track failed tests count (in last resultset table) $testsfailed = $sqlresult.tables[$numresulttables-1].rows[0].numfailed if( $testsfailed -gt 0) { write-warning "test [$classname]: [$testsfailed] test failure(s) detected" $numfailed += $testsfailed } } # evaluate outcome ( $numfailed -gt $maxerrors ) { $testresult = $false $errormessages.appendline( "[${testname}] tests: detected failed tests ($numfailed) maximum ($maxerrors)" ) } } } catch { $testresult = $false $errormessages.appendline( $_ ) } ($testresult -eq $false ) { write-error $errormessages.tostring() } else { write-information "tests completed successfully." write-information "wrote $numclasses test result files $outputpath" }</file><file name="src\synapse\studio\publish_config.json">{"publishbranch":"synapse_publish","enablegitcomment":true}</file><file name="src\synapse\studio\template-parameters-definition.json">{ "microsoft.synapse/workspaces/notebooks": { "properties": { "bigdatapool": { "referencename": "-:referencename:string" }, "sessionproperties": { "runasworkspacesystemidentity": "-:runasworkspacesystemidentity:bool" } } }, "microsoft.synapse/workspaces/sqlscripts": { "properties": { "content": { "currentconnection": { "*": "-" } } } }, "microsoft.synapse/workspaces/triggers": { "properties": { "typeproperties": { "recurrence": { "*": "=", "interval": "=:triggersuffix:int", "frequency": "=:-freq" }, "maxconcurrency": "=" } } }, "microsoft.synapse/workspaces/linkedservices": { "*": { "properties": { "typeproperties": { "accountname": "=", "username": "=", "connectionstring": "|:-connectionstring:securestring", "secretaccesskey": "|" } } }, "azuredatalakestore": { "properties": { "typeproperties": { "datalakestoreuri": "=" } } }, "azurekeyvault": { "properties": { "typeproperties": { "baseurl": "|:baseurl:securestring" }, "parameters": { "keyvaulturl": { "type": "=", "defaultvalue": "|:defaultvalue:securestring" } } } }, "azureblobfs": { "properties": { "typeproperties": { "url": "=:url:string" } } } }, "microsoft.synapse/workspaces/datasets": { "*": { "properties": { "typeproperties": { "folderpath": "=", "filename": "=" } } } }, "microsoft.synapse/workspaces/credentials" : { "properties": { "typeproperties": { "resourceid": "=" } } }, "microsoft.synapse/workspaces/pipelines":{ "properties": { "variables": { "env_code": { "defaultvalue":"=:defaultvalue:string" }, "spark_pool": { "defaultvalue":"=:defaultvalue:string" } } } }, "microsoft.synapse/workspaces/sparkconfigurations":{ "properties": { "configs": { "spark.environment_code": "-:environment_code:string" } } } }</file><file name="src\synapse\studio\credential\WorkspaceSystemIdentity.json">{ "name": "workspacesystemidentity", "properties": { "type": "managedidentity" } }</file><file name="src\synapse\studio\dataset\ds_binary_file.json">{ "name": "ds_binary_file", "properties": { "linkedservicename": { "referencename": "ls_dap_adls_01", "type": "linkedservicereference" }, "parameters": { "container_name": { "type": "string" }, "folder_directory": { "type": "string" }, "file_name": { "type": "string" } }, "annotations": [], "type": "binary", "typeproperties": { "location": { "type": "azureblobfslocation", "filename": { "value": "@dataset().file_name", "type": "expression" }, "folderpath": { "value": "@dataset().folder_directory", "type": "expression" }, "filesystem": { "value": "@dataset().container_name", "type": "expression" } } } } }</file><file name="src\synapse\studio\dataset\ds_binary_zip_folder.json">{ "name": "ds_binary_zip_folder", "properties": { "linkedservicename": { "referencename": "ls_dap_adls_01", "type": "linkedservicereference" }, "parameters": { "container_name": { "type": "string" }, "folder_directory": { "type": "string" }, "file_name": { "type": "string" } }, "annotations": [], "type": "binary", "typeproperties": { "location": { "type": "azureblobfslocation", "filename": { "value": "@dataset().file_name", "type": "expression" }, "folderpath": { "value": "@dataset().folder_directory", "type": "expression" }, "filesystem": { "value": "@dataset().container_name", "type": "expression" } }, "compression": { "type": "zipdeflate", "level": "optimal" } } } }</file><file name="src\synapse\studio\dataset\ds_dap_sql_meta.json">{ "name": "ds_dap_sql_meta", "properties": { "description": "this dataset used execute stored procedures look tables inside dev-dap-sql-core (hosted azure)", "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" }, "parameters": { "schema": { "type": "string", "defaultvalue": "meta" }, "target": { "type": "string" } }, "annotations": [], "type": "azuresqltable", "schema": [], "typeproperties": { "schema": { "value": "@dataset().schema", "type": "expression" }, "table": { "value": "@dataset().target", "type": "expression" } } } }</file><file name="src\synapse\studio\integrationRuntime\AutoResolveIntegrationRuntime.json">{ "name": "autoresolveintegrationruntime", "properties": { "type": "managed", "typeproperties": { "computeproperties": { "location": "autoresolve", "dataflowproperties": { "computetype": "general", "corecount": 8, "timetolive": 0 } } }, "managedvirtualnetwork": { "type": "managedvirtualnetworkreference", "referencename": "default" } } }</file><file name="src\synapse\studio\linkedService\dev-dap-syn-core-WorkspaceDefaultSqlServer.json">{ "name": "dev-dap-syn-core-workspacedefaultsqlserver", "type": "microsoft.synapse/workspaces/linkedservices", "properties": { "typeproperties": { "connectionstring": "data source=tcp:dev-dap-syn-core.sql.azuresynapse.net,1433;initial catalog=@{linkedservice().dbname}" }, "parameters": { "dbname": { "type": "string" } }, "type": "azuresqldw", "connectvia": { "referencename": "autoresolveintegrationruntime", "type": "integrationruntimereference" }, "annotations": [] } }</file><file name="src\synapse\studio\linkedService\dev-dap-syn-core-WorkspaceDefaultStorage.json">{ "name": "dev-dap-syn-core-workspacedefaultstorage", "type": "microsoft.synapse/workspaces/linkedservices", "properties": { "typeproperties": { "url": "https://devdapstcoresyn.dfs.core.windows.net/" }, "type": "azureblobfs", "connectvia": { "referencename": "autoresolveintegrationruntime", "type": "integrationruntimereference" }, "annotations": [] } }</file><file name="src\synapse\studio\linkedService\ls_dap_adls_01.json">{ "name": "ls_dap_adls_01", "properties": { "annotations": [], "type": "azureblobfs", "typeproperties": { "url": "https://devdapstdala1.dfs.core.windows.net/" }, "connectvia": { "referencename": "autoresolveintegrationruntime", "type": "integrationruntimereference" } }, "type": "microsoft.synapse/workspaces/linkedservices" }</file><file name="src\synapse\studio\linkedService\ls_dap_kv_core.json">{ "name": "ls_dap_kv_core", "type": "microsoft.synapse/workspaces/linkedservices", "properties": { "annotations": [], "type": "azurekeyvault", "typeproperties": { "baseurl": "https://dev-dap-key-core-syn.vault.azure.net/" } } }</file><file name="src\synapse\studio\linkedService\ls_dap_sql_meta.json">{ "name": "ls_dap_sql_meta", "properties": { "description": "sql database meta data", "annotations": [], "type": "azuresqldatabase", "typeproperties": { "connectionstring": "integrated security=false;encrypt=true;connection timeout=30;data source=dev-dap-sql-core.database.windows.net;initial catalog=dev-dap-sqldb-core-meta" }, "connectvia": { "referencename": "autoresolveintegrationruntime", "type": "integrationruntimereference" } } }</file><file name="src\synapse\studio\managedVirtualNetwork\default.json">{ "name": "default", "type": "microsoft.synapse/workspaces/managedvirtualnetworks" }</file><file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-custstgacct--sbx-dap-syn-01-sbxdapadls01.json">{ "name": "synapse-ws-custstgacct--sbx-dap-syn-01-sbxdapadls01", "properties": { "privatelinkresourceid": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-network/providers/microsoft.storage/storageaccounts/sbxdapadls01", "groupid": "dfs", "fqdns": [ "sbxdapadls01.dfs.core.windows.net" ] } }</file><file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-dap-key-core-syn.json">{ "name": "synapse-ws-dap-key-core-syn", "properties": { "privatelinkresourceid": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.keyvault/vaults/dev-dap-key-core-syn", "groupid": "vault", "fqdns": [ "dev-dap-key-core-syn.vault.azure.net" ] } }</file><file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-dap-sql-core.json">{ "name": "synapse-ws-dap-sql-core", "properties": { "privatelinkresourceid": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.sql/servers/dev-dap-sql-core", "groupid": "sqlserver" } }</file><file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-dapstdala1.json">{ "name": "synapse-ws-dapstdala1", "properties": { "privatelinkresourceid": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-dala/providers/microsoft.storage/storageaccounts/devdapstdala1", "groupid": "dfs" } }</file><file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-sql--sbx-dap-syn-01.json">{ "name": "synapse-ws-sql--sbx-dap-syn-01", "properties": { "privatelinkresourceid": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-network/providers/microsoft.synapse/workspaces/sbx-dap-syn-01", "groupid": "sql", "fqdns": [ "sbx-dap-syn-01.c87ced63-969a-42fd-8878-843403cf71eb.sql.azuresynapse.net" ] } }</file><file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-sqlOnDemand--sbx-dap-syn-01.json">{ "name": "synapse-ws-sqlondemand--sbx-dap-syn-01", "properties": { "privatelinkresourceid": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-network/providers/microsoft.synapse/workspaces/sbx-dap-syn-01", "groupid": "sqlondemand", "fqdns": [ "sbx-dap-syn-01-ondemand.c87ced63-969a-42fd-8878-843403cf71eb.sql.azuresynapse.net" ] } }</file><file name="src\synapse\studio\notebook\Classes.json">{ "name": "classes", "properties": { "folder": { "name": "modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "a30fc411-79c3-491d-919d-fae0c710276f" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# classes\n", "\n", "the classes notebook contains classes relevant communication sql meta database logging purposes. contains 4 classes, related logging tables sql meta databases:\n", "- sqlserverconnection: class establish connection database execute queries database\n", "- plan: class contains necessary functionalities need occur plan (logging, etc.)\n", "- task: class contains necessary functionalities need occur task (logging, file-collection, etc.)\n", "- file: class contains necessary functionalities need occur source file (logging, moving, etc.)" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## script contents:\n", "1. import relevant packages\n", "2. sqlserverconnection class\n", "3. plan class\n", "4. task class\n", "5. file class" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 1. import packages" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# import specific methods python-native libraries\n", "from notebookutils import mssparkutils\n", "from json import loads\n", "\n", "# import regex datetime package\n", "import re\n", "import datetime\n", "import time\n", "\n", "# import delta.tables dt\n", "# import numpy np\n", "# import os\n", "# pyspark.sql.functions import col, when, to_timestamp, to_date\n", "# pyspark.sql import row, dataframe\n", "# import pyspark.sql sql\n", "# pyspark.sql.types import structtype" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class sqlserverconnection(object):\n", "\n", " def __init__(self:object, env_code:str, port:int, debug:bool=false) -&gt; object:\n", " \"\"\"\n", " initialize set class-arguments related sql server connections: server_url linked_service\n", " validate input parameters creating arguments\n", "\n", " :param env_code: environment code sql server (dev, int, tst, acc, prd)\n", " :param port: port used connect sql server (1433)\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", " \"\"\"\n", "\n", " debug: print(f\"[classes:sqlserverconnection] : initialize sql server connection arguments...\")\n", " self.debug:bool = debug\n", "\n", " # variable initialisation: validate arguments contain expected values\n", " self.debug: print(f\"[classes:sqlserverconnection]: validate environment code: {env_code}\")\n", " self.validate_env_argument(environment_code=env_code)\n", " self.debug: print(f\"[classes:sqlserverconnection]: validate port: {port}\")\n", " self.validate_port_argument(port_argument=port)\n", "\n", " # set relevant member-values\n", " # dev-info: class using semi-static values members. need connect servers sql meta database.\n", " # would need this, class made generic replacing semi-static values arguments passed class\n", " self.server:str = f\"{env_code}-dap-sql-core.database.windows.net\"\n", " self.database:str = f\"{env_code}-dap-sqldb-core-meta\"\n", " \n", " # initiliaze member-values server_url related linked_service\n", " # dev-info: linked_service variable used retrieve access tokens server\n", " self.server_url:str = f\"jdbc:sqlserver://{self.server}:{port};database={self.database}\"\n", " self.linked_service:str = \"ls_dap_sql_meta\"\n", " \n", " # validate linked_service allowed value\n", " self.debug: print(f\"[classes:sqlserverconnection]: validate linked service: {self.linked_service}\")\n", " self.validate_sql_linked_service()\n", "\n", " # initialize empty access_token\n", " # first query executed using sqlserverconnection initialize access_token using self.linked_service\n", " self.access_token = none\n", "\n", " self.debug: print(f\"[classes:sqlserverconnection]: set initialisation arguments: server_url={self.server_url}; linked_service={self.linked_service}\")\n", " \n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------- methods validate argument values --------------------------------------------------------------\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " @staticmethod\n", " def validate_env_argument(environment_code:str):\n", " \"\"\"\n", " validate given value environment_code one expected/allowed values\n", " \n", " :param environment_code: code environmet sql database located (dev, int, tst, acc, prd)\n", " \"\"\"\n", "\n", " # set list allowed values\n", " allowed_envs = ['dev', 'int', 'tst', 'acc', 'prd']\n", " # [functions/genericfunctions] validate argument environment_code item allowed_envs list\n", " validate_argument('env_code', environment_code, allowed_envs)\n", "\n", " @staticmethod\n", " def validate_port_argument(port_argument:int):\n", " \"\"\"\n", " validate given value port_argument one expected/allowed values\n", " \n", " :param port_argument: tpc port communication microsoft sql server\n", " \"\"\"\n", "\n", " # set list allowed values\n", " allowed_ports = [1433]\n", " # [functions/genericfunctions] validate argument port_argument item allowed_ports list\n", " validate_argument('port', port_argument, allowed_ports)\n", "\n", " def validate_sql_linked_service(self:object):\n", " \"\"\"\n", " validate linked service actually configured\n", " \"\"\"\n", " # [functions/genericfunctions] validate argument self.linked_service actually configured\n", " validate_linked_service(self.linked_service)\n", "\n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ----------------------------------------------------------- methods establish connection sql server ------------------------------------------------------\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " def set_access_token(self: object) -&gt; str:\n", " \"\"\"\n", " return new access token using linked_service\n", " \"\"\"\n", "\n", " access_token = mssparkutils.credentials.getconnectionstringorcreds(self.linked_service)\n", " self.debug: print(f\"[classes:sqlserverconnection]: {self.linked_service}: new access token: {access_token}\")\n", " return access_token\n", "\n", " def set_sql_connection(self: object) -&gt; list:\n", " \"\"\"\n", " return new access token connection-object sql server\n", " \"\"\"\n", "\n", " self.debug: print(\"[classes:sqlserverconnection]: establish sql connection\")\n", "\n", " # [classes:sqlserverconnection]: set new access token establishing connection\n", " access_token:str = self.set_access_token()\n", " # set drivermanager object instance manage jdbc driver (-&gt; see: self.server_url)\n", " # dev-info: spark run java virtual machine (jvm), drivermanager-class defined.\n", " # need access specific underlying spark classes, gateway structure used\n", " driver_manager:object = spark._sc._gateway.jvm.java.sql.drivermanager\n", "\n", " # set properties object instance pass access_token part dictionary connection-objects\n", " connection_properties:object = spark._sc._gateway.jvm.java.util.properties()\n", " connection_dictionary:dict = {\n", " 'accesstoken': access_token\n", " }\n", " connection_properties.putall(connection_dictionary)\n", "\n", " # try establish connection sql server using driver_manager connection_properties instances\n", " try:\n", " connection:object = driver_manager.getconnection(self.server_url, connection_properties)\n", " except exception e:\n", " re.search(r\"no suitable driver found for\", str(e)):\n", " raise valueerror(f\"no suitable driver found sql server {self.server_url}\")\n", " elif re.search(r'the tcp/ip connection host.' , str(e)):\n", " raise valueerror(f\"connection sql server {self.server_url} cannot made. validate url-value check firewall/access settings\")\n", " else:\n", " raise exception(e)\n", "\n", " # return access_token string connection object instance\n", " return access_token, connection\n", "\n", " def check_access_token(self: object) -&gt; list:\n", " \"\"\"\n", " validate whether access_token expired yet\n", " set new access token (and connection) expired\n", " \"\"\"\n", "\n", " self.debug: print(\"[classes:sqlserverconnection]: check access token\")\n", "\n", " # token already set\n", " (self.access_token != none):\n", " # validate access_token yet expired\n", " tokenisvalid:bool = mssparkutils.credentials.isvalidtoken(self.access_token)\n", "\n", " tokenisvalid:\n", " self.debug: print(\"[classes:sqlserverconnection]: access token expired. setting new one\")\n", " # [classes:sqlserverconnection]: create new access token connection\n", " token, connection = self.set_sql_connection()\n", " # return re-established values\n", " return token, connection\n", " else:\n", " self.debug: print(\"[classes:sqlserverconnection]: access token expired. return existing arguments\")\n", " # return valid (not expired) values\n", " return self.access_token, self.connection\n", "\n", " # first time setting access token\n", " else:\n", " self.debug: print(\"[classes:sqlserverconnection]: establishing first sql connection\")\n", " # [classes:sqlserverconnection]: create new access token connection\n", " token, connection = self.set_sql_connection()\n", " # return established values\n", " return token, connection\n", "\n", "\n", " def check_lifetime_db(self: object):\n", " \"\"\"\n", " function try execute query sql database. result throws error, database likely paused start resuming query\n", " start-up time takes average 1 minute. query executed every 20 seconds, 5 times. response still invalid this, likely different issue\n", " \"\"\"\n", "\n", " self.debug: print(\"[classes:sqlserverconnection]: resume sql database...\")\n", "\n", " # set variables query execution\n", " serverissleeping = true # assume sql database sleeping\n", " sleep_time = 20 # time sending new calls sql database\n", " max_calls = 5 # total number calls send sql database failure\n", " total_calls = 0 # current total number calls made sql database\n", "\n", " (serverissleeping):\n", " total_calls += 1\n", " try:\n", " # [classes:sqlserverconnection]: execute query sql database\n", " self.execute_query('select top 1 * meta.source_configuration', primary_checks=false, expect_return=true)\n", "\n", " except:\n", " # throw error total number calls exceeds maximum allowed calls\n", " total_calls &gt;= max_calls:\n", " self.debug: print(f\"[classes:sqlserverconnection]: could resume sql database {self.database} server {self.server} (url: {self.server_url})...\")\n", " raise connectionerror(\"connecting sql metadb takes long. validate connection...\")\n", " \n", " # sleep {sleep_time} retry execute_query()\n", " self.debug: print(f\"[classes:sqlserverconnection]: response yet {total_calls} calls, sleeping {sleep_time} seconds...\")\n", " time.sleep(sleep_time)\n", "\n", " else:\n", " # try successful, database awake while-loop exited\n", " self.debug: print(f\"[classes:sqlserverconnection]: database resumed!\")\n", " serverissleeping = false\n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods execute sql queries sql server\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", " @staticmethod\n", " def execute_query_without_return(sql_statement:object, statement:str):\n", " \"\"\"\n", " use sql_statement object instance execute query without expecting return value\n", " \n", " :param sql_statement: object createstatement() used execute sql statements retrieve results.\n", " :param statement: sql query executed\n", " \"\"\"\n", " sql_statement.execute(statement)\n", " \n", "\n", " @staticmethod\n", " def execute_query_with_return(sql_statement, statement)-&gt; list:\n", " \"\"\"\n", " use sql_statement object instance execute query expect return value\n", " \n", " :param sql_statement: object createstatement() used execute sql statements retrieve results.\n", " :param statement: sql query executed\n", "\n", " :return row_objects:list: list json objects, object represents row name column value column \n", "\n", " example:\n", " [\n", " {column_name_1:row_value_11, column_name_2:row_value_12, etc.},\n", " {column_name_1:row_value_21, column_name_2:row_value_22, etc.},\n", " etc.\n", " ]\n", " \"\"\"\n", "\n", " # use sql_statement object instance execute query\n", " sql_statement.executequery(statement)\n", " # get resultset query\n", " # dev-note: check different getresultset() fetchall()\n", " query_result = sql_statement.getresultset()\n", "\n", "\n", " # get column names resulting query\n", " # dev-note: 1 resultset expected returned current set-up\n", " query_result_metadata = query_result.getmetadata()\n", " column_count = query_result_metadata.getcolumncount()\n", " column_names = [query_result_metadata.getcolumnname(i) range (1, column_count+1)]\n", "\n", "\n", " row_objects = []\n", " (query_result.next()):\n", " result = {column:query_result.getstring(column) column column_names}\n", " row_objects.append(result)\n", " return row_objects\n", "\n", "\n", "\n", " def execute_query(self: object, statement: str, primary_checks: bool = true, expect_return: bool = false) -&gt; list:\n", " \"\"\"\n", " execute query sql database\n", "\n", " :param statement: query execute sql database\n", " :param primary_checks: boolean indicating whether execute connection checks trying execute query\n", " :param expect_return: boolean indicating whether statement expects something returned sql database\n", "\n", "\n", " :note expected_return:\n", " depending value expected_return, one following methods executed:\n", " expected_return = false: .execute() -&gt; executes statement\n", " expected_return = true: .executequery() -&gt; executes statement expects resultset returned\n", " \n", " .execute() also used expecting resultsets returned, considers non-empty return value first command executed statement returns resultset. \n", " is, statement calls stored procedure first statement return resultset (say, declare-statement), .execute()-method say resultsets\n", " returned, even though stored procedure might contain several select-statements declare-statement. therefore, boolean value 'expected_return' make sure the\n", " correct method called avoid errors.\n", "\n", " \"\"\"\n", "\n", " self.debug: print(f\"[classes:sqlserverconnection]: execute query: {statement}\")\n", "\n", " # self.connection exist none -&gt; primary checks need executed\n", " hasattr(self, 'connection') getattr(self, 'connection', none) none:\n", " primary_checks = true\n", "\n", " # first check access_token valid whether database paused trying execute query\n", " primary_checks:\n", " self.debug: print(f\"[classes:sqlserverconnection]: execute primary checks\")\n", "\n", " # [classes:sqlserverconnection]: check access_token expired\n", " self.access_token, self.connection = self.check_access_token()\n", "\n", " # [classes:sqlserverconnection]: check database paused\n", " self.check_lifetime_db()\n", " \n", " \n", " # create createstatement object instance\n", " # dev-info: statement object used send sql commands database retrieve results.\n", " sql_statement:object = self.connection.createstatement()\n", " expect_return:\n", " self.debug: print(f\"[classes:sqlserverconnection]: expect return statement: {statement}\")\n", " # [classes:sqlserverconnection]: execute athe statement expect resultset returned\n", " result:list = self.execute_query_with_return(sql_statement, statement)\n", " sql_statement.close()\n", " self.debug: print(f\"[classes:sqlserverconnection]: return statement: {result}\")\n", " return result\n", "\n", " else:\n", " self.debug: print(f\"[classes:sqlserverconnection]: expect return statement: {statement}\")\n", " # [classes:sqlserverconnection]: execute statement\n", " self.execute_query_without_return(sql_statement, statement)\n", " sql_statement.close()\n", "" ], "execution_count": 2 }, { "cell_type": "code", "source": [ "class plan():\n", "\n", " def __init__(self: object, plan_id: int, plan_name: str, sqlserverconnection: object, debug:bool=false) -&gt; object:\n", " \"\"\"\n", " initialize object class 'plan'\n", " \n", " :param plan_id: id plan meta.log_plans\n", " :param plan_name: name plan meta.plan_configuration\n", " :param sqlserverconnection: object-instance class sqlserverconnection\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", "\n", " :note:\n", " class real purpose now, could relevant towards future.\n", " already included framework avoid issues future\n", " \"\"\"\n", "\n", " debug: print(f\"[classes:plan] : initialize plan arguments...\")\n", " self.plan_id:int = plan_id\n", " self.sqlserverconnection:object = sqlserverconnection\n", " self.plan_name:str = plan_name\n", " self.debug:bool = debug" ], "execution_count": 3 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class task(object):\n", " def __init__(self: object, task_name: str, task_id: int, plan_id: int, sqlserverconnection: object, worker_name:str, debug:bool=false) -&gt; object:\n", " \"\"\"\n", " initialize object class 'task'\n", "\n", " :param task_name: nmae task meta.task_configuration\n", " :param task_id: id task meta.log_tasks\n", " :param plan_id: id plan meta.log_plans\n", " :param sqlserverconnection: object-instance class sqlserverconnection\n", " :param worker_name: name worker execute task\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", " \"\"\"\n", "\n", " debug: print(f\"[classes:task] : initialize task arguments...\")\n", " self.plan_id:int = plan_id\n", " self.task_id :int= task_id\n", " self.task_name:str = task_name\n", " self.sqlserverconnection:object = sqlserverconnection\n", " self.worker_name = worker_name\n", " self.debug:bool = debug\n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods execute sql queries sql server: specifically related task\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " def start_task(self: object):\n", " \"\"\"\n", " execute sql query start task set status 'in progress'\n", " \"\"\"\n", " self.debug: print(f\"[classes:task] : start task...\")\n", "\n", " # set query executed sql database\n", " statement:str = f'exec meta.usp_start_task @task_id = {self.task_id}, @plan_id = {self.plan_id}'\n", " \n", " # [classes:sqlserverconnection]: execute statement\n", " self.sqlserverconnection.execute_query(statement)\n", "\n", "\n", " def end_task(self: object, success_flag: bool, comment: str = 'succes'):\n", " \"\"\"\n", " execute sql query start task set status 'succeeded' 'failed'\n", "\n", " :param success_flag: boolean indicating whether task succeeded failed\n", " :param comment: string value giving additional information. status failed, error-message passed here\n", " \"\"\"\n", " self.debug: print(f\"[classes:task] : end task...\")\n", "\n", " # check comment-argument dictionary value\n", " # yes -&gt; object coming notebook [errorhandling] contains \n", " isinstance(comment, dict):\n", " # object contains key 'custom_message':\n", " # replace token-values (e.g. %{&lt;token&gt;}% ) actual values\n", " comment.get('custom_message'):\n", " comment['custom_message'] = comment['custom_message'].replace(\"%{task_id}%\", str(self.task_id))\n", " comment['custom_message'] = comment['custom_message'].replace(\"%{task_name}%\", self.task_name)\n", " comment['custom_message'] = comment['custom_message'].replace(\"%{plan_id}%\", str(self.plan_id))\n", " # convert dictionary json-object sql deal it\n", " comment = json.dumps(comment)\n", " else:\n", " # replace ' values comment mess query-statement\n", " comment:str = comment.replace(\"\\'\", \"\")\n", " # set query executed sql database\n", " statement:str = f'exec meta.usp_end_task @task_id = {self.task_id}, @plan_id = {self.plan_id}, @success_flag = {success_flag}, @comment = \\'{comment}\\''\n", " \n", " # [classes:sqlserverconnection]: execute statement\n", " self.sqlserverconnection.execute_query(statement)\n", "\n", " def run_usp_gettaskmetadata(self: object) -&gt; dict:\n", " \"\"\"\n", " execute sql query get metadata related task\n", " \"\"\"\n", " self.debug: print(f\"[classes:task] : invoke meta.usp_get_task_metadata...\")\n", "\n", " # set query executed sql database\n", " statement:str = f\"exec meta.usp_get_task_metadata @task_id={str(self.task_id)}\"\n", " # [classes:sqlserverconnection]: execute statement\n", " task_metadata:dict = self.sqlserverconnection.execute_query(statement, expect_return=true)\n", "\n", " return task_metadata\n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods set members task instance\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", "\n", " def get_task_metadata(self, variables:list):\n", " \"\"\"\n", " set keys values dictionary self.variables\n", "\n", " :param variables: list contains columns names expected returned\n", " \"\"\"\n", "\n", " self.debug: print(f\"[classes:task] : get task metadata\")\n", " \n", " # [classes:task]: execute stored procedure meta.usp_get_task_metadata\n", " task_metadata:list = self.run_usp_gettaskmetadata()\n", "\n", " # convert variables-list dictionary keys column names values still empty\n", " self.variables:dict = dict.fromkeys(variables)\n", "\n", " # keys, multiple values returned. added list items\n", " # this, preventively specified value keys list\n", " list_columns:list = ['source_name', 'sink_name', 'dimension', 'data_type', 'check_name', 'config_params', 'column_info']\n", " column list_columns:\n", " self.variables[column] = []\n", " \n", " \n", " task_metadata:\n", " raise unboundlocalerror(f\"no task_metadata found task_id {str(self.task_id)}\")\n", " else:\n", "\n", " # meta.usp_get_task_metadata returns one row values\n", " # preventively, first object task_metadata-list analyzed.\n", " column_name task_metadata[0]:\n", " # meta.usp_get_task_metadata returns object row (per column). object contains relevant metadata configuration-table queried\n", " # try convert object json-object \n", " try: \n", " # returned value column expected array json-objects, formatted string\n", " # convert string list looping objects\n", " column_contents:object = loads(task_metadata[0][column_name])\n", "\n", " # loop columns json-object: value parent keys consists array json-objects\n", " columns column_contents:\n", " # loop item json-object \n", " column columns:\n", " # make sure set variables asked for\n", " column self.variables:\n", " # column one list_columns: use append append value list\n", " (column list_columns):\n", " self.variables[column].append(columns[column])\n", " # else: set value column-name\n", " else:\n", " self.variables[column] = columns[column]\n", " except:\n", " print(f\"empty json {column_name}\")\n", " \n", " def set_ingestion_variables(self: object, storage_account: str):\n", " \"\"\"\n", " set arguments needed file ingestion\n", "\n", " :param storage_account: name storage account files located\n", "\n", " :return silver_path: location delta table located (storage_account, container_name, table_name)\n", " :return raw_format: data-format file raw (default: parquet)\n", " :return target_options: options related target-location (=delta table) need taken account: partitioning, etc.\n", " :return skip_first_lines: ingestion, might need skip first lines file, indicated member\n", " \"\"\"\n", "\n", "\n", " self.debug: print(f\"[classes:task] : set ingestion variables\")\n", "\n", " self.silver_path:str = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', storage_account, self.table_name)\n", " # self.raw_format:str = \"parquet\"\n", " self.skip_first_lines:int = self.variables['skip_first_lines']\n", "\n", "\n", " # value target_options key, try converting json. possible, throw configuration error\n", " # value: return empty dictionary\n", " self.variables['target_options']:\n", " try:\n", " self.target_options:dict = loads(self.variables['target_options'])\n", " except:\n", " raise valueerror(f\"given target_options cannot converted json. validate configuration {self.task_name}: {self.variables['target_options']}\")\n", " else:\n", " self.target_options:dict = dict()\n", "\n", "\n", " def set_dataset_variables(self: object, storage_account: str):\n", " \"\"\"\n", " set arguments needed dataset manipulations\n", "\n", " :param storage_account: name storage account files located\n", "\n", " :return table_name: name delta table source file needs ingested into\n", " :return container_name: name container sources files expected\n", " :return file_path: naming-pattern source file expected follow\n", " :return sink_column_names: list column names silver table\n", " :return source_column_names: list column names source file\n", " :return column_datatypes: list datatypes different columns\n", " :return dimensions: list dimensions different columns (pk, scd2,...)\n", " :return separator: column-delimiter used file\n", " :return file_extension: extension file (csv, json, fil, txt,...)\n", " :return landing_path: overarching path datasets landed (not detail: &lt;env_code&gt;/&lt;timestamp&gt;/&lt;datasource&gt;)\n", " :return file_kind: type file dealth (csv, json, zip...)\n", " :return header: boolean value indicating whether dataframe expected headerline\n", " :return file_pattern: (regex) filename-template task meant ingest\n", " \"\"\"\n", "\n", " self.debug: print(f\"[classes:task] : set dataset variables\")\n", "\n", " self.table_name:str = self.variables['table_name']\n", " self.container_name:str = self.variables['container_name']\n", " self.file_path:str = self.variables['source_folder']\n", " self.sink_column_names:list = self.variables['sink_name']\n", " self.source_column_names:list = self.variables['source_name']\n", " self.column_datatypes:list = self.variables['data_type']\n", " self.dimensions:list = self.variables['dimension']\n", " self.column_information:list = [json.loads(object_) object_ self.variables['column_info']] # convert strings objects\n", " self.separator:str = self.variables['column_delimiter']\n", " self.file_extension:str = self.variables['file_extension']\n", " self.landing_path:str = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.container_name, storage_account, self.file_path)\n", " self.file_kind:str = self.variables['file_kind']\n", " self.file_pattern:str = self.variables['file_pattern']\n", " self.escape_character:str = self.variables['escape_character']\n", " self.quote_character:str = self.variables['quote_character']\n", " # dev-note: header string value sql db, therefore returned 'true' 'false'. combat issue, solution implemented\n", " # solution ideal, header value stored binary value sql instead\n", " string_header:str = str(self.variables['header'])\n", " (string_header.lower() == 'true'): # true true\n", " self.header = true\n", " elif (string_header.lower() == 'false'): # false false\n", " self.header = false\n", " else:\n", " raise valueerror(\"invalid boolean value header: {}\".format(string_header))\n", "\n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods get files need handled task instance\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " def get_files_objects(self: object, storage_account: str) -&gt; list:\n", " \"\"\"\n", " recursively get list files task.landing_path create instance class 'file'\n", "\n", " :param storage_account: azure data lake storage account files located\n", "\n", " :return file_objects: list object-instances class 'file'\n", "\n", " :dev-note:\n", " parameter 'storage account' could extracted landing_path instead pass parameter\n", "\n", " \"\"\"\n", " self.debug: print(f\"[classes:task] : get file objects\")\n", " # [functions/genericfunctions] get list files folders specific abfss-path\n", " files, folders = list_directory_content(self.landing_path, list(), list())\n", " self.debug: print(f\"[classes:task] : found files: {files}\") \n", " self.debug: print(f\"[classes:task] : found folders: {folders}\") \n", " # [functions/genericfunctions] filter file-list return items (regex) match file_pattern\n", " patterned_files:list = filter_list(full_list=files, pattern=self.file_pattern, extension=self.file_extension)\n", " self.debug: print(f\"[classes:task] : filtered files: {patterned_files}\")\n", " # set dictionary {file-path: file-name}\n", " found_files:dict = {file.path:file.name.split('.')[0] file patterned_files}\n", "\n", " # [classes/file] create set file-class objects based dictionary\n", " file_objects:list = [\n", " file(\n", " task=self, \n", " sqlserverconnection=self.sqlserverconnection, \n", " landing_path=file, \n", " filename=found_files[file], \n", " timestamp_folder=file.split('/')[-2], \n", " storage_account=storage_account, \n", " debug=self.debug\n", " ) \n", " file found_files\n", " ]\n", "\n", " return file_objects" ], "execution_count": 4 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class file(object):\n", " # dev-note: sqlserverconnection already part 'task' object passing extra argument could redundant\n", " def __init__(self:object, task:object, sqlserverconnection:object, landing_path:str, filename:str, timestamp_folder:str, storage_account:str, debug:bool=false):\n", " \"\"\"\n", " initialise set arguments file-instance log file meta.log_files\n", "\n", " :param task: instance class 'task'\n", " :param sqlserverconnection: instance class 'sqlserverconnection'\n", " :param landing_path: abfss-path source file landing zone\n", " :param filename: name source file landing zone\n", " :param timestamp_folder: name subfolder source file located (expected timestamp)\n", " :param storage_account: name storage account source file located\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", "\n", " :return raw_path: abfss-path source file copied raw-container\n", " :return silver_path: abfss-path delta table silver-container source file ingested into\n", " :return archive_path: abfss-path source file moved ingestion\n", " :return target_source_columnnames: dictionary matching column names delta table source file ({target_column: source_column})\n", " :return checks_parameters: list parameters needed certain check-methods \n", " \n", " \"\"\"\n", " debug: print(f\"[classes:file] : initialize file arguments: {filename}\")\n", "\n", " # ------------------------------------------ use arguments create class-members ------------------------------------------\n", "\n", " self.task:object = task\n", " self.sqlserverconnection:object = sqlserverconnection\n", " self.file_name:str = filename\n", " self.landing_path:str = landing_path\n", " self.extended_filename:str = filename + '_' + timestamp_folder\n", " self.storage_account = storage_account\n", " self.debug = debug\n", "\n", "\n", " # ------------------------------------------ set path-variables file movements ------------------------------------------\n", "\n", " # [functions/ingestionfunctions] remove extension filename create raw_path\n", " # dev-info: files raw parquet files, 'part-&lt;xxx&gt;.parquet'. control filename, raw_path folder-directory file landed\n", " # therefore: raw, filename name folder parquet file landed actual name file\n", " # removing extension makes sure name folder little bit cleaner\n", " self.raw_path:str = remove_extension(string_with_extension=landing_path.replace(\"landing\", \"raw\"), extension_value=self.task.file_extension)\n", " self.silver_path:str = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', self.storage_account, self.task.table_name)\n", " self.archive_path:str = self.landing_path.replace(\"landing\", \"archive\")\n", " self.target_source_columnnames:dict = self.get_sinksource_columnname_dictionary()\n", " # checks, convert list parameters needed execute checks list json-objects\n", " # dev-note: make passed argument creation self.spark_dataframe_instance\n", " self.checks_parameters:dict = jsons_to_dict(self.task.variables['config_params'])\n", "\n", "\n", " # execute preliminary methods \n", " # [classes/file] log file meta.log_files tables sql meta database\n", " self.log_file()\n", "\n", "\n", " def create_dataframe_instance(self, source_path:str, file_kind:str, skip_lines:int=none, source_kind:str=none) -&gt; object:\n", " \"\"\"\n", " return object class \"sparkdataframechecks\", load dataframe memory (during creation object) \n", "\n", " :param source_path: exact path file needs loaded\n", " :param file_kind: kind file loaded (csv, json, parquet)\n", " :param skip_lines: optional argument, declaring number rows need skipped beginning file\n", " :param source_kind: optional argument, specifying kind original source file. relevant parameter executing header_checks raw silver\n", "\n", " :return checks_object: instance class 'sparkdataframechecks'\n", " \"\"\"\n", " self.debug: print(f\"[classes:file] : create checks-class object\")\n", "\n", " # set list-argument specifying checks need executed task source file/dataframe\n", " check_names:list = self.task.variables['check_name']\n", "\n", " # [classes/file] get dictionary {source_name: datatype} match column names raw file expected datatype\n", " columnname_datatypes:dict = self.get_columndatatype_dictionary()\n", " column_information:list = self.task.column_information + [none]*9 # add none-type technical columns\n", " \n", " # [classes/checks] create checks-object instance file execute quality checks \n", " checks_object:object = sparkdataframechecks(\n", " source_path = source_path, \n", " header = self.task.header, \n", " separator = self.task.separator, \n", " file_kind = file_kind, \n", " column_names = list(columnname_datatypes.keys()), \n", " data_types = list(columnname_datatypes.values()), \n", " checks = check_names, \n", " column_information = column_information,\n", " skip_lines = skip_lines,\n", " debug = self.debug,\n", " source_kind = source_kind,\n", " quote_character = self.task.quote_character,\n", " escape_character = self.task.escape_character\n", " )\n", " return checks_object\n", "\n", " \n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods column handling: create dictionaries column_names matching data types, well match raw silver column names\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " def get_primary_key_dictionary(self) -&gt; dict:\n", "\n", " \"\"\"\n", " generate dictionary {sink_name: source_name}, matching column names delta table raw data source primary keys\n", " \"\"\"\n", " pk_column_names:dict = {self.task.sink_column_names[index]:self.task.source_column_names[index] index, dimension enumerate(self.task.dimensions) dimension == 'pk'}\n", " return pk_column_names\n", "\n", " @staticmethod\n", " def get_technical_column_names() -&gt; dict:\n", " \"\"\"\n", " return dictionary {sink_column:source_column} technical fields delta table raw data file\n", " \"\"\"\n", " technical_column_names:dict = {'t_load_date_raw': 't_load_date_raw', 't_load_date_silver': 't_load_date_silver', 't_extract_date': 't_extract_date', 't_update_date': 't_update_date','t_insert_date': 't_insert_date', 't_plan_id': 't_plan_id', 't_task_id': 't_task_id', 't_file_id': 't_file_id', 't_file_name': 't_file_name'}\n", " return technical_column_names\n", "\n", " @staticmethod\n", " def get_technical_column_datatypes() -&gt; dict:\n", " \"\"\"\n", " return dictionary {source_column:datatype} technical fields raw data file\n", " \"\"\"\n", " technical_column_datatypes = {'t_load_date_raw': 'timestamp', 't_load_date_silver': 'timestamp', 't_extract_date':'timestamp','t_update_date':'timestamp','t_insert_date':'timestamp','t_plan_id': 'integer', 't_task_id': 'integer', 't_file_id': 'integer', 't_file_name': 'string'}\n", " return technical_column_datatypes\n", "\n", " def get_sinksource_columnname_dictionary(self) -&gt; dict:\n", " \"\"\"\n", " create dictionary non-techincal column names: {sink_name: source_name}\n", " \"\"\"\n", " configured_column_names:dict = dict(zip(self.task.sink_column_names, self.task.source_column_names))\n", " # [classes/file] get dictionary techincal column names: {sink_name: source_name}\n", " technical_column_names:dict = self.get_technical_column_names()\n", " target_source_columnnames:dict = {**configured_column_names, **technical_column_names}\n", "\n", " return target_source_columnnames\n", "\n", " def get_columndatatype_dictionary(self) -&gt; dict:\n", " \"\"\"\n", " create dictionary non-techincal column names: {source_name: datatype}\n", " \"\"\"\n", " configured_column_datatypes:dict = dict(zip(self.task.source_column_names, self.task.column_datatypes))\n", " # [classes/file] get dictionary techincal column names: {source_name: datatype}\n", " technical_column_datatypes:dict = self.get_technical_column_datatypes()\n", " # merge dictionaries\n", " columnname_datatypes:dict = {**configured_column_datatypes, **technical_column_datatypes} \n", "\n", " return columnname_datatypes\n", " \n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods log file movements sql meta database\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " def log_file(self: object):\n", " \"\"\"\n", " execute stored procedure meta.usp_new_file set file_id argument \n", " \"\"\"\n", " self.debug: print(f\"[classes:file] : log file sql database: {self.file_name}\")\n", "\n", " # set query executed sql database\n", " statement:str = f\"\"\"exec meta.usp_new_file @filename='{self.file_name}', @task_id={self.task.task_id}, @plan_id ={self.task.plan_id}, @extended_filename = '{self.extended_filename}', @info_message = '{self.landing_path}', @no_select = 0\"\"\"\n", " # [classes:sqlserverconnection]: execute statement\n", " result:list = self.sqlserverconnection.execute_query(statement, expect_return=true)\n", " self.debug: print(f\"[classes:file]: logging result: {result}\")\n", " # result empty -&gt; throw error\n", " result:\n", " raise unboundlocalerror(f\"no file_id returned logging file {self.extended_filename} plan_id ({str(self.task.plan_id)}) task_id ({str(self.task.task_id)})\")\n", " \n", " # get value key 'file_id'\n", " key result[0]:\n", " key == 'file_id':\n", " self.file_id = result[0][key]\n", "\n", " # validate file_id member set\n", " try:\n", " self.file_id\n", " self.debug: print(f\"[classes:file]: file id set: {self.file_name}:{self.file_id}\")\n", " except attributeerror e:\n", " raise attributeerror(f\"there file_id set file {self.extended_filename} plan_id ({str(self.task.plan_id)}) task_id ({str(self.task.task_id)})\")\n", "\n", "\n", "\n", " def update_file_activity(self: object, activity: str, success: bool, info_message: str):\n", " \"\"\"\n", " execute stored procedure meta.usp_update_file_activity new status file\n", " \"\"\"\n", " self.debug: print(f\"[classes:file] : update file sql database: {self.file_name}:{activity}\")\n", "\n", " # set query executed sql database\n", " statement = f\"\"\"exec meta.usp_update_file_activity @extended_filename='{self.extended_filename}', @activity={activity}, @success={success}, @info_message = '{info_message}'\"\"\"\n", " \n", " # [classes:sqlserverconnection]: execute statement\n", " self.sqlserverconnection.execute_query(statement)\n", "\n", "\n", "\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # methods move files different stages: raw, silver, archive\n", " # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " def landing_to_raw(self):\n", " print(\"=========================\")\n", " print(f\"[classes:file]: execute landing_to_raw: file={self.file_name} ({datetime.datetime.now().strftime('%h:%m:%s')})\")\n", " print(\"=========================\")\n", "\n", " # [classes:checks] executing ingestion logic: execute quality checks landing-phase\n", " landing_dataframe_class_instance:object = self.create_dataframe_instance(source_path=self.landing_path, file_kind=self.task.file_kind, skip_lines=self.task.skip_first_lines)\n", " landing_dataframe_class_instance.start_checks(phase=\"landing\", parameters=self.checks_parameters)\n", "\n", " # [dataframes_v2:dataframeclass] add technical columns dataframe\n", " landing_dataframe_class_instance.add_logging_columns_to_dataframe(task_id=self.task.task_id, plan_id=self.task.plan_id, file_id=self.file_id, file_name=self.file_name)\n", "\n", " # [dataframes_v2:dataframeclass]write dataframe raw_path overwrite already existing files path\n", " landing_dataframe_class_instance.write_dataframe(write_format=\"parquet\", write_mode=\"overwrite\", destination_path=self.raw_path)\n", " raw_amount=landing_dataframe_class_instance.dataframe.count()\n", " # [dataframes_v2] uncache dataframe\n", " landing_dataframe_class_instance.unpersist_dataframe()\n", " raw_message=create_json_object(path=self.raw_path,raw_amount=raw_amount)\n", " # [classes:file] update logging tables: \n", " self.update_file_activity(activity=\"raw\", success=true, info_message=raw_message)\n", "\n", "\n", " def raw_to_silver(self):\n", " print(\"=========================\")\n", " print(f\"[classes:file]: execute raw_to_silver: file={self.file_name} ({datetime.datetime.now().strftime('%h:%m:%s')})\")\n", " print(\"=========================\")\n", "\n", " # [classes:file] get dictionary matches primary key columns delta table (sink) raw data file (source) \n", " pk_column_dict:dict = self.get_primary_key_dictionary()\n", " self.checks_parameters[\"primary_key_columns\"] = list(pk_column_dict.values())\n", "\n", " # [classes:checks] load raw data file execute raw-phase checks using checks_object instance\n", " raw_dataframe_class_instance:object = self.create_dataframe_instance(source_path=self.raw_path, file_kind=\"parquet\", source_kind=self.task.file_kind)\n", " raw_dataframe_class_instance.start_checks(phase=\"raw\", parameters=self.checks_parameters)\n", " rawdata = raw_dataframe_class_instance.dataframe\n", " \n", " # load delta table raw data file needs merged into\n", " delta_table_class = deltatableclass_v2(debug=self.debug)\n", " deltatable = delta_table_class.load_delta_table(source_path=self.silver_path)\n", "\n", " # [ingestionfunctions] ingest raw data file delta table\n", " mergefile(deltatable=deltatable, rawdata=rawdata, pk_columns_dict=pk_column_dict, column_names_dict=self.target_source_columnnames, target_options=self.task.target_options)\n", " # dev-note: temporary solution still inserting empty files\n", " history = deltatable.history().filter(\"operation = 'merge'\").select(\"operationmetrics.numtargetrowsinserted\")\n", " history.isempty(): \n", " num_inserted_rows=0\n", " num_updated_rows=0\n", " num_sourcefile_rows=0\n", " else :\n", " num_inserted_rows,num_updated_rows,num_sourcefile_rows=delta_table_class.count_delta_table()\n", " self.debug : print (f'the amount inserted row {num_inserted_rows}, amount updated row {num_updated_rows}, amount row {num_sourcefile_rows}')\n", " # [dataframes_v2] uncache dataframe\n", " raw_dataframe_class_instance.unpersist_dataframe()\n", " silver_message=create_json_object(path=self.silver_path,row_inserted=num_inserted_rows,row_updated=num_updated_rows,source_row=num_sourcefile_rows)\n", " \n", " # [classes:file] update logging tables:\n", " self.update_file_activity(activity=\"silver\", success=true, info_message=silver_message)\n", " \n", " def landing_to_archive(self):\n", " # move file landing archive\n", " self.debug: print(\"moving file\", self.landing_path, self.archive_path)\n", " mssparkutils.fs.mv(src=self.landing_path, dest=self.archive_path, create_path=true, overwrite=false)\n", " #remove empty folders file called finished\n", " self.debug: print(\"remove dropfolder\", self.landing_path)\n", " remove_dropfolder(self.landing_path,true)\n", " # [classes:file] update logging tables: \n", " self.update_file_activity(activity=\"archive\", success=true, info_message=self.archive_path)" ], "execution_count": 5 } ] } }</file><file name="src\synapse\studio\notebook\CleanWorkspace.json">{ "name": "cleanworkspace", "properties": { "folder": { "name": "functions" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "599a7d0d-33c3-4eb9-99a4-fba188d958f0" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# clean workspace" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## code" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def clean_containers(env_code: str, containers: list = ['landing', 'silver', 'raw', 'archive', 'logs']):\r\n", " \"\"\"\r\n", " recursively clean set containers specific storage account\r\n", "\r\n", " parameters:\r\n", " env_code (str): environment storage account located. used form name storage account: {env_code}dapstdala1. following allowed: ['dev', 'int']\r\n", " containers (list): list containers need cleaned. following allowed: ['landing', 'silver', 'raw', 'archive', 'logs']\r\n", " \"\"\"\r\n", "\r\n", " # variables: define list expected/allowed containers environments\r\n", " expected_containers = ['landing', 'silver', 'raw', 'archive', 'logs']\r\n", " expected_environments = ['dev', 'int']\r\n", "\r\n", " # validate parameters env_code containers contained allowed lists\r\n", " (env_code expected_environments):\r\n", " raise valueerror(f'not allowed clean containers environment {env_code}. dev int cleanups allowed')\r\n", " set(containers).issubset(expected_containers):\r\n", " unexpected_containers = set(containers) - set(expected_containers)\r\n", " raise valueerror(f'list containers ({containers}) contains one unexpected containers: {list(unexpected_containers)}')\r\n", "\r\n", " # tests passed: loop containers clean recursively\r\n", " container containers:\r\n", " mssparkutils.fs.rm(dir=f'abfss://{container}@{env_code}dapstdala1.dfs.core.windows.net/', recurse=true)" ], "execution_count": 50 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def clean_delta_table(env_code: str, delta_lake: str, table_name: str):\r\n", " \"\"\"\r\n", " remove delta table delta lake \r\n", "\r\n", " parameters:\r\n", " env_code (str): environment delta lake located. used form name delta lake: {env_code}dapstdala1. following allowed: ['dev', 'int']\r\n", " delta_lake (list): name delta lake \"database\". following allowed: silver\r\n", " table_name (str): name table delete \r\n", " \"\"\"\r\n", "\r\n", " # variables: define list expected/allowed containers environments\r\n", " expected_deltalake = ['silver', 'unittest']\r\n", " expected_environments = ['dev', 'int']\r\n", "\r\n", " # validate parameters env_code delta_lake contained allowed lists\r\n", " (env_code expected_environments):\r\n", " raise valueerror(f'not allowed clean containers environment {env_code}. dev int cleanups allowed')\r\n", "\r\n", " delta_lake expected_deltalake:\r\n", " raise valueerror(f\"only allowed clean delta lake 'silver', {delta_lake}\")\r\n", "\r\n", " # try remove delta table recursively removing files\r\n", " try:\r\n", " mssparkutils.fs.rm(dir=f'abfss://{delta_lake}@{env_code}dapstdala1.dfs.core.windows.net/{table_name}', recurse=true)\r\n", " \r\n", " # catch error: error contains 'pathnotfoundexception' -&gt; delta table exist, need throw error\r\n", " except exception exception:\r\n", " 'pathnotfoundexception' str(exception):\r\n", " print(\"delta table exist\")\r\n", " else:\r\n", " raise f\"clean_delta_table got unexpected exectption: {exception}\"" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def clean_table(table_name:str):\r\n", " \"\"\"\r\n", " remove external table lake database\r\n", "\r\n", " parameters:\r\n", " table_name (str): name table delete \r\n", "\r\n", " note:\r\n", " remove reference underlying delta table, contents delta table itself\r\n", " \"\"\"\r\n", " \r\n", " spark.sql(\"drop table exists silver.\"+table_name)" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def clean_folder(path):\r\n", " try:\r\n", " mssparkutils.fs.rm(path, recurse=false)\r\n", " except exception exception:\r\n", " raise exception(f\"the path could deleted {path}\") " ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\Dataframes.json">{ "name": "dataframes", "properties": { "description": "a collection classes contain logic alter structure dataframe", "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "d2777023-c64a-4d38-804c-2aa830aea1d9" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "from pyspark.sql.functions import lit, col\r\n", "from pyspark.sql import dataframe\r\n", "import datetime\r\n", "import delta.tables dt\r\n", "import delta\r\n", "import pyspark\r\n", "from delta.tables import deltaoptimizebuilder, deltatable" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 15 }, { "cell_type": "code", "source": [ "class calculatedfieldheaders:\r\n", " def __init__(self: object, path: str, source_folder: str, destination_folder:str, hasheader: bool, column_delimiter: str, file_extension: str, columns: list, start_index: int, end_index: int = none):\r\n", " self.path = path\r\n", " self.preprocess_path = path.replace(source_folder, destination_folder)\r\n", " \r\n", " self.header = hasheader\r\n", " self.column_delimiter = column_delimiter\r\n", " self.file_extension = file_extension\r\n", "\r\n", " self.columns = columns\r\n", " \r\n", " self.start_index = start_index\r\n", " self.end_index = end_index\r\n", " \r\n", " # execute checks creating object\r\n", " self.object_checks()\r\n", "\r\n", " def object_checks(self):\r\n", " # checks allowing full init calculatedfieldheaders class-object\r\n", " # dev-note: currently, expect \"false\", future also possible \"true\"-case\r\n", " # dev-note: so, needs updated future\r\n", " self.header != \"false\":\r\n", " # header can't anything except 'false'. else whole logic fail\r\n", " raise exception(f\"calculatedfieldheaders: dataframe header option set '{self.header}' '{self.path}'. expected: 'false'\")\r\n", "\r\n", " self.end_index:\r\n", " # end index provided (end_index == none), set end_index == last index column list\r\n", " self.end_index = len(self.columns) - 1\r\n", " elif self.end_index &gt; (len(self.columns) - 1):\r\n", " # end_index cannot bigger last index configured columns\r\n", " raise indexerror(f\"calculatedfieldheaders: end index '{self.end_index}' select secondary columns cannot higher last index configured columns '{len(self.columns) - 1}'\")\r\n", " elif self.end_index &lt; self.start_index:\r\n", " # end_index cannot smaller starting index, avoid confusion (-1 would return second last column, tricky people understand python works)\r\n", " # also: working negative indexes break logic, keep 'em positive (outofbounds errors, potentially)\r\n", " raise indexerror(f\"calculatedfieldheaders: end index '{self.end_index}' cannot smaller starting index '{self.start_index}'\")\r\n", "\r\n", " self.start_index &lt; 0:\r\n", " # start index cannot smaller 0 (don't start looking back columns)\r\n", " raise indexerror(f\"calculatedfieldheaders: start index '{self.start_index}' select secondary columns cannot lower 0\")\r\n", "\r\n", " def convertfirstrowtocols(self: object):\r\n", " ## method extract first row data dataframe, adds extra column dataframe\r\n", " ## columns value row, found first row original dataframe\r\n", "\r\n", " # create dataframe going work with\r\n", " df = spark.read.load(self.path, header=self.header, sep=self.column_delimiter, format=self.file_extension)\r\n", "\r\n", " # see many primary columns / need\r\n", " ## first get count secondary columns, know much left (= primary columns)\r\n", " count_secondary_columns = len(self.columns[self.start_index:self.end_index+1]) # get count primary columns (add +1, since excl number)\r\n", " secondary_columns = self.columns[-count_secondary_columns:] # get secondary columns (= extra columns going add)\r\n", "\r\n", " ## then, know many secondary columns have, extract primary columns\r\n", " ### primary columns count always amount columns + 1, (e.g. 'count_primary_columns 5, take first 4 columns)\r\n", " count_primary_columns = len(self.columns) - count_secondary_columns # based count secondary colums, know many 'primary columns' expect\r\n", " df = df.select(df.columns[0:count_primary_columns]) # get primary columns dataset (makes sure trailing column delimiters filtered out)\r\n", " \r\n", " primary_columns = self.columns[0:count_primary_columns] # then, take primary columns full list &amp; add df (right sequence config files needed)\r\n", " # count_primary_columns = len(df.columns) # see many 'primary columns' dataframe (= amount columns adding extra ones via function)\r\n", " df = df.todf(*primary_columns) # header == 'false', need paste headers dataframe\r\n", "\r\n", " # print(df.columns)\r\n", " # print(count_secondary_columns)\r\n", " # print(len(self.columns))\r\n", "\r\n", " # map extra headers first row data\r\n", " ## extract first row data (end_index +1, since number excl index)\r\n", " first_row_data = list(df.collect()[0][self.start_index:self.end_index+1])\r\n", " \r\n", " ## then, save dictionary\r\n", " mapped_headers = dict(zip(secondary_columns, first_row_data))\r\n", "\r\n", " ## now, add dataframe new column value available rows\r\n", " key list(mapped_headers.keys()):\r\n", " df = df.withcolumn(key, lit(mapped_headers[key]))\r\n", " \r\n", " ## dynamically remove first row dataframe (so re-use function)\r\n", " ## 0 -&gt; first row case\r\n", " ## found in: ingestionfunctions\r\n", " df = removerowsdf(df, [0])\r\n", "\r\n", " # save dataframe (after row conversion)\r\n", " self.savedataframe(df)\r\n", " \r\n", " df.show()\r\n", "\r\n", " def savedataframe(self, dataframe: dataframe):\r\n", " # dev-note: seems really great ingestion function --&gt; oops\r\n", " # dev-note: pull away class put ingestionfunction\r\n", " # write whole dataset storage account\r\n", " ## coalesce(1) -&gt; make sure written one file, use format &amp; delimiter original dataset, save inside folder name file\r\n", " # print(\"dataframe inside function:\")\r\n", " dataframe.coalesce(1).write.format(self.file_extension).option(\"delimiter\", f\"{self.column_delimiter}\").save(f'{remove_extension(self.preprocess_path)}')\r\n", " \r\n", " # now, file saved folder name original file (without extension)\r\n", " # inside folder, part-... file self.file_extension, rename one\r\n", " # can't immediately give file proper name, loop it\r\n", " ## get contents folder written to\r\n", " ## contain '__success' 'part-...' file -&gt; last one trying rename\r\n", " saved_files, saved_folders = list_directory_content(remove_extension(self.preprocess_path), list(), list())\r\n", "\r\n", " ## go contents\r\n", " sub_file saved_files:\r\n", " # file right extension, go rename it\r\n", " sub_file.isfile sub_file.name.endswith(f'.{self.file_extension}'):\r\n", " # first, get name parent folder file in, original folder name\r\n", " original_file_name = get_parent_folder(f'{remove_extension(self.preprocess_path)}/{sub_file.name}')\r\n", "\r\n", " # then, overwrite old path (1) new path (2)\r\n", " mssparkutils.fs.mv(sub_file.path, f'{remove_extension(self.preprocess_path)}/{original_file_name}.{self.file_extension}', false)\r\n", " else:\r\n", " # remove '__success' file\r\n", " mssparkutils.fs.rm(sub_file.path, false)\r\n", "\r\n", "# df_object = calculatedfieldheaders(path=\"abfss://unittest@devdapstdala1.dfs.core.windows.net/finwin/prd/timestamp/mock_firstrow_to_headers_trailing_sep.fil\", source_folder='finwin', destination_folder='finwin/preprocess', hasheader=\"false\", column_delimiter=\"\\t\", file_extension=\"csv\", columns=[\"fk_connection\", \"fk_game\", \"cat_class\", \"nb_payment\", \"amt_payment\", \"x_dt_extract\", \"x_dt_transaction\", \"drawnumber\"], start_index=1, end_index=3)\r\n", "# df_object.convertfirstrowtocols()" ], "execution_count": 18 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (\"unittest\", \"devdapstdala1\", \"modules_dataframes\")\r\n", "# test = filter_directory_content(path, \"*\")\r\n", "# # print(test)\r\n", "\r\n", "# df = spark.read.load(\"abfss://unittest@devdapstdala1.dfs.core.windows.net/modules_dataframes/mock_firstrow_to_headers_trailing_sep.fil\", header=\"false\", sep='\\t', format=\"csv\")\r\n", "# columns = [\"fk_connection\", \"fk_game\", \"cat_class\", \"nb_payment\", \"amt_payment\", \"x_dt_extract\", \"x_dt_transaction\", \"drawnumber\"]\r\n", "# secondary_columns_len = len(columns[1:3+1])\r\n", "# secondary_columns = columns[-secondary_columns_len:]\r\n", "# # print(secondary_columns)\r\n", "# # print(columns[1:3+1])\r\n", "\r\n", "# primary_cols_count = len(columns) - secondary_columns_len\r\n", "# # print(primary_cols_count)\r\n", "\r\n", "# df = df.select(df.columns[0:primary_cols_count])\r\n", "# df = df.todf(*columns[0:primary_cols_count])\r\n", "\r\n", "# # print(primary_cols_count)\r\n", "# # print(len(df.columns))\r\n", "# # print(secondary_columns_len)\r\n", "# df.show()\r\n", "# # df.select(df.columns[:5]).show()" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class dataframeclass:\r\n", " # define set arguments class-object\r\n", " def __init__(self, dataframe):\r\n", " self.dataframe = dataframe\r\n", " \r\n", " # add set literal columns dataframe\r\n", " # columns expected part delta tables improve tracking logging\r\n", " def add_logging_columns(self, task_id, plan_id, file_id, file_name):\r\n", "\r\n", " # get current datetime -&gt; datetime given load_date-column\r\n", " current_time = datetime.datetime.now()\r\n", "\r\n", " # add literal columns dataframe: load_date, plan_id, task_id, file_id\r\n", " self.dataframe = (self.dataframe\r\n", " .withcolumn('t_load_date_raw', lit(current_time).cast(sql.types.timestamptype()))\r\n", " .withcolumn('t_load_date_silver', lit(none).cast(sql.types.timestamptype()))\r\n", " .withcolumn('t_plan_id', lit(plan_id).cast(sql.types.integertype()))\r\n", " .withcolumn('t_task_id', lit(task_id).cast(sql.types.integertype()))\r\n", " .withcolumn('t_file_id', lit(file_id).cast(sql.types.integertype()))\r\n", " .withcolumn('t_file_name', lit(file_name).cast(sql.types.stringtype()))\r\n", " )\r\n", "\r\n", " # return enhanced dataframe\r\n", " return self.dataframe\r\n", "" ], "execution_count": 11 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class deltatableoptimizeclass:\r\n", " # class defined methods related deltatable-modifications\r\n", " # example: optimizations\r\n", " def __init__(self:object, table_name:str, env_code:str, debug:bool=false) -&gt; pyspark.sql.dataframe.dataframe :\r\n", " debug: print(f\"[dataframes:deltatableoptimizeclass] initialize object {table_name} environment {env_code}\")\r\n", " self.table_name = table_name\r\n", " self.delta_table_path = f'abfss://silver@{env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\r\n", " \r\n", " # load delta table silver-container\r\n", " self.delta_table = dt.deltatable.forpath(spark, self.delta_table_path)\r\n", " \r\n", " self.debug=debug\r\n", "\r\n", " def optimize_deltatable(self):\r\n", " # execute .optimize() method delta table\r\n", " # create deltaoptimizebuilder-object instance, used optimization methods\r\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass]: optimization object {self.table_name}...\")\r\n", " self.delta_table_optimized = self.delta_table.optimize()\r\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass]: optimized delta table {self.delta_table_optimized}...\")\r\n", "\r\n", "\r\n", " def compact_deltatable(self):\r\n", " # execute .executecompaction() method deltaoptimizebuilder-object\r\n", " # dev-info: method group together multiple smaller files one bigger file based limits set configuration\r\n", " # default limit spark session configuration 1gb.\r\n", " # executecompaction() use bin-packing method sort data files equal size, respecting 1gb limit\r\n", "\r\n", " # first: validate deltaoptimizebuilder-object exists class-member\r\n", " hasattr(self, \"delta_table_optimized\"):\r\n", " isinstance(self.delta_table_optimized, deltaoptimizebuilder):\r\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass]: compact delta table {self.table_name}...\")\r\n", " # execute compaction delta table\r\n", " compaction_metrics:object = self.delta_table_optimized.executecompaction()\r\n", " else:\r\n", " raise typeerror(\"[dataframes:deltatableoptimizeclass] delta_table_optimized member class deltaoptimizebuilder compaction\")\r\n", " \r\n", " else:\r\n", " raise valueerror(\"[dataframes:deltatableoptimizeclass] cannot execute compaction optimize object exist\")\r\n", "\r\n", " # print select set metrics give initial insight compaction\r\n", " self.debug: compaction_metrics.select(\"path\", \"metrics.numfilesadded\", \"metrics.numfilesremoved\").show(truncate=false)\r\n", "\r\n", " def z_order_deltatable(self, z_order_columns:list):\r\n", " # execute .executezorderby() method deltaoptimizebuilder-object\r\n", " # dev-info: method cluster set rows belong together within file\r\n", " # zorder provides clustering related data inside files (for specific partition, applicable) may contain multiple possible values given column.\r\n", " # also allow data skipping querying data, improving overall performance\r\n", "\r\n", " # first: validate deltaoptimizebuilder-object exists class-member\r\n", " hasattr(self, \"delta_table_optimized\"):\r\n", " isinstance(self.delta_table_optimized, deltaoptimizebuilder):\r\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass]: z-order delta table {self.table_name} column(s): {z_order_columns}...\")\r\n", " # execute z-order delta table\r\n", " self.delta_table_optimized.executezorderby(z_order_columns=z_order_columns)\r\n", " else:\r\n", " raise typeerror(\"[dataframes:deltatableoptimizeclass] delta_table_optimized member class deltaoptimizebuilder z-order\")\r\n", " \r\n", " else:\r\n", " raise valueerror(\"[dataframes:deltatableoptimizeclass] cannot execute z-order optimize object exist\")\r\n", "\r\n", "\r\n", " def optimize_table_storage(self, z_order_columns:list=none):\r\n", " # orchestrator function create deltaoptimizebuilder-object execute optimization methods (compaction z-order) object\r\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass] optimize table storage...\")\r\n", "\r\n", " # create deltaoptimizebuilder-object\r\n", " self.optimize_deltatable()\r\n", " # execute compaction object\r\n", " self.compact_deltatable()\r\n", "\r\n", " # list columns given, make sure also execute executezorderby-method\r\n", " z_order_columns:\r\n", " self.z_order_deltatable(z_order_columns)\r\n", "\r\n", " def vacuum_deltatable(self, retention_period:int=168):\r\n", " # remove files used anymore\r\n", " # often related files compacted larger files\r\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass] vacuum table storage...\")\r\n", "\r\n", " # dev-note: set \"spark.databricks.delta.vacuum.paralleldelete.enabled\" \"true\" vacuum takes much time\r\n", " self.delta_table.vacuum(retention_period)\r\n", "\r\n", " # print select set metrics give initial insight vacuum\r\n", " # self.debug: print(f\"[dataframes:deltatableoptimizeclass]: vacuum metrics:\")\r\n", " # self.debug: vacuum_metrics.select(\"numdeletedfiles\", \"numvacuumeddirectories\", \"numfilestodelete\").show(truncate=false)" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# set configuration property\r\n", "# spark.conf.set(\"spark.databricks.delta.optimize.maxfilesize\", \"50m\") # default = 1gb\r\n", "# spark.conf.set(\"spark.databricks.delta.optimizewrite.enabled\", true) # -&gt; writing \r\n", "# spark.conf.set(\"spark.databricks.delta.autocompact.enabled\", true) # -&gt; writing\r\n", "# spark.conf.set(\"spark.databricks.delta.retentiondurationcheck.enabled\", false)\r\n", "# delta_table.optimize().executecompaction()" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "optimized write combines many small writes partition one larger write operation. optimization performed data written delta table.\r\n", "\r\n", "auto compaction combines many small files larger, efficient files. optimization performed data written delta table. may need perform vacuum operation afterwards clean remaining small files." ] } ] } }</file><file name="src\synapse\studio\notebook\Dataframes_v2.json">{ "name": "dataframes_v2", "properties": { "folder": { "name": "modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "618bbc52-3f56-450a-8d63-00e2e7729465" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# dataframes_v2\n", "this second version dataframes notebook.\n", "the notebook contains 2 classes: sparkdataframeclass sparkdataframechecks.\n", "\n", "- sparkdataframeclass: class contains set methods used create, load, manipulate, write, etc. objects class pyspark.sql.dataframes\n", "- sparkdataframechecks: class contains set methods used apply quality checks objects class pyspark.sql.dataframes\n", "\n", "**note**: creating object class sparkdataframechecks, object class sparkdataframeclass initialized well, inherits methods latter." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import python libraries" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "from pyspark.sql.types import structtype, structfield, arraytype, binarytype, booleantype, bytetype, datetype, decimaltype, floattype, integertype, longtype, stringtype, timestamptype\n", "import pyspark.sql\n", "from pyspark.sql.functions import lit, col, when, to_timestamp, to_date, monotonically_increasing_id, regexp_replace, from_unixtime\n", "from pyspark.sql import dataframe\n", "\n", "import datetime\n", "import re" ], "execution_count": 1 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" notebooks referenced class-methods" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 49 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/genericfunctions" ], "execution_count": 50 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## class definitions" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class sparkdataframeclass():\n", " \"\"\"\n", " class contain set methods used load, write, create, manipulate, etc. dataframes class pyspark.sql.dataframe\n", " intended use: \n", " * every time new functionality needs added ingestion framework related dataframe manipulation, added method class\n", " * every reference spark dataframe initialized using class, dataframe-object able call methods defined class\n", " \"\"\"\n", "\n", " # defining class object, debug-attribute defined class\n", " # depending situation, load_dataframe create_dataframe used initialise self.dataframe attribute class-instance\n", " def __init__(self, debug:bool=false):\n", " self.debug:bool = debug\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------ dataframe definition --------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " # given source_path: load file dataframe-object\n", " def load_dataframe(self, source_path:str, header:bool, separator:str, file_kind:str, skip_lines:int=none, escape_character:str='\\\\', quote_character:str='\"') -&gt; pyspark.sql.dataframe:\n", " \"\"\"\n", " load source file spark session dataframe\n", "\n", " :param source_path: full abfss-path source file\n", " :param header: boolean value indicating whether source file headerline \n", " :param separator: column delimiter used source file\n", " :param file_kind: kind file loaded (csv, json, parquet, etc)\n", " :param skip_lines: integer indicating number lines need skipped beginning file\n", "\n", " :return dataframe: instance class pyspark.sql.dataframe\n", " \"\"\"\n", "\n", " self.debug: print(\"[dataframes_v2:sparkdataframeclass] loading dataframe file...\")\n", "\n", " skip_lines:\n", " self.debug: print(\"[dataframes_v2:sparkdataframeclass] executing skip_line dataframe load...\")\n", " # [ingestionfunctions]: return dataframe first x lines skipped\n", " self.dataframe:object = skip_first_lines(landing_path=source_path, delimiter=separator, header=header, skiplines=skip_lines)\n", "\n", " elif file_kind == 'json':\n", " self.debug: print(\"[dataframes_v2:sparkdataframeclass] executing json dataframe load...\")\n", " # self.dataframe:object = spark.read.option(\"multiline\", \"true\").json(path=source_path, linesep=separator)\n", " # default: file_kind == json -&gt; use multiline option load file\n", " self.dataframe:object = spark.read.json(path=source_path, linesep=separator, multiline=true)\n", "\n", " else:\n", " self.debug: print(\"[dataframes_v2:sparkdataframeclass] executing standard dataframe load...\")\n", " # load entire dataframe without preliminary changes\n", " # dev-info: load using cache()\n", " # spark uses concept delayed execution. executing \"read\", actually executed action needs taken (ingest, write, etc)\n", " # cache() materialises dataframe load instead using delayed execution.\n", " # done column-specific actions cause issues execution.\n", " # example: string-column starts comma-separator preceeding column empty, comma interpreted new column: \"\", \", text\" -&gt; \"\", \"\", \"text\"\n", " self.dataframe:object = spark.read.load(path=source_path, header=header, sep=separator, format=file_kind, quote=quote_character, escape=escape_character).cache()\n", "\n", " self.debug: self.dataframe.show(10)\n", " return self.dataframe\n", "\n", " @staticmethod\n", " def load_dataframe_with_schema( \n", " source_path:str,\n", " header:bool,\n", " schema:structtype,\n", " column_selection:list,\n", " separator:str=',', \n", " escape_character:str='\\\\', \n", " quote_character:str='\"',\n", " locale:str='en-us'\n", " ):\n", "\n", "\n", " # dev-note: locale used importing csv files. since transferable file formats,\n", " # decision made use function current format.\n", " # code kept code-base, might use function future\n", " # function unit tested (test_dataframes_v2), current set-up\n", " # integrate function framework, additional try-except blocks introduced make sure function\n", " # executed csv files (and potential fitting file formats (to discovered)). \n", " file_kind = source_path.rsplit('.', 1)[1]\n", " file_kind == 'csv':\n", "\n", " corrupt_records_column =\"_corrupt_record\"\n", " column_selection += [corrupt_records_column]\n", " structfield = structfield(name=corrupt_records_column, datatype=stringtype(), nullable=true, metadata={corrupt_records_column: f\"gathering records data-casting went wrong\"})\n", " schema = schema.add(structfield)\n", " formatted_dataframe = spark.read.load(path=source_path, \n", " schema=schema, \n", " header=header, \n", " sep=separator, \n", " format=file_kind, \n", " quote=quote_character, \n", " escape=escape_character,\n", " columnnameofcorruptrecord=corrupt_records_column,\n", " locale=locale\n", " ).select(column_selection)\n", "\n", " return formatted_dataframe\n", "\n", " else:\n", " raise attributeerror(\"[dataframes_v2:sparkdataframeclass:loaddataframewithschema] parquet files cannot reloaded schema support locale versions.\")\n", "\n", "\n", " # given list column-objects: create dataframe using attributes defined object\n", " def create_dataframe(self:object, column_information:list, table_description:str=none) -&gt; pyspark.sql.dataframe:\n", " self.debug: print(\"[dataframes_v2:sparkdataframeclass] creating new dataframe...\")\n", "\n", " \"\"\"\n", " parse list column_objects convert empty dataframe-object defined schema\n", " \n", " :param column_information: list objects containing information metadata column.\n", " :param table_description: description data table \n", " \n", " :return dataframe: object class pyspark.sql.dataframe \n", "\n", " :example column_information\n", " structure: { \"column_sequence\": x, \"column_name\": &lt;name column&gt;, \"dimension\": &lt;pk,scd2, etc.&gt;, \"data_type\": &lt;string, integer, varchar(x), etc.&gt;}\n", " example: { \"column_sequence\": 1, \"column_name\": \"firstname\", \"dimension\": \"pk\", \"data_type\": \"varchar(100)\"}\n", " \"\"\"\n", "\n", "\n", " # [dataframes_v2:sparkdataframeclass]: create list structfield-objects, used define schema dataframe\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] create structfield objects column information\")\n", " structfield_objects:list = self.create_structfield_objects(column_information=column_information)\n", "\n", " # [dataframes_v2:sparkdataframeclass]: add technical fields type structfield dataframe \n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] add technical fields structfield-objects dataframe-schema\")\n", " structfield_objects:list = self.add_logging_columns_to_schema(structfields=structfield_objects)\n", "\n", " # create empty dataframe class pyspark.sql.dataframe\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] list structfield objects defining dataframe schema: {structfield_objects}...\")\n", " self.dataframe = spark.createdataframe(data=[], schema=structtype(structfield_objects)) \n", " self.debug: self.dataframe.show()\n", "\n", "\n", " def unpersist_dataframe(self):\n", " \"\"\"\n", " remove dataframe cache\n", " \"\"\"\n", " hasattr(self, 'dataframe'):\n", " self.dataframe.unpersist()\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------- write dataframe manipulations ----------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", "\n", " def write_dataframe(self, write_format:str, write_mode:str, destination_path:str, partitioning_columns:list=none, column_delimiter:str=',', merge_schema:bool=false):\n", " \"\"\"\n", " given in-memory dataframe: write dataframe specific location\n", "\n", " :param write_format: csv, json, delta, parquet, etc.\n", " :param write_mode: overwrite append\n", " :param destination_path: abfss-path location dataframe needs written to\n", " :param partitioning_columns: list column names partition on\n", " :param merge_schema: boolean, indicating whether schema dataframe needs merged schema destination_path\n", "\n", " :note partitioning merge_schema cannot true function. reasoning prevent much possible schema conflicts. \n", " merge_schema true, dataframe also allowed data. strict assumption prevent possible data overrides\n", " \"\"\"\n", "\n", " ((merge_schema) (write_mode!='append')):\n", " raise valueerror(\"[dataframes_v2:sparkdataframeclass] allowed mergeschema write_mode 'append'. overwrite entire dataframe\")\n", " ((merge_schema) (self.dataframe.count() != 0)):\n", "\n", " raise valueerror(\"[dataframes_v2:sparkdataframeclass] allowed mergeschema dataframe empty. strict data override prevention mode\")\n", "\n", " # check self.dataframe attribute exists\n", " hasattr(self, 'dataframe'):\n", " partitioning_columns:\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] writing partitioned dataframe {destination_path}\")\n", " # use partitionby-option write dataframe\n", " self.dataframe.write.format(write_format).mode(write_mode).partitionby(*partitioning_columns).save(destination_path)\n", " else:\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] writing non-partitioned dataframe {destination_path}\")\n", " self.dataframe.write.format(write_format).mode(write_mode).option(\"delimiter\", column_delimiter).option(\"mergeschema\", merge_schema).save(destination_path)\n", " else:\n", " raise attributeerror(f\"[sparkdataframeclass] dataframe attribute defined. cannot write dataframe destination {destination_path}. failing task...\")\n", "\n", "\n", " @staticmethod\n", " def rename_dataframe_file(source_path:str, file_extension:str):\n", " \"\"\"\n", " given source_path: rename file source_path name subfolder\n", "\n", " :param source_path: abfss-path folder contains files renamed\n", " :param file_extension: expected extension source file\n", "\n", " :note method works maximum 2 files folder, one \"_success\" file\n", " would multiple files, would get name possible course\n", " \"\"\"\n", " saved_files, saved_folders = list_directory_content(path=source_path, files=list(), folders=list())\n", " saved_file_names = [file.name file saved_files]\n", "\n", " len(saved_files) &gt; 2:\n", " raise valueerror(\"too many files destination path. cannot rename files path.\")\n", "\n", " '_success' saved_file_names:\n", " raise valueerror(\"no successfile present path. files writen spark therefore renamed.\")\n", "\n", " # rename file expected file_extension\n", " sub_file saved_files:\n", " sub_file.isfile sub_file.name.endswith(f'.{file_extension}'):\n", " # get name folder re-use name file\n", " subfolder_name = get_parent_folder(path=f'{source_path}/{sub_file.name}')\n", " # dev-note: overwrite = false -&gt; overwrite files dictionary (strictly necessary case)\n", " mssparkutils.fs.mv(src=sub_file.path, dest=f'{source_path}/{subfolder_name}.{file_extension}', create_path=false, overwrite=false)\n", " else:\n", " # remove '_success' file\n", " mssparkutils.fs.rm(dir=sub_file.path, recurse=false)\n", "\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------- technical field manipulations ----------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", " def add_logging_columns_to_dataframe(self:object, task_id:int, plan_id:int, file_id:int, file_name:str):\n", " \"\"\"\n", " given dataframe: add technical fields dataframe set literal values\n", "\n", " :params task_id: id task meta.log_tasks\n", " :params plan_id: id plan meta.log_plans\n", " :params file_id: id file meta.log_files\n", " :params file_name: name file dataframe coming from\n", " \"\"\"\n", "\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] add technical fields existing dataframe\")\n", "\n", " # check self.dataframe attribute exists\n", " hasattr(self, 'dataframe'):\n", " # get current datetime -&gt; datetime given t_load_date_raw-field\n", " current_time = datetime.datetime.now()\n", "\n", " # add technical fields literal values dataframe\n", " self.dataframe = (self.dataframe\n", " .withcolumn(colname='t_load_date_raw', col=lit(current_time).cast(timestamptype()))\n", " .withcolumn(colname='t_load_date_silver', col=lit(none).cast(timestamptype()))\n", " .withcolumn(colname='t_extract_date', col=lit(none).cast(timestamptype()))\n", " .withcolumn(colname='t_update_date', col=lit(none).cast(timestamptype()))\n", " .withcolumn(colname='t_insert_date', col=lit(none).cast(timestamptype()))\n", " .withcolumn(colname='t_plan_id', col=lit(plan_id).cast(integertype()))\n", " .withcolumn(colname='t_task_id', col=lit(task_id).cast(integertype()))\n", " .withcolumn(colname='t_file_id', col=lit(file_id).cast(integertype()))\n", " .withcolumn(colname='t_file_name', col=lit(file_name).cast(stringtype()))\n", " )\n", " else:\n", " raise attributeerror(\"[sparkdataframeclass] dataframe attribute defined. cannot add logging columns. failing task...\")\n", "\n", "\n", " @staticmethod\n", " def add_logging_columns_to_schema(structfields:list) -&gt; list:\n", " \"\"\"\n", " given list structfield-objects schema, add structfield-objects technical fields\n", "\n", " :param structfields: list instances class 'structfield', used define column spark dataframe\n", " \"\"\"\n", "\n", " structfields.append(structfield(name='t_load_date_raw', datatype=timestamptype(), nullable=true, metadata={'t_load_date_raw': f\"date row loaded raw layer table\"}))\n", " structfields.append(structfield(name='t_load_date_silver', datatype=timestamptype(), nullable=true, metadata={'t_load_date_silver': f\"date row loaded delta table\"}))\n", " structfields.append(structfield(name='t_extract_date', datatype=timestamptype(), nullable=true, metadata={'t_extract_date': f\"date row extracted source system\"}))\n", " structfields.append(structfield(name='t_update_date', datatype=timestamptype(), nullable=true, metadata={'t_update_date': f\"date row extracted source system\"}))\n", " structfields.append(structfield(name='t_insert_date', datatype=timestamptype(), nullable=true, metadata={'t_insert_date': f\"date row extracted source system\"}))\n", " structfields.append(structfield(name='t_file_name', datatype=stringtype(), nullable=true, metadata={'t_load_date': f\"name file data coming from\"}))\n", " structfields.append(structfield(name='t_plan_id', datatype=integertype(), nullable=false, metadata={'t_plan_id': f\"the id plan meta.log_plans table\"}))\n", " structfields.append(structfield(name='t_task_id', datatype=integertype(), nullable=false, metadata={'t_task_id': f\"the id task meta.log_tasks table\"}))\n", " structfields.append(structfield(name='t_file_id', datatype=integertype(), nullable=false, metadata={'t_file_id': f\"the id file meta.log_files table\"}))\n", "\n", " return structfields\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------- schema manipulations -------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", " def create_structfield_objects(self:object, column_information:list) -&gt; list:\n", " \"\"\"\n", " create list structfield-objects used initialize dataframe schema\n", "\n", " :param column_information: list objects containing information metadata column.\n", " :example\n", " structure: { \"column_sequence\": x, \"column_name\": &lt;name column&gt;, \"dimension\": &lt;pk,scd2, etc.&gt;, \"data_type\": &lt;string, integer, varchar(x), etc.&gt;}\n", " example: { \"column_sequence\": 1, \"column_name\": \"firstname\", \"dimension\": \"pk\", \"data_type\": \"varchar(100)\"}\n", "\n", " :result structfield_objects: list structfield objects\n", " :example\n", " structure: structfield(name, datatype, nullable, metadata)\n", " example: structfield(\"firstname\", stringtype(), true, {'__char_varchar_type_string': \"varchar(100)})\n", "\n", " :note\n", " dictionary (datatypes) initialized configure_datatypes() method. dictionary maps set datatypes spark datatype\n", " column_object reference datatype. reference varchar, additional metadata added structfield instance.\n", " pyspark not, default, set \"length\" limitation stringtype-datatypes. additional metadata forces constraints.\n", " \"\"\"\n", "\n", " # derive dictionary pyspark.sql.types functions match given datatypes\n", " # structure: { &lt;datatype_reference&gt;: &lt;pyspark_datatype_reference&gt; }\n", " # example: { 'string': pyspark.sql.type.stringtype(), 'integer': pyspark.sql.type.integertype()}\n", " table_data_types:list = [column_object['data_type'] column_object column_information]\n", " configured_datatypes:dict = self.configure_datatypes(table_data_types=table_data_types)\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] datatypes configured dataframe: {configured_datatypes}...\")\n", " \n", " \n", " # dev-note: for-loop: check objects valid -&gt; contain keys column_name, data_type, dimension\n", " structfield_objects = list()\n", " column_object column_information:\n", " \n", " # data_type column varchar, additional metadata needs added column-definition\n", " # dev-info: delta force \"limits\" varchar-datatypes converts everything string (which similar varchar(max)). \n", " # control data goes delta table, constraint added using '__char_varchar_type_string' option enforce constraints length values added column\n", " varchar_pattern = r'^varchar\\((\\d{1,2})\\)$'\n", " re.match(varchar_pattern, column_object['data_type']):\n", " structfield_objects.append(structfield(name=column_object['column_name'],datatype=configured_datatypes[column_object['data_type']],nullable=true, metadata={column_object['column_name']: f\"metadata column {column_object['column_name']}\", '__char_varchar_type_string': column_object['data_type']}))\n", " \n", " # special case: add column without additional metadata-cases\n", " else:\n", " structfield_objects.append(structfield(name=column_object['column_name'],datatype=configured_datatypes[column_object['data_type']],nullable=true, metadata={column_object['column_name']: f\"metadata column {column_object['column_name']}\"}))\n", "\n", " return structfield_objects\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------- datatype manipulations -----------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", " @staticmethod\n", " def add_varchar_datatype(varchar_value:str, datatypes:dict) -&gt; dict:\n", " \"\"\"\n", " add varchar-datatype datatypes dictionary match datatype stringtype()\n", "\n", " :param varchar_value: string-value looking like 'varchar(&lt;xx&gt;)' needs added datatypes dictionary\n", " :param datatypes: dictionary datatypes matching spark-datatype function\n", " :example\n", " {'string': stringtype(), 'int': integertype(), etc.}\n", "\n", " :note\n", " varchar(x), direct mapping available spark datatype\n", " combat this, varchar-column added stringtype additional constraint added columns\n", " function add varchar-datatype datatypes-dictionary {varchar(x): stringtype}\n", " \"\"\"\n", "\n", " # regex pattern: varchar expected look like 'varchar(&lt;xx&gt;)'\n", " pattern = r\"varchar\\(\\d+\\)\"\n", "\n", " # varchar(max) already part datatypes dictionary special case needs added -&gt; spark-datatype: stringtype()\n", " varchar_value == 'varchar(max)':\n", " return datatypes\n", "\n", " # check match regex\n", " elif re.match(pattern=pattern, string=varchar_value):\n", " # match, add value datatypes-dictionary stringtype() datatype\n", " # assignment, value 'varchar_value' added metadata column\n", " datatypes[varchar_value] = stringtype()\n", " return datatypes\n", "\n", " # also match: varchar-value configured incorrectly -&gt; error\n", " else:\n", " raise valueerror(f'the varchar value {varchar_value} valid datatype value cannot configured correctly. aborting delta table deployment...')\n", "\n", " \n", " @staticmethod\n", " def add_decimal_datatype(decimal_value:str, datatypes:dict) -&gt; dict:\n", " \"\"\"\n", " add decimal-datatype datatypes dictionary match datatype decimaltype(&lt;xx&gt;, &lt;yy&gt;)\n", "\n", " :param varchar_value: string-value looking like 'decimal(&lt;xx&gt;, &lt;yy&gt;)' needs added datatypes dictionary\n", " :param datatypes: dictionary datatypes matching spark-datatype function\n", " :example\n", " {'string': stringtype(), 'int': integertype(), etc.}\n", "\n", " :note\n", " instead foresee every possible instance decimaltype(&lt;xx&gt;, &lt;yy&gt;) datatypes, \n", " method add instances needed specific delta table source file\n", " \"\"\"\n", "\n", " # regex pattern: decimal expected look like 'decimal(&lt;xx&gt;,&lt;yy&gt;)'\n", " pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\n", "\n", " # first check decimal contains additional parameters\n", " # decimal already part datatypes dictionary special case needs added -&gt; spark-datatype: decimaltype(38,4)\n", " decimal_value == 'decimal':\n", " return datatypes\n", "\n", " # check match regex\n", " elif re.match(pattern=pattern, string=decimal_value):\n", " # match: extract numbers string\n", " # assign numerical values total number characters numbers past comma\n", " match = re.match(pattern=pattern, string=decimal_value)\n", " num_characters = int(match.group(1))\n", " num_post_comma = int(match.group(2))\n", "\n", " # dev-note: add check make sure num_characters exceed 38 pyspark cannot handle more, num_characters &gt; num_post_comma\n", " datatypes[decimal_value] = decimaltype(precision=num_characters, scale=num_post_comma)\n", " return datatypes\n", "\n", " # also match: decimal-value configured incorrectly -&gt; error\n", " else:\n", " raise valueerror(f'the decimal value {decimal_value} valid datatype value cannot configured correctly. aborting delta table deployment...')\n", "\n", "\n", " def configure_datatypes(self:object, table_data_types:list) -&gt; dict:\n", " \"\"\"\n", " parse dictionary table_data_types add relevant datatypes dictionary called datatypes\n", " \n", " :param table_data_types: list possible datatypes could occur delta table\n", "\n", " :result datatypes: dictionary matching string values spark datatype\n", " :example\n", " {'string': stringtype(), 'int': integertype(), etc.}\n", "\n", "\n", " \"\"\"\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] configure datatypes dictionary...\")\n", "\n", " # create dictionary data types read spark\n", " # keys dictionary values used metadata configuration files (sql database)\n", " # values dictionary pyspark.sql.types datatypes \n", " # dev-info: default definition used decimals is: 38 characters total, 4 comma. \n", " # specifics given column_info (eg. decimal(12,2)), new input added types-dictionary accomodate datatype\n", " # https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.decimaltype.html\n", "\n", " datatypes = {\n", " \"array\": arraytype(stringtype()), \"binary\": binarytype(), \"boolean\": booleantype(), \"date\": datetype(), \"string\": stringtype(), \n", " \"varchar(max)\": stringtype(), \"timestamp\": timestamptype(), \"decimal\": decimaltype(38, 4), \"float\": floattype(), \"byte\": bytetype(), \n", " \"integer\": integertype(), \"int\": integertype(), \"long_integer\": longtype()\n", " }\n", "\n", "\n", " # datatypes come additional (set of) parameters. function add datatypes initial types-dictionary defined above.\n", " data_type table_data_types:\n", " # data_type contains value substring 'decimal' -&gt; invoke add_decimal_datatype function\n", " 'decimal' data_type:\n", "\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] adding datatype {data_type} datatypes-dictionary\")\n", " datatypes = self.add_decimal_datatype(decimal_value=data_type, datatypes=datatypes)\n", "\n", " # data_type contains value substring 'varchar' -&gt; invoke add_varchar_datatype function\n", " elif 'varchar' data_type:\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] adding datatype {data_type} datatypes-dictionary\")\n", " datatypes = self.add_varchar_datatype(varchar_value=data_type, datatypes=datatypes)\n", "\n", " # throw error value part datatypes dictionary if-statement configured add dictionary\n", " elif data_type datatypes.keys():\n", " raise valueerror(f\"the datatype '{data_type}' configured datatype current set-up.\")\n", "\n", " return datatypes\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------- dataframe manipulations ----------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " \n", " @staticmethod\n", " def validate_column_indexes(start_index:int, end_index:int, column_names:list, header:bool):\n", " \"\"\"\n", " validate given argument values within set expected boundaries\n", "\n", " :param start_index: column index operations need start at\n", " :param end_index: column index operations need end \n", " :param column_names: list column names\n", " :param header: boolean indicating whether header present\n", "\n", " :note\n", " method specifically written validation method convert_firstrow_to_columns()\n", " generic changes added whenever would need case future\n", " \"\"\"\n", "\n", " # dev-note: currently, expect \"false\", future also possible \"true\"-case\n", " header != false:\n", " # header can't anything except 'false'. else whole logic fail\n", " raise exception(f\"[sparkdataframeclass]: dataframe header option set '{header}'. expected: 'false'\")\n", "\n", " end_index &gt; (len(column_names) - 1):\n", " # end_index cannot bigger last index configured columns\n", " raise indexerror(f\"[sparkdataframeclass]: end index '{end_index}' select secondary columns cannot higher last index configured columns '{len(column_names) - 1}'\")\n", " \n", " end_index &lt; start_index:\n", " # end_index cannot smaller starting index, avoid confusion (-1 would return second last column, tricky people understand python works)\n", " raise indexerror(f\"[sparkdataframeclass]: end index '{end_index}' cannot smaller starting index '{start_index}'\")\n", "\n", " start_index &lt; 0:\n", " # start index cannot smaller 0 (don't start looking back columns)\n", " raise indexerror(f\"[sparkdataframeclass]: start index '{start_index}' select secondary columns cannot lower 0\")\n", "\n", " end_index &lt; 0:\n", " # start index cannot smaller 0 (don't start looking back columns)\n", " raise indexerror(f\"[sparkdataframeclass]: end index '{end_index}' select secondary columns cannot lower 0\")\n", "\n", " # given dataframe: convert (some of) values first row columns literal values\n", " def convert_firstrow_to_columns(self:object, start_index:int, end_index:int, column_names:list, header:bool, destination_path:str, file_extension:str, column_delimiter:str):\n", " \"\"\"\n", " convert set values first row dataframe separate, literal columns \n", " use case: first column contains relevant information date id. keep information, values added row separate column (timestamp id case)\n", "\n", " :param start_index: column index start operation from\n", " :param end_index: column index end operation\n", " :param column_names: list column names delta table (so already including column names added well)\n", " :param header: boolean indicating whether header already present\n", " :param destionation_path: abfss-path write dataframe modifications\n", " :param file_extension: extension newly written file (should ideally match original source file avoid confusion)\n", " :param column_delimiter: delimiter newly written file (should ideally match original source file avoid confusion)\n", "\n", " :example:\n", " arguments: start_index = 1, end_index = 2, column_names = [firstname, lastname, address, date, employee_id], header=false\n", " \n", " original dataframe:\n", " 999 1900-01-01 pla917 xxx\n", " simon plancke \"rue belliard 25\"\n", "\n", " converted dataframe: \n", " firstname lastname address date employee_id\n", " simon plancke \"rue belliard 25\" 1900-01-01 pla917\n", " \"\"\"\n", "\n", " hasattr(self, 'dataframe'):\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] convert first row separate columns...\")\n", " # executing function: execute quality checks\n", " \n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] validate column indexes...\")\n", " self.validate_column_indexes(start_index=start_index, end_index=end_index, column_names=column_names, header=header)\n", "\n", " # dev-info: \n", " # primary = columns already present dataframe (without column names)\n", " # secondary = columns need added dataframe\n", "\n", " # list column_names, get yet dataframe\n", " count_secondary_columns = end_index - start_index + 1 # get count primary columns (add +1, since excl number)\n", " secondary_columns = column_names[-count_secondary_columns:] # get secondary columns (= extra columns going add)\n", "\n", " # list column_names, get already dataframe\n", " ## count_primary_columns always amount columns + 1, (e.g. 'count_primary_columns 5, take first 4 columns)\n", " count_primary_columns = len(column_names) - count_secondary_columns \n", " primary_columns = column_names[0:count_primary_columns]\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] primary columns: {primary_columns} ; count: {count_primary_columns}\")\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] secondary columns: {secondary_columns}; count: {count_secondary_columns}\")\n", "\n", "\n", " # create local dataframe contains primary columns add primary column names\n", " primary_column_dataframe = self.dataframe.select(self.dataframe.columns[0:count_primary_columns]) # create local dataframe primary columns\n", " primary_column_dataframe = primary_column_dataframe.todf(*primary_columns)\n", " \n", " # since first row contains values added dataframe, extract first row data\n", " first_row_data = list(primary_column_dataframe.collect()[0][start_index:end_index+1])\n", " \n", " # map names columns literal values need added: {column_name:literal_value}\n", " mapped_headers = dict(zip(secondary_columns, first_row_data))\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass] mapped headers: {mapped_headers}\")\n", "\n", " # add columns dataframe, row containing literal value\n", " key list(mapped_headers.keys()):\n", " primary_column_dataframe = primary_column_dataframe.withcolumn(key, lit(mapped_headers[key]))\n", " \n", " # remove first row dataframe -&gt; 0: first row case\n", " # [ingestionfunctions]\n", " self.dataframe = removerowsdf(primary_column_dataframe, [0])\n", "\n", "\n", " self.write_dataframe(write_format='csv', write_mode='overwrite', destination_path=remove_extension(string_with_extension=destination_path, extension_value=file_extension), column_delimiter=column_delimiter)\n", " self.rename_dataframe_file(source_path=remove_extension(string_with_extension=destination_path, extension_value=file_extension), file_extension='csv')\n", " else:\n", " raise attributeerror(\"[sparkdataframeclass] dataframe attribute defined. cannot convert first row columns. failing task...\")\n", "\n", " def replace_dataframe_columns(self, replacing_dataframe:dataframe, columns_to_replace:list):\n", " \"\"\"\n", " replace one columns self.dataframe columns another dataframe\n", "\n", " :param replacing_dataframe: dataframe contains columns replace self.dataframe columns by\n", " :param columns_to_repalce: list columns replaced\n", " \"\"\" \n", "\n", " # dev-note: function integrated ingestion framework need unit testing\n", " # 1. dataframes actually order rows, rows read different order?\n", " # current uncertainty question, function integrated.\n", " # would need function, improvements also made current functionality function\n", " # funtion unit tested deployments (test_dataframes_v2), tests improved.\n", "\n", " self.debug: self.dataframe.show()\n", " self.debug: replacing_dataframe.show()\n", " # check: dataframes need lenght\n", " row_count_df = self.dataframe.count()\n", " row_count_repl_df = replacing_dataframe.count()\n", "\n", " row_count_df != row_count_repl_df:\n", " raise valueerror(\"[dataframes_v2:sparkdataframeclass] number rows original dataframe match number rows replace dataframe\")\n", "\n", "\n", " # get current list columns\n", " column_list = self.dataframe.columns\n", "\n", " # add row_number dataframes\n", " self.dataframe = self.dataframe.withcolumn(\"row_num\", monotonically_increasing_id())\n", " replacing_dataframe = replacing_dataframe.withcolumn(\"row_num\", monotonically_increasing_id())\n", "\n", " # drop columns replace self.dataframe\n", " col_name columns_to_replace:\n", " self.dataframe = self.dataframe.drop(self.dataframe[col_name])\n", "\n", "\n", " # select subset columns replace replace_dataframe\n", " columns_to_replace += ['row_num']\n", " replacing_dataframe_subset = replacing_dataframe.select(columns_to_replace)\n", "\n", " # use join replace columns self.dataframe\n", " self.dataframe = self.dataframe.join(replacing_dataframe_subset, on='row_num', how='left').select(column_list)\n", "" ], "execution_count": 51 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class sparkdataframechecks(sparkdataframeclass):\n", "\n", " def __init__(self, source_path:str, header:bool, separator:str, file_kind:str, column_names:list, data_types:list, checks:list, column_information:list,\n", " skip_lines:int=none, debug:bool=false, deploy:bool=false, source_kind:str=none, escape_character:str='\\\\', quote_character:str='\"'):\n", " \"\"\"\n", " initialize sparkdataframechecks instance load dataframe object call deploy-stage\n", "\n", " :param source_path: abfss-path source file checked\n", " :param header: boolean value indicating whether source file contains headerline\n", " :param separator: column delimiter used source file\n", " :param file_kind: type file dealth (csv, json, parquet, etc.)\n", " :param column_names: list column names source file\n", " :param data_types: list datatypes columns\n", " :param checks: list data quality checks need done source file\n", " :param skip_lines: value indicating whether source file contains rows skipped dataframe load\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", " :param deploy: boolean indicating whether class called deploy-mode\n", " :param source_kind: checks raw data (parquet), original type file (csv, json, parquet, etc.)\n", " :param qoute_character: character used indicate string values source file\n", " :param escapce_character: character used escape special characters source file\n", "\n", " :note deploy-mode\n", " deploy argument indiciates whether instance load dataframe whether dataframe exists already.\n", " deploy -&gt; use arguments load source file dataframe\n", " deploy -&gt; set dataframe-argument creating class instance\n", "\n", " dev-note would possible call load_dataframe separatly __init__ function remove deploy-mode option\n", " \n", " \"\"\"\n", " self.debug = debug\n", "\n", " deploy:\n", " # [dataframes_v2:sparkdataframeclass] load dataframe source path\n", " self.dataframe = self.load_dataframe(\n", " source_path=source_path, \n", " header=header, \n", " separator=separator, \n", " file_kind=file_kind, \n", " skip_lines=skip_lines, \n", " escape_character=escape_character, \n", " quote_character=quote_character\n", " )\n", "\n", " # set class-member values\n", " self.source_path = source_path\n", " self.header:bool = header\n", " self.separator = separator\n", " self.file_kind = file_kind\n", " self.column_names:list = column_names\n", " self.data_types:list = data_types \n", " self.column_information:list = column_information\n", " self.checks:list = checks\n", " self.source_kind:str = source_kind\n", "\n", " # add mandatory checks self.checks list\n", " self.add_mandatory_checks()\n", " \n", "\n", " def add_mandatory_checks(self:object):\n", " \"\"\"\n", " add list mandatory checks self.checks member checks-class\n", " \"\"\"\n", "\n", " # list mandatory checks phases\n", " mandatory_checks = ['header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n", "\n", " # mandatory checks self.checks, add them\n", " mandatory_check mandatory_checks:\n", " mandatory_check self.checks:\n", " self.checks.append(mandatory_check)\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------ orchestrate check execution -------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", " def start_checks(self, phase:str, parameters:dict=none):\n", " \"\"\"\n", " orchestrator method: calls necessary methods execute checks\n", " \n", " :param phase: medallion architecture phase: landing, raw, silver -&gt; distinguish possible tests per phase\n", " :param parameters: optional parameters needed check-execution\n", "\n", " \"\"\"\n", "\n", " # given phase: return phase-specific list configured checks executed\n", " filtered_checks:dict = self.get_checks_for_phase(phase=phase)\n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks]: executing checks phase {phase}: {filtered_checks}\")\n", " # checks executed:\n", " (len(filtered_checks) &gt; 0):\n", "\n", " # executing checks:\n", " # parameters passed start_checks(), make sure parameter-keys valid\n", " parameters:\n", " self.validate_parameter_keys(parameters=parameters)\n", " \n", " # loop configured tests\n", " check filtered_checks: \n", " filtered_checks[check]()\n", " \n", " # checks execute\n", " else:\n", " self.debug: print(f\"[dataframes_v2:sparkdataframeclass]: checks executed phase {phase}.\")\n", " \n", " return 0\n", "\n", " # ------------------------------------------------------------- validate arguments passed start_checks() -------------------------------------------------------------\n", "\n", " def validate_phase(self, phase:str):\n", " \"\"\"\n", " validate phase-argument passed start_checks() valid\n", " \"\"\"\n", "\n", " # validate given value self.phase one expected/allowed values\n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks]: validate phase: {phase}\")\n", "\n", " # set list allowed values\n", " allowed_phases = ['landing', 'raw', 'silver']\n", "\n", " # [functions/genericfunctions] validate argument self.phase item allowed_phases list\n", " validate_argument('phase', phase, allowed_phases)\n", "\n", " def validate_parameter_keys(self, parameters: dict):\n", " \"\"\"\n", " validate parameters-argument passed start_checks() valid\n", " \"\"\"\n", " # dev-note: move dataconfig module avoid unexpected parameters configuration\n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks]: validate checks_parameters\")\n", "\n", " # set list allowed values\n", " allowed_parameters = ['landing_rows_expected', 'primary_key_columns']\n", " \n", " # [functions/genericfunctions] validate keys parameters-object allowed_parameters list\n", " validate_argument_list('check_parameters', list(parameters.keys()), allowed_parameters)\n", "\n", " # error thrown: set class-member self.parameters\n", " self.parameters:dict = parameters\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # ------------------------------------------------------------------------ prepare check execution -------------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " def get_checks_for_phase(self, phase:str):\n", " \"\"\"\n", " filter self.checks return checks specific phase\n", " \"\"\"\n", "\n", " # validate phase-argument expected argument\n", " self.validate_phase(phase=phase) # allowed phase-values: [landing, raw, silver]\n", "\n", " # phase landing: filter self.checks return possible checks landing\n", " phase == 'landing':\n", " possible_landing_checks = ['landing_rows']\n", " filtered_checks:list = self.filter_checks(possible_phase_checks=possible_landing_checks)\n", "\n", " # phase landing: filter self.checks return possible checks raw\n", " elif phase == 'raw':\n", " possible_raw_checks = ['header','default_replace', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n", " filtered_checks:list = self.filter_checks(possible_phase_checks=possible_raw_checks)\n", "\n", " # phase allowed, found if-structure, means tasks available (yet) phase\n", " else:\n", " raise attributeerror(f\"[dataframes_v2:sparkdataframeclass] checks implemented phase '{phase}'\")\n", "\n", " return filtered_checks\n", "\n", "\n", " def filter_checks(self, possible_phase_checks: list) -&gt; dict:\n", " \"\"\"\n", " return list checks corresponding method executed phase file\n", "\n", " :param possible_phase_checks: list checks could executed certain phase\n", " \"\"\"\n", "\n", " # map possible check-values corresponding function \n", " checks_functions = { \n", " 'header': self.check_dataframe_headers, \n", " 'default_replace': self.check_values_to_replace,\n", " 'data_type': self.check_dataframe_datatypes, \n", " 'landing_rows': self.check_minimum_rowcount,\n", " 'primary_keys': self.check_primary_keys,\n", " 'duplicate_primary_keys': self.check_duplicate_primary_keys \n", " }\n", " \n", " # filtered_checks contain function executed phase, corresponding function: {check_name: function_name}\n", " filtered_checks = dict()\n", "\n", " # loop list possible checks phase\n", " check_name possible_phase_checks:\n", " # check_name self.checks list, map check corresponding function filtered_checks-dictionary\n", " (check_name self.checks):\n", " try:\n", " filtered_checks[check_name] = checks_functions[check_name]\n", " except exception:\n", " # extraction checks_functions fails, available allowed key yet mapped method class\n", " raise keyerror(f\"check '{check_name}' mapped check method. check (yet) implemented.\")\n", "\n", " # return dictionary {check_name: function_name}\n", " return filtered_checks\n", " \n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", " # --------------------------------------------------------------------------------------- check functions ----------------------------------------------------------------\n", " # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " def check_minimum_rowcount(self):\n", " \"\"\"\n", " check dataframe contains minimal number rows\n", " \"\"\"\n", "\n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks]: check minimum row count {self.source_path}\")\n", "\n", " actual_amount_rows:int = self.dataframe.count()\n", " \n", " self.parameters.get(\"landing_rows_expected\") none:\n", " actual_amount_rows &lt; self.parameters[\"landing_rows_expected\"]:\n", " raise assertionerror(f\"[dataframes_v2:sparkdataframechecks] minimum rows {self.parameters['landing_rows_expected']}, got {actual_amount_rows}\")\n", " else:\n", " raise valueerror(\"[dataframes_v2:sparkdataframechecks] cannot check minimum rowcount without landing_rows_expected parameter\")\n", "\n", " return 0\n", "\n", " \n", " def check_dataframe_headers(self):\n", " \"\"\"\n", " headers = true: check configured column names match headers dataframe\n", " headers = false: add configured column names validate present first line dataframe\n", " \"\"\"\n", "\n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks]: check header-line {self.source_path} header={self.header}\")\n", "\n", " # header present:\n", " self.header == true:\n", " \n", " # column_names configuration match column names dataframe -&gt; continue\n", " self.column_names == self.dataframe.columns:\n", " return 0 ## unit test\n", " \n", " # column_names configuration match column names dataframe first column _c0 -&gt; continue\n", " # current configuration: allow first column likely index column column name\n", " # dev-note: might necessary add _c0 column configuration avoid potential future bugs use case\n", " elif self.column_names == self.dataframe.columns[1:] self.dataframe.columns[0] == '_c0':\n", " # dev-info: first column empty, named _c&lt;number&gt;, match configured names config files\n", " return 0 ## unit test\n", " \n", " # column_names configuration match column names dataframe, sequence -&gt; continue json; error other: configuration needs changed\n", " # dev-info: working json files, column names stored alphabethically since schema often fixed.\n", " elif sorted(self.column_names) == sorted(self.dataframe.columns):\n", " self.source_kind == 'json':\n", " return 0\n", " else:\n", " raise configurationerror(\n", " custom_message=f\"column sequence configuration ({self.column_names}) match column sequence dataframe ({self.dataframe.columns})\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_headers\"\n", " )\n", " # column_names configuration match column names dataframe, sequence first column _c0 -&gt; error: configuration needs changed\n", " elif sorted(self.column_names) == sorted(self.dataframe.columns[1:]) self.dataframe.columns[0] == '_c0':\n", " raise configurationerror(\n", " custom_message=f\"column sequence configuration ({self.column_names}) match column sequence dataframe ({self.dataframe.columns[1:]}) (case: index column _c0)\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_headers\"\n", " )\n", " # none previous cases match:\n", " else:\n", " # use self.column_information get list mandatory column names\n", " local_column_names:list = self.column_names\n", " local_column_information:list = self.column_information\n", " column_information_dict = dict(zip(local_column_names, local_column_information))\n", " self.debug: print(\"column information dictionary: \", column_information_dict)\n", "\n", " mandatory_columns = return_filtered_keys(column_information_dict, 'optional', reverse=true)\n", " # non-empty list returned\n", " self.debug: print(\"mandatory columns: \",mandatory_columns)\n", " self.debug: print(\"dataframe columns': \", self.dataframe.columns)\n", " self.debug: print(\"configured columns': \", local_column_names)\n", "\n", "\n", " mandatory_columns:\n", " # mandatory_list subset dataframe_columns: successful execution\n", " set(mandatory_columns).issubset(set(self.dataframe.columns)):\n", " set(self.dataframe.columns).issubset(set(local_column_names)):\n", " return 0\n", " else:\n", " unconfigured_columns = list(filter(lambda item: item local_column_names, self.dataframe.columns))\n", " raise configurationerror(\n", " custom_message=f\"the following columns found source file configuration: {unconfigured_columns}\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_headers\"\n", " )\n", " else:\n", " missing_mandatory_columns = list(filter(lambda item: item self.dataframe.columns, mandatory_columns))\n", " raise configurationerror(\n", " custom_message=f\"the following columns non-optional configuration found source file: {missing_mandatory_columns}\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_headers\"\n", " )\n", "\n", "\n", "\n", " # return happened yet: raise error\n", " raise configurationerror(\n", " custom_message=f\"header mismatch: actual ({self.dataframe.columns}) vs expected ({self.column_names}).\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_headers\"\n", " )\n", "\n", " # header present:\n", " elif self.header == false:\n", " # replace '_cx' headers actual header-line configuration\n", " self.dataframe = self.dataframe.todf(*self.column_names) ## unpack list *, include one column list column names\n", " \n", " # get first row dataframe\n", " first_row = self.dataframe.head(1)[0] # returns row-object\n", " first_row_values:list = list(first_row.asdict().values())\n", "\n", " # validate first row contain column names\n", " all(x self.column_names x first_row_values):\n", " raise configurationerror(\n", " custom_message=f\"file {self.source_path} expected contain header-line does\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_headers\"\n", " )\n", "\n", " else:\n", " return 0 ## unit test\n", "\n", " else:\n", " # dev-note: check datatype enforcement -&gt; need 'else' case function enforces boolean\n", " raise valueerror(f\"[dataframes_v2:sparkdataframechecks] invalid configuration header: {self.header}\")\n", "\n", "\n", " def check_primary_keys(self):\n", " \"\"\"\n", " validate primary keys null\n", " \"\"\"\n", " self.debug: print(\"[dataframes_v2:sparkdataframechecks] check primary keys null\")\n", " self.parameters.get('primary_key_columns') none:\n", " primary_key_dataframe = self.dataframe.select(self.parameters['primary_key_columns'])\n", "\n", " null_values = false\n", " column_nullcount:dict = dict()\n", " column self.parameters['primary_key_columns']:\n", " null_count = primary_key_dataframe.filter(\n", " col(column).isnull()\n", " ).count()\n", "\n", " null_count &gt; 0:\n", " primary_key_value=self.check_pk_values_to_replace(column)\n", " primary_key_value==1 :\n", " null_values = true\n", "\n", " column_nullcount[column] = null_count\n", "\n", " null_values:\n", " raise assertionerror(f\"[checks:primarykeys] primary keys contain null values. number null values pk columns: {column_nullcount}\")\n", " else:\n", " return 0\n", "\n", " else:\n", " raise valueerror(f\"[checks:primarykeys]: cannot execute primary_key check without list primary keys\")\n", "\n", " def check_duplicate_primary_keys(self):\n", "\n", " self.parameters.get('primary_key_columns') none:\n", " duplcate_pk_dataframe = self.dataframe.groupby(self.parameters['primary_key_columns']).count().where('count &gt; 1')\n", " duplcate_pk_dataframe.isempty():\n", " return 0\n", "\n", " duplcate_pk_dataframe.show()\n", " raise middlewareerror(\n", " custom_message=f\"[checks:duplicatprimarykeys] duplicate primary keys found source file {duplcate_pk_dataframe.count()} instances\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name=\"sparkdataframechecks\",\n", " function_name=\"check_duplicate_primary_keys\")\n", " \n", "\n", " \n", " else:\n", " raise valueerror(f\"[checks:duplicatprimarykeys]: cannot execute duplicate_primary_keys check without list primary keys\")\n", "\n", "\n", " def check_values_to_replace(self):\n", " \"\"\"\n", " replace invalid values certain datatype default value\n", "\n", " :dev-info\n", " certain values cannot read spark replaced default/null values ingesting\n", " example: dates 1900 cannot read spark therefore set null -&gt; limitation present many modern environmnets (sql, etc.).\n", " since exclusion given resulting delta tables ready using spark, use-case made implementation method\n", " note part test-class configuration contains explicit reference replace method, making visible developers/managers.\n", "\n", " \"\"\"\n", " self.debug: print(\"[dataframes_v2:sparkdataframechecks] replace dataframe values defaults\")\n", " local_column_names:list = self.column_names\n", " local_datatypes:list = self.data_types\n", " column_datatype_dict:dict = dict(zip(local_column_names, local_datatypes))\n", "\n", " column_name, data_type column_datatype_dict.items():\n", " data_type == 'timestamp':\n", " self.dataframe = self.dataframe.withcolumn(column_name, when(col(column_name) &lt; '1900-01-01', none).otherwise(col(column_name)))\n", " data_type == 'date':\n", " self.dataframe = self.dataframe.withcolumn(column_name, when(col(column_name) &lt; '1900-01-01', none).otherwise(col(column_name)))\n", " # 'decimal' data_type:\n", " # self.dataframe = self.dataframe.withcolumn(column_name, sql.functions.regexp_replace(col(column_name), \",\", \".\"))\n", "\n", "\n", "\n", " def check_dataframe_datatypes(self):\n", " \"\"\"\n", " check columns casted configured datatypes\n", " \n", " :dev-info: spark converts values null casting cannot done, method throw explicit error conversion cannot done\n", " \"\"\"\n", " # define method-lists data_types column_names\n", " # changes lists might happen method, changes relevant method itself\n", " local_data_types:list = self.data_types\n", " local_column_names:list = self.column_names\n", " local_column_information:list = self.column_information\n", "\n", " # cases might occur first column dataframe contains indices indicating row, configured metadata\n", " # spark-load, unnamed column renamed _c0 (if self.header == true)\n", " # avoid errors this: add column _c0 column_names give integer datatype\n", " (len(self.dataframe.columns) == (len(local_column_names) + 1) (self.dataframe.columns[0] == '_c0')):\n", " local_data_types = ['integer'] + local_data_types\n", " local_column_names = ['_c0'] + local_column_names\n", " local_column_information = [none] + local_column_information\n", "\n", " # map columns data_types: {column_name: data_type}\n", " column_datatype_dict = dict(zip(local_column_names, local_data_types))\n", " column_information_dict = dict(zip(local_column_names, local_column_information))\n", " self.debug: print(\"[dataframes_v2:sparkdataframechecks] column_datatype_dict:\", column_datatype_dict)\n", " self.debug: print(\"[dataframes_v2:sparkdataframechecks] column_information_dict:\", column_information_dict)\n", "\n", " # number columns dataframe match number configured columns -&gt; error\n", " optional_columns:list = return_filtered_keys(column_information_dict, 'optional', reverse=false)\n", "\n", " len(self.dataframe.columns) != len(local_column_names):\n", " present_and_optional_columns = list(set(self.dataframe.columns) | set(optional_columns))\n", " len(present_and_optional_columns) != len(local_column_names):\n", " raise configurationerror(\n", " custom_message=f\"total number configured columns match total number source file columns: \\n{local_column_names} ({len(local_column_names)}) versus {self.dataframe.columns} ({len(self.dataframe.columns)})\",\n", " notebook_name=\"dataframes_v2\" ,\n", " class_name= \"sparkdataframechecks\",\n", " function_name= \"check_dataframe_datatypes\"\n", " )\n", "\n", " # [dataframes_v2:sparkdataframechecks] cast column expected datatype validate null-conversions\n", " result = dict()\n", " successfullcasting = true\n", " column local_column_names:\n", " column self.dataframe.columns:\n", " try:\n", " # try cast column individually expected datatype. possible, error thrown\n", " # catch error continue conversion. dictionary return columns (and number rows) could converted correct datatype\n", " result[column] = self.cast_column(dataframe_subset=self.dataframe.select(column), column=column, data_type=column_datatype_dict[column], formatting = column_information_dict[column])\n", " except assertionerror casting_mismatch:\n", " result[column] = casting_mismatch\n", " successfullcasting = false\n", " else:\n", " column optional_columns:\n", " self.debug: print(\"skip column check {} dataframe flagged optional\")\n", " result[column] = 'optional_skip'\n", " else: configurationerror(\n", " custom_message=\"skipping datatype check column {} possible optional, also present dataframe\",\n", " notebook_name=\"dataframes_v2\",\n", " class_name=\"sparkdataframechecks\",\n", " function_name=\"check_dataframe_datatypes\"\n", " )\n", " \n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks] casting results: {result}\")\n", " successfullcasting:\n", " # example result: {column_name: 0, column_name2: 3, etc.} -&gt; 3 values column_name2 could converted configured datatype\n", " raise valueerror(f\"error casting: {result}\")\n", " return result\n", "\n", "\n", " def cast_column(self:object, dataframe_subset, column:str, data_type:str, formatting:dict=dict()):\n", " \"\"\"\n", " cast one column datatype\n", "\n", " :param dataframe_subset: dataframe 1 column\n", " :param data_type: datatype column needs casted\n", " \"\"\"\n", " self.debug: print(f\"[dataframes_v2:sparkdataframechecks] casting column {column} datatype {data_type}\")\n", "\n", " # data_type: string value resembles data type spark (string, int, array, etc.)\n", " # values allowed metadata-configuration recognized spark (eg. long_integer)\n", " # use cases, specific if-statement included\n", " #\n", " # additionally, pyspark recognize varchar(x) metadata-configuration gives support datatype\n", " # hack, using udf-function, metadata column configured enforce varchar(x) constraint anyway\n", " # dev-note: udf's impact performance spark code -&gt; would nice find way around udf possible.\n", "\n", " default_formatting = {\n", " \"format\": \"yyyy-mm-dd hh:mm:ss\",\n", " \"locale\": \"en-us\",\n", " \"thousand_separator\": \"\",\n", " \"decimal_separator\": \".\"\n", " }\n", " formatting:\n", " formatting = dict()\n", " local_formatting = {**default_formatting, **formatting}\n", " self.debug: print(local_formatting)\n", "\n", " try:\n", " # data_type == 'long_integer' -&gt; cast column longtype()\n", " data_type == 'long_integer':\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", col(column).cast(sql.types.longtype()))\n", "\n", " # datatype == varchar(max)-&gt; cast column stringtype()\n", " elif data_type == 'varchar(max)':\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", col(column).cast('string'))\n", "\n", " # datatype == varchar(&lt;xx&gt;) (not varchar(max) handled above) -&gt; use udf enforce_varchar_constraint column enforce \"max_length\"\n", " elif 'varchar' data_type:\n", " # create udf defined ingestionfunctions notebook\n", " enforce_length_udf = enforce_varchar_constraint({'__char_varchar_type_string': data_type})\n", " # apply udf column add metadata\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", enforce_length_udf(dataframe_subset[column]).alias(column, metadata={'__char_varchar_type_string': data_type}))\n", " elif 'decimal' data_type:\n", " # decimal separator defined :\n", " (local_formatting['decimal_separator'] != '.'):\n", " local_formatting['thousand_separator'] != \"\":\n", " self.dataframe = self.dataframe.withcolumn(column, regexp_replace(column, local_formatting['thousand_separator'], ''))\n", " self.dataframe = self.dataframe.withcolumn(column, regexp_replace(column, local_formatting['decimal_separator'], '.'))\n", " dataframe_subset = self.dataframe.select(column)\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", col(column).cast(data_type))\n", "\n", " elif data_type == 'timestamp':\n", " timestamp_format = local_formatting['format']\n", " timestamp_format == 'epoch':\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", from_unixtime(dataframe_subset[column]))\n", " else:\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", to_timestamp(dataframe_subset[column], timestamp_format))\n", "\n", " # original dataframe (self.dataframe) load original columns using format\n", " # merging values deltatable, values cannot casted properly set null\n", " # prevent this, overwrite columns self.dataframe columns loaded using formatting-setting\n", " timestamp_format != \"yyyy-mm-dd hh:mm:ss\":\n", " timestamp_format == 'epoch':\n", " self.dataframe = self.dataframe.withcolumn(f\"{column}\", from_unixtime(dataframe_subset[column]))\n", " else:\n", " self.dataframe = self.dataframe.withcolumn(f\"{column}\", to_timestamp(dataframe_subset[column], timestamp_format))\n", " \n", "\n", " elif data_type == 'date':\n", " date_format = local_formatting['format']\n", " date_format == \"yyyy-mm-dd hh:mm:ss\": date_format = \"yyyy-mm-dd\"\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", to_date(dataframe_subset[column], date_format))\n", " \n", " # original dataframe (self.dataframe) load original columns using specified format\n", " # merging values deltatable, values cannot casted properly set null\n", " # prevent this, overwrite columns self.dataframe columns loaded using locale-setting\n", " data_type != 'yyyy-mm-dd':\n", " self.dataframe = self.dataframe.withcolumn(f\"{column}\", to_date(dataframe_subset[column], date_format))\n", " \n", " else:\n", " dataframe_subset = dataframe_subset.withcolumn(f\"casted_{column}\", col(column).cast(data_type))\n", "\n", " # data_type passed spark support specific if-statement built -&gt; error\n", " except exception e:\n", " raise valueerror(f'casting error casting column {column} data type {data_type}: {e}')\n", "\n", " # [dataframes_v2:sparkdataframechecks] check casted dataframe matches original\n", " result = self.count_miscasted_column_values(dataframe_subset, column)\n", " return result\n", "\n", "\n", " def count_miscasted_column_values(self, casting_dataframe: pyspark.sql.dataframe, column:str):\n", " \"\"\"\n", " compare 2 columns (one casted, one not) check casting issues checking null values one column other\n", "\n", " :param casting_dataframe: dataframe 2 columns, one casted one casted\n", " :param column: name original column\n", " \"\"\"\n", " self.debug: print(\"[classes:checks] casted dataframe:\")\n", " self.debug: casting_dataframe.show()\n", " self.debug: print(f\"column: {column}\")\n", " # compare casted column original columns: error casted column null original contains value\n", " mismatch_count = casting_dataframe.filter(\n", " col(f'casted_{column}').isnull() &amp;\n", " col(column).isnotnull()\n", " ).count()\n", " self.debug: print(\"finish count\")\n", " ## count &gt; 0 -&gt; error: casting problem\n", " mismatch_count &gt; 0:\n", " raise assertionerror(f\"casting error: null values column {column} casting: could convert {mismatch_count} columns\")\n", " else:\n", " return 0\n", " def check_pk_values_to_replace (self,column_name:str):\n", " \"\"\"\n", " replace null values dataframe column specified values replace_value option provided column.\n", " :param column_name : name dataframe column \n", " \"\"\"\n", " column_information_dict = dict(zip(self.column_names, self.column_information))\n", " column_information_object=column_information_dict[column_name]\n", " filter=\"replace_value\"\n", " isinstance(column_information_object, dict):\n", " filter column_information_object.keys():\n", " replace_value=column_information_object[filter]\n", " self.dataframe=self.dataframe.fillna({column_name: replace_value})\n", " return 0\n", " return 1\n", "\n", "" ], "execution_count": 1 } ] } }</file><file name="src\synapse\studio\notebook\DeltaTables.json">{ "name": "deltatables", "properties": { "folder": { "name": "modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 2, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "2", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "7cc89550-de4b-4933-a98e-2c231ec2b653" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "import delta.tables dt" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/dataframes_v2" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/genericfunctions" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class deltatableclass_v2(sparkdataframeclass):\n", " \"\"\"\n", " class defined methods related deltatables\n", " class inherits methods defined class sparkdataframeclass\n", " \"\"\"\n", " \n", " def __init(self:object, debug:bool=false):\n", " self.debug = debug\n", " # dev-note: define source_path argument creating class-instance\n", "\n", "\n", " def load_delta_table(self, source_path:str):\n", " \"\"\"\n", " load delta table memory instance class deltatable\n", "\n", " :param source_path: abfss-path delta table\n", " \"\"\"\n", " self.debug: print(f\"[dataframes:deltatableoptimizeclass] initialize delta table object {source_path}\")\n", " \n", " # load delta table silver-container\n", " self.delta_table = dt.deltatable.forpath(spark, source_path)\n", " return self.delta_table # used unit testing\n", "\n", " def create_delta_table(self:object, destination_path:str, table_description:str, column_objects:object, partition_objects:dict=[]):\n", " \"\"\"\n", " create empty (partitioned) delta table columns datatypes defined\n", "\n", " :param destination_path: abfss-path silver container delta table needs created\n", " :param table_description: general description delta table contents\n", " :param column_objects: list objects, representing metadata needed create structfield object\n", " :param partition_objects: dictionary containing names delta table partition delta table\n", " \"\"\"\n", "\n", " # [dataframes_v2:sparkdataframeclass] create empty dataframe column names\n", " self.create_dataframe(column_information=column_objects, table_description=table_description)\n", "\n", " partition_objects:\n", " # add partitioning columns dataframe write delta table partitioning\n", " self.dataframe, partitioning_columns = create_partioning_list(dataframe=self.dataframe, partitioning_objects=partition_objects) \n", " self.write_dataframe(write_format='delta', write_mode='overwrite', destination_path=destination_path, partitioning_columns=partitioning_columns) \n", " else:\n", " # write delta table without partitioning \n", " self.write_dataframe(write_format='delta', write_mode='overwrite', destination_path=destination_path)\n", "\n", " def count_delta_table (self):\n", " \"\"\"\n", " delta table, count total number rows, well number rows inserted updated last operation\n", " \"\"\"\n", "\n", " inserted_row=self.delta_table.history().filter(\"operation = 'merge'\").select(\"operationmetrics.numtargetrowsinserted\").collect()[0][0]\n", " updated_row= self.delta_table.history().filter(\"operation = 'merge'\").select(\"operationmetrics.numtargetrowsupdated\").collect()[0][0]\n", " source_row= self.delta_table.history().filter(\"operation = 'merge'\").select(\"operationmetrics.numsourcerows\").collect()[0][0]\n", " return inserted_row,updated_row,source_row\n", "\n", " @staticmethod\n", " def add_notnull_constraints(source_path:str, column_dimensions:dict):\n", " \"\"\"\n", " primary key columns, add null constraints delta table\n", " \n", " :param source_path: path delta table\n", " :param column_dimensions: dictionary keys = column_names values = dimension column\n", " :example {\"column_name\": pk/scd2/etc.}\n", " \"\"\"\n", " # get constraints defined delta table\n", " constraints = spark.sql(f\"describe detail delta.`{source_path}`\").select('properties').collect()[0][0]\n", " constraint_names = list(constraints.keys())\n", "\n", " # loop column_dimensions\n", " column_name, dimension column_dimensions.items():\n", " # dimension == \"pk\" -&gt; add not-null constraint\n", " dimension == 'pk':\n", " constraint_name = f'pk__notnull_{column_name}'\n", " f'delta.constraints.{constraint_name}' constraint_names:\n", " spark.sql(f\"alter table delta.`{source_path}` add constraint {constraint_name} check ({column_name} null)\")\n", " else:\n", " spark.sql(f\"alter table delta.`{source_path}` drop constraint {constraint_name}\")\n", " spark.sql(f\"alter table delta.`{source_path}` add constraint {constraint_name} check ({column_name} null)\")\n", "\n", " def validate_delta_table(self, table_name:str, container_name:str, storage_account:str, column_objects:list):\n", " \"\"\"\n", " delta table already expected exist, execute validation checks make sure table still par\n", "\n", " :param table_name: name delta table\n", " :param container_name: name container delta table located\n", " :param storage_account: name storage account delta table located\n", " :param column_objects: list objects containing metatadata column delta table\n", " \"\"\"\n", "\n", " # path delta table, given parameter arguments\n", " delta_path:str = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n", "\n", " # validate metadata delta table expected\n", " self.validate_deltatable_metadata(delta_path=delta_path, table_name=table_name, container_name=container_name, storage_account=storage_account)\n", " self.validate_columnnames(delta_path=delta_path, column_objects=column_objects)\n", " \n", " def validate_deltatable_metadata(self, delta_path:str, table_name:str, container_name:str, storage_account:str):\n", " \"\"\"\n", " go metadata delta table execute validation checks \n", "\n", " :param delta_path: abfss-path delta table\n", " :param table_name: name delta table\n", " :param container_name: name container delta table located\n", " :param storage_account: name storage account delta table located\n", " \"\"\" \n", " \n", " # get delta table metadata\n", " table_details = spark.sql(f\"describe detail delta.`{delta_path}`\")\n", "\n", " # print metadata objects running debug-mode\n", " self.debug: \n", " detail table_details.columns:\n", " expression = table_details.selectexpr(f'first({detail})').collect()[0][0]\n", " print(str(detail), ':', str(expression))\n", "\n", " # get relevant metadata\n", " sql_table_format = table_details.selectexpr(f\"first(format)\").collect()[0][0]\n", " sql_table_name = table_details.selectexpr(f\"first(name)\").collect()[0][0]\n", " sql_table_location = table_details.selectexpr(f\"first(location)\").collect()[0][0]\n", " sql_table_partitioncolumns = table_details.selectexpr(f\"first(partitioncolumns)\").collect()[0][0]\n", " \n", " # execute quality checks table\n", " sql_table_format != 'delta':\n", " raise typeerror(\"the format table delta table\")\n", " # dev-note: might cause issues when, example, storage_account name table container\n", " storage_account sql_table_location:\n", " raise valueerror(f\"the table stored given storage account {storage_account}. storage location {sql_table_location}\")\n", " # dev-note: might cause issues when, example, database name table storage_account\n", " container_name sql_table_location:\n", " raise valueerror(f\"the table stored expected container {container_name}. storage location {sql_table_location}\")\n", " dt.deltatable.isdeltatable(spark, delta_path):\n", " raise typeerror(f\"the files stored path {delta_path} table {table_name} delta table\")\n", " return\n", "\n", " def validate_columnnames(self, delta_path:str, column_objects:list):\n", " \"\"\"\n", " validate columns column_objects delta table, add case\n", " method: empty database made columns need added, partition columns\n", "\n", " :param delta_path: abfss-path delta table checked\n", " :param column_objects: list objects containing metadata columns delta table\n", "\n", " :example column_objects\n", " column_objects = [\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"},\n", " {\"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\n", " ]\n", " \"\"\"\n", "\n", " # load existing delta table memory\n", " self.load_delta_table(delta_path) # initiate self.delta_table\n", " # get list existing columns delta table \n", " existing_columns:list = self.delta_table.todf().columns\n", "\n", " # loop column_objects\n", " # column_name columns -&gt; add object columns_to_add\n", " columns_to_add = []\n", " column_object column_objects:\n", " column_object['column_name'] existing_columns:\n", " columns_to_add.append(column_object)\n", " \n", " # new columns add delta table:\n", " columns_to_add:\n", " # dev-info: delta table partitioned, partition columns need part dataframe added\n", " # get list partition_columns existing delta table\n", " table_details = spark.sql(f\"describe detail delta.`{delta_path}`\")\n", " sql_table_partitioncolumns = table_details.selectexpr(f\"first(partitioncolumns)\").collect()[0][0]\n", " # add partition columns columns_to_add object\n", " partition_column sql_table_partitioncolumns:\n", " column_object = {\"column_name\": partition_column, \"data_type\": \"string\", \"dimension\": \"scd2\"}\n", " columns_to_add.append(column_object)\n", "\n", " # create dataframe columns need added\n", " self.create_dataframe(column_information=columns_to_add)\n", " self.debug: print(f\"adding following columns delta table {delta_path}\")\n", " self.debug: self.dataframe.show()\n", "\n", " # add dataframe existing delta table\n", " self.write_dataframe(write_format='delta', write_mode='append', destination_path=delta_path, merge_schema=true)\n", " return" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class optimizedeltatable(deltatableclass_v2):\n", " \"\"\"\n", " class defined methods related deltatable-optimizations\n", " class contain methods executed deltaoptimizebuilder-objects\n", " \"\"\"\n", " def __init__(self:object, table_name:str, env_code:str, debug:bool=false):\n", " \"\"\"\n", " initialize object class optimizedeltatable create instances class deltatable deltaoptimizebuilder creation\n", "\n", " :param table_name: name delta table optimize\n", " :param env_code: environment code indicating environment tables created (dev, int, tst, acc, prd)\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", "\n", " :return delta_table_path: path delta table (assume table located silver-container)\n", " :return delta_table: instance class deltatable\n", " :return delta_table_optimized: instance class deltaoptimizebuilder\n", " \"\"\"\n", " self.debug = debug\n", " self.env_code = env_code \n", " self.table_name = table_name\n", "\n", " self.delta_table_path = f'abfss://silver@{self.env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\n", " # create object deltatable\n", " self.delta_table =self.load_delta_table(source_path=self.delta_table_path)\n", " # create object deltaoptimizebuilder\n", " self.optimize_deltatable()\n", "\n", "\n", " def optimize_deltatable(self):\n", " \"\"\"\n", " execute .optimize() method delta table\n", " create deltaoptimizebuilder-object instance, used optimization methods\n", " \"\"\"\n", " \n", " self.debug: print(f\"[dataframes:optimizedeltatable]: optimization object {self.table_name}...\")\n", " self.delta_table_optimized = self.delta_table.optimize()\n", " self.debug: print(f\"[dataframes:optimizedeltatable]: optimized delta table {self.delta_table_optimized}...\")\n", "\n", "\n", " def compact_deltatable(self):\n", "\n", " \"\"\" \n", " execute .executecompaction() method deltaoptimizebuilder-object\n", " \n", " :dev-info: \n", " method group together multiple smaller files one bigger file based limits set configuration\n", " default limit spark session configuration 1gb.\n", " executecompaction() use bin-packing method sort data files equal size, respecting 1gb limit\n", " \"\"\"\n", "\n", " # first: validate deltaoptimizebuilder-object exists class-member\n", " hasattr(self, \"delta_table_optimized\"):\n", " isinstance(self.delta_table_optimized, dt.deltaoptimizebuilder):\n", " self.debug: print(f\"[dataframes:optimizedeltatable]: compact delta table {self.table_name}...\")\n", " # execute compaction delta table\n", " compaction_metrics:object = self.delta_table_optimized.executecompaction()\n", " else:\n", " raise typeerror(\"[dataframes:optimizedeltatable] delta_table_optimized member class deltaoptimizebuilder compaction\")\n", " \n", " else:\n", " raise valueerror(\"[dataframes:optimizedeltatable] cannot execute compaction optimize object exist\")\n", "\n", " # print select set metrics give initial insight compaction\n", " self.debug: compaction_metrics.select(\"path\", \"metrics.numfilesadded\", \"metrics.numfilesremoved\").show(truncate=false)\n", "\n", " def z_order_deltatable(self, z_order_columns:list):\n", " \"\"\"\n", " execute .executezorderby() method deltaoptimizebuilder-object\n", " \n", " :dev-info: \n", " method cluster set rows belong together within file\n", " zorder provides clustering related data inside files (for specific partition, applicable) may contain multiple possible values given column.\n", " also allow data skipping querying data, improving overall performance\n", " \"\"\"\n", " # first: validate deltaoptimizebuilder-object exists class-member\n", " hasattr(self, \"delta_table_optimized\"):\n", " isinstance(self.delta_table_optimized, dt.deltaoptimizebuilder):\n", " self.debug: print(f\"[dataframes:optimizedeltatable]: z-order delta table {self.table_name} column(s): {z_order_columns}...\")\n", " # execute z-order delta table\n", " self.delta_table_optimized.executezorderby(z_order_columns=z_order_columns)\n", " else:\n", " raise typeerror(\"[dataframes:optimizedeltatable] delta_table_optimized member class deltaoptimizebuilder z-order\")\n", " \n", " else:\n", " raise valueerror(\"[dataframes:optimizedeltatable] cannot execute z-order optimize object exist\")\n", "\n", "\n", " def optimize_table_storage(self, z_order_columns:list=none):\n", " \"\"\" \n", " orchestrator function create deltaoptimizebuilder-object execute optimization methods (compaction z-order) object\n", " \"\"\"\n", "\n", " self.debug: print(f\"[dataframes:optimizedeltatable] optimize table storage...\")\n", "\n", " # create deltaoptimizebuilder-object\n", " self.optimize_deltatable()\n", " # execute compaction object\n", " self.compact_deltatable()\n", "\n", " # list columns given, make sure also execute executezorderby-method\n", " z_order_columns:\n", " self.z_order_deltatable(z_order_columns=z_order_columns)\n", "\n", " def vacuum_deltatable(self, retention_period:int):\n", " \"\"\"\n", " remove files used anymore. often related files compacted larger files\n", " \"\"\"\n", " self.debug: print(f\"[dataframes:optimizedeltatable] vacuum table storage...\")\n", " \n", " # dev-note: set \"spark.databricks.delta.vacuum.paralleldelete.enabled\" \"true\" vacuum takes much time\n", " self.delta_table.vacuum(retention_period)\n", "\n", " # print select set metrics give initial insight vacuum\n", " # self.debug: print(f\"[dataframes:optimizedeltatable]: vacuum metrics:\")\n", " # self.debug: vacuum_metrics.select(\"numdeletedfiles\", \"numvacuumeddirectories\", \"numfilestodelete\").show(truncate=false)\n", " " ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class deploydeltatable(deltatableclass_v2):\n", " \"\"\"\n", " class call methods functions relevant deployment delta tables\n", " \"\"\"\n", "\n", " def __init__(self:object, table_name:str, storage_account:str, container_name:str, table_description:str, column_info:str, partitioning: dict=[], debug:bool=false):\n", " \"\"\"\n", " set list arguments use delta table deployment: silver_path column_objects\n", "\n", " :param table_name: name delta table deploy/check\n", " :param storage_account: name storage account deploy table\n", " :param container_name: name container deploy table\n", " :param table_description: general description table contents\n", " :param column_info: string resembles list objects contain necessary values create structfield objects\n", " :param partitioning: dictionary containing names columns partition delta table\n", " :param debug: boolean indicating whether class initiated debug mode. output informational print-statements terminal\n", "\n", " :result silver_path: abfss-path delta table found\n", " :result column_objects: list objects contain necessary values create structfield objects (same column_info converted proper list)\n", " \"\"\"\n", " self.debug = debug\n", "\n", " # initialize class-object parameters\n", " self.container_name = container_name\n", " self.storage_account = storage_account\n", " self.table_description = table_description\n", " self.column_info = column_info\n", " self.partitioning = partitioning\n", " self.table_name = table_name\n", "\n", "\n", " # based given parameters, create set derived parameters \n", " # self.full_name = self.container_name + '.' + self.table_name # name delta table wit schema/namespace: &lt;namespace&gt;.&lt;table_name&gt;\n", " self.silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.container_name, self.storage_account, self.table_name) # path table data lake storage account\n", "\n", " # create list json objects column information\n", " # structure: {&lt;column_metadata&gt;: &lt;metadata_value&gt;}\n", " # example: \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'bk', 'data_type': 'string' }]\"\n", " self.column_objects = process_string_to_json(string_object=self.column_info)\n", "\n", " # # create dictionary matching column_names data types\n", " # # structure: {&lt;column_name&gt;: &lt;column_datatype&gt;}\n", " # # example: { 'column_1': 'string', 'column_2': 'integer', ...}\n", " # self.column_datatypes = {column_object['column_name']: column_object['data_type'] column_object self.column_objects}\n", "\n", " def deploy_delta_table(self):\n", " \"\"\"\n", " orchestrator function call set methods depending whether delta table already exists not\n", " \"\"\"\n", " print(f\"starting table deployment table {self.table_name}\")\n", " \n", " # check delta-table already exists execute methods accordingly\n", " # true -&gt; validate\n", " # false -&gt; create new table\n", " mssparkutils.fs.exists(self.silver_path):\n", " print(f\"table {self.table_name} exists already, validating structure...\")\n", " self.validate_delta_table(table_name=self.table_name, storage_account=self.storage_account, container_name=self.container_name, column_objects=self.column_objects)\n", "\n", " else:\n", " print(f\"table {self.table_name} exist, creating table...\")\n", " column_dimensions = {column_object['column_name']: column_object['dimension'] column_object self.column_objects}\n", " self.create_delta_table(destination_path=self.silver_path, table_description=self.table_description, column_objects=self.column_objects, partition_objects=self.partitioning)\n", " self.add_notnull_constraints(source_path=self.silver_path, column_dimensions=column_dimensions)\n", " " ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\DeployDeltaTable.json">{ "name": "deploydeltatable", "properties": { "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "4a01aeda-bd7e-4ca4-b95c-1ff0aea25a37" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# deploydeltatable\r\n", "this script called upon changes made configuration data. deployment configuration data, script validates whether configured table exists. case, validitiy checks executed make sure table resembles configured metadata. table exist, created." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 76 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 1. import necessary packages" ] }, { "cell_type": "code", "source": [ "# import necessary packages\r\n", "import delta.tables dt\r\n", "# pyspark.sql.functions import col\r\n", "# pyspark.sql import row, dataframe\r\n", "import pyspark.sql sql\r\n", "import json\r\n", "import re\r\n", "import delta.tables dt" ], "execution_count": 77 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 2. define script parameters" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "# parameters need given create delta table\r\n", "table_name = 'delta_test' # name delta table\r\n", "database_name = 'silver' # namespace delta table synapse database\r\n", "column_info = '[{\"column_name\": \"id\", \"data_type\": \"integer\"}, {\"column_name\": \"column_2\", \"data_type\": \"string\"}]' # datatypes columns\r\n", "storage_account = 'devdapstdala1' # storage account delta table expected stored\r\n", "table_description = \"\" # general description delta table: contents table describing" ], "execution_count": 78 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "json_column_info = json.loads(column_info)\r\n", "\r\n", "column_dict = dict()\r\n", "for structfield json_column_info:\r\n", " column_dict[structfield[\"column_name\"]] = structfield[\"data_type\"]" ], "execution_count": 79 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "print(\"parameter values\")\r\n", "print(\"table_name: \", table_name)\r\n", "print(\"database_name: \", database_name)\r\n", "print(\"storage_account: \", storage_account)\r\n", "print(\"table_description: \", table_description)\r\n", "print(\"column dictionnary: \", column_dict)" ], "execution_count": 80 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# based given parameters, create set derived parameters \r\n", "full_name = database_name + '.' + table_name # name delta table wit schema/namespace: &lt;namespace&gt;.&lt;table_name&gt;\r\n", "silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (database_name, storage_account, table_name) # path table data lake storage account\r\n", "\r\n", "# list data types read spark\r\n", "# dev-note: decimaltype 'precision' (how many numbers long number be) 'scale' (how many numbers allow comma?)\r\n", "# dev-note: currently default \"38 numbers long\" allows \"4 numbers comma (23.23 gets turned 23.2300)\" -&gt; parameter future\r\n", "# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.decimaltype.html\r\n", "types = {\"array\": sql.types.arraytype(sql.types.stringtype()), \"binary\": sql.types.binarytype(), \r\n", " \"boolean\": sql.types.booleantype(), \"date\": sql.types.datetype(), \"string\": sql.types.stringtype(), \r\n", " \"timestamp\": sql.types.timestamptype(), \"decimal\": sql.types.decimaltype(38, 4), \"float\": sql.types.floattype(), \r\n", " \"byte\": sql.types.bytetype(), \"integer\": sql.types.integertype(), \"long_integer\": sql.types.longtype()}" ], "execution_count": 81 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_decimal_datatype(decimal_value, datatypes):\r\n", "\r\n", " pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\r\n", "\r\n", " decimal_value == 'decimal':\r\n", " return datatypes\r\n", " elif re.match(pattern, decimal_value):\r\n", " match = re.match(pattern, decimal_value)\r\n", " num_characters = int(match.group(1))\r\n", " num_post_comma = int(match.group(2))\r\n", " # dev-note: add check make sure num_characters exceed 38 pyspark cannot handle more, num_characters &gt; num_post_comma\r\n", " datatypes[decimal_value] = sql.types.decimaltype(num_characters, num_post_comma)\r\n", " return datatypes\r\n", " else:\r\n", " raise valueerror(f'the decimal value {decimal_value} valid datatype value cannot configured correctly. aborting delta table deployment...')" ], "execution_count": 82 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_datatypes(datatypes, column_dictionary):\r\n", " # datatypes come additionanal (set of) parameters. function add datatypes initial types-dictionary defined above.\r\n", " value column_dictionary.values():\r\n", " 'decimal' value:\r\n", " datatypes = add_decimal_datatype(value, datatypes)\r\n", "\r\n", " return types" ], "execution_count": 83 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 3. define functions validate create delta table" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def validate_table():\r\n", " table_details = spark.sql(f\"describe detail silver.{table_name}\")\r\n", "\r\n", " # print details table\r\n", " # detail table_details.columns:\r\n", " # expression = table_details.selectexpr(f\"first({detail})\").collect()[0][0]\r\n", " # print(str(detail) +\": \"+ str(expression))\r\n", "\r\n", " # expressions validate\r\n", " sql_table_format = table_details.selectexpr(f\"first(format)\").collect()[0][0]\r\n", " sql_table_name = table_details.selectexpr(f\"first(name)\").collect()[0][0]\r\n", " sql_table_location = table_details.selectexpr(f\"first(location)\").collect()[0][0]\r\n", " # sql_table_partitioncolumns = table_details.selectexpr(f\"first(partitioncolumns)\").collect()[0][0] -&gt; uncomment integration\r\n", "\r\n", " sql_table_format != 'delta':\r\n", " raise typeerror(\"the format table delta table\")\r\n", "\r\n", " sql_table_name != full_name:\r\n", " raise typeerror(f\"the name table {table_name} namespace {database_name} match name table {sql_table_name}\")\r\n", "\r\n", " storage_account sql_table_location:\r\n", " raise valueerror(f\"the table stored given storage account {storage_account}. storage location {sql_table_location}\")\r\n", "\r\n", " database_name sql_table_location:\r\n", " raise valueerror(f\"the table stored expected container {database_name}. storage location {sql_table_location}\")\r\n", "\r\n", " dt.deltatable.isdeltatable(spark, silver_path):\r\n", " raise typeerror(f\"the files stored path {silver_path} table {table_name} delta table\")\r\n", "\r\n", " # dev-note: validate folder actually exists container\r\n", " # dev-note: make sure columns config also defined table\r\n", "\r\n", " print(\"all validity checks passed.\")\r\n", " return true" ], "execution_count": 84 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def create_table():\r\n", " print(\"creating table schema\")\r\n", " add_columns = list()\r\n", "\r\n", " column_name column_dict:\r\n", " add_columns.append(dt.structfield(name=column_name,datatype=types[column_dict[column_name]],nullable=true, metadata={column_name: f\"metadata column {column_name}\"}))\r\n", "\r\n", " print(\"creating delta table\")\r\n", " (dt.deltatable.createifnotexists(spark)\r\n", " .tablename(full_name)\r\n", " .addcolumns(add_columns)\r\n", " .comment(table_description)\r\n", " .location(f\"{silver_path}\")\r\n", " .execute()\r\n", " )" ], "execution_count": 85 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 4. orchestration: run functions" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "if __name__ == \"__main__\":\r\n", "\r\n", " types = add_datatypes(types, column_dict)\r\n", "\r\n", " print(f\"starting table deployment notebook {full_name}\")\r\n", " table_list = []\r\n", " table spark.sql(f'show tables {database_name}').collect():\r\n", " table_list.append(table[1])\r\n", " print(table_list)\r\n", "\r\n", " table_name table_list:\r\n", " mssparkutils.fs.exists(silver_path):\r\n", " print(f\"table {full_name} exists already, validating structure...\")\r\n", " validate_table()\r\n", " else:\r\n", " print(f\"table {full_name} exist, inside silver container.\")\r\n", " print(f\"removing {full_name} silver db, creating new table &amp; link\")\r\n", " clean_table(table_name)\r\n", " create_table()\r\n", "\r\n", " else:\r\n", " print(f\"table {full_name} exist, creating table...\")\r\n", " create_table()" ], "execution_count": 86 } ] } }</file><file name="src\synapse\studio\notebook\DeployDeltaTable_v2.json">{ "name": "deploydeltatable_v2", "properties": { "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "24a137f4-13b3-4983-b084-1c23b2e77f0c" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28 }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# deploydeltatable\r\n", "this script called upon changes made configuration data. deployment configuration data, script validates whether configured table exists. case, validitiy checks executed make sure table resembles configured metadata. table exist, created." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 1. import necessary packages" ] }, { "cell_type": "code", "source": [ "# import necessary packages\r\n", "import delta.tables dt\r\n", "import pyspark.sql sql\r\n", "import json\r\n", "import re" ], "execution_count": 3 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 2. define functions validate create delta table" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_decimal_datatype(decimal_value:str, datatypes:object) -&gt; object:\r\n", "\r\n", " # regex pattern\r\n", " # decimal expected look like 'decimal(&lt;xx&gt;,&lt;yy&gt;)'\r\n", " pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\r\n", "\r\n", "\r\n", " # first check decimal contains additional parameters\r\n", " # not: nothing changes decimal assigned parameters (38,4) (38 characters, 4 comma)\r\n", " decimal_value == 'decimal':\r\n", " return datatypes\r\n", " # otherwise: check match regex\r\n", " elif re.match(pattern, decimal_value):\r\n", " match = re.match(pattern, decimal_value)\r\n", " num_characters = int(match.group(1))\r\n", " num_post_comma = int(match.group(2))\r\n", " # dev-note: add check make sure num_characters exceed 38 pyspark cannot handle more, num_characters &gt; num_post_comma\r\n", " datatypes[decimal_value] = sql.types.decimaltype(num_characters, num_post_comma)\r\n", "\r\n", " return datatypes\r\n", " # also match: decimal-value configured incorrectly\r\n", " else:\r\n", " raise valueerror(f'the decimal value {decimal_value} valid datatype value cannot configured correctly. aborting delta table deployment...')" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def configure_datatypes(column_dictionary:dict) -&gt; dict:\r\n", " # create dictionary data types read spark\r\n", " # keys dictionary values used metadata configuration files (sql database)\r\n", " # values dictionary pyspark.sql datatypes \r\n", " # dev-note: default definition used decimals is: 38 characters total, 4 comma. \r\n", " # specifics given column_info (eg. decimal(12,2)), new input added types-dictionary accomodate datatype\r\n", " # https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.decimaltype.html\r\n", "\r\n", " datatypes = {\"array\": sql.types.arraytype(sql.types.stringtype()), \"binary\": sql.types.binarytype(), \r\n", " \"boolean\": sql.types.booleantype(), \"date\": sql.types.datetype(), \"string\": sql.types.stringtype(), \r\n", " \"timestamp\": sql.types.timestamptype(), \"decimal\": sql.types.decimaltype(38, 4), \"float\": sql.types.floattype(), \r\n", " \"byte\": sql.types.bytetype(), \"integer\": sql.types.integertype(), \"long_integer\": sql.types.longtype()}\r\n", "\r\n", "\r\n", " # datatypes come additionanal (set of) parameters. function add datatypes initial types-dictionary defined above.\r\n", " value column_dictionary.values():\r\n", " # column_dictionary value contains value substring 'decimal' -&gt; invoke add_decimal_datatype function\r\n", " 'decimal' value:\r\n", " datatypes = add_decimal_datatype(value, datatypes)\r\n", "\r\n", " # throw error value part datatypes dictionary if-statement configured add dictionary\r\n", " elif value datatypes.keys():\r\n", " raise valueerror(f\"the value '{value}' configured datatype current set-up.\")\r\n", "\r\n", " return datatypes" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def process_column_info_string(string_object:str) -&gt; object:\r\n", " # converting string_object json-object, replace '-characters \" first\r\n", " column_info_string = (string_object.replace('\\'', '\"'))\r\n", " # convert string_object string json object\r\n", " try:\r\n", " column_info_object = json.loads(column_info_string)\r\n", " # return json object\r\n", " return column_info_object\r\n", " except:\r\n", " raise typeerror(f\"the string_object passed cannot converted json object. passed string: {string_object}\")" ], "execution_count": 8 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 3. class object: define deltatable methods" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class deltatableclass(object):\r\n", " # class used define methods needed creating new delta table validating structure existing one\r\n", " def __init__(self, table_name:str, database_name:str, storage_account:str, table_description:str, column_info:object) -&gt; object:\r\n", "\r\n", " # initialize class-object parameters\r\n", " self.table_name = table_name\r\n", " self.database_name = database_name\r\n", " self.storage_account = storage_account\r\n", " self.table_description = table_description\r\n", " self.column_info = column_info\r\n", "\r\n", "\r\n", " # based given parameters, create set derived parameters \r\n", " self.full_name = self.database_name + '.' + self.table_name # name delta table wit schema/namespace: &lt;namespace&gt;.&lt;table_name&gt;\r\n", " self.silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.database_name, self.storage_account, self.table_name) # path table data lake storage account\r\n", "\r\n", " # create list json objects column information\r\n", " # structure: {&lt;column_metadata&gt;: &lt;metadata_value&gt;}\r\n", " # example: \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'pk', 'data_type': 'string' }]\"\r\n", " column_info_object = process_column_info_string(self.column_info)\r\n", "\r\n", " # create dictionary matching column_names data types\r\n", " # structure: {&lt;column_name&gt;: &lt;column_datatype&gt;}\r\n", " # example: { 'column_1': 'string', 'column_2': 'integer', ...}\r\n", " self.column_dict = dict()\r\n", " structfield column_info_object:\r\n", " self.column_dict[structfield[\"column_name\"]] = structfield[\"data_type\"]\r\n", " \r\n", " # derive dictionary pyspark.sql.types functions match given datatypes\r\n", " # structure: { &lt;datatype_reference&gt;: &lt;pyspark_datatype_reference&gt; }\r\n", " # example: { 'string': pyspark.sql.type.stringtype(), 'integer': pyspark.sql.type.integertype()}\r\n", " self.types = configure_datatypes(self.column_dict)\r\n", "\r\n", " \r\n", " def deploy_delta_table(self):\r\n", " # orchestrator function\r\n", " # check delta-table already exists execute methods accordingly\r\n", " print(f\"starting table deployment table {self.full_name}\")\r\n", "\r\n", " # get list existing tables database\r\n", " table_list = []\r\n", " table spark.sql(f'show tables {self.database_name}').collect():\r\n", " table_list.append(table[1])\r\n", "\r\n", " # delta_table object contains to-be configured table\r\n", " # -&gt; check exists folder storage account\r\n", " # true -&gt; validate\r\n", " # false -&gt; clean table database recreate table\r\n", " self.table_name table_list:\r\n", " mssparkutils.fs.exists(self.silver_path):\r\n", " print(f\"table {self.full_name} exists already, validating structure...\")\r\n", " self.validate_table()\r\n", " else:\r\n", " print(f\"table {self.full_name} exist, inside silver container.\")\r\n", " print(f\"removing {self.full_name} silver db, creating new table &amp; link\")\r\n", " clean_table(self.table_name)\r\n", " self.create_table()\r\n", "\r\n", " # else: create table scratch\r\n", " else:\r\n", " print(f\"table {self.full_name} exist, creating table...\")\r\n", " self.create_table()\r\n", "\r\n", "\r\n", " def create_table(self):\r\n", "\r\n", " # add columns add table list structfield-objects\r\n", " add_columns = list()\r\n", " column_name self.column_dict:\r\n", " add_columns.append(dt.structfield(name=column_name,datatype=self.types[self.column_dict[column_name]],nullable=true, metadata={column_name: f\"metadata column {column_name}\"}))\r\n", "\r\n", " # create table exist\r\n", " (dt.deltatable.createifnotexists(spark)\r\n", " .tablename(self.full_name)\r\n", " .addcolumns(add_columns)\r\n", " .comment(self.table_description)\r\n", " .location(f\"{self.silver_path}\")\r\n", " .execute()\r\n", " )\r\n", "\r\n", "\r\n", " def validate_table(self):\r\n", " table_details = spark.sql(f\"describe detail silver.{self.table_name}\")\r\n", "\r\n", " # print details table\r\n", " # detail table_details.columns:\r\n", " # expression = table_details.selectexpr(f\"first({detail})\").collect()[0][0]\r\n", " # print(str(detail) +\": \"+ str(expression))\r\n", "\r\n", " # expressions validate\r\n", " sql_table_format = table_details.selectexpr(f\"first(format)\").collect()[0][0]\r\n", " sql_table_name = table_details.selectexpr(f\"first(name)\").collect()[0][0]\r\n", " sql_table_location = table_details.selectexpr(f\"first(location)\").collect()[0][0]\r\n", " # sql_table_partitioncolumns = table_details.selectexpr(f\"first(partitioncolumns)\").collect()[0][0] -&gt; uncomment integration\r\n", "\r\n", " sql_table_format != 'delta':\r\n", " raise typeerror(\"the format table delta table\")\r\n", "\r\n", " sql_table_name != self.full_name:\r\n", " raise typeerror(f\"the name table {self.table_name} namespace {self.database_name} match name table {sql_table_name}\")\r\n", "\r\n", " # dev-note: might cause issues when, example, storage_account name table container\r\n", " self.storage_account sql_table_location:\r\n", " raise valueerror(f\"the table stored given storage account {self.storage_account}. storage location {sql_table_location}\")\r\n", " \r\n", " # dev-note: might cause issues when, example, database name table storage_account\r\n", " self.database_name sql_table_location:\r\n", " raise valueerror(f\"the table stored expected container {self.database_name}. storage location {sql_table_location}\")\r\n", "\r\n", " dt.deltatable.isdeltatable(spark, self.silver_path):\r\n", " raise typeerror(f\"the files stored path {self.silver_path} table {self.table_name} delta table\")\r\n", "\r\n", " # dev-note: validate folder actually exists container\r\n", " # dev-note: make sure columns config also defined table\r\n", "\r\n", " # print(\"all validity checks passed.\")\r\n", " return true" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\DeployDeltaTable_v3.json">{ "name": "deploydeltatable_v3", "properties": { "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "1bd537ea-4bd2-4ce2-8b25-feaf10036ec8" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# deploydeltatable\n", "this script called upon changes made configuration data. deployment configuration data, script validates whether configured table exists. case, validitiy checks executed make sure table resembles configured metadata. table exist, created." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 1. import necessary packages" ] }, { "cell_type": "code", "source": [ "# import necessary packages\n", "import delta.tables dt\n", "import pyspark.sql sql\n", "import json\n", "import re" ], "execution_count": 3 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 2. define functions validate create delta table" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_varchar_datatype(varchar_value:str, datatypes:object) -&gt; object:\n", "\n", " # regex pattern\n", " # varchar expected look like 'varchar(&lt;xxxx&gt;)'\n", " pattern = r\"varchar\\(\\d+\\)\"\n", "\n", " # first check varchar value contains 'max' argument\n", " varchar_value == 'varchar(max)':\n", " return datatypes\n", " # otherwise: check match regex\n", " elif re.match(pattern, varchar_value):\n", " # match, add value datatypes-dictionary stringtype() datatype\n", " # assignment, value 'varchar_value' added metadata column\n", " datatypes[varchar_value] = sql.types.stringtype()\n", " return datatypes\n", " # also match: varchar-value configured incorrectly\n", " else:\n", " raise valueerror(f'the varchar value {varchar_value} valid datatype value cannot configured correctly. aborting delta table deployment...')" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_decimal_datatype(decimal_value:str, datatypes:object) -&gt; object:\n", "\n", " # regex pattern\n", " # decimal expected look like 'decimal(&lt;xx&gt;,&lt;yy&gt;)'\n", " pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\n", "\n", " # first check decimal contains additional parameters\n", " # not: nothing changes decimal assigned parameters (38,4) (38 characters, 4 comma)\n", " decimal_value == 'decimal':\n", " return datatypes\n", " # otherwise: check match regex\n", " elif re.match(pattern, decimal_value):\n", " # match: extract numbers string\n", " # assign numerical values total number characters numbers past comma\n", " match = re.match(pattern, decimal_value)\n", " num_characters = int(match.group(1))\n", " num_post_comma = int(match.group(2))\n", " # dev-note: add check make sure num_characters exceed 38 pyspark cannot handle more, num_characters &gt; num_post_comma\n", " datatypes[decimal_value] = sql.types.decimaltype(num_characters, num_post_comma)\n", " return datatypes\n", " # also match: decimal-value configured incorrectly\n", " else:\n", " raise valueerror(f'the decimal value {decimal_value} valid datatype value cannot configured correctly. aborting delta table deployment...')" ], "execution_count": 12 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def configure_datatypes(column_dictionary:dict) -&gt; dict:\n", " # create dictionary data types read spark\n", " # keys dictionary values used metadata configuration files (sql database)\n", " # values dictionary pyspark.sql datatypes \n", " # dev-note: default definition used decimals is: 38 characters total, 4 comma. \n", " # specifics given column_info (eg. decimal(12,2)), new input added types-dictionary accomodate datatype\n", " # https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.decimaltype.html\n", "\n", " datatypes = {\"array\": sql.types.arraytype(sql.types.stringtype()), \"binary\": sql.types.binarytype(), \n", " \"boolean\": sql.types.booleantype(), \"date\": sql.types.datetype(), \"string\": sql.types.stringtype(), \"varchar(max)\": sql.types.stringtype(), \n", " \"timestamp\": sql.types.timestamptype(), \"decimal\": sql.types.decimaltype(38, 4), \"float\": sql.types.floattype(), \n", " \"byte\": sql.types.bytetype(), \"integer\": sql.types.integertype(), \"int\": sql.types.integertype(), \"long_integer\": sql.types.longtype()}\n", "\n", "\n", " # datatypes come additionanal (set of) parameters. function add datatypes initial types-dictionary defined above.\n", " value column_dictionary.values():\n", " # column_dictionary value contains value substring 'decimal' -&gt; invoke add_decimal_datatype function\n", " 'decimal' value:\n", " datatypes = add_decimal_datatype(value, datatypes)\n", " elif 'varchar' value:\n", " datatypes = add_varchar_datatype(value, datatypes)\n", "\n", " # throw error value part datatypes dictionary if-statement configured add dictionary\n", " elif value datatypes.keys():\n", " raise valueerror(f\"the value '{value}' configured datatype current set-up.\")\n", "\n", " return datatypes" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def process_column_info_string(string_object:str) -&gt; object:\n", " # converting string_object json-object, replace '-characters \" first\n", " column_info_string = (string_object.replace('\\'', '\"'))\n", " # convert string_object string json object\n", " try:\n", " column_info_object = json.loads(column_info_string)\n", " # return json object\n", " return column_info_object\n", " except:\n", " raise typeerror(f\"the string_object passed cannot converted json object. passed string: {string_object}\")" ], "execution_count": 8 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_structfield_logging_columns(structfield_list):\n", " structfield_list.append(dt.structfield(name='t_load_date_raw' ,datatype=sql.types.timestamptype(),nullable=true, metadata={'t_load_date_raw': f\"date row loaded raw layer table\"}))\n", " structfield_list.append(dt.structfield(name='t_load_date_silver' ,datatype=sql.types.timestamptype(),nullable=true, metadata={'t_load_date_silver': f\"date row loaded delta table\"}))\n", " structfield_list.append(dt.structfield(name='t_file_name' ,datatype=sql.types.stringtype(),nullable=true, metadata={'t_load_date': f\"name file data coming from\"}))\n", " structfield_list.append(dt.structfield(name='t_plan_id' ,datatype=sql.types.integertype(),nullable=false, metadata={'t_plan_id': f\"the id plan meta.log_plans table\"}))\n", " structfield_list.append(dt.structfield(name='t_task_id' ,datatype=sql.types.integertype(),nullable=false, metadata={'t_task_id': f\"the id task meta.log_tasks table\"}))\n", " structfield_list.append(dt.structfield(name='t_file_id' ,datatype=sql.types.integertype(),nullable=false, metadata={'t_file_id': f\"the id file meta.log_files table\"}))\n", "\n", " return structfield_list" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\DeployOrchestrator.json">{ "name": "deployorchestrator", "properties": { "folder": { "name": "deploy" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "5", "spark.autotune.trackingid": "01bce0d1-1cf0-4f19-9a7c-39b343326906" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# deploy orchestrator\n", "this orchestrator notebook used deploy set delta tables.\n", "the set delta tables, properties, passed string notebook. string converted list objects, for-loop loop object. object converted class-object deltatable-class (see notebook: deploydeltatable_v2) necessary functions called deploy object delta table.\n", "" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 1. parameters\n", "define necessary parameters" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "import json" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "# parameters\n", "## get list objects, object resembles metadata certain delta table\n", "table_definition_list = '''[\n", " {\n", " \"table_name\": \"deploy_test\",\n", " \"container_name\": \"silver\",\n", " \"storage_account\": \"devdapstdala1\",\n", " \"table_description\": \"test deploy script works\",\n", " \"target_options\": {},\n", " \"column_info\": \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'pk', 'data_type': 'timestamp' }, { 'column_sequence': 2, 'column_name': 'column_2', 'dimension': 'scd2', 'data_type': 'int' }, { 'column_sequence': 3, 'column_name': 'column_3', 'dimension': 'scd2', 'data_type': 'decimal(12,3)' }]\"\n", " },\n", " {\n", " \"table_name\": \"deploy_test\",\n", " \"container_name\": \"silver\",\n", " \"storage_account\": \"devdapstdala1\",\n", " \"table_description\": \"test deploy script works\",\n", " \"target_options\": {},\n", " \"column_info\": \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'pk', 'data_type': 'varchar(10)' }, { 'column_sequence': 2, 'column_name': 'column_2', 'dimension': 'scd2', 'data_type': 'int' }, { 'column_sequence': 3, 'column_name': 'column_3', 'dimension': 'scd2', 'data_type': 'decimal(12,3)' }]\"\n", " },\n", " {\n", " \"table_name\": \"deploy_test_v2\",\n", " \"container_name\": \"silver\",\n", " \"storage_account\": \"devdapstdala1\",\n", " \"table_description\": \"test happens decimal datatypes\",\n", " \"target_options\": {},\n", " \"column_info\": \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'pk', 'data_type': 'varchar(max)' }, { 'column_sequence': 2, 'column_name': 'column_2', 'dimension': 'scd2', 'data_type': 'int' }, { 'column_sequence': 3, 'column_name': 'column_3', 'dimension': 'scd2', 'data_type': 'decimal(12,5)' }]\"\n", " }\n", "]'''" ], "execution_count": 5 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 2. \"import\" relevant notebooks functions/methods" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/genericfunctions" ], "execution_count": 6 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 7 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 8 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 9 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/classes" ], "execution_count": 10 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## 3. execute\n", "loop delta-table objects run deploy_delta_table method deltatable-class" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# convert 'table_definition_list' parameter list json objects\n", "table_definition_list:str = replace_string_value(table_definition_list, '\\\\', '\\\\\\\\')\n", "table_definition_objects = json.loads(table_definition_list)\n", "\n", "default_target_options = {'partitioning': []}\n", "# loop object\n", "for table_definition table_definition_objects:\n", " # create dictionary overwrite default values specified table_definition-dictionary\n", "\n", " # dev-info: dictionary unpacking keyword argument unpacking using **-syntax\n", " # use ** operator dictionary, unpacks dictionary's key-value pairs separate keyword arguments\n", " # example: = {'a': 1, 'b': 2}} ; function_name(**d) pass dictionary values a=1,b=2 instead dev explicitly define arguments like this\n", " # use {**dict1, **dict2} syntax, python merges key-value pairs dictionaries new dictionary. \n", " # duplicate keys, values second dictionary (dict2) override values first dictionary (dict1).\n", " target_options_dict = table_definition['target_options']\n", " full_target_options_dict = {**default_target_options, **target_options_dict}\n", "\n", " # create deltatableclass-object defaulted_table_definition keys\n", " delta_table = deploydeltatable(\n", " table_name = table_definition['table_name'], \n", " container_name = table_definition['container_name'], \n", " storage_account = table_definition['storage_account'],\n", " table_description = table_definition['table_description'], \n", " column_info = table_definition['column_info'],\n", " partitioning = full_target_options_dict['partitioning']\n", " )\n", "\n", " # run deploy_delta_table-method class-object\n", " delta_table.deploy_delta_table()" ], "execution_count": 11 } ] } }</file><file name="src\synapse\studio\notebook\ErrorHandling.json">{ "name": "errorhandling", "properties": { "folder": { "name": "modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore2", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "4c0bdd64-16d8-4170-b9be-d6d11c3c1bd0" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore2", "name": "devdapsspcore2", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore2", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "code", "source": [ "def get_exception_information(thrown_error:str) -&gt; tuple:\r\n", " \"\"\"\r\n", " get underlying error class message error_block thrown python.\r\n", " dealing spark related issues, py4jjavaerror class error, aims say \"java throw error\"\r\n", " get java error, scripting necessary figure specific issue\r\n", "\r\n", " :param thrown_error: entire error thrown notebook\r\n", "\r\n", " :return exception_class: string name error_class (valueerror (python), filenotfounderror (spark), configurationerror (custom), etc.)\r\n", " :return execption_message: string entire error message\r\n", " \"\"\"\r\n", "\r\n", " # get class-type thrown error, extract name error\r\n", " error_type = type(thrown_error).__name__\r\n", " # error type py4jjavaerror:\r\n", " # get underlying argument 'java_exception', contains error class-name error message\r\n", " error_type == 'py4jjavaerror':\r\n", " exception_class = thrown_error.java_exception.getclass().getname().rsplit('.', 1)[-1]\r\n", " exception_message = thrown_error.java_exception.getmessage()\r\n", " # error java error: get message error class\r\n", " else:\r\n", " exception_class = error_type\r\n", " exception_message = str(thrown_error)\r\n", " \r\n", " return exception_class, exception_message" ], "execution_count": 14 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def is_custom_error(error_class:str) -&gt; bool:\r\n", " \"\"\"\r\n", " returns true error custom error, false otherwise.\r\n", "\r\n", " :param error_class: contains class-type error (valueerror, filenotfounderror, configurationerror, etc.)\r\n", "\r\n", " :return boolean: value indicating whether error_class custom error \r\n", " \"\"\"\r\n", "\r\n", " # get list custom error classes\r\n", " # assumption: custom error classes subclasses genericerrorclass\r\n", " custom_error_classes = [cls.__name__ cls globals().values() isinstance(cls, type) issubclass(cls, genericerrorclass)]\r\n", " error_class custom_error_classes:\r\n", " return true\r\n", " return false" ], "execution_count": 15 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def handle_exceptions(exception_class:str, exception_message:str, exception_object:object) -&gt; dict:\r\n", " \"\"\"\r\n", " exception_class custom error, create genericerrorclass object custom messages\r\n", " otherwise, return object already created initial error-raise\r\n", "\r\n", " :param exception_class: error-class error object\r\n", " :param exception_message: error message thrown error class\r\n", " :param exception_object: entire error object\r\n", "\r\n", " :result error_object: object containing error returned logging tables sql meta database\r\n", " \"\"\"\r\n", "\r\n", " # [errorhandling] check exception_class custom error class not\r\n", " custom_error = is_custom_error(exception_class)\r\n", "\r\n", " custom_error:\r\n", " # error custom error:\r\n", " # 1. define set arguments\r\n", " # dev-info: idea dealing exception part custom classes, likely something code-issue\r\n", " # arguments warn user this, make sure exception flagged something needs dealt dap core engineers\r\n", " # final error-message also contain original error message, general guide dap core engineers replicate error \r\n", " custom_message = \"exception: unknown exception. rerun task %{task_name}% (task_id: %{task_id}% , plan_id: %{plan_id}%) debug-mode see error coming from. debugging, make sure add use case errorhandling notebook.\"\r\n", " custom_error_class = \"exception\"\r\n", " notebook_name = \"metanotebook\"\r\n", " responsible_team = \"dap core engineers\"\r\n", " exception_message = str(exception_message).replace(\"'\", \"\")\r\n", "\r\n", " # 2. pass arguments genericerrorclass object set error_object\r\n", " generic_error_object = genericerrorclass()\r\n", " error_object = generic_error_object.__report__(custom_error_class=custom_error_class, custom_message=custom_message, notebook_name=notebook_name, responsible_team=responsible_team, python_error_class=exception_class, python_error_message=exception_message)\r\n", " return error_object\r\n", "\r\n", " # dealing custom error, error_object already made therefore returned whole\r\n", " else:\r\n", " return exception_object.error_object" ], "execution_count": 16 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class genericerrorclass(exception):\r\n", " \"\"\"\r\n", " generic class custom errors based on\r\n", " purpose: whenever custom error (middlewareerror, configurationerror, etc.) thrown, creates error_object. object returned logging tables sql server\r\n", " \"\"\"\r\n", " def __report__(self, custom_error_class:str, custom_message:str, notebook_name:str, class_name:str=none, function_name:str=none, *args:list, **kwargs:dict) -&gt; dict:\r\n", " \"\"\"\r\n", " __report__ function create error_object, \"report\" error occured\r\n", "\r\n", " :param custom_error_class: name custom error class (genericerrorclass, middlewareerror, configurationerror) \r\n", " :param custom_message: custom error message thrown custom class\r\n", " :param notebook_name: name notebook error coming from\r\n", " :param class_name: name class error coming (if any)\r\n", " :param function_name: name function error coming (if any)\r\n", " :param args: additional list arguments passed user/system\r\n", " :param kwargs: additional key-word (aka dictionary) arguments passed user/system\r\n", "\r\n", " :return error_object: dictionary containing relevant information error thrown\r\n", " \"\"\"\r\n", "\r\n", " # create dictionary 'full_message', containing arguments passed __report__ method\r\n", " self.custom_error_class = custom_error_class\r\n", " self.custom_message = custom_message.replace(\"'\", \"\")\r\n", " self.error_location = f\"[{notebook_name ''}:{class_name ''}:{function_name ''}]\"\r\n", " self.full_message = {\r\n", " \"custom_message\": self.custom_message,\r\n", " \"custom_error_class\": self.custom_error_class,\r\n", " \"error_location_in_notebooks\": self.error_location\r\n", " }\r\n", "\r\n", " # unite 'full_message' kwargs-dictionary create complete dictionary \r\n", " self.error_object = {**self.full_message, **kwargs}\r\n", " \r\n", " # initialize exception-class error_object representing kwargs-argument class\r\n", " super().__init__(self.error_object)\r\n", "\r\n", " # return object\r\n", " return self.error_object" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class middlewareerror(genericerrorclass):\r\n", " \"\"\"\r\n", " custom error class, used errors related middleware team \r\n", " examples:\r\n", " 1. expected files landed\r\n", " 2. name file match expectations\r\n", " etc.\r\n", " \"\"\"\r\n", " def __init__(self, custom_message:str, notebook_name:str, class_name:str=none, function_name:str=none, *args:list, **kwargs:dict):\r\n", " \"\"\"\r\n", " :param custom_message: custom error message thrown custom class\r\n", " :param notebook_name: name notebook error coming from\r\n", " :param class_name: name class error coming (if any)\r\n", " :param function_name: name function error coming (if any)\r\n", " :param args: additional list arguments passed user/system\r\n", " :param kwargs: additional key-word (aka dictionary) arguments passed user/system\r\n", "\r\n", " \"\"\"\r\n", "\r\n", " custom_error_class=\"middlewareerror\"\r\n", " responsible_team = \"middleware team (add email)\"\r\n", "\r\n", " # call __report__ method genericerrorclass create error_object\r\n", " super().__report__(\r\n", " custom_error_class=custom_error_class, \r\n", " custom_message=custom_message, \r\n", " notebook_name=notebook_name, \r\n", " class_name=class_name,\r\n", " function_name=function_name,\r\n", " responsible_team=responsible_team, \r\n", " **kwargs\r\n", " ) " ], "execution_count": 18 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class configurationerror(genericerrorclass):\r\n", " \"\"\"\r\n", " custom error class, used error related file configuration\r\n", " examples\r\n", " 1. configured headers match headers source file\r\n", " 2. datatype column match configuration\r\n", " etc.\r\n", " \"\"\"\r\n", " def __init__(self, custom_message:str, notebook_name:str, class_name:str=none, function_name:str=none, *args:list, **kwargs:dict):\r\n", " \"\"\"\r\n", " :param custom_message: custom error message thrown custom class\r\n", " :param notebook_name: name notebook error coming from\r\n", " :param class_name: name class error coming (if any)\r\n", " :param function_name: name function error coming (if any)\r\n", " :param args: additional list arguments passed user/system\r\n", " :param kwargs: additional key-word (aka dictionary) arguments passed user/system\r\n", "\r\n", " \"\"\"\r\n", "\r\n", " custom_error_class=\"configurationerror\"\r\n", " responsible_team = \"dap engineers (add email)\"\r\n", "\r\n", " # call __report__ method genericerrorclass create error_object\r\n", " super().__report__(\r\n", " custom_error_class=custom_error_class, \r\n", " custom_message=custom_message, \r\n", " notebook_name=notebook_name, \r\n", " class_name=class_name,\r\n", " function_name=function_name,\r\n", " responsible_team=responsible_team, \r\n", " **kwargs\r\n", " )" ], "execution_count": 19 } ] } }</file><file name="src\synapse\studio\notebook\FilWorker.json">{ "name": "filworker", "properties": { "folder": { "name": "workers" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "f4a3c2ff-eed4-4f6a-ae48-5301ff01eda9" } }, "metadata": { "saveoutput": true, "enabledebugmode": false, "kernelspec": { "name": "synapse_pyspark", "display_name": "python" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## filworker\n", "\n", "\n", "the filworker used preprocessing. purpose worker to:\n", "- obtain set header-names source file, move separate columns" ] }, { "cell_type": "code", "source": [ "def run_fil_worker(task:object, env_code:str, debug:bool=true):\n", " '''\n", " inputs\n", " task: object task-class [modules/classes]\n", " env_code: code used indicate environment functions running (dev, int, tst, acc, prd)\n", " debug: boolean indicating whether session run debug mode. trigger information (print statements) user\n", "\n", " functionality:\n", " file handled worker, first line values extracted.\n", " indicated indices, values added new column file\n", " new set header values assigned file\n", "\n", " example:\n", " convert 2nd column row rename columns [header_a, header_b, header_c, header_d]\n", " file example:\n", " column_a, column_b, column_c\n", " value_a1, value_b1, value_c1\n", " value_a2, value_b2, value_c2\n", "\n", " resulting dataframe:\n", " header_a, header_b, header_c, header_d\n", " value_a1, value_b1, value_c1, column_b\n", " value_a2, value_b2, value_c2, column_b\n", "\n", " '''\n", "\n", "\n", " # set variable 'data_lake_storage_account' using env_code link relevant storage account\n", " data_lake_storage_account:str = f\"{env_code}dapstdala1\"\n", " debug: print(f\"[filworker] data lake storage account: {data_lake_storage_account}\")\n", "\n", "\n", "\n", " # set list metadata needs extracted sql meta db\n", " metadata_keys:list = [\n", " 'file_pattern', \n", " 'file_kind',\n", " 'source_folder', \n", " 'column_delimiter', \n", " 'file_extension', \n", " 'container_name', \n", " 'source_name', \n", " 'header', \n", " 'table_name',\n", " 'escape_character', \n", " 'quote_character'\n", " ]\n", " debug: print(f\"[filworker] metadata variables: {', '.join(metadata_keys)}\")\n", "\n", " # [modules/classes] run stored procedure 'usp_get_task_metadata' set task-object class-member values\n", " task.get_task_metadata(metadata_keys)\n", " task.set_dataset_variables(data_lake_storage_account)\n", " debug: print(f\"[filworker] task variables: {task.variables}\")\n", "\n", "\n", " debug: print(f\"[filworker] filtering directory contents: path = '{task.landing_path}'; file_pattern: {task.variables['file_pattern']}\")\n", " \n", " # [classes:task] get list files process\n", " file_objects:list = task.get_files_objects(data_lake_storage_account)\n", "\n", " # [modules/classes] execute set functions successfully execute first-line conversion file\n", " file_object file_objects:\n", " print(f\"[filworker] processing file: {file_object.file_name} ({datetime.datetime.now().strftime('%h:%m:%s')})\")\n", " \n", " debug: print(f\"[filworker] log file\")\n", " file_object.log_file()\n", "\n", " debug: print(\"[filworker] create object sparkdataframeclass-class\")\n", " # [modules/dataframes_v2] create object calculatedfieldheaders-class\n", " dataframe_object = sparkdataframeclass(debug=debug)\n", " dataframe_object.load_dataframe(source_path=file_object.landing_path, header=file_object.task.header, separator=file_object.task.separator, file_kind=file_object.task.file_kind, quote_character=file_object.task.quote_character, escape_character=file_object.task.escape_character)\n", " debug: print(\"[filworker] convert first row extra columns\")\n", " destination_path = file_object.landing_path.replace(task.file_path, task.table_name)\n", " # dev-note: start_index end_index hardcoded -&gt; make dynamic\n", " dataframe_object.convert_firstrow_to_columns(start_index = 1, end_index=3, column_names=task.source_column_names, header=task.header, destination_path=destination_path, file_extension=task.file_extension, column_delimiter=task.separator)\n", "\n", " debug: print(f\"[filworker] move file archive\")\n", " file_object.landing_to_archive()\n", " dataframe_object.unpersist_dataframe()" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\GenericFunctions.json">{ "name": "genericfunctions", "properties": { "folder": { "name": "functions" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "15ec3da0-b18e-4b91-8a23-b335c3db9303" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "import re\r\n", "import json\r\n", "import py4j\r\n", "import os" ], "execution_count": 196 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 197 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": 198 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define global variables" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# define set regex special characters\r\n", "regex_special_characters = r\"[*+?^${}()|[\\]]\" # note: forward slash included regex-matching executed path-like variables" ], "execution_count": 199 }, { "cell_type": "code", "source": [ "# dev-note: # https://github.com/allrod5/parameters-validation -&gt; use instead\n", "# dev-note: possible install module -&gt; relevant \n", "def validate_argument(argument_name, argument_value, allowed_list):\n", " argument_value allowed_list:\n", " raise valueerror(f\"the {argument_name}-argument '{str(argument_value)}' listed allowed {argument_name} list: {allowed_list}\")" ], "execution_count": 200 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def validate_argument_list(argument_name:str, argument_list:list, allowed_list:list):\n", " # create empty object store 'not allowed parameters'\n", " not_allowed_list = []\n", "\n", " # check every item argument_list contained allowed_list\n", " # not: add item not_allowed_list\n", " argument argument_list:\n", " argument allowed_list:\n", " not_allowed_list.append(argument)\n", "\n", " # not_allowed_list empty: raise error\n", " len(not_allowed_list) &gt; 0:\n", " raise valueerror(f\"[genericfunctions] {argument_name}-argument take following list give parameters: '{', '.join(not_allowed_list)}'; allowed values: {', '.join(allowed_list)}\")\n", "" ], "execution_count": 201 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def validate_linked_service(linked_service):\n", " # validate linked_service value exist list linked services\n", " try:\n", " mssparkutils.credentials.getpropertiesall(linked_service)\n", "\n", " except exception exception:\n", " # error states linked_service exist, throw custom error\n", " 'the linked service exist published' str(exception):\n", " raise valueerror(f\"linked service {linked_service} exist published\") none\n", " # different error thrown, throw entire error message\n", " else:\n", " raise exception" ], "execution_count": 202 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def filter_list(full_list:list, pattern:str, extension:str=none) -&gt; list:\r\n", " \"\"\"\r\n", " return list based pattern extension\r\n", "\r\n", " :param full_list: full list items may may contain pattern\r\n", " :param pattern: pattern used filter on\r\n", " :param extension: extension expected end item (if applicable)\r\n", "\r\n", " :result patterned_list: list items contain pattern extension\r\n", "\r\n", " :note inclusion 'extension' avoid 'finished' files also loaded silver layer\r\n", "\r\n", " \"\"\"\r\n", "\r\n", " # error handling: configuration contains * match_pattern -&gt; set .* comply regex rules\r\n", " pattern == '*':\r\n", " pattern = '.*'\r\n", "\r\n", " # compile regular expression pattern regular expression object, used matching\r\n", " compiled_pattern = re.compile(pattern)\r\n", " # return list fileobjects matches pattern\r\n", " patterned_list= [file file full_list compiled_pattern.search(file.name)]\r\n", " \r\n", " # extension given:\r\n", " extension:\r\n", " # compile regular expression pattern regular expression object, used matching\r\n", " compiled_pattern = re.compile(extension)\r\n", " # return list fileobjects matches extension\r\n", " patterned_list= [file file patterned_list compiled_pattern.search(file.name)]\r\n", "\r\n", " # return filtered list fileobjects\r\n", " return patterned_list" ], "execution_count": 203 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def contains_regex_special_chars(value:str) -&gt; str:\r\n", " \"\"\" \r\n", " check character string regex special character\r\n", "\r\n", " :param value: string value regex characters\r\n", "\r\n", " :result boolean: indication whether value-parameter contains regex character\r\n", " \"\"\" \r\n", " return any(char regex_special_characters char value)" ], "execution_count": 204 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def split_string_at_regex(value:str, split_character:str) -&gt; str:\r\n", " \"\"\"\r\n", " given string separator values (;, /, etc.), split string return first part string first regex.\r\n", " function part greater construct gives list values need match regex, function lists values iteratively.\r\n", "\r\n", " :param value: string value (potentially) contains regex-pattern\r\n", " :param split_character: character string split value \r\n", "\r\n", " :return path: splitted string value regex-like pattern contained\r\n", "\r\n", " :example\r\n", " value = this/is/a/$w+/regex/path\r\n", " split_character = /\r\n", "\r\n", " path = this/is/a/\r\n", "\r\n", " :example\r\n", " value = 'this *w+ regex .* sentence'\r\n", " split_character = ' ' (space)\r\n", "\r\n", " path = is\r\n", " \"\"\"\r\n", "\r\n", " # value empty, return configurationerror\r\n", " value:\r\n", " raise configurationerror(\r\n", " custom_message=f\"user provide value source_folder: {value=}\",\r\n", " notebook_name = \"genericfunctions\",\r\n", " function_name = \"split_string_at_regex\"\r\n", " )\r\n", "\r\n", " # separate value-string using split_character\r\n", " parts: list = value.split(sep=split_character)\r\n", " path_parts: list = []\r\n", "\r\n", " # loop parts string regex-like expression picked up\r\n", " # break loop happens\r\n", " part parts:\r\n", " contains_regex_special_chars(part):\r\n", " path_parts.append(part)\r\n", " else:\r\n", " break\r\n", " \r\n", " # re-join path parts split character\r\n", " path = split_character.join(path_parts)\r\n", "\r\n", " # path empty string contain regex-character:\r\n", " # return initial value -&gt; split \r\n", " path all(contains_regex_special_chars(part) part parts):\r\n", " return value\r\n", "\r\n", " return path" ], "execution_count": 205 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def locate_static_path(regex_path:str) -&gt; tuple:\r\n", " \"\"\"\r\n", " given regex abfss-path, return files regex-match path looping root-folder (non_regex_path)\r\n", " getting back files, regex-search keep files file-path matches regex_path\r\n", "\r\n", " :param regex_path: abfss-path least one regex character \r\n", "\r\n", " :result remaining_files: list files match regex_path\r\n", " :result folders: folders non-regex path \r\n", "\r\n", " :note filter folder-list would remove parent folders higher level regex_path\r\n", " :example regex_path = &lt;parent&gt;/.*/&lt;child&gt; -&gt; folder &lt;parent&gt;/acc match regex_path\r\n", "\r\n", " :example\r\n", " regex_path = this/is/a/$w+/path/with/regex\r\n", " remaining_files = list files path complies regex_path\r\n", " folder = list folders path like non_regex_path\r\n", " \"\"\"\r\n", " # split string recursive places\r\n", " non_regex_path = split_string_at_regex(regex_path, split_character='/')\r\n", " regex_path = re.compile(regex_path)\r\n", " \r\n", " # look directory content first part path (non regex)\r\n", " files, folders = list_directory_content(non_regex_path, [], [])\r\n", "\r\n", " # use regex-path filter returned file list\r\n", " remaining_files = [file file files regex_path.search(file.path)]\r\n", " \r\n", " return remaining_files, folders" ], "execution_count": 218 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def list_directory_content(path:str, files:list, folders:list) -&gt; tuple:\n", " '''\n", " list contents path/container (non recursive)\n", "\n", " :param path: contains path used search folders files\n", " :param files: contains list files found\n", " :param folders:contains list folders found\n", "\n", " :result files,folders : return files/folders recursively found container/path\n", " '''\n", "\n", " # path regex:\n", " # recall function w specific regex properties\n", " contains_regex_special_chars(path):\n", " files, folders = locate_static_path(regex_path=path)\n", " return files, folders # return lists\n", "\n", " else:\n", " try:\n", " path_contents = mssparkutils.fs.ls(path)\n", " item path_contents:\n", " # content directory, append folder-list\n", " # use directory-path new starting point new iteration\n", " item.isdir:\n", " folders.append(item)\n", " files, folders = list_directory_content(item.path, files, folders)\n", " # content file, append files-list\n", " elif item.isfile:\n", " files.append(item)\n", " return files, folders # return lists\n", " except exception e:\n", " error_class, error_message = get_exception_information(e)\n", " error_class==\"filenotfoundexception\":\n", " folders:\n", " return files, folders\n", " raise middlewareerror (custom_message=f\"there issue parent folder {folders}\",notebook_name=\"genericfunctions\",class_name=\"list_directory_content\")" ], "execution_count": 208 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# remove file extension certain path / folder / ...\n", " \n", "def remove_extension(string_with_extension: str, extension_value:str):\n", " \"\"\"\n", " remove extension (.csv, .json, .parquet, etc) string\n", "\n", " :param string_with_extension: string value containing extension needs removed\n", " :param extension_value: value remove string\n", "\n", " :result string_without_extension: string value without extension\n", " \"\"\"\n", "\n", " extension_value:\n", " extension_with_dot = f'.{extension_value}'\n", " # check file string ends extension\n", " string_with_extension.endswith(extension_with_dot):\n", " # remove extension applcatble\n", " string_without_extension = string_with_extension[:-len(extension_with_dot)]\n", " return string_without_extension\n", "\n", " # string extension, something went wrong config\n", " raise configurationerror(\n", " custom_message = f\"trying remove extension {extension_value} {string_with_extension}, extension end string\",\n", " notebook_name = \"genericfunctions\",\n", " function_name = \"remove_extension\"\n", " )\n", " \n", " # extension remove, return initial string\n", " else:\n", " return string_with_extension" ], "execution_count": 209 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def process_string_to_json(string_object:str) -&gt; object:\n", " # converting string_object json-object, replace '-characters \" first\n", " column_info_string = (string_object.replace('\\'', '\"'))\n", " # convert string_object string json object\n", " try:\n", " column_info_object = json.loads(column_info_string)\n", " # return json object\n", " return column_info_object\n", " except:\n", " raise typeerror(f\"the string_object passed cannot converted json object. passed string: {string_object}\")" ], "execution_count": 210 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def rename_blob(file_path, file_format, new_filename):\n", " path_contents = mssparkutils.fs.ls(dir=file_path)\n", " content_names = [file_object.name file_object path_contents]\n", " len(path_contents) &gt; 2 '_success' content_names:\n", " raise exception(f\"too many files {file_path}. files overwritten\")\n", " \n", " item path_contents:\n", " print(item)\n", " # content csv file, append files-list\n", " item.isfile:\n", " item.name.endswith(f'.{file_format}'):\n", " filename = f'{new_filename}.{file_format}'\n", " item_path = os.path.split(item.path)[0]\n", " mssparkutils.fs.mv(src=item.path, dest=f'{item_path}/{filename}', create_path=true, overwrite=true)" ], "execution_count": 211 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def locate_dropfolder(path:str)-&gt;str :\r\n", " '''\r\n", " last folder timestamp folder, function go folders finds one timestamp.\r\n", " find any, throws error.\r\n", " \r\n", " :example abfss://landing@devdapstdala1.dfs.core.windows.net/pdm/unzip/prd/pdm/20240808_110331/pdm_extracts_20240630.gz, \r\n", " return path abfss://landing@devdapstdala1.dfs.core.windows.net/pdm/unzip/prd/pdm/20240808_110331\r\n", "\r\n", " :param path: contains path folder type timestamp must found\r\n", "\r\n", " :result path: path whose last folder timestamp\r\n", " '''\r\n", "\r\n", " # dropfolder expected timestamp structure yyyymmdd_hhmmss\r\n", " dropfolder_pattern:str = r'^\\d{8}_\\d{6}$'\r\n", " last_folder:str = os.path.basename(path)\r\n", " len_path:int =len([f f path os.path.isdir(os.path.join(path, f))])\r\n", " \r\n", " # directories references path, parent folder reached\r\n", " # dropfolder, throw error\r\n", " len_path &lt;= 0:\r\n", " raise exception(\"you reached parent folder without finding drop folder\")\r\n", " \r\n", " # check last folder follows dropfolder_pattern\r\n", " # not, go one level higher call (recursively) function \r\n", " re.search(dropfolder_pattern, last_folder):\r\n", " folder = last_folder\r\n", " return path\r\n", " else:\r\n", " folder = os.path.dirname(path)\r\n", " return locate_dropfolder(folder)" ], "execution_count": 212 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def remove_dropfolder(path:str, look_for_dropfolder:bool=true):\r\n", " \"\"\"\r\n", " function remove :\r\n", " 1. empty folders\r\n", " 2. file called \"finished\" one present folder\r\n", " 3. folders contain one file called \"finished\"\r\n", "\r\n", " :param path: contains path folder(s) file(s) need deleted\r\n", " :param look_for_dropfolder : boolean value indicates correct folder (look_for_dropfolder=false), \r\n", " not(look_for_dropfolder=true), locate_dropfolder used.\r\n", " \"\"\"\r\n", "\r\n", " # yet dropfolder -&gt; change path argument\r\n", " look_for_dropfolder:\r\n", " path=locate_dropfolder(path)\r\n", "\r\n", " # get contents path\r\n", " files, folders = list_directory_content(path, list(), list())\r\n", " \r\n", " # reserve list folders: clean lowest directories first\r\n", " folders.sort(key=lambda x: x.path, reverse=true)\r\n", "\r\n", " # path contains folders: remove first\r\n", " len(folders)!=0:\r\n", " folder folders:\r\n", " remove_dropfolder(folder.path,false)\r\n", " files, folders = list_directory_content(path, list(), list())\r\n", " \r\n", " # files: remove folder\r\n", " len(files) == 0:\r\n", " clean_folder(path)\r\n", "\r\n", " # files:\r\n", " # file called 'finished', remove file folder\r\n", " len(files) == 1 files[0].name==\"finished\":\r\n", " clean_folder(files[0].path)\r\n", " clean_folder(path)\r\n", "\r\n", " # else: anything" ], "execution_count": 213 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def replace_string_value(string:str, value_to_replace:str, replace_value:str) -&gt; str:\r\n", " \"\"\"\r\n", " replace string value another value\r\n", "\r\n", " :param string: string value contains value_to_replace\r\n", " :param value_to_replace: value needs replaced string\r\n", " :param replace_value: value replace initial value string\r\n", "\r\n", " :result string: function return full string, values replaced\r\n", " \"\"\"\r\n", "\r\n", " (isinstance(string, str)):\r\n", " return string.replace(value_to_replace, replace_value)\r\n", " else:\r\n", " raise genericerror(custom_message=f\"you trying exectue string-replace value {value_to_replace} {replace_value}, {str(string)} string!\", notebook_name=\"genericfunctions\", function_name=\"replace_string_value\")" ], "execution_count": 214 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def return_filtered_keys(obj:dict, prop:str, reverse:bool=false) -&gt; list:\r\n", " \"\"\"\r\n", " return list dictionary keys value contains property 'prop' 'true'\r\n", "\r\n", " :param obj: dictionary argument contains keys values check\r\n", " :param prop: property looked value key\r\n", " :param reverse: boolean property indicating resulting list return values without prop-property\r\n", "\r\n", " :return filtered_keys: list keys (not) prop-property\r\n", "\r\n", " :example\r\n", " obj: {key1: {prop:true}, key2: {prop:false}, key3: none}\r\n", "\r\n", " reverse = false: result: [key1]\r\n", " reverse = true: result: [key2, key3]\r\n", " \"\"\"\r\n", " \r\n", " # initialize empty list\r\n", " filtered_keys = list()\r\n", "\r\n", " # get keys object\r\n", " key_list = obj.keys()\r\n", " key key_list:\r\n", " # key value (so none)\r\n", " obj[key]:\r\n", " # check property found value\r\n", " prop obj[key]:\r\n", " # check value true\r\n", " obj[key][prop]: \r\n", " filtered_keys.append(key)\r\n", "\r\n", " # reverse: return keys prop-property set true\r\n", " reverse:\r\n", " filtered_keys = [item item key_list item filtered_keys]\r\n", "\r\n", " return filtered_keys" ], "execution_count": 215 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def create_json_object(**kwargs):\r\n", " \"\"\"\r\n", " return json string\r\n", " :param **kwargs : dictionnary argument needs converted\r\n", "\r\n", " :return info : json string converted\r\n", " \"\"\"\r\n", " info=json.dumps(kwargs)\r\n", " return info" ], "execution_count": 216 } ] } }</file><file name="src\synapse\studio\notebook\IngestionFunctions.json">{ "name": "ingestionfunctions", "properties": { "folder": { "name": "functions" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "b6ca4e0f-f5b4-4c8a-a595-b876db781f93" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# ingestion functions\n", "this notebook contains functions used ingestion process. main functions contained notebook are:\n", "1. merge_file: merge new data file existing delta table based primary keys table\n", "2. column_match: primary key match, update row columns\n", "3. no_match: primary key match, insert row columns\n", "4. list_directory_contents: returns list files list folders contained specific (directory a) container\n", "\n", "**language**: pyspark" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": null }, { "cell_type": "code", "source": [ "# import packages\n", "# import delta.tables dt \n", "import # regex package: used analyse patterns strings (e.g. file names, col)\n", "import json\n", "import pyspark.sql sql\n", "from pyspark.sql.functions import monotonically_increasing_id, lit, udf\n", "from pyspark.sql.functions import year,month,days,col,to_timestamp,date_format, regexp_extract\n", "\n", "import datetime" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "## function move generic functions would cause conflict development done notebook -&gt; adding temporarily\r\n", "\r\n", "def convert_lists_to_list_of_dicts(lists:list, keys:list) -&gt; list:\r\n", " \"\"\"\r\n", " convert set lists [\"value11\", \"value12\"] [\"value21\", \"value22\"] list dictionaries: [{\"key1\": \"value11\", \"key2\": \"value21\"}, {\"key1\": \"value12\", \"key2\": \"value22\"}]\r\n", "\r\n", " :param lists: contains list lists need converted\r\n", " :param keys: list dictionary keys need used\r\n", "\r\n", " :result dict_list: list dictionary objects\r\n", " \"\"\"\r\n", "\r\n", " len(lists) != len(keys):\r\n", " raise valueerror(f\"number keys match number lists: \\n{keys}\\n{lists}\")\r\n", "\r\n", " # transpose list lists\r\n", " transposed_lists = list(zip(*lists))\r\n", "\r\n", " # zip list dictionary object\r\n", " dict_list = [dict(zip(keys, inner_list)) inner_list transposed_lists]\r\n", "\r\n", " return dict_list\r\n", "" ], "execution_count": 26 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "## function move generic functions would cause conflict development done notebook -&gt; adding temporarily\r\n", "\r\n", "def replace_listitems_by_keyvalues(keys_to_replace:list, key_value_pairs:dict) -&gt; list:\r\n", " \"\"\"\r\n", " return list items based dictionary:\r\n", " \r\n", " :param keys_to_replace: list items keys key_value_pairs parameter\r\n", " :param key_value_pairs: dictionary key-value pairs\r\n", "\r\n", " :return replaced_values: list items values key_value_pairs\r\n", "\r\n", " :example\r\n", " keys_to_replace = [\"foo\", \"bar\"]\r\n", " key_value_pairs = {\"test\": \"test\", \"foo\":\"foo\", \"bar\": \"bar\"}\r\n", " replaced_values = [\"foo\", \"bar\"]\r\n", "\r\n", " :note key keys_to_replace key_value_pairs, error thrown. avoid mix keys values, missing items returned list\r\n", "\r\n", " \"\"\"\r\n", " replaced_values = list()\r\n", " key keys_to_replace:\r\n", " key key_value_pairs:\r\n", " replaced_values.append(key_value_pairs[key])\r\n", " else:\r\n", " raise valueerror(f\"key {key} cannot replaced value. process stopped safety reasons...\")\r\n", " \r\n", " \r\n", " return replaced_values" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## partitioning functions" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "partitioning big part reading writing dataframes. functions defined subsection oriented towards functionality" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# %run modules/classes # \"active\", causes issue referencing many notebooks testnotebook. assumption: notebook never called individually always called modules/classes \"imported\"" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# whenever partition-object contains reference datepart (year, month, day), validation needs occur make sure referenced column indeed convertable timestamp date datatype\n", "# function \n", "# 1. cast column timestamp datatype\n", "# 2. create checks-class object (classes-notebook) \n", "# 3. execute casted_values-method checks-class validate correct datatype conversion\n", "\n", "def cast_partitioning_timestamp(dataframe, partition_key):\n", " '''\n", " inputs:\n", " dataframe: dataframe containing column datetime-partitioning needs done\n", " partition_key: name column needs converted timestamp\n", " '''\n", "\n", " # create instance checks-class execute casted_values-method correct datatype conversion\n", " checks_object = sparkdataframechecks(source_path='', header=true, separator='', file_kind='', column_names=list(), data_types=list(), checks=list(), skip_lines='', column_information=list(), deploy=true)\n", " checks_object.dataframe = dataframe\n", " checks_object.cast_column(dataframe_subset=dataframe.select(partition_key), column=partition_key, data_type='timestamp', formatting={\"format\": \"yyyy-mm-dd\"})\n", "\n", " # duplicate partition-column (casted_&lt;name&gt;) cast original column\n", " casted_dataframe = dataframe.select(partition_key).withcolumn(f\"casted_{partition_key}\", col(partition_key))\n", " casted_dataframe = casted_dataframe.withcolumn(f\"{partition_key}\", to_timestamp(col(partition_key)))\n", " # casted_dataframe.show()\n", " return casted_dataframe" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# function called dealing partitioning dataframes/delta tables\n", "# function return:\n", "# 1. dataframe potentially added columns related timestamps\n", "# 2. list column names partitioning executed\n", "\n", "def create_partioning_list(dataframe, partitioning_objects: dict):\n", " '''\n", " inputs:\n", " dataframe: data potentially new columns need added partitioning\n", " partitioning_objects: json-object containing names columns partition\n", "\n", " output:\n", " partition_column_list = [platformname, p_year, p_month]\n", " dataframe = original dataframe + [col(p_year), col(p_month)]\n", "\n", " functionality:\n", " function loop partitioning_objects dictionary\n", " object, check key 'datepart' present\n", " yes:\n", " cast column timestamp validate conversion (cast_partitioning_timestamp())\n", " add 'p_' column dataframe partition_column_list\n", " no:\n", " add column partition_column_list\n", " \n", " raise error datepart-key present contain valid value (year, month, day)\n", "\n", "\n", " example arguments:\n", " partitioning_objects: [\n", " {\"name\": \"platform_name\", \"sequence\": 1},\n", " {\"name\": \"creation_date\", \"sequence\": 2, \"datepart\": \"year\"},\n", " {\"name\": \"creation_date\", \"sequence\": 3, \"datepart\": \"month\"},\n", " ]\n", "\n", " -&gt; datepart-key, new column added dataframe. datepart takes part data partition rather entire date\n", " added columns prefix 'p_' indicate part original dataframe added partitioning-columns\n", "\n", "\n", " '''\n", "\n", " # initiate empty list\n", " partition_column_list = []\n", "\n", " # loop objects partitioning_objects dictionary\n", " partition_key partitioning_objects:\n", " # get value key datepart\n", " datepart = partition_key.get('datepart')\n", "\n", " # value == 'year', add p_year partition_column_list dataframe\n", " datepart == 'year':\n", " cast_partitioning_timestamp(dataframe, partition_key.get('name'))\n", "\n", " partition_column_list.append('p_year')\n", " dataframe = dataframe.withcolumn(\"p_year\", date_format(col(partition_key.get('name')), \"yyyy\"))\n", "\n", " # value == 'month', add p_month partition_column_list dataframe\n", " elif datepart == 'month':\n", " cast_partitioning_timestamp(dataframe, partition_key.get('name'))\n", "\n", " partition_column_list.append('p_month')\n", " dataframe = dataframe.withcolumn(\"p_month\", date_format(col(partition_key.get('name')), \"mm\"))\n", "\n", " # value == 'day', add p_day partition_column_list dataframe\n", " elif datepart == 'day':\n", " cast_partitioning_timestamp(dataframe, partition_key.get('name'))\n", "\n", " partition_column_list.append('p_day')\n", " dataframe = dataframe.withcolumn(\"p_day\", date_format(col(partition_key.get('name')), \"dd\"))\n", "\n", " # value == none, add column_name partition_column_list\n", " elif datepart == none:\n", " partition_column_list.append(partition_key.get('name'))\n", "\n", " # other, throw invalid error\n", " else:\n", " raise valueerror(f'invalid key value datepart: {datepart}. values year, month, day allowed.')\n", "\n", " return dataframe, partition_column_list" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## standalone functions" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "user defined function: force varchar-constrains casting dataframe" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# purpose: skip first x lines dataframe\n", "def skip_first_lines(landing_path:str, delimiter:str, header:bool, skiplines:int):\n", " '''\n", " inputs:\n", " landing_path: string-value abfss-path source file landing zone\n", " delimiter: column-delimiter used source file\n", " header: boolean value indicating whether, skipping first lines, headerline first row\n", " skiplines: integer value, indicating many rows skip, starting top\n", "\n", " output:\n", " dataframe: pyspark-dataframe object first x lines skipped \n", "\n", " functionality:\n", " load source file one-column text file\n", " add column 'row_id' remove columns row_id &lt;= skiplines\n", " split resulting dataframe based delimter header\n", "\n", "\n", " notes:\n", " first line initial file headerline, recommended use function include headerline rows skip set headers=false dataset configuration\n", " new headers (_c0, _c1, etc.) added returned dataframe moving raw silver, headerline added based dataset configuration\n", "\n", " '''\n", " # read full file one-column dataframe (column_name: value)\n", " text_df = spark.read.text(landing_path)\n", " \n", " # add column 'row_id' remove columns row_id lower skiplines argument\n", " filtered_df = text_df.filter(text_df.value != '').withcolumn('row_id', sql.functions.monotonically_increasing_id()).filter(sql.functions.col('row_id') &gt;= skiplines).drop('row_id')\n", " \n", " # map resulting dataframe based one-column (value) return contents (basically: remove column name 'value')\n", " filtered_rdd = filtered_df.rdd.map(lambda x: x.value)\n", "\n", " # read filtered dataframe proper delimiter pass first line headerline header=true \n", " dataframe = spark.read.options(delimiter=delimiter, header=header).csv(filtered_rdd)\n", "\n", " return dataframe" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# udf (user defined function) allows enforce varchar-constraint column\n", "# used casting-step, ingestion framework validates whether incoming data correct datatype\n", "def enforce_varchar_constraint(metadata):\n", " # first get varchar(&lt;maxlenght&gt;) metadata string, look like {..., '__char_varchar_type_string': 'varchar(&lt;maxstring&gt;),...}\n", " # throw error metadata string contain expected metadata value (defined above)\n", " try:\n", " max_length = int(metadata.get('__char_varchar_type_string', '').split('(')[1].split(')')[0])\n", " except indexerror:\n", " raise valueerror(f\"metadata contain correct varchar-definition: {metadata}. expected something like {{'__char_varchar_type_string': 'varchar(xx)'}}\")\n", "\n", " # return string-value shorter max_length\n", " # return none string-value longer max_length\n", " def check_length(s):\n", " none:\n", " return s\n", " elif len(s) &gt; max_length:\n", " return none\n", " else:\n", " return s\n", " \n", " # return udf-object combination check_lenght-function stringtype-datatype\n", " # applying udf-object column, \n", " return udf(check_length, sql.types.stringtype())" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def filter_directory_content(path: str, file_pattern: str):\n", " # get files / folders certain path, filter based certain file_pattern\n", " files, folders = list_directory_content(path, list(), list())\n", "\n", " patterned_files = []\n", " file_pattern != '*':\n", " # file_pattern != wildcard, match pattern\n", " patterned_files = [file file files re.match(re.compile(file_pattern), file.name)]\n", " else:\n", " # else: return whole list\n", " patterned_files = files\n", "\n", " return patterned_files\n", "" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def removerowsdf(dataframe: sql.dataframe, rows_to_remove: list):\n", " # purpose: add n-rows dataframe, based index starting 0 n\n", "\n", " # pre-checks:\n", " max_df_index = dataframe.rdd.count() - 1\n", " row rows_to_remove:\n", " ## digits rows_to_remove\n", " isinstance(row, int):\n", " raise typeerror(f\"removerowsdf: item '{row}' inside rows_to_remove integer. integers allowed\")\n", "\n", " ## row trying remove greater last index dataframe, return error\n", " elif row &gt; max_df_index:\n", " raise indexerror(f\"removerowsdf: max index dataframe {max_df_index}. index trying remove '{row}' outofbounds\")\n", "\n", " # first, save dataframe copy, also add index column end\n", " # add incremental start removing rows want\n", " new_dataframe = dataframe.withcolumn('remove_index', monotonically_increasing_id())\n", "\n", " # every row specific index number 'rows_to_remove' removed (0 = first row, 1 = second row, ...)\n", " # that, drop index columns longer need it\n", " new_dataframe = new_dataframe.filter(~new_dataframe.remove_index.isin(rows_to_remove))\n", " new_dataframe = new_dataframe.drop('remove_index')\n", "\n", " return new_dataframe" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## merge functions\n", "**main function: merge_file()**\n", "this function main orchestration merging new raw file existing delta table.\n", "it calls following functions:\n", "1. **pk_match()**: create string shows primary key columns raw file match primary key columns existing delta table.\n", "2. **column_match()**: specific row: match primary keys raw data delta table, return dictionary columns update\n", "3. **no_match()**: specific row: match primary keys raw data delta labe, return dictionary columns insert" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def pk_match(old_alias: str, new_alias: str, pk_columns: dict):\n", " \"\"\"\n", " based dictionary primary key columns ({ delta lake : raw data }), create statement matches primary keys delta table raw data\n", "\n", " parameters:\n", " :param old_alias: alias used delta lake merge_file() function\n", " :param new_alias: alias used refer raw data merge_file() function\n", " :param pk_columns: dictionary primary columns delta lake raw data paired up: { 'id' : 'id', 'date': 'date' }\n", "\n", " :result condition: concatenated string matches primary keys old_alias new_alias\n", "\n", " :example\n", " primary key delta table [id, date] raw data [id, date]\n", " function return: 'olddata.id = newdata.id olddata.date = newdata.date'\n", "\n", " :note\n", " notice new_alias columns contain ` beginning end column name. column names could contain spaces could mess concatenated string\n", " case old_alias (=delta table columns) team agreed standard use spaces _ instead.\n", " \"\"\"\n", " condition = ' '.join(f'{old_alias}.{pk} = {new_alias}.`{pk_columns[pk]}`' pk pk_columns)\n", " return condition" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# return dictionary columns update row already exists table\n", "# now: rows updated\n", "\n", "# parameters:\n", "# columns: list columns updated\n", "# data: alias used refer raw data merge_file function\n", "\n", "def columnmatch(columns: dict, data: str):\n", " match_dictionary = dict()\n", " column columns:\n", " # match -&gt; row updated\n", " # need \"touch\" t_insert_date column\n", " # therefore, include resulting dictionary\n", " column == 't_insert_date':\n", " continue\n", " match_dictionary[column] = sql.functions.col(\"%s.%s\" %(data, columns[column]))\n", "\n", " return match_dictionary" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# return dictionary columns update row exists already table\n", "# now: rows updated\n", "\n", "# parameters:\n", "# columns: list columns updated\n", "# data: alias used refer raw data merge_file function\n", "\n", "def nomatch(columns: dict, data: str):\n", " no_match_dictionary = dict()\n", " column columns:\n", " no_match_dictionary[column] = sql.functions.col(\"%s.%s\" %(data, columns[column]))\n", " \n", " return no_match_dictionary" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def fill_extraction_date(dataframe, target_options:dict):\r\n", " \"\"\"\r\n", " set values technical column t_extract_date\r\n", "\r\n", " :param dataframe: current rawdata handled\r\n", " :param target_options: dictionary information extract data t_extract_date\r\n", "\r\n", " :result dataframe: current rawdata t_extract_date column non-empty\r\n", " \"\"\"\r\n", "\r\n", " # property 'extract_date' defined: \r\n", " 'extract_date' target_options.keys():\r\n", " # get extract_info separate object\r\n", " extract_info:dict = target_options['extract_date']\r\n", "\r\n", " # get name column use determine extraction time\r\n", " extract_column:str = extract_info['column_name']\r\n", "\r\n", " # column part dataframe\r\n", " extract_column dataframe.columns:\r\n", " # extract date column\r\n", " 'regex_expression' extract_info.keys():\r\n", " # define regex expression allow get timestamp column\r\n", " regex_expression:str = extract_info['regex_expression']\r\n", "\r\n", " # define extract_date_format: format timestamp regex extraction\r\n", " 'extract_date_format' extract_info.keys():\r\n", " extract_date_format:str = extract_info['extract_date_format']\r\n", " else:\r\n", " extract_date_format = 'yyyymmdd_hhmmss'\r\n", " \r\n", " # extract timestamp using regex expression date_format\r\n", " dataframe = dataframe.withcolumn('t_extract_date', to_timestamp(regexp_extract(extract_column, regex_expression, 1), extract_date_format))\r\n", " \r\n", " else:\r\n", " # dev-info: regex-expression needed, assumption made column already datatype 'timestamp'\r\n", " # means already format given column date_format assumed relevance\r\n", " dataframe = dataframe.withcolumn('t_extract_date', extract_column)\r\n", "\r\n", " ## t_extract_date could set -&gt; throw error\r\n", " dataframe.filter(col('t_extract_date').isnull()).count() &gt; 0:\r\n", " raise configurationerror(\r\n", " custom_message=f\"extract date could set using extract_column {extract_column}\", \r\n", " notebook_name=\"ingestionfunctions\", \r\n", " function_name=\"fill_extraction_date\"\r\n", " )\r\n", " \r\n", " return dataframe\r\n", " \r\n", " # column name could found -&gt; throw error\r\n", " raise configurationerror(\r\n", " custom_message=f'column {extract_column} cannot found given dataframe', \r\n", " notebook_name='ingestionfunctions', \r\n", " function_name='fill_extraction_date'\r\n", " )\r\n", "\r\n", " # key defined\r\n", " else:\r\n", " # dev-note: back-up action needs exist long extract_date mandatory field\r\n", " # field mandatory, error thrown extract data cannot found\r\n", " current_time = datetime.datetime.now()\r\n", " dataframe = dataframe.withcolumn('t_extract_date', lit(current_time).cast(sql.types.timestamptype()))\r\n", " return dataframe" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def mergefile(deltatable, rawdata, pk_columns_dict:dict, column_names_dict:dict, target_options:dict={}):\n", " \"\"\"\n", " execute merge new raw data file existing delta table\n", "\n", " :param deltatable: dataframe existing delta table\n", " :param rawdata: dataframe to-be ingested raw data file\n", " :param pk_columns_dict: dictionary primary columns delta lake raw data paired up\n", " :param column_names_dict: dictionary columns delta lake raw data paired up\n", " :param target_options: dictionary object containing additional checks configurations writing delta table: partitioning, extract_date, etc\n", " \"\"\"\n", " # alias: used refer new raw data old/existing delta table\n", " new_alias = \"newdata\"\n", " old_alias = \"olddata\"\n", "\n", " ############################################################################################\n", " #################################### t_load_date_silver ####################################\n", "\n", " # validate column 't_load_date_silver' exist rawdata dataframe adding timestamp\n", " 't_load_date_silver' rawdata.columns:\n", " raise valueerror(\"column 't_load_date_silver' exist raw dataframe. something must gone wrong writing landing raw...\")\n", "\n", " # add current time t_load_date_silver column rawdata dataframe\n", " current_time = datetime.datetime.now()\n", " rawdata = rawdata.withcolumn('t_load_date_silver', lit(current_time).cast(sql.types.timestamptype()))\n", "\n", " ############################################################################################\n", "\n", " ############################################################################################\n", " #################################### t_extract_date ########################################\n", "\n", " # validate column 't_extract_date' exist rawdata dataframe adding timestamp\n", " 't_extract_date' rawdata.columns:\n", " raise valueerror(\"column 't_extract_date' exist raw dataframe. something must gone wrong writing landing raw...\")\n", "\n", " # add extract time t_extract_date column rawdata dataframe\n", " rawdata = fill_extraction_date(dataframe=rawdata, target_options=target_options)\n", "\n", " ############################################################################################\n", "\n", "\n", " ############################################################################################\n", " ################################### optional_columns #######################################\n", "\n", " # columns optional, still configured dataset configuration\n", " # given mandatory checks headers datatypes, columns occur rawdata \n", " # removed column_names_dict\n", " # removed, issue matching olddata newdata columns\n", " keys_to_remove = list()\n", " key, value column_names_dict.items():\n", " value rawdata.columns:\n", " keys_to_remove.append(key)\n", "\n", " column_names_dict = {k: v k, v column_names_dict.items() k keys_to_remove}\n", "\n", " ############################################################################################\n", "\n", " ############################################################################################\n", " ####################################### partitioning #######################################\n", " \n", " # rawdata = rawdata.withcolumn(partitioning_timestamp,to_timestamp(col(partitioning_timestamp))).withcolumn(\"day\", date_format(col(partitioning_timestamp), \"dd\")).withcolumn(\"year\", date_format(col(partitioning_timestamp), \"yyyy\")).withcolumn(\"month\", date_format(col(partitioning_timestamp), \"mm\"))\n", " # add columns partitioning\n", " # needs happen quality checks, \n", " 'partitioning' target_options.keys():\n", " # get list columns partitioning executed\n", " partition_target_columns:list = [partition_object['name'] partition_object target_options['partitioning']]\n", " # return list columns source file partitioning executed\n", " partition_source_columns:list = replace_listitems_by_keyvalues(partition_target_columns, column_names_dict)\n", " # create list objects references target columns replaced references source\n", " source_partitioning:list = [{**dct, 'name': value} dct, value zip(target_options['partitioning'], partition_source_columns)]\n", " # print(source_partitioning)\n", " \n", " rawdata, partioning_column_list = create_partioning_list(rawdata, source_partitioning)\n", "\n", "\n", " 'p_year' partioning_column_list:\n", " column_names_dict['p_year'] = 'p_year'\n", "\n", " 'p_month' partioning_column_list:\n", " column_names_dict['p_month'] = 'p_month'\n", "\n", " 'p_day' partioning_column_list:\n", " column_names_dict['p_day'] = 'p_day'\n", " \n", " # repartitioing dataframe based partitioning columns\n", " # dev-info: benefit optimize-functions less file-movements needed\n", " # original df: multiple nodes contain subsets dataframe. subset contain multiple partitions\n", " # writing df, file created partition per node\n", " # say 3 nodes 4 partitions -&gt; 12 files created \n", " # repartitioned df: partition grouped one node (simplified)\n", " # writing df, file created per partition (simplified)\n", " # say 3 nodes 4 partitions -&gt; 4 files created (1 partition)\n", " # note: simplified: partition might able contained 1 node large, idea abstracted explanation\n", " rawdata.repartition(*partioning_column_list)\n", "\n", " else:\n", " partioning_column_list = []\n", " \n", " target_partition_columns:list = deltatable.detail().select(\"partitioncolumns\").collect()[0][0]\n", " source_partition_columns:list = replace_listitems_by_keyvalues(target_partition_columns, column_names_dict)\n", "\n", " source_partition_columns != partioning_column_list:\n", " raise valueerror(f\"given list partition columns {partioning_column_list} match partition columns specified raw data: {source_partition_columns}\")\n", "\n", " ############################################################################################\n", "\n", "\n", " #####################################################################################################\n", " ################################### t_update_date &amp; t_insert_date ###################################\n", "\n", " # validate column 't_update_date' exist rawdata dataframe adding timestamp\n", " 't_update_date' rawdata.columns:\n", " raise valueerror(\"column 't_update_date' exist raw dataframe. something must gone wrong writing landing raw...\")\n", " \n", " # validate column 't_insert_date' exist rawdata dataframe adding timestamp\n", " 't_insert_date' rawdata.columns:\n", " raise valueerror(\"column 't_insert_date' exist raw dataframe. something must gone wrong writing landing raw...\")\n", "\n", " # add current time t_update_data t_insert_date column rawdata dataframe\n", " current_time = datetime.datetime.now()\n", " rawdata = rawdata.withcolumn('t_update_date', lit(current_time).cast(sql.types.timestamptype()))\n", " rawdata = rawdata.withcolumn('t_insert_date', lit(current_time).cast(sql.types.timestamptype()))\n", "\n", "\n", " ############################################################################################\n", " ########################################## merge ###########################################\n", " # execute merge raw data delta table based primary columns dataframes\n", " \n", " deltatable.alias(old_alias) \\\n", " .merge(\n", " rawdata.alias(new_alias), pk_match(old_alias, new_alias, pk_columns_dict)) \\\n", " .whenmatchedupdate(set=columnmatch(column_names_dict, new_alias)) \\\n", " .whennotmatchedinsert(values = nomatch(column_names_dict, new_alias)) \\\n", " .execute()\n", " \n", "# potential future use (soft-delete, insert, update):\n", "# (targetdf\n", "# .merge(sourcedf, \"source.key = target.key\")\n", "# .whenmatchedupdateall()\n", "# .whennotmatchedinsertall()\n", "# .whennotmatchedbysourceupdate()\n", "# .execute()\n", "# )\n", "# -&gt; https://docs.delta.io/latest/delta-update.html" ], "execution_count": 12 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# extract data subpath\n", " \n", "def get_parent_folder(path: str):\n", " # rsplit (= start splitting right), character '/', get name folder file in\n", " # parent folder second place array, since splitting first two '/' starting right\n", " '/' path:\n", " # '/', parent\n", " raise exception(f\"get_parent_folder: path '{path}' qualify path parent folder\")\n", "\n", " path.count('/') == 1:\n", " # happens depth folders parent/child\n", " path_part = path.rsplit('/', 1)[0]\n", " else:\n", " # cases (e.g. parent/parent/child, parent/parent/.../child)\n", " path_part = path.rsplit('/', 2)[1]\n", "\n", " return path_part" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# create dictionary list flat-json objects stored strings\n", "\n", "def jsons_to_dict(json_list:list):\n", " # init place want store whole dictionary\n", " big_dictionary = dict()\n", "\n", " json_str json_list:\n", " (json_str.count('{') &gt; 1):\n", " # count &gt; 1, means multiple json objects inside\n", " # so: create array handle situation\n", "\n", " # first remove white space, object inside whitespaces\n", " # matter using split(',')\n", " # create array loop over\n", " json_without_whitespaces = json_str.replace(\" \", \"\")\n", " jsons_array = json_without_whitespaces.split(',')\n", "\n", " obj jsons_array:\n", " try:\n", " # parse object\n", " dict_part_array = json.loads(obj)\n", "\n", " # add dictionary\n", " big_dictionary.update(dict_part_array)\n", " except exception:\n", " raise typeerror(f\"unable parse string {obj} json\")\n", " else:\n", " try:\n", " # parse object\n", " dict_part = json.loads(json_str)\n", "\n", " # add dictionary\n", " big_dictionary.update(dict_part)\n", " \n", " except exception:\n", " raise typeerror(f\"unable parse string {json_str} json\")\n", "\n", " return big_dictionary" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# def list_files(path, to_remove):\n", "# files = mssparkutils.fs.ls(path)\n", "# file files:\n", "# file.isdir:\n", "# new_files, new_remove = list_files(file.path, to_remove)\n", "# files += new_files\n", "# to_remove += new_remove\n", "# to_remove.append(file)\n", "# return files, to_remove" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# def move_to_raw(variables, data_lake_storage_account, sqlcon, task):\n", "# storage_account = data_lake_storage_account\n", "# container_name = variables['container_name']\n", "# file_path = variables['source_folder'].replace('*', '')\n", "# header = variables['header']\n", "# separator = variables['column_delimiter']\n", "# format = variables['file_extension']\n", "\n", "# path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, file_path)\n", "\n", "# files, to_remove = list_files(path, [])\n", "# [files.remove(file) file to_remove file files]\n", "# task.variables['file_pattern'] != '*':\n", "# patterned_files = [file file files re.match(re.compile(task.variables['file_pattern']),file.name) ]\n", "# else:\n", "# patterned_files = files\n", "# found_files = {file.path:file.name.split('.')[0] file patterned_files}\n", "# # print(files, patterned_files, found_files, to_remove) \n", "# file_objects = [file(task, sqlcon, file, found_files[file], file.split('/')[-2], storage_account) file found_files]\n", "# file file_objects:\n", "# landingdata = spark.read.load(file.landing_path, header=header, sep=separator, format = format)\n", "# landingdata.write.mode(\"overwrite\").parquet(file.raw_path)\n", "# file.update_file_activity(\"raw\", true, file.raw_path)\n", "# return file_objects" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# def ingest_to_silver(variables: dict, file: object, data_lake_storage_account: str):\n", "# storage_account = data_lake_storage_account\n", "# table_name = variables['table_name']\n", "# header = variables['header']\n", "# separator = variables['column_delimiter']\n", "# format = 'parquet'\n", "# dimensions = variables['dimension']\n", "\n", "# pk_column_dict = {variables['sink_name'][index]:variables['source_name'][index] index, dimension enumerate(dimensions) dimension == 'pk'}\n", "# column_names_dict = dict(zip(variables['sink_name'], variables['source_name']))\n", "# silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', storage_account, table_name)\n", "# #file.set_silver_path(silver_path)\n", "\n", "# deltatable = dt.deltatable.forpath(spark, silver_path)\n", "# rawdata = spark.read.load(file.raw_path, header=header, sep=separator, format = format)\n", "# mergefile(deltatable, rawdata, pk_column_dict, column_names_dict)\n", "# file.update_file_activity(\"silver\", true, silver_path)" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# def move_to_archive(file):\n", "# mssparkutils.fs.mv(file.landing_path, file.archive_path, true)\n", "# file.update_file_activity(\"archive\", true, file.archive_path)\n", "# return" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# def merge_ingestion(variables, data_lake_storage_account):\n", "# # source sink location\n", "# storage_account = data_lake_storage_account\n", "# container_name = variables['container_name']\n", "# table_name = variables['table_name']\n", "# file_path = variables['source_folder']\n", "\n", "# # file layout variables\n", "# file_name = variables['file_pattern']+'.'+variables['file_type']\n", "# format = variables['file_type']\n", "# header = variables['header']\n", "# separator = variables['column_delimiter']\n", "\n", "\n", "# # file path definitions\n", "# source_path_full = \"abfss://%s@%s.dfs.core.windows.net/%s/%s\" % (container_name, storage_account, file_path, file_name)\n", "# silver_path_full = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', storage_account, table_name)\n", "\n", "# # column names primary keys\n", "# pk_column_names_dict = {variables['sink_name'][index]:variables['source_name'][index] index, dimension enumerate(dimensions) dimension == 'pk'}\n", "# column_names_dict = dict(zip(variables['sink_name'], variables['source_name']))\n", "\n", " # list directories, including timestamps, found landing container (and following source_path)\n", " # list used dump files raw archive\n", "\n", " # file_path_stripped = file_path.replace('*', '')\n", " # file_list = mssparkutils.fs.ls(\"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, file_path_stripped))\n", " # source_path_timestamps = list()\n", " # raw_path_dict = dict()\n", " # archive_path_dict = dict()\n", " # file file_list:\n", " # source_path_timestamps.append(file.path)\n", " # raw_path_dict[file.path] = \"abfss://%s@%s.dfs.core.windows.net/%s/%s\" % ('raw', storage_account, file_path_stripped, file.name)\n", " # archive_path_dict[file.path] = \"abfss://%s@%s.dfs.core.windows.net/%s/%s\" % ('archive', storage_account, file_path_stripped, file.name)\n", "\n", "\n", " # # loop files within landing folders\n", " # source_file source_path_timestamps:\n", " # deltatable = dt.deltatable.forpath(spark, silver_path_full)\n", " # newdata = spark.read.load(source_file, header=header, sep=separator, format = format)\n", " # newdata.write.mode(\"overwrite\").parquet(raw_path_dict[source_file])\n", " # mergefile(deltatable, newdata, pk_column_names, column_names_dict)\n", " # mssparkutils.fs.mv(source_file, archive_path_dict[source_file], true)" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\IngestionWorker.json">{ "name": "ingestionworker", "properties": { "folder": { "name": "workers" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "a36328c8-c080-484e-b0fa-9e45437365ea" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# ingestion worker\n", "the ingestionworker used ingestion. purpose worker to:\n", "- collect list files processed task\n", "- move files landing raw\n", "- ingest files raw dataframe silver delta table\n", "- move processed files landing archive" ] }, { "cell_type": "code", "source": [ "def run_ingestion_worker(task: object, env_code: str, debug:bool=true):\n", " '''\n", " inputs\n", " task: object task-class [modules/classes]\n", " env_code: code used indicate environment functions running (dev, int, tst, acc, prd)\n", " debug: boolean indicating whether session run debug mode. trigger information (print statements) user\n", "\n", " functionality:\n", " file handled worker will:\n", " - move landing raw, converted parquet file process\n", " - ingested silver delta table\n", " - move landing archive, clearing landing container already processed files\n", " '''\n", "\n", " # set variable 'data_lake_storage_account' using env_code link relevant storage account\n", " data_lake_storage_account:str = f\"{env_code}dapstdala1\"\n", " debug: print(f\"[ingestionworker] data lake storage account: {data_lake_storage_account}\")\n", "\n", " # set list metadata needs extracted sql meta db\n", " metadata_keys:list = [\n", " 'file_layout',\n", " 'file_pattern', \n", " 'file_kind',\n", " 'file_extension', \n", " 'target_options', \n", " 'skip_first_lines',\n", " 'column_delimiter', \n", " 'header', \n", " 'source_name', \n", " 'sink_name', \n", " 'dimension', \n", " 'source_folder', \n", " 'data_type', \n", " 'column_info',\n", " 'table_name', \n", " 'container_name',\n", " 'escape_character',\n", " 'quote_character'\n", " ]\n", " debug: print(f\"[ingestionworker] metadata variables: {', '.join(metadata_keys)}\")\n", "\n", " # [modules/classes] run stored procedure 'usp_get_task_metadata' set task-object class-member values\n", " task.get_task_metadata(metadata_keys)\n", " task.set_dataset_variables(data_lake_storage_account)\n", " task.set_ingestion_variables(data_lake_storage_account)\n", " debug: print(f\"[ingestionworker] task variables: {task.variables}\")\n", " \n", "\n", " # table_name known, ingestion start\n", " # without target table_name, landing location source file\n", " task.table_name:\n", "\n", " # retrieve files need ingested task hand\n", " debug: print(\"[ingestionworker] set file objects\")\n", " file_objects:list = task.get_files_objects(data_lake_storage_account)\n", " sorted_file_objects = sorted(file_objects, key=lambda obj: obj.landing_path,reverse=false)\n", " # loop file execute file movements (and necessary data quality checks)\n", " file sorted_file_objects:\n", " print(f'[ingestionworker] processing file: {file.file_name} ({datetime.datetime.now().strftime(\"%h:%m:%s\")})')\n", "\n", " try:\n", " debug: print('[ingestionworker] copy file raw')\n", " file.landing_to_raw()\n", " except exception error:\n", " print(f'[ingestionworker] error raw {error}')\n", " e_class, e_msg=get_exception_information(error)\n", " error_object = handle_exceptions(e_class, e_msg, error)\n", " file.update_file_activity(activity=\"raw\", success=false, info_message=json.dumps(error_object))\n", " raise error\n", "\n", " try:\n", " debug: print('[ingestionworker] ingest file silver')\n", " file.raw_to_silver()\n", " except exception error:\n", " print(f'[ingestionworker] error silver {error}')\n", " e_class, e_msg=get_exception_information(error)\n", " error_object = handle_exceptions(e_class, e_msg, error) \n", " file.update_file_activity(activity=\"silver\", success=false, info_message=json.dumps(error_object))\n", " raise error\n", "\n", " try:\n", " debug: print('[ingestionworker] move file archive')\n", " file.landing_to_archive()\n", " except exception error:\n", " print(f'[ingestionworker] error archive {error}')\n", " e_class, e_msg=get_exception_information(error)\n", " error_object = handle_exceptions(e_class, e_msg, error)\n", " file.update_file_activity(activity=\"archive\", success=false, info_message=json.dumps(error_object))\n", " raise error\n", "\n", "\n", " # ingesting files delta table, optimize storage table\n", " debug: print('[ingestionworker] optimize table storage')\n", " delta_table:object = optimizedeltatable(table_name=task.table_name, env_code=env_code, debug=debug)\n", " delta_table.optimize_table_storage()\n", " hasattr(task, 'target_options') task.target_options.get('retention_time'):\n", " retention_time=task.target_options.get('retention_time')\n", " else :\n", " retention_time=720\n", " debug : print('retention time :', retention_time)\n", " delta_table.vacuum_deltatable(retention_time)" ], "execution_count": 1 } ] } }</file><file name="src\synapse\studio\notebook\MetaNotebook.json">{ "name": "metanotebook", "properties": { "folder": { "name": "workers" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "b9993b9d-c5a8-47e1-919d-a6e42a28d345" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# meta notebook\n", "the purpose metanotebook create sql connection, plan, task class-objects. task-object, worker script called depending specified task. class-objects objects contain different methods, separate scripts created functions called worker scripts. functions methods defined scripts following reasons:\n", "1. increase flexibility script\n", "2. clear separation *worker scripts* make things happen *functional scripts* define things happen\n", "\n", "**language**: pyspark (python)\n", "" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## script logic\n", "1. import necessary packages\n", "2. run relevant scripts: scripts contain functions methods need \"run\" first able use them\n", "3. create class-objects sql connection, plans tasks\n", "4. loop execute task objects: task needs assigned worker-script actually execute task" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 1. import packages" ] }, { "cell_type": "code", "source": [ "# import packages\n", "import json\n", "import sys\n", "import datetime\n", "# operator import itemgetter" ], "execution_count": 1 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 2. prepare spark session\n", "the following section prepare spark session. is, invoking notebook starts set default configurations functions. stated, notebooks, metanotebook, contain functions classes. execute anything without called upon. happen functions/classes 'imported' spark session.\n", "\n", "same goes parameter-list passed calling metanotebook. preprocessing activities executed parameters \"prepare\" environment execution." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 2.1 parameters\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "'''\n", "inputs\n", " task_list -&gt; contains set task-objects, set metadata defined used throughout execution communication sql meta database\n", " plan_id -&gt; id plan task part of\n", " env_code -&gt; code used indicate environment functions running (dev, int, tst, acc, prd)\n", " debug -&gt; boolean indicating whether session run debug mode. trigger information (print statements) user\n", "\n", " task_list = {\n", " plan_id = id plan task part of\n", " plan_name = name plan started task\n", " task_group = name task group task part (preprocess, ingest, etc.)\n", " task_id = id task invoked\n", " task_name = name task invoked\n", " worker_name = name (spark notebook) worker execute task\n", " task_sequence = whithin plan task group, sequence task executed (arbitrary value since tasks independent one another)\n", " original_plan_id = id plan originally tried executing task (should plan_id run 'retry' run failed tasks)\n", " current_status = latest status plan (pending/failed)\n", " }\n", "'''\n", "\n", "# list task-objects execute\n", "task_list:str = '''[{\n", " \"plan_id\": ,\n", " \"plan_name\": \"\",\n", " \"task_group\": \"ingest\",\n", " \"task_id\": ,\n", " \"task_name\": \"\",\n", " \"worker_name\": \"ingestionworker\",\n", " \"task_sequence\":,\n", " \"original_plan_id\": ,\n", " \"current_status\": \"pending\"\n", " }]'''\n", "\n", "# id plan executed\n", "# dev-info: one notebook call theory handle multiple plans times (depending task-objects passed), recommended\n", "plan_id:int = 1\n", "\n", "## environment base phase notebook currently (dev/int/tst/acc/prod)\n", "env_code:str = \"dev\"\n", "\n", "# boolean indicating whether session run debug mode. trigger information (print statements) user\n", "debug:bool = true" ], "execution_count": 25 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# try converting task_list (string) list json-objects\n", "try:\n", " task_list:list = json.loads(task_list)\n", "except:\n", " raise valueerror(\"could convert task_list json object. ending notebook session...\")" ], "execution_count": 26 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# use default port sql server connection\n", "port = 1433\n", "spark.conf.set(\"spark.sql.legacy.timeparserpolicy\", \"legacy\")\n", "spark.conf.set(\"spark.sql.parquet.int96rebasemodeinwrite\", 'corrected')" ], "execution_count": 27 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 2.2. load notebooks\n", "import relevant notebooks spark session using %run. action, function classes defined notebooks able called within spark session. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "1. modules" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/classes" ], "execution_count": 28 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/dataframes_v2" ], "execution_count": 29 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 30 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": 31 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "2. functions" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 32 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 33 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/genericfunctions" ], "execution_count": 34 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "3. workers" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run workers/ingestionworker" ], "execution_count": 35 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run workers/filworker" ], "execution_count": 36 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 3. class objects\n", "1. sqlserverconnection: class creates connection sql server. class-object passed classes able execute sql queries, stored procedures, etc... define connection once\n", "2. plan: class creates plan-object containing plan_id plan_name.\n", "3. task: class creates task-object containing ids names plan task. " ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# modules/classes] create object sqlserverconnection class sql meta database\n", "metadb:object = sqlserverconnection(env_code, port, debug)" ], "execution_count": 37 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# [modules/classes] create object plan class relevant plan-metadata \n", "plan:object = plan(plan_id, task_list[0]['plan_name'], metadb, debug)" ], "execution_count": 38 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# [modules/classes] create object task class task object relevant task-metadata\n", "task_objects:list = [task(task['task_name'], task['task_id'], task['original_plan_id'], metadb, task[\"worker_name\"], debug) task task_list]" ], "execution_count": 39 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 4. execute tasks\n", "\n", "loop list task-objects. object, worker defined. worker linked worker-notebook executed.\n", "all tasks executed, errorflag set true least one task fails. prevent tasks (which independend one another) executed." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [] }, "source": [ "# flag indicate whether task failed for-loop.\n", "# set true, error thrown end for-loop indicate least one tasks failed\n", "errorflag:bool = false" ], "execution_count": 40 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "for task task_objects:\n", " print(f'start task {task.task_name}: {datetime.datetime.now().strftime(\"%h:%m:%s\")}')\n", " # [modules/classes] set status task 'in progress'\n", " task.start_task()\n", " try:\n", " # invoke worker needed execute task\n", " task.worker_name == 'filworker':\n", " run_fil_worker(task, env_code, debug)\n", "\n", " elif task.worker_name == 'ingestionworker':\n", " run_ingestion_worker(task, env_code, debug)\n", "\n", " # dummyworker: case used unit testing validate integration pipelines abd notebooks\n", " elif task.worker_name == 'dummyworker':\n", " pass\n", " \n", " # given worker name existing configuration, raise error\n", " else:\n", " raise valueerror(\"worker cannot found\", task.worker_name)\n", " print(f'end task {task.task_name} successfully: {datetime.datetime.now().strftime(\"%h:%m:%s\")}\\n\\n')\n", " # [modules/classes] set status task 'success'\n", " task.end_task(true)\n", " \n", " # catch errors thrown execution\n", " except exception generalerror:\n", " print(f'end task {task.task_name} unsuccessfully: {datetime.datetime.now().strftime(\"%h:%m:%s\")}')\n", "\n", " # [errorhandling:genericerrorclass] convert error object class genericerrorclass\n", " e_class, e_msg=get_exception_information(generalerror)\n", " error_object = handle_exceptions(e_class, e_msg, generalerror)\n", " print(error_object)\n", " # [modules/classes] set status task 'failed' pass custom error_object\n", " task.end_task(false, error_object)\n", "\n", " errorflag = true" ], "execution_count": 41 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# errorflag = true: make sure notebook execution fails alert users\n", "if (errorflag):\n", " sys.exit(1)" ], "execution_count": 42 } ] } }</file><file name="src\synapse\studio\notebook\SchemaDriftTemplates.json">{ "name": "schemadrifttemplates", "properties": { "folder": { "name": "templates" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "de2801b6-f53d-4e4b-8eaf-c3e0c16cb255" } }, "metadata": { "saveoutput": true, "enabledebugmode": false, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# schemadrifttemplates\r\n", "this script contains set functions used change schema specific source file. related changing column names, data types, partitioning columns, etc. functions allow standardized approach changing schema existing delta table without risk unforeseen implicit impact. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## resolve standard scenarios\r\n", "- change column datatype (ok)\r\n", "- add generic column delta tables (ok)\r\n", "- change column name (ok)\r\n", "- remove column name (ok)\r\n", "- change pk column regular column (ok)\r\n", "- change column pk column (ok)\r\n", "- changed partitioning column (ok)\r\n", "\r\n", "note: adding new column already integrated framework happens automatically updating configuration file" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## developer notes\r\n", "when changing schema delta table, options. depending type change needs executed, following functions (options) exist delta architecture:\r\n", "\r\n", "- mergeschema\r\n", "\r\n", "this functionality used two dataframes (potentially) different schema without impact one another. is, columns name expected schema. case, error thrown.\r\n", "in theory, function allow types changes (remove rename columns), would require settings table change. recommended practice implicit impact operations occur delta tables. \r\n", "https://learn.microsoft.com/en-us/azure/databricks/delta/table-properties\r\n", "\r\n", "- overwriteschema\r\n", "\r\n", "this functionality fully overwrites existing schema content existing delta table \"new\" dataframe. execute without underlying impact, following must considered:\r\n", "1. structfield objects (or columns) metadata described (this especially important varchar-columns)\r\n", "2. dataframe contain data (that needs kept) current delta table? \r\n", "3. current delta table partition fields, need replicated overwriting data\r\n", "" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## set-up environment\r\n", "- import libraries notebooks\r\n", "- set global variables" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# standard python packages\r\n", "import os\r\n", "import re\r\n", "\r\n", "# spark delta functionalities\r\n", "from notebookutils import mssparkutils\r\n", "from delta.tables import deltatable\r\n", "from pyspark.sql.functions import col\r\n", "from pyspark.sql.types import structfield, structtype" ], "execution_count": 69 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 70 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "print_statements = false # set true want print additional information\r\n", "env_code = 'dev' # change working environment (will give time-out connection properly set)\r\n", "container_name = 'silver' # name container delta tables stored\r\n", "\r\n", "storage_account = f'{env_code}dapstdala1' # name storage account delta tables stored\r\n", "delta_lake_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net' # full path container delta tables stored" ], "execution_count": 71 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## helper functions\r\n", "additional functions used throughout different schema-drift functions" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def is_delta_table(table_path:str) -&gt; bool:\r\n", " \"\"\"\r\n", " check table_path delta table\r\n", "\r\n", " :param table_path: abfss-path delta lake\r\n", " :return boolean: indicating whether table_path delta table\r\n", " \"\"\"\r\n", " try:\r\n", " print_statements: print(table_path)\r\n", " # check table_path loaded delta table\r\n", " delta_table = deltatable.forpath(spark, table_path)\r\n", " # get table metadata validate format 'delta'\r\n", " table_details = spark.sql(f\"describe detail delta.`{table_path}`\")\r\n", " table_details.selectexpr(f\"first(format)\").collect()[0][0] == 'delta':\r\n", " print_statements: print(\" delta table\")\r\n", " return true\r\n", " print_statements: print(\" delta table\")\r\n", " return false\r\n", " except exception e:\r\n", " print_statements: print(\" delta table\")\r\n", " return false" ], "execution_count": 52 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def get_partitioning_columns(table_path:str) -&gt; list:\r\n", " \"\"\"\r\n", " return list columns partitioning done\r\n", "\r\n", " :param table_path: abfss-path delta lake\r\n", " :return sql_table_partitioncolumns: list columns table partitioned\r\n", " \"\"\"\r\n", " table_details = spark.sql(f\"describe detail delta.`{table_path}`\")\r\n", " sql_table_partitioncolumns:list = table_details.selectexpr(f\"first(partitioncolumns)\").collect()[0][0]\r\n", " print_statements: print(f\"list partition columns {table_path}: {sql_table_partitioncolumns}\")\r\n", "\r\n", " return sql_table_partitioncolumns" ], "execution_count": 53 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def get_constraints(table_path:str) -&gt; list:\r\n", " \"\"\"\r\n", " return list constraints placed table\r\n", "\r\n", " :param table_path: abfss-path delta lake\r\n", " :return constraint_names: list constraints enforced table\r\n", " \"\"\"\r\n", " constraints:dict = spark.sql(f\"describe detail delta.`{table_path}`\").select('properties').collect()[0][0]\r\n", " constraint_names:list = list(constraints.keys())\r\n", " print_statements: print(f\"list constraints {table_path}: {constraint_names}\")\r\n", " return constraint_names" ], "execution_count": 54 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def get_column_metadata(table_path:str):\r\n", " \"\"\"\r\n", " print structfield object column, including metadata\r\n", " mainly used manual validation changes schema\r\n", "\r\n", " :param table_path: abfss-path delta lake\r\n", "\r\n", " :example\r\n", " table_schema[column_id] = structfield(&lt;column_name&gt;, &lt;datatype&gt;, &lt;nullable&gt;)\r\n", " table_schema[column_id].metadata = {&lt;dictionary metadata column&gt;} = {\"&lt;column_name&gt;\": \"&lt;description purpose column&gt;\", \"__char_varchar_type_string\": \"varchar(&lt;xx&gt;)\"}\r\n", " \"\"\" \r\n", " delta_table = spark.read.load(path=table_path, format='delta')\r\n", " table_schema:dict = delta_table.schema\r\n", "\r\n", " column_id range (len(table_schema)):\r\n", " print_statements: print(table_schema[column_id])\r\n", " print_statements: print(table_schema[column_id].metadata)" ], "execution_count": 55 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## schema drift functions" ] }, { "cell_type": "code", "source": [ "def add_generic_column_to_all_delta_tables(column_name:str, data_type:str, dimension:str='scd2'):\r\n", " \"\"\"\r\n", " add &lt;column_name&gt; &lt;data_type&gt; existing delta tables yet column\r\n", "\r\n", " :param column_name: name column needs added delta tables\r\n", " :param data_type: data type column needs added delta tables\r\n", " :param dimension: dimension column (primary key, scd2, etc.)\r\n", "\r\n", " :notes\r\n", " recommended use function add primary key (pk) columns!\r\n", " refer function add_new_column refactor_pk_to_data_column\r\n", " \"\"\"\r\n", " \r\n", "#region get list existing delta tables\r\n", " # dev-info: assume delta tables defined directly delta_lake_path (so recursive search done)\r\n", " found_folders = mssparkutils.fs.ls(delta_lake_path)\r\n", " # filter list contains delta tables (and files/folders)\r\n", " delta_tables = [possible_delta_table_folder possible_delta_table_folder found_folders is_delta_table(possible_delta_table_folder.path)] \r\n", " print_statements: print(\"list found delta tables:\")\r\n", " print_statements: [print(f\" {delta_table}\") delta_table delta_tables]\r\n", "#endregion\r\n", "\r\n", "#region loop delta tables add column exist table already\r\n", " delta_table delta_tables:\r\n", " add_new_column(delta_table.name, column_name=column_name, data_type=data_type, dimension=dimension)\r\n", "#endregion" ], "execution_count": 56 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def add_new_column(table_name:str, column_name:str, data_type:str, dimension:str=\"scd2\"):\r\n", " \"\"\"\r\n", " add &lt;column_name&gt; &lt;data_type&gt; existing delta tables yet column\r\n", "\r\n", " :param column_name: name column needs added delta tables\r\n", " :param data_type: data type column needs added delta tables\r\n", " :param dimension: dimension column (primary key, scd2, etc.)\r\n", "\r\n", " :notes\r\n", " recommended use function add primary key (pk) columns!\r\n", " refer function refactor_pk_to_data_column\r\n", " \"\"\"\r\n", "\r\n", " \"\"\"\r\n", " first get list existing delta tables\r\n", " table: column yet part table:\r\n", " 1. create dataframe contains new column existing list partition columns\r\n", " dev-note: without adding partitioning columns, merge cannot executed properly\r\n", " 2. merge dataframe existing delta table \r\n", " \"\"\"\r\n", "\r\n", " delta_table_path = f\"{delta_lake_path}/{table_name}\"\r\n", " dataframe = spark.read.load(path=delta_table_path, format='delta')\r\n", " column_name dataframe.columns:\r\n", " #region create list &lt;columns_to_add&gt; dictionaries contain column information new column\r\n", " print_statements: print(f\"adding column {column_name} {delta_table.name}...\")\r\n", " \r\n", " # dev-note: refactor function would possible add multiple new columns once\r\n", " columns_to_add = [\r\n", " {\"column_name\": column_name, \"data_type\": data_type, \"dimension\": dimension}\r\n", " ]\r\n", " #endregion\r\n", "\r\n", " #region add partition columns column information object &lt;columns_to_add&gt;\r\n", " sql_table_partitioncolumns = get_partitioning_columns(table_path=path)\r\n", " partition_column sql_table_partitioncolumns:\r\n", " column_object = {\"column_name\": partition_column, \"data_type\": \"string\", \"dimension\": \"scd2\"}\r\n", " columns_to_add.append(column_object)\r\n", " #endregion\r\n", "\r\n", " #region crearte merge new dataframe existing delta table\r\n", " # [dataframes_v2:sparkdataframeclass:create_dataframe] create dataframe using column_information object\r\n", " df_object = sparkdataframeclass()\r\n", " df_object.create_dataframe(column_information=columns_to_add)\r\n", "\r\n", " print_statements: print(f\"adding following dataframe delta table {path}\")\r\n", " print_statements: df_object.dataframe.show()\r\n", "\r\n", " new_df = df_object.dataframe\r\n", " # add dataframe existing delta table\r\n", " new_df.write.format('delta').mode('append').option(\"mergeschema\", true).save(path)\r\n", " #endregion" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def change_column_datatype(table_name:str, column_name:str, new_data_type:str, validate_casting:bool=false):\r\n", " \"\"\"\r\n", " change data type specified column delta table.\r\n", "\r\n", " :param table_name: name delta table.\r\n", " :param column_name: name column change.\r\n", " :param new_data_type: new data type column.\r\n", " :param validate_casting: whether validate casting column.\r\n", " \"\"\"\r\n", "#region define function variables\r\n", " delta_table_path = f'{delta_lake_path}/{table_name}'\r\n", " df = spark.read.format(\"delta\").load(delta_table_path)\r\n", " # get list current partition_columns delta table\r\n", " partition_columns:list = get_partitioning_columns(delta_table_path)\r\n", " # return list configuration datatypes spark-opposite\r\n", " data_types:dict = class_obj.configure_datatypes([new_data_type])\r\n", "#endregion\r\n", "\r\n", "#region define metadata field changed column\r\n", " new_metadata = {column_name: f\"metadata column {column_name}\"}\r\n", " varchar_pattern = r'^varchar\\((\\d{1,2})\\)$'\r\n", " # datatype = varchar(&lt;xx&gt;): alter metadata\r\n", " re.match(varchar_pattern, new_data_type):\r\n", " # dealing varchar, change metadata column add condition\r\n", " new_metadata['__char_varchar_type_string'] = new_data_type\r\n", " new_data_type = 'string'\r\n", "#endregion\r\n", "\r\n", "#region create new schema updated data type metadata\r\n", " new_schema = []\r\n", " # create structfield object column, update new column\r\n", " field df.schema.fields:\r\n", " field.name == column_name:\r\n", " # create new structfield updated data type metadata\r\n", " new_field = structfield(field.name, data_types[new_data_type], field.nullable, new_metadata)\r\n", " new_schema.append(new_field)\r\n", " else:\r\n", " new_field = structfield(field.name, field.datatype, field.nullable, field.metadata)\r\n", " new_schema.append(new_field)\r\n", "\r\n", " # create new structtype new schema\r\n", " new_struct_type = structtype(new_schema)\r\n", "#endregion\r\n", "\r\n", "#region apply new schema dataframe\r\n", " field new_schema:\r\n", " df = df.withcolumn(field.name, col(field.name).cast(field.datatype))\r\n", " df = df.withmetadata(field.name, field.metadata)\r\n", "\r\n", " validate_casting:\r\n", " class_obj = sparkdataframechecks(source_path=delta_table_path, header=true, separator=',', file_kind='', column_names='', data_types='', checks=[], column_information={}, deploy=true)\r\n", " class_obj.dataframe = df\r\n", " class_obj.cast_column(dataframe_subset=df.select(column_name), column=column_name, data_type=new_data_type)\r\n", "\r\n", " df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteschema\", \"true\").partitionby(*partition_columns).save(delta_table_path)\r\n", "#endregion" ], "execution_count": 57 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def rename_column(table_name:str, current_column_name:str, new_column_name:str):\r\n", " \"\"\"\r\n", " change data type specified column delta table.\r\n", "\r\n", " :param table_name: name delta table.\r\n", " :param current_column_name: name column change.\r\n", " :param new_column_name: new name column.\r\n", " \"\"\"\r\n", "#region define function variables\r\n", " delta_table_path = f'{delta_lake_path}/{table_name}'\r\n", " df = spark.read.format(\"delta\").load(delta_table_path)\r\n", " # get list current partition_columns delta table\r\n", " partition_columns = get_partitioning_columns(delta_table_path)\r\n", "#endregion\r\n", "\r\n", "#region check current_column_name pk column\r\n", " constraint_list = get_constraints(delta_table_path)\r\n", " not_null_constraint = f'pk__notnull_{current_column_name}'\r\n", "\r\n", " add_pk_constraint = false\r\n", " f'delta.constraints.{not_null_constraint}' constraint_list:\r\n", " add_pk_constraint = true\r\n", " spark.sql(f\"alter table delta.`{delta_table_path}` drop constraint {not_null_constraint}\")\r\n", "#endregion\r\n", "\r\n", "#region check partitioning current_column_name\r\n", " current_column_name partition_columns:\r\n", " partition_columns = [x.replace('overwrite', 'debug') x partition_columns]\r\n", "#endregion\r\n", "\r\n", "#region\r\n", " # rename column (for example, renaming 'old_column_name' 'new_column_name')\r\n", " df = df.withcolumnrenamed(current_column_name, new_column_name)\r\n", " # write dataframe back delta table\r\n", " df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteschema\", \"true\").partitionby(*partition_columns).save(delta_table_path)\r\n", "#endregion\r\n", "\r\n", "#region\r\n", " add_pk_constraint:\r\n", " new_not_null_constraint = f'pk__notnull_{new_column_name}'\r\n", " spark.sql(f\"alter table delta.`{delta_table_path}` add constraint {new_not_null_constraint} check ({new_column_name} null)\")\r\n", " \r\n", "#endregion" ], "execution_count": 59 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def drop_column_name(table_name:str, column_name:str):\r\n", " \"\"\"\r\n", " change data type specified column delta table.\r\n", "\r\n", " :param table_name: name delta table.\r\n", " :param column_name: name column drop.\r\n", " \"\"\"\r\n", "#region define function variables\r\n", " delta_table_path = f'{delta_lake_path}/{table_name}'\r\n", " df = spark.read.format(\"delta\").load(delta_table_path)\r\n", "#endregion\r\n", "\r\n", "#region check current_column_name pk column\r\n", " constraint_list = get_constraints(delta_table_path)\r\n", " not_null_constraint = f'pk__notnull_{column_name}'\r\n", "\r\n", " add_pk_constraint = false\r\n", " f'delta.constraints.{not_null_constraint}' constraint_list:\r\n", " add_pk_constraint = true\r\n", " spark.sql(f\"alter table delta.`{delta_table_path}` drop constraint {not_null_constraint}\")\r\n", "#endregion\r\n", "\r\n", "#region check partitioning current_column_name\r\n", " column_name partition_columns:\r\n", " partition_columns.remove(column_name)\r\n", "#endregion\r\n", "\r\n", "#region\r\n", " # rename column (for example, renaming 'old_column_name' 'new_column_name')\r\n", " df = df.drop(column_name)\r\n", " # write dataframe back delta table\r\n", " df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteschema\", \"true\").partitionby(*partition_columns).save(delta_table_path)\r\n", "#endregion" ], "execution_count": 60 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def refactor_pk_to_data_column(table_name:str, column_name:str, validate_unique:bool=true):\r\n", " \"\"\"\r\n", " remove null constraint column\r\n", "\r\n", " :param table_name: name delta table.\r\n", " :param column_name: name pk column refactor regular data_column.\r\n", " \"\"\"\r\n", "#region define function variables\r\n", " delta_table_path = f'{delta_lake_path}/{table_name}'\r\n", " df = spark.read.format(\"delta\").load(delta_table_path)\r\n", "#endregion\r\n", "\r\n", "#region check current_column_name pk column\r\n", " constraint_list = get_constraints(delta_table_path)\r\n", " not_null_constraint = f'pk__notnull_{column_name}'\r\n", "\r\n", " f'delta.constraints.{not_null_constraint}' constraint_list:\r\n", " spark.sql(f\"alter table delta.`{delta_table_path}` drop constraint {not_null_constraint}\")\r\n", " # validate_unique:\r\n", "\r\n", "#endregion" ], "execution_count": 62 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def refactor_data_column_to_pk(table_name:str, column_name:str):\r\n", " \"\"\"\r\n", " remove null constraint column\r\n", "\r\n", " :param table_name: name delta table.\r\n", " :param column_name: name pk column refactor regular data_column.\r\n", " \"\"\"\r\n", "#region define function variables\r\n", " delta_table_path = f'{delta_lake_path}/{table_name}'\r\n", " df = spark.read.format(\"delta\").load(delta_table_path)\r\n", "#endregion\r\n", "\r\n", "#region check current_column_name pk column\r\n", " constraint_list = get_constraints(delta_table_path)\r\n", " not_null_constraint = f'pk__notnull_{column_name}'\r\n", "\r\n", " f'delta.constraints.{not_null_constraint}' constraint_list:\r\n", " spark.sql(f\"alter table delta.`{delta_table_path}` add constraint {new_not_null_constraint} check ({column_name} null)\")\r\n", "#endregion" ], "execution_count": 63 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "def change_partitioning_columns(table_name, new_partitioning_list:list):\r\n", " \"\"\"\r\n", " alter existing list partitioning columns new list\r\n", "\r\n", " :param table_name: name delta table.\r\n", " :param new_partitioning_list: list column names partition on\r\n", " \"\"\"\r\n", "\r\n", "#region define function variables\r\n", " delta_table_path = f'{delta_lake_path}/{table_name}'\r\n", " df = spark.read.format(\"delta\").load(delta_table_path)\r\n", "#endregion\r\n", "\r\n", "#region\r\n", " df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteschema\", \"true\").partitionby(*new_partitioning_list).save(delta_table_path)\r\n", "#endregion" ], "execution_count": 64 } ] } }</file><file name="src\synapse\studio\notebook\TestNotebook.json">{ "name": "testnotebook", "properties": { "folder": { "name": "workers" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "e39bc7ff-34b6-455f-b7a5-31cea916c1c8" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test notebook\n", "\n", "the purpose test notebook script run test classes unit testing synapse notebook section one organised space. notebook \"import\" test classes written test notebooks (test_modules, test_classes, test_ingestionfunctions, etc.) invoke classes once. results tests handled notebook wil written text-file 'logs' container relevant data lake storage account.\n", "\n", "this script compared metanotebook script, also serves orchestrator rather actual executor.\n", "\n", "**language**: pyspark (python)" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## script logic\n", "1. import necessary python packages\n", "2. prepare spark session: set parameters run test notebooks \"import\" test cases\n", "3. create test-case runner object run testcases \"imported\"\n", "4. process test results write result text file logs container" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 1. import packages" ] }, { "cell_type": "code", "source": [ "import unittest\n", "import os\n", "from notebookutils import mssparkutils\n", "\n", "# dev-info: import pyspark.testing.utils # pyspark version 3.5 contains useful functions (assertdataframeequal, assertschemaequal...) exist current 3.3.1 pyspark version" ], "execution_count": 15 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 2. prepare spark session\n", "the following section prepare spark session. is, invoking notebook starts set default configurations functions. notebooks used testing, testnotebook, contain functions classes. execute anything without called upon. happen functions/classes 'imported' spark session.\n", "\n", "same goes parameter-list passed calling testnotebook. preprocessing activities executed parameters \"prepare\" environment execution." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### parameters\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "'''\n", "inputs:\n", " environment_code:the code used indicate environment functions running (dev, int)\n", "'''\n", "\n", "environment_code:str = ''" ], "execution_count": 16 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "set variables use notebook execution, passed parameters." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "spark.conf.set('spark.environment_code', environment_code)" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# set env_code variable based spark configuration\n", "# configuration argument 'environment_code' exists, use this. not, use parameter passed \n", "# dev-info: synapse workspace deployment, notebook forced core_configuration spark configuration contains spark.environment_code=&lt;target_environment&gt;\n", "env_code:str = spark.conf.get('spark.environment_code', environment_code)\n", "print(env_code)\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# set variables used export results file to\n", "test_adls_instance:str = f'{env_code}dapstdala1'\n", "test_export_container:str = 'logs'\n", "test_output_folder:str = 'test_results'\n", "output_file:str = 'output.txt'" ], "execution_count": 18 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 2.2. load notebooks\n", "import relevant notebooks spark session using %run. action, function classes defined notebooks able called within spark session. \n", "\n", "dev-info: notebooks invoked using %run-option. option pass parameter-arguments function, take variables. is, possible add {'env_code'=env_code} function call shown below. static variables (that is, variable names actual string values) passed (so {'env_code'='dev'} works). reason, test notebook contains core_configuration env_code:str = spark.conf.get('spark.environment_code', environment_code) code snippet. \n", "\n", "notebooks without params\n", "```\n", "%run functions/test_functions/test_functions\n", "```\n", "notebook params\n", "```\n", "%run modules/test_modules/test_modules {'env_code': &lt;static_string&gt; }\n", "```" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/test_functions/test_ingestionfunctions" ], "execution_count": 21 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/test_functions/test_cleanworkspace" ], "execution_count": 23 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/test_functions/test_genericfunctions" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/test_modules/test_classes" ], "execution_count": 19 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/test_modules/test_dataframes_v2" ], "execution_count": 24 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/test_modules/test_deltatables" ], "execution_count": 25 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### 3. run test cases\n", "\n", "each test notebook contains set classes inherit functionalities unittest.testcase.\n", "the runner created invoke testcase-classes thereby run configured tests" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# use output_file write tests results to\n", "log_file:str = output_file\n", "with open(log_file, \"w\") f:\n", " # create runner texttestrunner object, output_file write-location runner\n", " # basically tells unittest-class convert test results txt-format write specific file\n", " # dev-info: note file exist anywhere yet! empty textfile-like template created location assigned textfile\n", " runner = unittest.texttestrunner(f, verbosity=2)\n", "\n", " # run testcase classes store results findings-variable \n", " findings = unittest.main(argv=[''], testrunner=runner, verbosity=2, exit=false, warnings='ignore')" ], "execution_count": 26 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "collapsed": false }, "source": [ "# open log_file read contents. write lines one one location storage account\n", "with open(log_file, \"r\") f:\n", " test = f.read()\n", " test_list = [test]\n", "\n", " spark.createdataframe(test_list, \"string\").coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"overwrite\").save(f'abfss://{test_export_container}@{test_adls_instance}.dfs.core.windows.net/{test_output_folder}')" ], "execution_count": 27 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# findings-variable contains failures errors, make sure error thrown user alerted\n", "if len(findings.result.failures) != 0 len(findings.result.errors) != 0:\n", " raise exception(f\"failed tests: {findings.result.failures}, error tests: {findings.result.errors}\")\n", "elif len(findings.result.failures) != 0:\n", " raise exception(f\"failed tests: {findings.result.failures}\")\n", "elif len(findings.result.errors) != 0:\n", " raise exception(f\"error tests: {findings.result.errors}\")" ], "execution_count": 29 } ] } }</file><file name="src\synapse\studio\notebook\Test_Classes.json">{ "name": "test_classes", "properties": { "folder": { "name": "modules/test_modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "160bf83a-4a37-4a9d-b95b-ebdc510c8163" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test classes\n", "**purpose**: notebook using unittest library pyspark.testing.utils library python pyspark execute unit tests spark notebooks folder *modules*.\n", "\n", "unit tests: test method expect do, untit test written. unit test isolates method overall environment executes method. aim able define expected outcome method see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute method successfully\n", "\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary libraries" ] }, { "cell_type": "code", "source": [ "import unittest\n", "from unittest.mock import patch, magicmock, mock, call, any\n", "\n", "import pyspark.testing.utils # pyspark version 3.5 contains useful functions (assertdataframeequal, assertschemaequal...) exist current 3.3.1 pyspark version\n", "\n", "from pyspark.sql.types import structtype,structfield, stringtype, integertype\n", "from delta.tables import deltatable\n", "from pyspark.sql.functions import col\n", "import pyspark\n", "\n", "import pandas pd\n", "from pyspark.sql import sparksession\n", "\n", "from io import stringio\n", "import os\n", "from py4j.java_gateway import javaobject\n", "\n", "import time\n", "import datetime\n", "import delta.tables dt " ], "execution_count": 1 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## parameters\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\n", "environment_code = 'dev'" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\n", "\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 3 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" methods need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/classes" ], "execution_count": 4 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 5 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/genericfunctions" ], "execution_count": 6 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 7 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test classes" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: sqlserverconnection()\n", "class purpose: " ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sqlserverconnection\n", "# class test __init__ sqlserverconnection-class defined classes notebook\n", "class test_sqlserverconnection_initialisation(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown testcass class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls __init__ method\n", "\n", " # success: initialize sqlserverconnection class-object valid arguments \n", " def test_initialisation_sqlserverconnection_success(self):\n", " '''\n", " test logic:\n", " test happy path: given arguments valid, server_url linked_service name retrievable object\n", " '''\n", " # preprocess\n", " # define arguments passed sqlserverconnection object\n", " port_value = 1433\n", " env_code_value = self.env_code\n", " \n", " # define expected values class-instance members: \n", " expected_server_url = f\"jdbc:sqlserver://{env_code_value}-dap-sql-core.database.windows.net:{port_value};database={env_code_value}-dap-sqldb-core-meta\"\n", " expected_linked_service = 'ls_dap_sql_meta'\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: initiliaze sqlserverconnection class-object valid set argument values\n", " class_object = sqlserverconnection(env_code=env_code_value, port=port_value)\n", "\n", " # evaluate\n", " # validate initialised object parameter 'server_url' matches expected value\n", " actual_server_url = class_object.server_url\n", " actual_linked_service = class_object.linked_service\n", "\n", " self.assertequal(expected_server_url, actual_server_url, f\"[classes:sqlserverconnection:init] expected server url match actual server url: {expected_server_url} versus {actual_server_url}\")\n", " self.assertequal(expected_linked_service, actual_linked_service, f\"[classes:sqlserverconnection:init] expected linked service match actual server url: {expected_linked_service} versus {actual_linked_service}\")\n", "\n", " # success: validate function expected_validate_argument_calls initializing sqlserverconnection class-object valid arguments \n", " def test_initialisation_sqlserverconnection_expected_calls(self):\n", " '''\n", " test logic:\n", " test happy path: given arguments valid, __init__ function call set validation functions\n", " '''\n", " # preprocess\n", " # define arguments passed sqlserverconnection object\n", " port_value = 1433\n", " env_code_value = self.env_code\n", " linked_service = 'ls_dap_sql_meta'\n", "\n", " ## [genericfunctions]: define list expected calls validate_argument function\n", " expected_validate_argument_calls = [\n", " call('env_code', env_code_value, ['dev', 'int', 'tst', 'acc', 'prd']), \n", " call('port', port_value, [1433])\n", " ]\n", " expected_validate_linked_service_calls = [call(linked_service)]\n", "\n", "\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: initiliaze sqlserverconnection class-object valid set argument values\n", " patch.object(sqlserverconnection, 'validate_env_argument') mock_validate_env_argument, \\\n", " patch.object(sqlserverconnection, 'validate_port_argument') mock_validate_port_argument, \\\n", " patch.object(sqlserverconnection, 'validate_sql_linked_service') mock_validate_sql_linked_service, \\\n", " patch(f'__main__.validate_argument') mock_validate_argument, \\\n", " patch(f'__main__.validate_linked_service') mock_validate_linked_service:\n", "\n", " class_object = sqlserverconnection(env_code=env_code_value, port=port_value)\n", "\n", "\n", " # evaluate\n", " # validate mocked functions called expected set arguments\n", " ## [classes:sqlserverconnection]: validate methods sqlserverconnection-class called\n", " mock_validate_env_argument.assert_called_once()\n", " mock_validate_port_argument.assert_called_once()\n", " mock_validate_sql_linked_service.assert_called_once()\n", " ## [genericfunctions]: validate methods genericfunctions notebook called specific arguments\n", " mock_validate_linked_service.called_once_with(expected_validate_linked_service_calls)\n", " mock_validate_argument.called_with(expected_validate_argument_calls)\n", "\n", "\n", " # failure: initialize sqlserverconnection class-object invalid value env_code-argument\n", " def test_initialisation_sqlserverconnection_invalid_environment(self):\n", " '''\n", " test logic:\n", " test unhappy path: given env_code-arguments invalid, error thrown\n", " '''\n", " # preprocess\n", " # define expected error thrown trying initialize class-object\n", " expected_error = f\"the env_code-argument 'invalid' listed allowed env_code list: ['dev', 'int', 'tst', 'acc', 'prd']\"\n", " \n", " # execute\n", " # [classes:sqlserverconnection]: initiliaze sqlserverconnection class-object, expecting error thrown\n", " self.assertraises(valueerror) error:\n", " class_object = sqlserverconnection(env_code='invalid', port=1433)\n", "\n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", "\n", " # failure: initialize sqlserverconnection class-object invalid value port-argument\n", " def test_initialisation_sqlserverconnection_invalid_port(self):\n", " '''\n", " test logic:\n", " test unhappy path: given port-arguments invalid, error thrown\n", " '''\n", " # preprocess\n", " # define expected error thrown trying initialize class-object\n", " expected_error = f\"the port-argument '-99' listed allowed port list: [1433]\"\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: initiliaze sqlserverconnection class-object, expecting error thrown\n", " self.assertraises(valueerror) error:\n", " class_object = sqlserverconnection(env_code='dev', port=-99)\n", " \n", " \n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:sqlserverconnection:init] expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sqlserverconnection_initialisation)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sqlserverconnection initialisation tests, something went wrong!\")" ], "execution_count": 8 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sqlserverconnection\n", "# class test set_sql_connection() sqlserverconnection-class defined classes notebook\n", "class test_sqlserverconnection_setsqlconnection(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing\n", " self.env_code = env_code\n", " # set valid sqlserverconnection object instance use testing\n", " self.sql_connection_object = sqlserverconnection(env_code=env_code, port=1433)\n", "\n", " self.get_connection_patcher = patch.object(spark._sc._gateway.jvm.java.sql.drivermanager, 'getconnection')\n", " self.mock_get_connection = self.get_connection_patcher.start()\n", " return\n", "\n", " def teardown(self):\n", " # teardown patch.objected functions\n", " self.get_connection_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls set_sql_connection() method\n", "\n", " # success: assert set_sql_connection returns 2 non-empty objects\n", " def test_setsqlconnection_sqlserverconnection_success(self):\n", " '''\n", " test logic:\n", " test happy path: call sql_connection_object method expect non-empty value access_token connection-object returned\n", " '''\n", " # preprocess\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: call method test\n", " access_token, connection = self.sql_connection_object.set_sql_connection()\n", "\n", " # validate\n", " # assert method returns expected values\n", " self.assertisnotnone(access_token)\n", " self.assertisnotnone(connection)\n", "\n", "\n", "\n", " # failure: pass string value server_url member sqlserverconnection object expect error\n", " def test_setsqlconnection_sqlserverconnection_failed_invalid_sqlserver_1(self):\n", " '''\n", " test logic:\n", " test unhappy path: try setting connection using string-value server_url \n", " expect error: suitable driver found sql server &lt;server_url&gt;\n", " '''\n", "\n", " # preprocess\n", " # set invalid url use throughout test\n", " invalid_url = 'invalid_url'\n", " self.sql_connection_object.server_url = invalid_url\n", " # expect following error returned\n", " expected_error = f'no suitable driver found sql server {invalid_url}'\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: call method test\n", " self.assertraises(exception) error:\n", " access_token, connection = self.sql_connection_object.set_sql_connection()\n", "\n", " # validate\n", " # validate returned error matches expectations\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:sqlserverconnection:setsqlconnection] expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", "\n", "\n", " # failure: pass non-existing server server_url member sqlserverconnection object expect error\n", " def test_setsqlconnection_sqlserverconnection_failed_invalid_sqlserver_2(self):\n", " '''\n", " test logic:\n", " test unhappy path: try making connection non-existing server server cannot accessed using access_token\n", " expect error: connection sql server &lt;server_url&gt; cannot made. validate url-value check firewall/access settings\n", " '''\n", "\n", " # preprocess\n", " # set invalid url use throughout test\n", " invalid_url = 'jdbc:sqlserver://%s:%s;database=%s' % ('non-existing-db.database.windows.net', 1433, 'does-not-matter')\n", " self.sql_connection_object.server_url = invalid_url\n", " # expect following error returned\n", " expected_error = f'connection sql server {invalid_url} cannot made. validate url-value check firewall/access settings'\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: call method test\n", " self.assertraises(exception) error:\n", " access_token, connection = self.sql_connection_object.set_sql_connection()\n", "\n", " # evaluate\n", " # validate returned error matches expectations\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:sqlserverconnection:setsqlconnection] expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", "\n", "\n", " # success: mock called functions make sure called expected arguments\n", " def test_setsqlconnection_sqlserverconnection_expected_calls(self):\n", " '''\n", " test logic:\n", " test happy path: call sql_connection_object method expect set (mocked) functions called\n", " - call set_access_token -&gt; call mssparkutils.credentials.getconnectionstringorcreds\n", " - create properties-instance call putall-method using connection_object returned access_tokenb\n", " - create drivermanager-instance get connection using properties-instance\n", " - validate values returned values\n", " '''\n", "\n", " # preprocess\n", " ## set return_value access_token\n", " expected_access_token = 'mocked_string'\n", " expected_connection_object = 'mocked_connection_object'\n", " # define magicmock object call putall function\n", " mock_properties = magicmock(spec=['putall'])\n", " ## expect connectionproperties.putall() function called following dictionary\n", " expected_connection_properties:dict = {\n", " 'accesstoken': expected_access_token\n", " }\n", " \n", " \n", " # execute\n", " # [classes:sqlserverconnection]: run sql_connection_object() mock called functions\n", " patch.object(sqlserverconnection, 'set_access_token') mock_setaccesstoken, \\\n", " patch.object(mssparkutils.credentials, 'getconnectionstringorcreds') mock_getconnectionstringorcreds, \\\n", " patch.object(spark._sc._gateway, 'jvm') mock_jvm:\n", "\n", " # dev-info: side_effect needed getconnectionstringorcreds directly called mocked object.\n", " # need side_effect validate getconnectionstringorcreds also called, focus set_access_token()\n", " # set_access_token still try calling original (non-mocked) version mock_getconnectionstringorcreds. side_effect forces call mock\n", " # refering functions main (see functions-folder), side-effect needed mock-object actually called original\n", " # bit confusing remember: functions main mocked directly, methods mocked indirectly. want refer mocked method, need explicitly \n", " mock_setaccesstoken.side_effect = mock_getconnectionstringorcreds\n", " \n", " # define return_value mocked functions\n", " mock_jvm.java.util.properties.return_value = mock_properties\n", " mock_getconnectionstringorcreds.return_value=expected_access_token\n", " mock_jvm.java.sql.drivermanager.getconnection.return_value = expected_connection_object\n", " \n", " # [classes:sqlserverconnection]: call method test\n", " actual_access_token, actual_connection_object = self.sql_connection_object.set_sql_connection()\n", "\n", "\n", " # evaluate\n", " # make sure mocked functions called expected\n", " ## set_access_token mssparkutils.credentials.getconnectionstringorcreds -&gt; need called set access_token\n", " mock_setaccesstoken.assert_called()\n", " mock_getconnectionstringorcreds.assert_called()\n", "\n", " ## properties drivermanager: class-instances need created set connection-object sql server\n", " mock_jvm.java.util.properties.assert_called()\n", "\n", " ## properties.putall drivermanager.getconnection(): use returned access_token set connectionproperties\n", " mock_properties.putall.assert_called_with(expected_connection_properties)\n", " mock_jvm.java.sql.drivermanager.getconnection.assert_called_with(self.sql_connection_object.server_url, mock_properties)\n", "\n", " ## validate final returned values match expectations -&gt; test return_values mocked functions match values returned set_sql_connection() \n", " self.assertequal(expected_access_token, actual_access_token, f\"[classes:sqlserverconnection:setsqlconnection] returned access token match expectations: {actual_access_token} versus {expected_access_token}\")\n", " self.assertequal(expected_connection_object, actual_connection_object, f\"[classes:sqlserverconnection:setsqlconnection] returned connection object match expectations: {actual_connection_object} versus {expected_connection_object}\") \n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sqlserverconnection_setsqlconnection)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 9 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sqlserverconnection\n", "# class test check_access_token() sqlserverconnection-class defined classes notebook\n", "class test_sqlserverconnection_checkaccesstoken(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing\n", " self.env_code = env_code\n", " self.sql_connection_object = sqlserverconnection(env_code=self.env_code, port=1433)\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown patched functions\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_access_token method\n", "\n", " # success: current access_token == none new value needs set\n", " def test_checkaccesstoken_sqlserverconnection_empty_token(self):\n", " '''\n", " test logic:\n", " access_token = none\n", " -&gt; check isvalidtoken exist\n", " -&gt; call set_sql_connection set connection access_token -&gt; validate connection access_token values\n", "\n", " '''\n", "\n", " # preprocess\n", " # set current access_token object = none\n", " access_token = none\n", " self.sql_connection_object.access_token = access_token\n", " # set expected return values access_token connection object\n", " expected_connection_object = 'new_connection_object'\n", " expected_access_token = 'new_token'\n", " \n", "\n", " # execute\n", " # invoke check_access_token method sqlserverconnection class, expect values actual_connection_object actual_access_token returned\n", " patch.object(mssparkutils.credentials, 'isvalidtoken') mock_isvalidtoken, \\\n", " patch.object(sqlserverconnection, 'set_sql_connection') mock_setsqlconnection:\n", " # set return values mocked functions -&gt; token none set_sql_object() called\n", " mock_setsqlconnection.return_value = expected_access_token, expected_connection_object\n", " actual_access_token, actual_connection_object = self.sql_connection_object.check_access_token()\n", "\n", " # evaluate\n", " # validate mocked functions called accordingly\n", " ## expect isvalidtoken called since original access_token == none\n", " mock_isvalidtoken.assert_not_called()\n", " mock_setsqlconnection.assert_called_once()\n", "\n", " # validate returned connection-object access_token match return_values set_sql_connection()\n", " self.assertequal(expected_connection_object, actual_connection_object, f\"[classes:sqlserverconnection:checkaccesstoken] connection objects match: {expected_connection_object} versus {actual_connection_object}\")\n", " self.assertequal(expected_access_token, actual_access_token, f\"[classes:sqlserverconnection:checkaccesstoken] access tokens match: {expected_access_token} versus {actual_access_token}\")\n", "\n", "\n", " # success: current access_token expired\n", " def test_checkaccesstoken_sqlserverconnection_nonempty_token_valid(self):\n", " '''\n", " test logic:\n", " access_token = none isvalidtoken = true\n", " -&gt; validate isvalidtoken called\n", " -&gt; call set_sql_connection since new value needed -&gt; validate change connection access_token values\n", " '''\n", " # preprocess\n", " # define value token argument\n", " access_token = 'token_value'\n", " self.sql_connection_object.access_token = access_token\n", " # set expected return values\n", " expected_connection_object = 'predefined_connection_object'\n", " self.sql_connection_object.connection = expected_connection_object\n", " expected_access_token = access_token\n", " # set return values set_sql_connection() -&gt; executed overwrite values executed\n", " not_expected_access_token = 'new_token_value'\n", " not_expected_connection_object = 'new_connection_object'\n", "\n", " \n", "\n", " # execute\n", " # invoke check_access_token method sqlserverconnection class, expect values actual_connection_object actual_access_token returned\n", " patch.object(mssparkutils.credentials, 'isvalidtoken') mock_isvalidtoken, \\\n", " patch.object(sqlserverconnection, 'set_sql_connection') mock_setsqlconnection:\n", " # set return values mocked functions -&gt; token valid original values returned\n", " mock_isvalidtoken.return_value = true\n", " mock_setsqlconnection.return_value = not_expected_access_token, not_expected_connection_object\n", "\n", " actual_access_token, actual_connection_object = self.sql_connection_object.check_access_token()\n", "\n", " # evaluate\n", " # validate mocked functions called accordingly\n", " ## expect set_sql_connection called since original access_token still valid need replaced\n", " mock_isvalidtoken.assert_called_once_with(access_token)\n", " mock_setsqlconnection.assert_not_called()\n", "\n", " # validate returned connection-object access_token match original_values values returned set_sql_connection()\n", " self.assertequal(expected_connection_object, actual_connection_object, f\"[classes:sqlserverconnection:checkaccesstoken] connection objects match: {expected_connection_object} versus {actual_connection_object}\")\n", " self.assertequal(expected_access_token, actual_access_token, f\"[classes:sqlserverconnection:checkaccesstoken] token values match: {expected_access_token} versus {actual_access_token}\")\n", "\n", "\n", " # success: current access_token expired new one needs set\n", " def test_checkaccesstoken_sqlserverconnection_nonempty_token_invalid(self):\n", " '''\n", " test logic:\n", " access_token = none isvalidtoken = false\n", " -&gt; validate isvalidtoken called\n", " -&gt; call set_sql_connection since new value needed -&gt; validate connection access_token values\n", " '''\n", " # preprocess\n", " # define value token argument\n", " access_token = 'token_value'\n", " self.sql_connection_object.access_token = access_token\n", " # set expected return values \n", " expected_connection_object = 'new_connection_object'\n", " expected_access_token = 'new_access_token'\n", " self.sql_connection_object.connection = 'predefined_connection_object'\n", " \n", "\n", " # execute\n", " # invoke check_access_token method sqlserverconnection class, expect values actual_connection_object returned\n", " patch.object(mssparkutils.credentials, 'isvalidtoken') mock_isvalidtoken, \\\n", " patch.object(sqlserverconnection, 'set_sql_connection') mock_setsqlconnection:\n", " # set return values mocked functions -&gt; token invalid needs recreated\n", " mock_isvalidtoken.return_value = false\n", " mock_setsqlconnection.return_value = expected_access_token, expected_connection_object\n", "\n", " actual_access_token, actual_connection_object = self.sql_connection_object.check_access_token()\n", "\n", " # evaluate\n", " # validate mocked functions called accordingly\n", " ## expect set_sql_connection called since original access_token still valid need replaced\n", " mock_isvalidtoken.assert_called_once_with(access_token)\n", " mock_setsqlconnection.assert_called_once()\n", "\n", " # validate returned connection-object access_token match return_values set_sql_connection()\n", " self.assertequal(expected_connection_object, actual_connection_object, f\"[classes:sqlserverconnection:checkaccesstoken] connection objects equal: {expected_connection_object} versus {actual_connection_object}\")\n", " self.assertequal(expected_access_token, actual_access_token, f\"[classes:sqlserverconnection:checkaccesstoken] access tokens equal: {expected_access_token} versus {actual_access_token}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sqlserverconnection_checkaccesstoken)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 10 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sqlserverconnection\n", "# class test execute_query() sqlserverconnection-class defined classes notebook\n", "class test_sqlserverconnection_executequery(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing\n", " self.env_code = env_code\n", " self.sql_connection_object = sqlserverconnection(env_code=self.env_code, port=1433)\n", " \n", " ## [classess:sqlserverconnection] patch functions actually run sql queries prevent making connections sql testing\n", " self.mock_execute_query_with_return_patcher = patch.object(sqlserverconnection, 'execute_query_with_return', return_value=true)\n", " self.mock_execute_query_with_return = self.mock_execute_query_with_return_patcher.start()\n", "\n", " self.mock_execute_query_without_return_patcher = patch.object(sqlserverconnection, 'execute_query_without_return')\n", " self.mock_execute_query_without_return = self.mock_execute_query_without_return_patcher.start()\n", "\n", " self.get_connection_patcher = patch.object(spark._sc._gateway.jvm.java.sql.drivermanager, 'getconnection')\n", " self.mock_get_connection = self.get_connection_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown patched functions\n", " self.mock_execute_query_with_return_patcher.stop()\n", " self.mock_execute_query_without_return_patcher.stop()\n", " self.get_connection_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls __init__ method\n", "\n", "\n", " # success: execute staticmethods: check_access_token actual execution\n", " # validate methods executed\n", " def test_setsqlconnection_executequery_functionaltest(self):\n", " ''' \n", " test logic:\n", " - run execute_query statement 'test', primary_key=true expectreturn=false\n", " - validate mocked functions called expected arguments\n", " '''\n", " # preprocess\n", "\n", " # execute\n", " self.sql_connection_object.execute_query('test', true, false)\n", "\n", " # evaluate\n", " self.mock_execute_query_without_return.assert_called_once_with(any, 'test')\n", " self.mock_execute_query_with_return.assert_called_once_with(any, 'select top 1 * meta.source_configuration')\n", "\n", " # success: execute primary checks expect return \n", " def test_setsqlconnection_executequery_primarychecks_return(self):\n", " ''' \n", " test logic:\n", " run execute_query statement 'test', primary_key=true expectreturn=true =&gt; mock functions\n", " - primary key = true -&gt; call set_access_token check_lifetime_db\n", " - expectreturn = true -&gt; call execute_query_with_return; call execute_query_without_return\n", " '''\n", " # preprocess\n", " # set query-statement execute\n", " query = 'test'\n", " # set magicmock-object createstatement-function -&gt; make sure still \"call\" close-function\n", " mock_statement = magicmock(spec='close')\n", " # set magicmock-object mock_checkaccesstoken-function return connection-object -&gt; make sure still \"call\" createstatement-function\n", " mock_connection = magicmock(spec='createstatement')\n", " # using mocked connection-object method createstatement, set returned object 'mock_statement' object defined above\n", " mock_connection.createstatement = mock_statement\n", "\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: run execute_query() mock called functions\n", " patch.object(sqlserverconnection, 'check_access_token') mock_checkaccesstoken, \\\n", " patch.object(sqlserverconnection, 'check_lifetime_db') mock_checklifetimedb:\n", " # set output mock_checkaccesstoken mock (access_token) mock_connection (connection_object)\n", " mock_checkaccesstoken.return_value = mock(), mock_connection\n", "\n", " # [classes:sqlserverconnection]: call method test\n", " self.sql_connection_object.execute_query(query, true, true)\n", "\n", "\n", " # evaluate\n", " # validate correct functions called\n", " ## primary checks == true -&gt; call set_access_token check_lifetime_db\n", " mock_checkaccesstoken.assert_called_once()\n", " mock_checklifetimedb.assert_called_once()\n", " ## expect_return == true -&gt; call with_return query\n", " self.mock_execute_query_without_return.assert_not_called()\n", " self.mock_execute_query_with_return.assert_called_once_with(mock_statement.return_value, query)\n", "\n", " # success: execute primary checks expect return\n", " def test_setsqlconnection_executequery_noprimarychecks_return(self):\n", " ''' \n", " test logic:\n", " run execute_query statement 'test', primary_key=true expectreturn=true =&gt; mock functions\n", " - primary key = false -&gt; call set_access_token check_lifetime_db\n", " - expectreturn = true -&gt; call execute_query_with_return; call execute_query_without_return\n", " '''\n", " # preprocess\n", " # set query-statement execute\n", " query = 'test'\n", " # set magicmock-object createstatement-function -&gt; make sure still \"call\" close-function\n", " mock_statement = magicmock(spec='close')\n", " # set magicmock-object mock_checkaccesstoken-function return connection-object -&gt; make sure still \"call\" createstatement-function\n", " mock_connection = magicmock(spec='createstatement')\n", " # using mocked connection-object method createstatement, set returned object 'mock_statement' object defined above\n", " mock_connection.createstatement = mock_statement\n", " # primary_checks=false -&gt; set connection none set primary checks \n", " self.sql_connection_object.connection = mock_connection\n", "\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: run execute_query() mock called functions\n", " patch.object(sqlserverconnection, 'check_access_token') mock_checkaccesstoken, \\\n", " patch.object(sqlserverconnection, 'check_lifetime_db') mock_checklifetimedb:\n", " # [classes:sqlserverconnection]: call method test\n", " mock_checkaccesstoken.return_value = mock(), mock_connection\n", " self.sql_connection_object.execute_query(query, false, true)\n", "\n", "\n", " # evaluate\n", " # validate correct functions called\n", " ## primary checks == false -&gt; call set_access_token check_lifetime_db\n", " mock_checkaccesstoken.assert_not_called()\n", " mock_checklifetimedb.assert_not_called()\n", " ## expect_return == true -&gt; call with_return query\n", " self.mock_execute_query_without_return.assert_not_called()\n", " self.mock_execute_query_with_return.assert_called_once_with(mock_statement.return_value, query)\n", "\n", " # success: execute primary checks expect return\n", " def test_setsqlconnection_executequery_primarychecks_noreturn(self):\n", " ''' \n", " test logic:\n", " run execute_query statement 'test', primary_key=true expectreturn=true =&gt; mock functions\n", " - primary key = true -&gt; call set_access_token check_lifetime_db\n", " - expectreturn = false -&gt; call execute_query_with_return; call execute_query_without_return\n", " '''\n", " # preprocess\n", " # set query-statement execute\n", " query = 'test'\n", " # set magicmock-object createstatement-function -&gt; make sure still \"call\" close-function\n", " mock_statement = magicmock(spec='close')\n", " # set magicmock-object mock_checkaccesstoken-function return connection-object -&gt; make sure still \"call\" createstatement-function\n", " mock_connection = magicmock(spec='createstatement')\n", " # using mocked connection-object method createstatement, set returned object 'mock_statement' object defined above\n", " mock_connection.createstatement = mock_statement\n", "\n", "\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: run execute_query() mock called functions\n", " patch.object(sqlserverconnection, 'check_access_token') mock_checkaccesstoken, \\\n", " patch.object(sqlserverconnection, 'check_lifetime_db') mock_checklifetimedb:\n", " # [classes:sqlserverconnection]: call method test\n", " mock_checkaccesstoken.return_value = mock(), mock_connection\n", " self.sql_connection_object.execute_query(query, true, false)\n", "\n", "\n", " # evaluate\n", " # validate correct functions called\n", " ## primary checks == true -&gt; call set_access_token check_lifetime_db\n", " mock_checkaccesstoken.assert_called_once()\n", " mock_checklifetimedb.assert_called_once()\n", " ## expect_return == false -&gt; call without_return query\n", " self.mock_execute_query_with_return.assert_not_called()\n", " self.mock_execute_query_without_return.assert_called_once_with(mock_statement.return_value, query)\n", "\n", " # success: execute primary checks expect return\n", " def test_setsqlconnection_executequery_noprimarychecks_noreturn(self):\n", " ''' \n", " test logic:\n", " run execute_query statement 'test', primary_key=true expectreturn=true =&gt; mock functions\n", " - primary key = false -&gt; call set_access_token check_lifetime_db\n", " - expectreturn = false -&gt; call execute_query_with_return; call execute_query_without_return\n", " '''\n", " # preprocess\n", " # set query-statement execute\n", " query = 'test'\n", " # set magicmock-object createstatement-function -&gt; make sure still \"call\" close-function\n", " mock_statement = magicmock(spec='close')\n", " # set magicmock-object mock_checkaccesstoken-function return connection-object -&gt; make sure still \"call\" createstatement-function\n", " mock_connection = magicmock(spec='createstatement')\n", " # using mocked connection-object method createstatement, set returned object 'mock_statement' object defined above\n", " mock_connection.createstatement = mock_statement\n", " # primary checks set connection one needs defined upfront\n", " self.sql_connection_object.connection = mock_connection\n", "\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: run execute_query() mock called functions\n", " patch.object(sqlserverconnection, 'check_access_token') mock_checkaccesstoken, \\\n", " patch.object(sqlserverconnection, 'check_lifetime_db') mock_checklifetimedb:\n", " # [classes:sqlserverconnection]: call method test\n", " mock_checkaccesstoken.return_value = mock(), mock_connection\n", " self.sql_connection_object.execute_query(query, false, false)\n", "\n", "\n", " # evaluate\n", " # validate correct functions called\n", " ## primary checks == false -&gt; call set_access_token check_lifetime_db\n", " mock_checkaccesstoken.assert_not_called()\n", " mock_checklifetimedb.assert_not_called()\n", " ## expect_return == false -&gt; call without_return query\n", " self.mock_execute_query_with_return.assert_not_called()\n", " self.mock_execute_query_without_return.assert_called_once_with(mock_statement.return_value, query)\n", "\n", "\n", " # success: execute primary checks expect return\n", " def test_setsqlconnection_executequery_no_connection(self):\n", " ''' \n", " test logic:\n", " run execute_query self.connection = none\n", " - primary_key false set true connection_object exists -&gt; call set_access_token check_lifetime_db\n", " - expectreturn = false -&gt; call execute_query_with_return; call execute_query_without_return\n", " '''\n", " # preprocess\n", " # set query-statement execute\n", " query = 'test'\n", " # set magicmock-object createstatement-function -&gt; make sure still \"call\" close-function\n", " mock_statement = magicmock(spec='close')\n", " # set magicmock-object mock_checkaccesstoken-function return connection-object -&gt; make sure still \"call\" createstatement-function\n", " mock_connection = magicmock(spec='createstatement')\n", " # using mocked connection-object method createstatement, set returned object 'mock_statement' object defined above\n", " mock_connection.createstatement = mock_statement\n", " # primary checks set connection one needs defined upfront\n", " self.sql_connection_object.connection = none\n", "\n", "\n", " # execute\n", " # [classes:sqlserverconnection]: run execute_query() mock called functions\n", " patch.object(sqlserverconnection, 'check_access_token') mock_checkaccesstoken, \\\n", " patch.object(sqlserverconnection, 'check_lifetime_db') mock_checklifetimedb:\n", " # [classes:sqlserverconnection]: call method test\n", " mock_checkaccesstoken.return_value = mock(), mock_connection\n", " self.sql_connection_object.execute_query(query, false, false)\n", "\n", "\n", " # evaluate\n", " # validate correct functions called\n", " ## self.connection = none -&gt; set primary_checks == true -&gt; call set_access_token check_lifetime_db\n", " mock_checkaccesstoken.assert_called_once()\n", " mock_checklifetimedb.assert_called_once()\n", " ## expect_return == false -&gt; call without_return query\n", " self.mock_execute_query_with_return.assert_not_called()\n", " self.mock_execute_query_without_return.assert_called_once_with(mock_statement.return_value, query)\n", " \n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sqlserverconnection_executequery)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 28 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sqlserverconnection\n", "# class test check_lifetime_db() sqlserverconnection-class defined classes notebook\n", "class test_sqlserverconnection_checklifetimedb(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing\n", " self.env_code = env_code\n", " self.sql_connection_object = sqlserverconnection(env_code=self.env_code, port=1433)\n", " \n", " # [classes:sqlserverconnection] mock execute_query method try execute actual calls database\n", " self.mock_execute_query_patcher = patch.object(sqlserverconnection, 'execute_query')\n", " self.mock_execute_query = self.mock_execute_query_patcher.start()\n", "\n", " # mock time.sleep test take long sleep actually executed\n", " self.mock_sleep_patcher = patch.object(time, 'sleep')\n", " self.mock_sleep = self.mock_sleep_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown patched functions\n", " self.mock_execute_query_patcher.stop()\n", " self.mock_sleep_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_lifetime_db method\n", "\n", " # success: execute execute_query method valid arguments\n", " def test_checklifetimedb_sqlserverconnection_sleep_once(self):\n", " # preprocess\n", " # [classes:sqlserverconnection] execute_query: first call throw error, second call successful\n", " self.mock_execute_query.side_effect = [valueerror, true]\n", " # time.sleep: time.sleep called, set wait time none (instead pre-set 20 seconds)\n", " self.mock_sleep.side_effect = [none]\n", "\n", " # execute\n", " # [classes:sqlserverconnection] call method test\n", " self.sql_connection_object.check_lifetime_db()\n", "\n", " # evaluate\n", " # validate execute_query called 2 times\n", " self.mock_execute_query.assert_called()\n", " self.assertequal(self.mock_execute_query.call_count, 2)\n", "\n", " self.mock_sleep.assert_called()\n", " self.assertequal(self.mock_sleep.call_count, 1)\n", "\n", "\n", " # success: execute execute_query method valid arguments\n", " def test_checklifetimedb_sqlserverconnection_fail_on_5_tries(self):\n", " # preprocess\n", " # call execute_query function 5 times throw error:\n", " expected_error = \"connecting sql metadb takes long. validate connection...\"\n", " # [classes:sqlserverconnection] execute_query: first 5 calls throw error, 6th call (which reached) successful\n", " self.mock_execute_query.side_effect = [exception()] * 5 + [true]\n", " # time.sleep: time.sleep called, set wait time none (instead pre-set 20 seconds) 5 calls executed (5 predefined check_lifetime_db method)\n", " self.mock_sleep.side_effect = [none]*4\n", "\n", "\n", " # execute\n", " # invoke check_lifetime_db method sqlserverconnection class, expect values actual_connection_object returned\n", " self.assertraises(connectionerror) error:\n", " # [classes:sqlserverconnection] call method test\n", " self.sql_connection_object.check_lifetime_db()\n", "\n", " # evaluate\n", " # validate putall-method called connectiondictionary defined function\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"expected error match actual: {expected_error} versus {actual_error}\")\n", " self.mock_execute_query.assert_called()\n", " self.assertequal(self.mock_execute_query.call_count, 5)\n", " self.mock_sleep.assert_called()\n", " self.assertequal(self.mock_sleep.call_count, 4)\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sqlserverconnection_checklifetimedb)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 12 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_plan_instance(unittest.testcase):\n", " def setup(self):\n", " return\n", " \n", " def teardown(self):\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls plan.__init__ method\n", "\n", " def test_initialisation_plan(self):\n", " ## test exist see instance variables configured properly, based input object\n", " # preprocess:\n", " expected_plan_id = -99\n", " expected_sql_server_connection = mock()\n", " expected_plan_name = 'mock_plan'\n", "\n", " # execute\n", " # [classes:plan:init] run method test\n", " planinstance = plan(expected_plan_id, expected_plan_name, expected_sql_server_connection)\n", "\n", " # evaluate\n", " # match actuals expectations\n", " actual_plan_id = planinstance.plan_id\n", " actual_plan_name = planinstance.plan_name\n", "\n", " self.assertequal(expected_plan_id, actual_plan_id, f\"[classes:plan:init] something went wrong initialisation plan_id: {expected_plan_id} versus {actual_plan_id}\")\n", " self.assertequal(expected_plan_name, actual_plan_name, f\"[classes:plan:init] something went wrong initialisation plan_name: {expected_plan_name} versus {actual_plan_name}\")\n", " self.assertin('mock', str(planinstance.sqlserverconnection), f\"[classes:plan:init] something went wrong initialisation sqlserverconnection: {str(planinstance.sqlserverconnection)}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_plan_instance)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 13 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_instance(unittest.testcase):\n", " def setup(self):\n", " return\n", "\n", " def teardown(self):\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls task.__init__ method\n", "\n", " def test_initialisation_task(self):\n", " ## test exist see instance variables configured properly, based input object\n", " # preprocess:\n", " expected_task_name = 'mock_task_name'\n", " expected_task_id = -99\n", " expected_plan_id = -88\n", " expected_sqlserverconnection = mock()\n", " expected_worker_name = 'test'\n", "\n", "\n", " # execute\n", " # [classes:task:init] run method test\n", " taskinstance = task(expected_task_name, expected_task_id, expected_plan_id, expected_sqlserverconnection, expected_worker_name)\n", "\n", " # evaluate\n", " # match actuals expectations\n", " actual_task_name = taskinstance.task_name\n", " actual_task_id = taskinstance.task_id\n", " actual_plan_id = taskinstance.plan_id\n", " actual_worker_name = taskinstance.worker_name\n", "\n", " self.assertequal(expected_task_name, actual_task_name, f\"[classes:task:init] something went wrong initialisation task_name: {expected_task_name} versus {actual_task_name}\")\n", " self.assertequal(expected_task_id, actual_task_id, f\"[classes:task:init] something went wrong initialisation task_id: {expected_task_id} versus {actual_task_id}\")\n", " self.assertequal(expected_plan_id, actual_plan_id, f\"[classes:task:init] something went wrong initialisation plan_id: {expected_plan_id} versus {actual_plan_id}\")\n", " self.assertin('mock', str(taskinstance.sqlserverconnection), f\"[classes:task:init] something went wrong initialisation sqlserverconnection: str(taskinstance.sqlserverconnection)\")\n", " self.assertequal(expected_worker_name, actual_worker_name, f\"[classes:task:init] something went wrong initialisation worker_name: {expected_worker_name} versus {actual_worker_name}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_instance)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 14 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_starttask(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " self.task_id = -99\n", " self.plan_id = -88\n", "\n", " self.task_object = task(mock(), self.task_id, self.plan_id, mock(), mock(), mock())\n", " return\n", "\n", " def teardown(self):\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls start_task method\n", "\n", "\n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_start_task'\n", " def test_starttask_task_success(self):\n", " # preprocess\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " mock_sql_object.execute_query.return_value = true\n", " self.task_object.sqlserverconnection = mock_sql_object\n", "\n", " # execute\n", " # [classes:task:starttask] run method test\n", " self.task_object.start_task()\n", "\n", " # evaluate\n", " # validate execute_query called \n", " mock_sql_object.execute_query.assert_called_once_with(f'exec meta.usp_start_task @task_id = {self.task_id}, @plan_id = {self.plan_id}')\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_starttask)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 15 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_endtask(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " self.task_id = -99\n", " self.plan_id = -88\n", "\n", " self.task_object = task(mock(), self.task_id, self.plan_id, mock(), mock(), mock())\n", " return\n", "\n", " def teardown(self):\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls end_task method\n", "\n", " # success: expect execute_query mock-object called statement 'exec meta.usp_end_task' positive success_flag\n", " def test_endtask_task_success(self):\n", " # preprocess\n", " success_flag = true\n", " comment = \"comment double quotes\"\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " mock_sql_object.execute_query.return_value = true\n", " self.task_object.sqlserverconnection = mock_sql_object\n", "\n", " # execute\n", " # [classes:task:endtask] run method test\n", " self.task_object.end_task(success_flag=success_flag, comment=comment)\n", "\n", " # evaluate\n", " # validate execute_query called \n", " mock_sql_object.execute_query.assert_called_once_with(f\"exec meta.usp_end_task @task_id = {self.task_id}, @plan_id = {self.plan_id}, @success_flag = {success_flag}, @comment = '{comment}'\")\n", "\n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_end_task' negative success_flag\n", " def test_endtask_task_failure(self):\n", " # preprocess\n", " success_flag = false\n", " comment = 'comment single quotes'\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " mock_sql_object.execute_query.return_value = true\n", " self.task_object.sqlserverconnection = mock_sql_object\n", "\n", " # execute\n", " # [classes:task:endtask] run method test\n", " self.task_object.end_task(success_flag=success_flag, comment=comment)\n", "\n", " # evaluate\n", " # validate execute_query called \n", " mock_sql_object.execute_query.assert_called_once_with(f\"exec meta.usp_end_task @task_id = {self.task_id}, @plan_id = {self.plan_id}, @success_flag = {success_flag}, @comment = '{comment}'\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_endtask)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 16 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_runuspgettaskmetadata(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " self.task_id = -99\n", "\n", " self.task_object = task(mock(), self.task_id, mock(), mock(), mock(), mock())\n", " return\n", "\n", " def teardown(self):\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls start_task method\n", "\n", "\n", " # success: expect execute_query mock-object called statement 'exec meta.usp_get_task_metadata' expect_return=true\n", " def test_run_usp_gettaskmetadata_task_success(self):\n", " # preprocess\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " mock_sql_object.execute_query.return_value = true\n", " self.task_object.sqlserverconnection = mock_sql_object\n", "\n", " # execute\n", " # [classes:task:run_usp_gettaskmetadata] run method test\n", " self.task_object.run_usp_gettaskmetadata()\n", "\n", " # evaluate\n", " # validate execute_query called \n", " mock_sql_object.execute_query.assert_called_once_with(f'exec meta.usp_get_task_metadata @task_id={self.task_id}', expect_return=true)\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_runuspgettaskmetadata)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_gettaskmetadata(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " self.task_id = -99\n", " self.task_object = task(mock(), self.task_id, mock(), mock(), mock())\n", " return\n", "\n", " def teardown(self):\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls start_task method\n", "\n", "\n", " # success: expect execute_query mock-object called statement 'exec meta.usp_get_task_metadata' expect_return=true\n", " def test_gettaskmetadata_task_function_calls(self):\n", " '''\n", " test logic:\n", " - expect get_task_metadata() call run_usp_gettaskmetadata, call execute_query with:\n", " statement = 'exec meta.usp_get_task_metadata @task_id = &lt;task_id&gt;' \n", " expect_return = true\n", " '''\n", " \n", " # preprocess\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " # set return_value execute_query empty list json-objects\n", " mock_sql_object.execute_query.return_value = [{}, {}]\n", " self.task_object.sqlserverconnection = mock_sql_object\n", " # pass empty list variables test validate function calls\n", " variables = []\n", "\n", " # execute\n", " # [classes:task:get_task_metadata] run method test\n", " self.task_object.get_task_metadata(variables)\n", "\n", " # evaluate\n", " # validate execute_query called expected arguments \n", " mock_sql_object.execute_query.assert_called_once_with(f'exec meta.usp_get_task_metadata @task_id={self.task_id}', expect_return=true)\n", "\n", " # success: expect self.task_object.variables-dictionary contain keys values passed variables-list\n", " def test_gettaskmetadata_task_set_simple_variables(self):\n", " '''\n", " test logic:\n", " pass non-empty list variables get_task_metadata method\n", " mock execute_query return list json-objects set columns\n", " validate variables-member task-instance task_object contains keys values executing get_task_metadata\n", " '''\n", " # preprocess\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " # exeucte_query returns non-empty object array\n", " mock_sql_object.execute_query.return_value = [\n", " {\n", " 'column_1': '[{\"test\": \"value1\"}]', \n", " 'column_2': '[{\"unittest\": \"value2\"}]',\n", " 'column_3': '[{\"keys\": \"value3\"}]'\n", " }\n", " ]\n", " self.task_object.sqlserverconnection = mock_sql_object\n", " # pass list get_task_metadata expect key-value pairs set self.task_object.variables dictionary\n", " variables = ['test', 'unittest', 'keys']\n", "\n", " # execute\n", " # [classes:task:get_task_metadata] run method test\n", " self.task_object.get_task_metadata(variables)\n", "\n", " # evaluate\n", " # validate execute_query successfully set values self.task_object.variables\n", " self.assertequal(self.task_object.variables['test'], \"value1\", f\"[classes:task:gettaskmetadata]: value key 'test' equal expectations: '{self.task_object.variables['test']}' versus 'value1'\")\n", " self.assertequal(self.task_object.variables['unittest'], \"value2\", f\"[classes:task:gettaskmetadata]: value key 'unittest' equal expectations: '{self.task_object.variables['test']}' versus 'value2'\")\n", " self.assertequal(self.task_object.variables['keys'], \"value3\", f\"[classes:task:gettaskmetadata]: value key 'keys' equal expectations: '{self.task_object.variables['test']}' versus 'value3'\")\n", " \n", "\n", " def test_gettaskmetadata_task_set_complex_variables(self):\n", " '''\n", " test logic:\n", " key-values need considered lists, individual values\n", " test validates values list_columns (see [classes:task:gettaskmetadata]) actually set list-values\n", " '''\n", " # preprocess\n", "\n", " # define list expected values list-specific keys\n", " # see [classes:task:gettaskmetadata]: list_columns = ['source_name', 'sink_name', 'dimension', 'data_type', 'check_name', 'config_params', 'column_info']\n", " expected_source_names = [\"value211\", \"value221\", \"value231\"]\n", " expected_sink_names = [\"value212\", \"value222\", \"value232\"]\n", " expected_dimensions = [\"value213\", \"value223\", \"value233\"]\n", " expected_datatypes = [\"value214\", \"value224\", \"value234\"]\n", " expected_check_names = [\"value215\", \"value225\", \"value235\"]\n", " expected_config_params = [\"value216\", \"value226\", \"value236\"]\n", " expected_column_info = [\"value217\", \"value227\", \"value237\"]\n", "\n", "\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " # exeucte_query returns non-empty object array\n", " mock_sql_object.execute_query.return_value = [\n", "{\n", " 'column_1': '[{\"test1\": \"value11\", \"test2\": \"value12\"}]', \n", " 'column_2': '''\n", " [{\"source_name\": \"value211\", \"sink_name\": \"value212\", \"dimension\": \"value213\", \"data_type\": \"value214\", \"check_name\": \"value215\", \"config_params\": \"value216\", \"column_info\": \"value217\"}, \n", " {\"source_name\": \"value221\", \"sink_name\": \"value222\", \"dimension\": \"value223\", \"data_type\": \"value224\", \"check_name\": \"value225\", \"config_params\": \"value226\", \"column_info\": \"value227\"}, \n", " {\"source_name\": \"value231\", \"sink_name\": \"value232\", \"dimension\": \"value233\", \"data_type\": \"value234\", \"check_name\": \"value235\", \"config_params\": \"value236\", \"column_info\": \"value237\"}]\n", " ''',\n", " 'column_3': '[{\"keys1\": \"value31\", \"keys2\": \"value32\"}]'\n", " }\n", " ]\n", " self.task_object.sqlserverconnection = mock_sql_object\n", " # pass list get_task_metadata expect key-value pairs set self.task_object.variables dictionary\n", " # dev-note: list_columns added automatically, even though mentioned variables-list. changed, current code allow it\n", " variables = ['test1', 'test2', 'keys1', 'keys2']\n", "\n", " # execute\n", " # [classes:task:get_task_metadata] run method test\n", " self.task_object.get_task_metadata(variables)\n", "\n", " # evaluate\n", " # validate execute_query successfully set values self.task_object.variables\n", " self.assertequal(self.task_object.variables['test1'], \"value11\", f\"[classes:task:gettaskmetadata]: value key 'test1' equal expectations: '{self.task_object.variables['test1']}' versus 'value11'\")\n", " self.assertequal(self.task_object.variables['keys1'], \"value31\", f\"[classes:task:gettaskmetadata]: value key 'keys1' equal expectations: '{self.task_object.variables['test1']}' versus 'value31'\")\n", " self.assertequal(self.task_object.variables['test2'], \"value12\", f\"[classes:task:gettaskmetadata]: value key 'test2' equal expectations: '{self.task_object.variables['test2']}' versus 'value12'\")\n", " self.assertequal(self.task_object.variables['keys2'], \"value32\", f\"[classes:task:gettaskmetadata]: value key 'keys2' equal expectations: '{self.task_object.variables['test2']}' versus 'value32'\")\n", " \n", " # validate execute_query successfully set values self.task_object.variables list_column keys\n", " self.assertequal(self.task_object.variables['source_name'], expected_source_names, f\"[classes:task:gettaskmetadata]: value key 'source_name' equal expectations: '{self.task_object.variables['source_name']}' versus {expected_source_names}\")\n", " self.assertequal(self.task_object.variables['sink_name'], expected_sink_names, f\"[classes:task:gettaskmetadata]: value key 'sink_names' equal expectations: '{self.task_object.variables['sink_name']}' versus {expected_sink_names}\")\n", " self.assertequal(self.task_object.variables['dimension'], expected_dimensions, f\"[classes:task:gettaskmetadata]: value key 'dimensions' equal expectations: '{self.task_object.variables['dimension']}' versus {expected_dimensions}\")\n", " self.assertequal(self.task_object.variables['data_type'], expected_datatypes, f\"[classes:task:gettaskmetadata]: value key 'datatypes' equal expectations: '{self.task_object.variables['data_type']}' versus {expected_datatypes}\")\n", " self.assertequal(self.task_object.variables['check_name'], expected_check_names, f\"[classes:task:gettaskmetadata]: value key 'check_names' equal expectations: '{self.task_object.variables['check_name']}' versus {expected_check_names}\")\n", " self.assertequal(self.task_object.variables['config_params'], expected_config_params, f\"[classes:task:gettaskmetadata]: value key 'config_params' equal expectations: '{self.task_object.variables['config_params']}' versus {expected_config_params}\")\n", " self.assertequal(self.task_object.variables['column_info'], expected_column_info, f\"[classes:task:gettaskmetadata]: value key 'column_info' equal expectations: '{self.task_object.variables['column_info']}' versus {expected_column_info}\")\n", "\n", "\n", " # failure: expect execute_query mock-object called statement 'exec meta.usp_get_task_metadata' expect_return=true\n", " def test_gettaskmetadata_task_return_empty_query(self):\n", " '''\n", " test logic:\n", " execute_query returns empty result, error thrown\n", " '''\n", " # preprocess\n", " expected_error = f\"no task_metadata found task_id {str(self.task_id)}\"\n", " # set sqlserverconnection-object task_object\n", " # define magicmock object sqlserverconnection -&gt; still able call execute_query\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " # exeucte_query returns none\n", " mock_sql_object.execute_query.return_value = none\n", " self.task_object.sqlserverconnection = mock_sql_object\n", " # pass empty list variables test validate function calls\n", " variables = []\n", "\n", " # execute\n", " # [classes:task:get_task_metadata] run method test\n", " self.assertraises(unboundlocalerror) error:\n", " self.task_object.get_task_metadata(variables) \n", "\n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:task:gettaskmetadata]: error match expectations: {actual_error} versus {expected_error}\")\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_gettaskmetadata)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 18 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_setingestionvariables(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " task_name = 'task_name'\n", " self.task_object = task(task_name, mock(), mock(), mock(), mock(), mock())\n", " self.task_object.table_name = 'unittest'\n", " return\n", "\n", "\n", " def teardown(self):\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls start_task method\n", "\n", "\n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_start_task'\n", " def test_set_ingestion_variables_task_success(self):\n", " # preprocess\n", " storage_account = 'unittest'\n", " expected_silver_path = \"abfss://silver@%s.dfs.core.windows.net/%s\" % ( storage_account, self.task_object.table_name)\n", " # expected_raw_format = 'parquet'\n", " expected_skip_first_lines = -1\n", " expected_target_options = {}\n", "\n", " self.task_object.variables = {\n", " 'skip_first_lines': expected_skip_first_lines,\n", " 'target_options': expected_target_options\n", " }\n", "\n", " # execute\n", " # [classes:task:setingestionvariables] run method test\n", " self.task_object.set_ingestion_variables(storage_account)\n", "\n", " # evaluate\n", " # validate execute_query called \n", " self.assertequal(self.task_object.silver_path, expected_silver_path, f\"[classes:task:gettaskmetadata]: value key 'silver_path' equal expectations: '{self.task_object.silver_path}' versus '{expected_silver_path}'\")\n", " # self.assertequal(self.task_object.raw_format, expected_raw_format, f\"[classes:task:gettaskmetadata]: value key 'raw_format' equal expectations: '{self.task_object.raw_format}' versus '{expected_raw_format}'\")\n", " self.assertequal(self.task_object.skip_first_lines, expected_skip_first_lines, f\"[classes:task:gettaskmetadata]: value key 'skip_first_lines' equal expectations: '{self.task_object.skip_first_lines}' versus '{expected_skip_first_lines}'\")\n", " self.assertequal(self.task_object.target_options, expected_target_options, f\"[classes:task:gettaskmetadata]: value key 'target_options' equal expectations: '{self.task_object.target_options}' versus '{expected_target_options}'\")\n", "\n", "\n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_start_task'\n", " def test_set_ingestion_variables_task_targetoptions(self):\n", " # preprocess\n", " storage_account = 'unittest'\n", " expected_target_options = '{\"valid\": \"json\", \"test\": {\"regex\": \"\\\\\\\\d{8}\"}}'\n", "\n", " self.task_object.variables = {\n", " 'skip_first_lines': -1,\n", " 'target_options': expected_target_options\n", " }\n", "\n", " # execute\n", " # [classes:task:setingestionvariables] run method test\n", " self.task_object.set_ingestion_variables(storage_account)\n", "\n", " # evaluate\n", " # validate execute_query called \n", " self.assertequal(self.task_object.target_options, json.loads(expected_target_options), f\"[classes:task:gettaskmetadata]: value key 'target_options' equal expectations: '{self.task_object.target_options}' versus '{expected_target_options}'\")\n", " \n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_start_task'\n", " def test_set_ingestion_variables_task_invalid_targetoptions(self):\n", " # preprocess\n", " storage_account = 'unittest'\n", " expected_target_options = 'string'\n", " expected_error = f\"given target_options cannot converted json. validate configuration {self.task_object.task_name}: {expected_target_options}\"\n", "\n", " self.task_object.variables = {\n", " 'skip_first_lines': -1,\n", " 'target_options': expected_target_options\n", " }\n", "\n", " # execute\n", " # [classes:task:setingestionvariables] run method test\n", " self.assertraises(valueerror) error:\n", " self.task_object.set_ingestion_variables(storage_account)\n", "\n", "\n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:task:gettaskmetadata]: error match expectations: {actual_error} versus {expected_error}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_setingestionvariables)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 19 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_setdatasetvariables(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " task_name = 'task_name'\n", " self.task_object = task(task_name, mock(), mock(), mock(), mock(), mock())\n", " self.task_object.table_name = 'unittest'\n", " return\n", "\n", "\n", " def teardown(self):\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls start_task method\n", "\n", "\n", " # success: expect dataset_variables set running set_dataset_variables()\n", " def test_set_dataset_variables_task_success(self):\n", " # preprocess\n", " # set expected values\n", " storage_account = 'unittest'\n", " expected_table_name = 'test_table'\n", " expected_container_name = 'test_container'\n", " expected_file_path = 'test_file/test_folder/'\n", " expected_header = true\n", " expected_sink_column_names = ['test', 'sink']\n", " expected_source_column_names = ['test', 'sink']\n", " expected_column_datatypes = ['test', 'sink']\n", " expected_dimensions = ['test', 'sink']\n", " expected_column_information = [{\"test\": \"dict\"}, {\"key\": \"value\"}, 'null']\n", " expected_separator = 'test_sep'\n", " expected_file_extension = 'test_file_extension'\n", " expected_file_kind = 'test_file_kind'\n", " expected_landing_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (expected_container_name, storage_account, expected_file_path)\n", " expected_file_pattern = 'test_file_pattern'\n", " expected_quote_character = '\"'\n", " expected_escape_character = '\\\\'\n", "\n", " # create task_object-member variables: dictionary key:value pairs dataset metadata\n", " self.task_object.variables = {\n", " 'table_name': expected_table_name,\n", " 'container_name': expected_container_name,\n", " 'source_folder': expected_file_path,\n", " 'header': expected_header,\n", " 'sink_name': expected_sink_column_names,\n", " 'source_name': expected_source_column_names,\n", " 'data_type': expected_column_datatypes,\n", " 'dimension': expected_dimensions,\n", " 'column_info': ['{\"test\": \"dict\"}', '{\"key\": \"value\"}', 'null'],\n", " 'column_delimiter': expected_separator,\n", " 'file_extension': expected_file_extension,\n", " 'file_kind': expected_file_kind,\n", " 'file_pattern': expected_file_pattern,\n", " 'quote_character': expected_quote_character,\n", " 'escape_character': expected_escape_character\n", " }\n", "\n", " # execute\n", " # [classes:task:setdatasetvariables] run method test\n", " self.task_object.set_dataset_variables(storage_account)\n", "\n", " # evaluate\n", " # validate members set correctly\n", " self.assertequal(self.task_object.table_name, expected_table_name, f\"[classes:task:gettaskmetadata]: value key 'table_name' equal expectations: '{self.task_object.table_name}' versus '{expected_table_name}'\")\n", " self.assertequal(self.task_object.container_name, expected_container_name, f\"[classes:task:gettaskmetadata]: value key 'container_name' equal expectations: '{self.task_object.container_name}' versus '{expected_container_name}'\")\n", " self.assertequal(self.task_object.file_path, expected_file_path, f\"[classes:task:gettaskmetadata]: value key 'file_path' equal expectations: '{self.task_object.file_path}' versus '{expected_file_path}'\")\n", " self.assertequal(self.task_object.header, expected_header, f\"[classes:task:gettaskmetadata]: value key 'header' equal expectations: '{self.task_object.header}' versus '{expected_header}'\")\n", " self.assertequal(self.task_object.sink_column_names, expected_sink_column_names, f\"[classes:task:gettaskmetadata]: value key 'sink_column_names' equal expectations: '{self.task_object.sink_column_names}' versus '{expected_sink_column_names}'\")\n", " self.assertequal(self.task_object.source_column_names, expected_source_column_names, f\"[classes:task:gettaskmetadata]: value key 'source_column_names' equal expectations: '{self.task_object.source_column_names}' versus '{expected_source_column_names}'\")\n", " self.assertequal(self.task_object.column_datatypes, expected_column_datatypes, f\"[classes:task:gettaskmetadata]: value key 'column_datatypes' equal expectations: '{self.task_object.column_datatypes}' versus '{expected_column_datatypes}'\")\n", " self.assertequal(self.task_object.dimensions, expected_dimensions, f\"[classes:task:gettaskmetadata]: value key 'dimensions' equal expectations: '{self.task_object.dimensions}' versus '{expected_dimensions}'\")\n", " self.assertequal(self.task_object.separator, expected_separator, f\"[classes:task:gettaskmetadata]: value key 'separator' equal expectations: '{self.task_object.separator}' versus '{expected_separator}'\")\n", " self.assertequal(self.task_object.file_extension, expected_file_extension, f\"[classes:task:gettaskmetadata]: value key 'file_extension' equal expectations: '{self.task_object.file_extension}' versus '{expected_file_extension}'\")\n", " self.assertequal(self.task_object.file_kind, expected_file_kind, f\"[classes:task:gettaskmetadata]: value key 'file_kind' equal expectations: '{self.task_object.file_kind}' versus '{expected_file_kind}'\")\n", " self.assertequal(self.task_object.landing_path, expected_landing_path, f\"[classes:task:gettaskmetadata]: value key 'landing_path' equal expectations: '{self.task_object.landing_path}' versus '{expected_landing_path}'\")\n", " self.assertequal(self.task_object.file_pattern, expected_file_pattern, f\"[classes:task:gettaskmetadata]: value key 'file_pattern' equal expectations: '{self.task_object.file_pattern}' versus '{expected_file_pattern}'\")\n", " self.assertequal(self.task_object.escape_character, expected_escape_character, f\"[classes:task:gettaskmetadata]: value key 'escape_character' equal expectations: '{self.task_object.escape_character}' versus '{expected_escape_character}'\")\n", " self.assertequal(self.task_object.quote_character, expected_quote_character, f\"[classes:task:gettaskmetadata]: value key 'quote_character' equal expectations: '{self.task_object.quote_character}' versus '{expected_quote_character}'\")\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_setdatasetvariables)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 20 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_task_getfilesobjects(unittest.testcase):\n", " def setup(self):\n", " # set-up object-instance task-class use test-methods\n", " self.task_object = task(mock(), mock(), mock(), mock(), mock(), mock())\n", " self.task_object.landing_path = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/task/get_file_objects/'\n", "\n", "\n", " self.landing_path_1 = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/task/get_file_objects/test_file_1'\n", " self.landing_path_2 = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/task/get_file_objects/test_file_2'\n", "\n", " dataframe_1 = spark.createdataframe(\n", " data=[],\n", " schema=\"test int, schema string\"\n", " )\n", "\n", " dataframe_1.repartition(1).write.mode('overwrite').csv(self.landing_path_1)\n", " dataframe_1.repartition(1).write.mode('append').csv(self.landing_path_1)\n", " dataframe_1.repartition(1).write.mode('overwrite').csv(self.landing_path_2)\n", "\n", " return\n", "\n", " def teardown(self):\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls start_task method\n", "\n", "\n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_start_task'\n", " def test_get_files_objects_task_success(self):\n", " # preprocess\n", " self.task_object.file_pattern = 'part'\n", " self.task_object.file_extension = none\n", "\n", " expected_calls = [\n", " call(\n", " task=self.task_object, \n", " sqlserverconnection=self.task_object.sqlserverconnection, \n", " landing_path=any,\n", " filename= any, \n", " timestamp_folder='test_file_1', \n", " storage_account='unittest', \n", " debug=false\n", " ),\n", " call(\n", " task=self.task_object, \n", " sqlserverconnection=self.task_object.sqlserverconnection, \n", " landing_path=any,\n", " filename= any, \n", " timestamp_folder='test_file_1', \n", " storage_account='unittest', \n", " debug=false\n", " ),\n", " call(\n", " task=self.task_object, \n", " sqlserverconnection=self.task_object.sqlserverconnection, \n", " landing_path=any,\n", " filename= any, \n", " timestamp_folder='test_file_2', \n", " storage_account='unittest', \n", " debug=false\n", " )\n", " ]\n", "\n", " # execute\n", " # [classes:task:getfilesobjects] run method test\n", " patch(__name__ + '.file') mock_file_class:\n", " mock_file_class.return_value = -1\n", " file_objects=self.task_object.get_files_objects('unittest')\n", "\n", " # evaluate\n", " # validate file-object called 3 times, dataframes\n", " self.assertequal(mock_file_class.call_count, 3)\n", " mock_file_class.assert_has_calls(expected_calls, any_order=true)\n", "\n", "\n", " # success: excpet execute_query mock-object called statement 'exec meta.usp_start_task'\n", " def test_get_files_objects_task_success(self):\n", " # preprocess\n", " self.task_object.file_pattern = 'unfound_pattern'\n", " self.task_object.file_extension = none\n", "\n", " # execute\n", " # [classes:task:getfilesobjects] run method test\n", " patch(f'{__name__}.file') mock_file_class, \\\n", " patch(f'{__name__}.list_directory_content') mock_list_directory_content, \\\n", " patch(f'{__name__}.filter_list') mock_filter_list:\n", "\n", " mock_list_directory_content.return_value = ['invalid_filename'], list()\n", " mock_filter_list.return_value = list()\n", "\n", " file_objects=self.task_object.get_files_objects('unittest')\n", "\n", " # evaluate\n", " # validate mocked functions called expected arguments\n", " mock_list_directory_content.assert_called_once_with(self.task_object.landing_path, list(), list())\n", " mock_filter_list.assert_called_once_with(full_list=['invalid_filename'], pattern=self.task_object.file_pattern, extension=none)\n", " # since filter render empty list found_files, file-object class called\n", " mock_file_class.assert_not_called()\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_task_getfilesobjects)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 21 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: file\n", "# class test __init__() file-class defined classes notebook\n", "class test_file_initialisation(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing \n", " self.task = task(task_name= mock(), task_id= mock(), plan_id=mock(), sqlserverconnection = mock(), worker_name=mock())\n", "\n", " self.task.column_datatypes = ['string', 'int']\n", " self.task.source_column_names = ['unit', 'test']\n", " self.task.sink_column_names = ['unit', 'test']\n", " self.task.table_name = ['unit_test'] \n", " self.task.header = mock()\n", " self.task.variables = {\"check_name\": [], \"config_params\": {}}\n", " self.task.file_extension = 'csv'\n", "\n", " return\n", "\n", " def teardown(self):\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_access_token method\n", "\n", " # success: test successful initialisation new file-object\n", " def test_init_success(self):\n", " # preprocess\n", " expected_task_object = self.task\n", " expected_task_object.table_name = 'unittest_silver'\n", "\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " expected_sql_server_object = mock_sql_object\n", "\n", " expected_file_name = 'unittest.csv'\n", " expected_storage_account = 'storage_account'\n", " expected_landing_path = f'abfss-path://landing@{expected_storage_account}.dfs.core.windows.net/{expected_file_name}'\n", " expected_timestamp = 'timestamp'\n", " expected_raw_path = f'abfss-path://raw@{expected_storage_account}.dfs.core.windows.net/unittest'\n", " expected_silver_path = f'abfss://silver@{expected_storage_account}.dfs.core.windows.net/{expected_task_object.table_name}'\n", " expected_archive_path = f'abfss-path://archive@{expected_storage_account}.dfs.core.windows.net/{expected_file_name}'\n", " expected_extended_filename = expected_file_name + '_' + expected_timestamp\n", "\n", "\n", " # execute\n", " # initialise file-object file-class\n", " patch.object(file, 'log_file') mock_log_file:\n", " file_object = file(expected_task_object, expected_sql_server_object, expected_landing_path, expected_file_name, expected_timestamp, expected_storage_account)\n", "\n", "\n", " # evaluate\n", " # validate correct functions called expected set arguments\n", " mock_log_file.assert_called_once()\n", "\n", " # validate instance checks class created\n", "\n", " # validate member-values set according expectations\n", " self.assertequal(expected_sql_server_object, file_object.sqlserverconnection, f'[classes:file:init] initialisation member \"sqlserverconnection\" match expectations: {expected_sql_server_object} versus {file_object.sqlserverconnection}') \n", " self.assertequal(expected_file_name, file_object.file_name, f'[classes:file:init] initialisation member \"file_name\" match expectations: {expected_file_name} versus {file_object.file_name}')\n", " self.assertequal(expected_storage_account, file_object.storage_account, f'[classes:file:init] initialisation member \"storage_account\" match expectations: {expected_storage_account} versus {file_object.storage_account}') \n", " self.assertequal(expected_landing_path, file_object.landing_path, f'[classes:file:init] initialisation member \"landing_path\" match expectations: {expected_landing_path} versus {file_object.landing_path}')\n", " self.assertequal(expected_extended_filename, file_object.extended_filename, f'[classes:file:init] initialisation member \"extended_filename\" match expectations: {expected_extended_filename} versus {file_object.extended_filename}') \n", " self.assertequal(expected_raw_path, file_object.raw_path, f'[classes:file:init] initialisation member \"raw_path\" match expectations: {expected_raw_path} versus {file_object.raw_path}')\n", " self.assertequal(expected_silver_path, file_object.silver_path, f'[classes:file:init] initialisation member \"silver_path\" match expectations: {expected_silver_path} versus {file_object.silver_path}')\n", " self.assertequal(expected_archive_path, file_object.archive_path, f'[classes:file:init] initialisation member \"archive_path\" match expectations: {expected_archive_path} versus {file_object.archive_path}')\n", "\n", "\n", " def test_init_query_execution(self):\n", " # preprocess\n", " expected_task_object = self.task\n", " expected_task_object.table_name = 'unittest_silver'\n", " expected_task_object.task_id = -1\n", " expected_task_object.plan_id = -1\n", "\n", " mock_sql_object = magicmock(spec=sqlserverconnection)\n", " expected_sql_server_object = mock_sql_object\n", " \n", " expected_file_name = 'unittest.csv'\n", " expected_storage_account = 'storage_account'\n", " expected_landing_path = f'abfss-path://landing@{expected_storage_account}.dfs.core.windows.net/{expected_file_name}'\n", " expected_timestamp = 'timestamp'\n", " expected_extended_filename = expected_file_name + '_' + expected_timestamp\n", "\n", "\n", " \n", " patch.object(mock_sql_object, 'execute_query') mock_execute_query:\n", " mock_execute_query.return_value = [{\"file_id\": -2}]\n", " file_object = file(expected_task_object, expected_sql_server_object, expected_landing_path, expected_file_name, expected_timestamp, expected_storage_account)\n", "\n", "\n", " # evaluate\n", " # validate execute_query called expected statement\n", " mock_execute_query.assert_called_once_with(f\"exec meta.usp_new_file @filename='{expected_file_name}', @task_id={expected_task_object.task_id}, @plan_id ={expected_task_object.plan_id}, @extended_filename = '{expected_extended_filename}', @info_message = '{expected_landing_path}', @no_select = 0\", expect_return=true)\n", " # validate log_files set expected file_id member value\n", " self.assertequal(-2, file_object.file_id, f'[classes:file:init] initialisation member \"file_id\" match expectations: {-2} versus {file_object.file_id}')\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_file_initialisation)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 22 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: file\n", "# class test __init__() file-class defined classes notebook\n", "class test_file_logfile(unittest.testcase):\n", "# ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing\n", " \n", " self.mock_file_class_patcher = patch.object(file, '__init__', return_value=none)\n", " self.mock_file_class = self.mock_file_class_patcher.start()\n", "\n", " self.file_object = file()\n", " self.file_object.file_name = mock()\n", " self.file_object.debug = false\n", " self.file_object.task = mock()\n", " self.file_object.task.plan_id = -98\n", " self.file_object.task.task_id = -97\n", " self.file_object.extended_filename = \"extended_filename\"\n", " self.file_object.landing_path = mock()\n", "\n", " self.file_object.sqlserverconnection = mock()\n", " \n", " self.mock_sqlserverconnection_execute_query_patcher = patch.object(self.file_object.sqlserverconnection, 'execute_query')\n", " self.mock_sqlserverconnection_execute_query = self.mock_sqlserverconnection_execute_query_patcher.start()\n", " \n", " return\n", "\n", " def teardown(self):\n", " self.mock_file_class_patcher.stop()\n", " self.mock_sqlserverconnection_execute_query_patcher.stop()\n", " # teardown patched functions\n", " return\n", "\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_access_token method\n", "\n", "\n", " # success: stored procedure call returned file_id argument\n", " def test_log_file_success(self):\n", " # preprocess\n", " # set expected results return_value mocked functions\n", " expected_file_id = -99\n", " self.mock_sqlserverconnection_execute_query.return_value = [{'file_id': expected_file_id}]\n", "\n", " # execute\n", " # run function/method test\n", " self.file_object.log_file()\n", "\n", " # evaluate\n", " # validate file_id set expected\n", " actual_file_id = self.file_object.file_id\n", " self.assertequal(expected_file_id, actual_file_id, f\"actual file id match expectation: {actual_file_id} versus {expected_file_id}\")\n", "\n", " # validate correct functions called expected arguments\n", " self.mock_sqlserverconnection_execute_query.assert_called_once_with(\n", " f\"\"\"exec meta.usp_new_file @filename='{self.file_object.file_name}', @task_id={self.file_object.task.task_id}, @plan_id ={self.file_object.task.plan_id}, @extended_filename = '{self.file_object.extended_filename}', @info_message = '{self.file_object.landing_path}', @no_select = 0\"\"\", \n", " expect_return=true\n", " )\n", "\n", "\n", " # failure: stored procedure call return result\n", " def test_log_file_noresult(self):\n", " # preprocess\n", " # set expected results return_value mocked functions\n", " self.mock_sqlserverconnection_execute_query.return_value = none\n", " expected_exception = f\"no file_id returned logging file {self.file_object.extended_filename} plan_id ({str(self.file_object.task.plan_id)}) task_id ({str(self.file_object.task.task_id)})\"\n", "\n", " \n", " # execute\n", " # run function/method test expect error thrown\n", " self.assertraises(unboundlocalerror) error:\n", " self.file_object.log_file()\n", " \n", "\n", " # evaluate\n", " # validate raised error matches expected error\n", " actual_exception = str(error.exception)\n", " self.assertequal(expected_exception, actual_exception, f\"the actual exception log_file function match expected exception: {actual_exception} versus {expected_exception}\")\n", "\n", " # failure: stored procedure call return file_id argument\n", " def test_log_file_nofileid(self):\n", " # preprocess\n", " # set expected results return_value mocked functions\n", " self.mock_sqlserverconnection_execute_query.return_value = [{'empty': 'dictionary'}]\n", " expected_exception = f\"there file_id set file {self.file_object.extended_filename} plan_id ({str(self.file_object.task.plan_id)}) task_id ({str(self.file_object.task.task_id)})\"\n", "\n", "\n", " # execute\n", " # run function/method test expect error thrown\n", " self.assertraises(attributeerror) error:\n", " self.file_object.log_file()\n", "\n", "\n", " # evaluate\n", " # validate raised error matches expected error\n", " actual_exception = str(error.exception)\n", " self.assertequal(expected_exception, actual_exception,f\"the actual exception log_file function match expected exception: {actual_exception} versus {expected_exception}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_file_logfile)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 23 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: file\n", "# class test landing_to_raw() file-class defined classes notebook\n", "class test_file_landingtoraw(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing \n", " self.task_id = -1\n", " self.plan_id = -1\n", " self.task = task(task_name='landig_to_raw', task_id= self.task_id, plan_id=self.task_id, sqlserverconnection = mock(), worker_name=mock())\n", "\n", " self.task.column_datatypes = ['string', 'int']\n", " self.task.source_column_names = ['unit', 'test']\n", " self.task.sink_column_names = ['unit', 'test']\n", " self.task.column_information = [none, none]\n", " self.task.table_name = ['unit_test'] \n", " self.task.header = true\n", " self.task.variables = {\"check_name\": [], \"config_params\": {}}\n", " self.task.skip_first_lines = none\n", " self.task.separator = ','\n", " self.task.file_kind = 'csv'\n", " self.task.quote_character = '\"'\n", " self.task.escape_character = '\\\\'\n", " self.task.file_extension = 'csv'\n", " \n", "\n", " # [classes:sqlserverconnection] mock sqlserverconnection: make sure queries executed\n", " self.mock_sql_object = magicmock(spec=sqlserverconnection)\n", " self.sql_server_object = self.mock_sql_object\n", "\n", " # [classes:file] make sure log_file execute throw error -&gt; cannot make sqlserverconnection\n", " self.log_file_patcher = patch.object(file, 'log_file')\n", " self.mock_log_file = self.log_file_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " self.log_file_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls landing_to_raw method\n", "\n", " # success: test successful landingtoraw new file-object\n", " def test_landingtoraw_file_success(self):\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.table_name = 'unittest_silver'\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'mock_data.csv'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/landing/move_to_raw'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " file_object.file_id = -1\n", "\n", "\n", " # execute\n", " # [classes:file:landingtoraw]: run method test\n", " patch.object(self.mock_sql_object, 'execute_query') mock_execute_query:\n", " file_object.landing_to_raw()\n", " \n", " # evaluate\n", " # get list files raw_filepath validate expected files landed there\n", " raw_filepath = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/classes/filemovement/raw/move_to_raw/mock_data'\n", " actual_raw_files, actual_raw_folders = list_directory_content(raw_filepath, list(), list())\n", " actual_raw_file_names = [file.name file actual_raw_files]\n", " self.assertin(\"_success\", actual_raw_file_names, \"[classes:file:landingtoraw] succes-file found raw\")\n", " self.asserttrue(len(actual_raw_file_names) == 2, f\"[classes:file:landingtoraw] files found raw {actual_raw_file_names}, equal 2 files expected.\")\n", "\n", "\n", " # success: validate expected function calls made\n", " def test_landingtoraw_file_function_calls(self):\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.table_name = 'unittest_silver'\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'mock_data.csv'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/landing/move_to_raw'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " file_object.file_id = -1\n", "\n", "\n", " # execute\n", " # [classes:file:landingtoraw]: run method test\n", " patch.object(self.mock_sql_object, 'execute_query') mock_execute_query, \\\n", " patch.object(sparkdataframechecks, 'start_checks') mock_start_checks, \\\n", " patch.object(sparkdataframeclass, 'add_logging_columns_to_dataframe') mock_add_logging_columns_to_dataframe:\n", " file_object.landing_to_raw()\n", " \n", " # evaluate\n", " # get list files raw_filepath validate expected files landed there\n", " raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/classes/filemovement/raw/move_to_raw/mock_data'\n", " info=create_json_object(path=raw_path,raw_amount=20)\n", " mock_execute_query.assert_called_once_with(f\"exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=raw, @success=true, @info_message = '{info}'\")\n", " mock_start_checks.assert_called_once_with(phase='landing', parameters={})\n", " mock_add_logging_columns_to_dataframe.assert_called_once_with(task_id=task_object.task_id, plan_id=task_object.plan_id, file_id=file_object.file_id, file_name=file_object.file_name)\n", "\n", "\n", " # success: validate expected function calls made\n", " def test_landingtoraw_file_skipline(self):\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.table_name = 'unittest_silver'\n", " task_object.skip_first_lines = 1\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'mock_skiplines_move.csv'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/landing/move_to_raw'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " file_object.file_id = -1\n", "\n", "\n", " # execute\n", " # [classes:file:landingtoraw]: run method test\n", " patch.object(self.mock_sql_object, 'execute_query') mock_execute_query, \\\n", " patch.object(sparkdataframechecks, 'start_checks') mock_start_checks, \\\n", " patch.object(sparkdataframeclass, 'add_logging_columns_to_dataframe') mock_add_logging_columns_to_dataframe:\n", " file_object.landing_to_raw()\n", " \n", " # evaluate\n", " # get list files raw_filepath validate expected files landed there\n", " raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/classes/filemovement/raw/move_to_raw/mock_skiplines_move'\n", " info=create_json_object(path=raw_path,raw_amount=6)\n", " mock_execute_query.assert_called_once_with(f\"exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=raw, @success=true, @info_message = '{info}'\")\n", " mock_start_checks.assert_called_once_with(phase='landing', parameters={})\n", " mock_add_logging_columns_to_dataframe.assert_called_once_with(task_id=task_object.task_id, plan_id=task_object.plan_id, file_id=file_object.file_id,file_name=file_object.file_name)\n", "\n", "\n", "\n", " # success: validate expected function calls made\n", " def test_landingtoraw_file_skipline_functioncalls(self):\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.table_name = 'unittest_silver'\n", " task_object.skip_first_lines = 1\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'mock_skiplines_move.csv'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/landing/move_to_raw'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " file_object.file_id = -1\n", "\n", "\n", " # execute\n", " # [classes:file:landingtoraw]: run method test\n", " patch.object(self.mock_sql_object, 'execute_query') mock_execute_query, \\\n", " patch.object(sparkdataframechecks, 'start_checks') mock_start_checks, \\\n", " patch.object(sparkdataframeclass, 'add_logging_columns_to_dataframe') mock_add_logging_columns_to_dataframe:\n", " file_object.landing_to_raw()\n", " \n", " # evaluate\n", " # get list files raw_filepath validate expected files landed there\n", " raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/classes/filemovement/raw/move_to_raw/mock_skiplines_move'\n", " info=create_json_object(path=raw_path,raw_amount=6)\n", " mock_execute_query.assert_called_once_with(f\"exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=raw, @success=true, @info_message = '{info}'\")\n", " mock_start_checks.assert_called_once_with(phase='landing', parameters={})\n", " mock_add_logging_columns_to_dataframe.assert_called_once_with(task_id=task_object.task_id, plan_id=task_object.plan_id, file_id=file_object.file_id, file_name=file_object.file_name)\n", "\n", " # success: validate expected function calls made\n", " def test_landingtoraw_file_jsonmultiline(self):\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.table_name = 'unittest_silver'\n", " task_object.file_extension = 'json'\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'mock_multiline_move.json'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/landing/move_to_raw'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account, true)\n", " file_object.file_id = -1\n", "\n", "\n", " # execute\n", " # [classes:file:landingtoraw]: run method test\n", " patch.object(self.mock_sql_object, 'execute_query') mock_execute_query: \n", " file_object.landing_to_raw()\n", " \n", " # evaluate\n", " # get list files raw_filepath validate expected files landed there\n", " raw_filepath = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/classes/filemovement/raw/move_to_raw/mock_multiline_move'\n", " actual_raw_files, actual_raw_folders = list_directory_content(raw_filepath, list(), list())\n", " actual_raw_file_names = [file.name file actual_raw_files]\n", " self.assertin(\"_success\", actual_raw_file_names, \"[classes:file:landingtoraw] succes-file found raw mock_multiline_move\")\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_file_landingtoraw)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 24 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: file\n", "# class test raw_to_silver() file-class defined classes notebook\n", "class test_file_rawtosilver(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing \n", " self.task_id = -1\n", " self.plan_id = -1\n", " self.task = task(task_name='landig_to_raw', task_id= self.task_id, plan_id=self.task_id, sqlserverconnection = mock(), worker_name=mock())\n", "\n", " self.task.column_datatypes = [\"string\",\"string\", \"string\", \"string\", \"string\"]\n", " self.task.source_column_names = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n", " self.task.sink_column_names = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n", " self.task.dimensions = ['scd2', 'pk','pk','scd2','scd2',]\n", " self.task.column_information = [none, none, none, none, none]\n", " self.task.table_name = ['unit_test'] \n", " self.task.header = true\n", " self.task.variables = {\"check_name\": [], \"config_params\": {}}\n", " self.task.separator = ';'\n", " self.task.file_kind = 'parquet'\n", " self.task.quote_character = '\"'\n", " self.task.escape_character = '\\\\' \n", " self.task.file_extension = none\n", "\n", " # [classes:sqlserverconnection] mock sqlserverconnection: make sure queries executed\n", " self.mock_sql_object = magicmock(spec=sqlserverconnection)\n", " self.sql_server_object = self.mock_sql_object\n", "\n", " # [classes:file] make sure log_file execute throw error -&gt; cannot make sqlserverconnection\n", " self.log_file_patcher = patch.object(file, 'log_file')\n", " self.mock_log_file = self.log_file_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " self.log_file_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls raw_to_silver method\n", "\n", "\n", " def test_rawtosilver_file_success(self):\n", " '''\n", " test logic:\n", " create empty delta table\n", " move parquet file raw silver\n", " validate contents file ingested silver table row_count\n", " '''\n", "\n", " # preprocess\n", " # set expected values\n", " expected_rowcount = 20\n", "\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.target_options = {}\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'part-00000-ded9c119-09f3-4c0a-9415-3536ded9342f-c000.snappy.parquet'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/raw/move_to_silver'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " \n", " # overwrite file_object members test-values \n", " file_object.file_id = -1\n", " file_object.raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " file_object.silver_path = f\"abfss://silver@{storage_account}.dfs.core.windows.net/mocking_table\"\n", "\n", "\n", " ## create empty delta table (with technical fields) upload rawdata into\n", " df = spark.createdataframe(data=[], schema=dt.structtype([\n", " dt.structfield(\"_c0\", sql.types.stringtype()),\n", " dt.structfield(\"first_name\", sql.types.stringtype()),\n", " dt.structfield(\"last_name\", sql.types.stringtype()),\n", " dt.structfield(\"age\", sql.types.stringtype()),\n", " dt.structfield(\"email\", sql.types.stringtype()),\n", " dt.structfield(\"t_load_date_raw\", sql.types.timestamptype()),\n", " dt.structfield(\"t_load_date_silver\", sql.types.timestamptype()),\n", " dt.structfield(\"t_extract_date\", sql.types.timestamptype()),\n", " dt.structfield(\"t_update_date\", sql.types.timestamptype()),\n", " dt.structfield(\"t_insert_date\", sql.types.timestamptype()),\n", " dt.structfield(\"t_plan_id\", sql.types.integertype()),\n", " dt.structfield(\"t_task_id\", sql.types.integertype()),\n", " dt.structfield(\"t_file_id\", sql.types.integertype()),\n", " dt.structfield(\"t_file_name\", sql.types.stringtype())\n", " ])\n", " )\n", " df.write.format(\"delta\").mode('overwrite').save(file_object.silver_path)\n", "\n", " # execute\n", " # [classes:file:rawtosilver] run method test\n", " file_object.raw_to_silver()\n", "\n", "\n", " # evaluate\n", " # validate delta table longer empty\n", " df = dt.deltatable.forpath(spark, file_object.silver_path)\n", " actual_rowcount = df.todf().count()\n", "\n", " self.assertequal(expected_rowcount, actual_rowcount, f'[classes:file:rawtosilver]: expected row count match actuals: {expected_rowcount} versus {actual_rowcount}')\n", "\n", "\n", " def test_rawtosilver_file_function_calls(self):\n", " '''\n", " test logic: validate function calls made method test\n", " - update_file_activity called, turn call [classes:sqlserverconnection] execute_query\n", " - mergefile called\n", " - [classes:checks] data quailty methods check_dataframe_headers() check_dataframe_datatypes() called whenever files moving raw silver\n", " '''\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.target_options = {}\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'part-00000-ded9c119-09f3-4c0a-9415-3536ded9342f-c000.snappy.parquet'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/raw/move_to_silver'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " \n", " # overwrite file_object members test-values \n", " file_object.file_id = -1\n", " file_object.raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " file_object.silver_path = f\"abfss://silver@{storage_account}.dfs.core.windows.net/mocking_table\"\n", "\n", "\n", " ## create empty delta table (with technical fields) upload rawdata into\n", " df = spark.createdataframe(data=[], schema=dt.structtype([\n", " dt.structfield(\"_c0\", sql.types.stringtype()),\n", " dt.structfield(\"first_name\", sql.types.stringtype()),\n", " dt.structfield(\"last_name\", sql.types.stringtype()),\n", " dt.structfield(\"age\", sql.types.stringtype()),\n", " dt.structfield(\"email\", sql.types.stringtype()),\n", " dt.structfield(\"t_load_date_raw\", sql.types.timestamptype()),\n", " dt.structfield(\"t_load_date_silver\", sql.types.timestamptype()),\n", " dt.structfield(\"t_extract_date\", sql.types.timestamptype()),\n", " dt.structfield(\"t_update_date\", sql.types.timestamptype()),\n", " dt.structfield(\"t_insert_date\", sql.types.timestamptype()),\n", " dt.structfield(\"t_plan_id\", sql.types.integertype()),\n", " dt.structfield(\"t_task_id\", sql.types.integertype()),\n", " dt.structfield(\"t_file_id\", sql.types.integertype()),\n", " dt.structfield(\"t_file_name\", sql.types.stringtype())\n", " ])\n", " )\n", " df.write.format(\"delta\").mode('overwrite').save(file_object.silver_path)\n", "\n", " expected_checks_parameters = {'primary_key_columns': ['first_name', 'last_name']}\n", "\n", " # execute\n", " # [classes:file:rawtosilver] run method test, mocking all/most function calls made\n", " patch.object(self.mock_sql_object, 'execute_query') mock_execute_query, \\\n", " patch.object(sparkdataframechecks, 'check_dataframe_headers') mock_headers_checks, \\\n", " patch.object(sparkdataframechecks, 'check_dataframe_datatypes') mock_casting_check, \\\n", " patch.object(sparkdataframechecks, 'check_primary_keys') mock_primary_keys, \\\n", " patch('__main__.mergefile') mock_mergefile: \n", " # [classes:file:rawtosilver] run method test\n", " file_object.raw_to_silver()\n", "\n", "\n", " # evaluate\n", " # validate mocked functions called expected arguments\n", " ## [classes:checks] expect header-checks datatypes-checks always called raw_to_silver stage mandatory\n", " info=create_json_object(path=file_object.silver_path,row_inserted=0,row_updated=0,source_row=0)\n", " mock_headers_checks.assert_called_once()\n", " mock_casting_check.assert_called_once()\n", " mock_primary_keys.assert_called_once()\n", " ## [ingestionfunctions] expect mergefile-function called\n", " mock_mergefile.assert_called_once()\n", " ## [classes:sqlserverconnection] expect execute_query called log file updates\n", " mock_execute_query.assert_called_once_with(f\"exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=silver, @success=true, @info_message = '{info}'\")\n", "\n", " \n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_file_rawtosilver)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 25 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: file\n", "# class test archive_to_silver() file-class defined classes notebook\n", "class test_file_landingtoarchive(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use later testing \n", " self.task_id = -1\n", " self.plan_id = -1\n", " self.task = task(task_name='landig_to_archive', task_id= self.task_id, plan_id=self.task_id, sqlserverconnection = mock(), worker_name=mock())\n", "\n", " self.task.column_datatypes = [\"string\",\"string\", \"string\", \"string\", \"string\"]\n", " self.task.source_column_names = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n", " self.task.sink_column_names = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n", " self.task.dimensions = ['scd2', 'pk','scd2','scd2','scd2',]\n", " self.task.table_name = ['unit_test'] \n", " self.task.header = true\n", " self.task.variables = {\"check_name\": [], \"config_params\": {}}\n", " self.task.separator = ';'\n", " self.task.file_extension = 'csv'\n", " \n", "\n", " # [classes:sqlserverconnection] mock sqlserverconnection: make sure queries executed\n", " self.mock_sql_object = magicmock(spec=sqlserverconnection)\n", " self.sql_server_object = self.mock_sql_object\n", "\n", " # [classes:file] make sure log_file execute throw error -&gt; cannot make sqlserverconnection\n", " self.log_file_patcher = patch.object(file, 'log_file')\n", " self.mock_log_file = self.log_file_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " self.log_file_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls archive_to_silver method\n", "\n", " def test_landingtoarchive_file_success(self):\n", " # preprocess\n", " # [classes:task] create task-object task-class\n", " task_object = self.task\n", " task_object.table_name = 'unittest_silver'\n", " task_object.skip_first_lines = 1\n", " task_object.target_options = {}\n", "\n", " # [classes:file] create file-object file-class\n", " file_name = 'mock_data_move.csv'\n", " storage_account = f'{env_code}dapstdala1'\n", " file_path = 'classes/filemovement/landing/move_to_archive/20240802_112046'\n", " container_name = 'unittest'\n", " landing_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n", " timestamp = 'timestamp'\n", "\n", " file_object = file(self.task, self.sql_server_object, landing_path, file_name, timestamp, storage_account)\n", " # overwrite file_object members test-values \n", " file_object.file_id = -1\n", " \n", " # execute\n", " # [classes:file:landingtoarchive] run method test\n", " file_object.landing_to_archive()\n", "\n", " # evaluate\n", " # validate file moved archive container\n", " actual_archive_files, actual_archive_folders = list_directory_content(file_object.archive_path, list(), list())\n", " actual_archive_file_names = [file.name file actual_archive_files]\n", " self.asserttrue(len(actual_archive_file_names) == 1, f\"[classes:file:landingtoarchive] files found archive {actual_archive_file_names}, equal 1 files expected.\")\n", " self.assertequal(actual_archive_file_names[0], file_name)\n", " \n", " # validate folders/file deleted landing\n", " actual_landing_files, actual_landing_folders= list_directory_content(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/classes/filemovement/landing/move_to_archive', list(), list())\n", " self.asserttrue(len(actual_landing_files) == 0, f\"[classes:file:landingtoarchive] file still landing, expected.\")\n", " self.asserttrue(len(actual_landing_folders) == 0, f\"[classes:file:landingtoarchive] folder still exist landing, expected.\")\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_file_landingtoarchive)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 26 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# # run tests see errors\n", "# findings = unittest.main(argv=[''], verbosity=2,exit=false, warnings='ignore')\n", "\n", "# len(findings.result.failures) != 0 len(findings.result.errors) != 0:\n", "# raise exception(\"error tests, something went wrong!\")" ], "execution_count": 27 } ] } }</file><file name="src\synapse\studio\notebook\Test_CleanWorkspace.json">{ "name": "test_cleanworkspace", "properties": { "folder": { "name": "functions/test_functions" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "75a4ba4e-5e53-4196-a432-f4d6b5db91af" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test cleanworkspace\r\n", "this script test functions coming cleanworkspace notebook." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary libraries" ] }, { "cell_type": "code", "source": [ "# import unittest unittest.mock modules\r\n", "import unittest\r\n", "from unittest.mock import patch, magicmock, call\r\n", "import os\r\n", "\r\n", "# import pyspark specific testing package\r\n", "# import pyspark.testing.utils # pyspark version 3.5 contains useful functions (assertdataframeequal, assertschemaequal...) exist current 3.3.1 pyspark version\r\n", "\r\n", "# import deltatable module read blob-folder delta table object\r\n", "from delta.tables import deltatable" ], "execution_count": 1 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## parameters" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\r\n", "environment_code = 'dev'" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# set environment parameter 'env_code'\r\n", "# first, check value spark configuration called 'environment_code'. not, use parameter defined above.\r\n", "env_code = spark.conf.get('spark.environment_code', environment_code)\r\n", "\r\n", "\r\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\r\n", "# according ci/cd, notebook never even deployed there, case ;)\r\n", "if env_code ['dev', 'int']:\r\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 3 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# define global parameters use entire notebook\r\n", "container_name = 'unittest'\r\n", "storage_account = f'{env_code}dapstdala1'" ], "execution_count": 4 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" methods need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 5 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test classes" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### function: clean_containers" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: cleancontainers\r\n", "# class run set methods clean_containers function \r\n", "class test_function_cleancontainers(unittest.testcase):\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " # patch python functions: clean_containers function needs run isolation. \r\n", " # function-calls need mocked avoid getting errors things need tested.\r\n", " def setup(self):\r\n", "\r\n", " ## method coming library pyspark.sql\r\n", " self.mock_mssparkutils_fs_rm_patcher = patch.object(mssparkutils.fs, 'rm')\r\n", " self.mock_mssparkutils_fs_rm = self.mock_mssparkutils_fs_rm_patcher.start()\r\n", "\r\n", " # tests executed, patched functions stopped\r\n", " def teardown(self):\r\n", " self.mock_mssparkutils_fs_rm_patcher.stop()\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls clean_containers function\r\n", "\r\n", " # success: test patched function called given arguments acceptable\r\n", " # clean 1 container\r\n", " def test_clean_containers_one_container(self):\r\n", " clean_containers(env_code='dev', containers=['logs'])\r\n", " self.mock_mssparkutils_fs_rm.assert_called_once()\r\n", "\r\n", " # success: test patched function called given arguments acceptable\r\n", " # clean multiple containers using 1 function call\r\n", " def test_clean_containers_multiple_containers(self):\r\n", " # preprocess\r\n", " # set variable 'expected_calls': \r\n", " # following calls expected executed calling function clean_containers\r\n", " # concept: clean_containers function expected called twice using parameters\r\n", " expected_calls = [ call(dir='abfss://logs@devdapstdala1.dfs.core.windows.net/', recurse=true), \r\n", " call(dir='abfss://silver@devdapstdala1.dfs.core.windows.net/', recurse=true)\r\n", " ]\r\n", "\r\n", "\r\n", " # execute\r\n", " clean_containers(env_code='dev', containers=['logs', 'silver'])\r\n", "\r\n", " # evaluate\r\n", " # make sure calls made\r\n", " self.mock_mssparkutils_fs_rm.assert_has_calls(expected_calls, any_order=true)\r\n", "\r\n", "\r\n", " # failure: container-argument datatype 'string', expect error thrown\r\n", " def test_clean_containers_invalid_container_1(self):\r\n", " # preprocess\r\n", " # error expected thrown\r\n", " expected_error= \"list containers (logs) contains one unexpected containers: ['l', 'o', 'g', 's']\"\r\n", "\r\n", " # execute\r\n", " self.assertraises(valueerror) error:\r\n", " clean_containers(env_code='dev', containers='logs')\r\n", "\r\n", " # evaluate \r\n", " # check thrown error matches expeced error\r\n", " self.assertequal(str(error.exception), expected_error)\r\n", "\r\n", " # failure: given list containers contains unexpected container-name, expect error thrown\r\n", " def test_clean_containers_invalid_container_2(self):\r\n", " # preprocess\r\n", " # error expected thrown\r\n", " expected_error= \"list containers (['logs', 'unittest']) contains one unexpected containers: ['unittest']\"\r\n", "\r\n", " # execute\r\n", " self.assertraises(valueerror) error:\r\n", " clean_containers(env_code='dev', containers=['logs', 'unittest'])\r\n", "\r\n", " # evaluate\r\n", " # check thrown error matches expeced error\r\n", " self.assertequal(str(error.exception), expected_error)\r\n", "\r\n", " # failure: given environment argument 'env_code' 'dev' 'int', expect error thrown\r\n", " def test_invalid_environment(self):\r\n", " # preprocess\r\n", " # error expected thrown\r\n", " expected_error= \"not allowed clean containers environment tst. dev int cleanups allowed\"\r\n", "\r\n", " # execute\r\n", " self.assertraises(valueerror) error:\r\n", " clean_containers(env_code='tst', containers=['logs'])\r\n", "\r\n", " # evaluate\r\n", " # check thrown error matches expeced error\r\n", " self.assertequal(str(error.exception), expected_error)\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_cleancontainers)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error clean containers test, something went wrong!\")" ], "execution_count": 6 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### function: clean_delta_table" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: cleandeltatable\r\n", "# class run set methods clean_delta_table function \r\n", "class test_function_cleandeltatable(unittest.testcase):\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " # patch python functions: clean_containers function needs run isolation. \r\n", " # function-calls need mocked avoid getting errors things need tested.\r\n", " def setup(self):\r\n", "\r\n", " ## method coming library pyspark.sql\r\n", " self.mock_mssparkutils_fs_rm_patcher = patch.object(mssparkutils.fs, 'rm')\r\n", " self.mock_mssparkutils_fs_rm = self.mock_mssparkutils_fs_rm_patcher.start()\r\n", "\r\n", " # patch print-function\r\n", " # purpose: evaluate calls made print-function python evaluate printed message\r\n", " self.mock_print_patcher = patch('builtins.print')\r\n", " self.mock_print = self.mock_print_patcher.start()\r\n", "\r\n", "\r\n", " # tests executed, patched functions stopped\r\n", " def teardown(self):\r\n", " self.mock_mssparkutils_fs_rm_patcher.stop()\r\n", " self.mock_print_patcher.stop()\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls clean_delta_table function\r\n", "\r\n", " # success: test that, set valid arguments, function calls mssparkutils.fs.rm correct set arguments\r\n", " def test_clean_delta_table_success(self):\r\n", " # preprocess\r\n", " # following call expected made \r\n", " expected_calls = [ call(dir=f'abfss://silver@devdapstdala1.dfs.core.windows.net/test', recurse=true) ]\r\n", "\r\n", " # execute\r\n", " # execute clean_delta_table function valid set arguments\r\n", " clean_delta_table(env_code='dev', delta_lake='silver', table_name='test')\r\n", "\r\n", " # evaluate\r\n", " # validate mssparkuitls.fs.rm function called expected set arguments\r\n", " self.mock_mssparkutils_fs_rm.assert_has_calls(expected_calls)\r\n", "\r\n", "\r\n", " # failure: argument 'delta_lake' take value 'silver' 'unittest'\r\n", " def test_clean_delta_table_invalid_deltalake(self):\r\n", " # preprocess\r\n", " # expect error thrown\r\n", " expected_error= 'only allowed clean delta lake \\'silver\\', landing'\r\n", "\r\n", " # execute\r\n", " # run function invalid value delta_lake argument\r\n", " self.assertraises(valueerror) error:\r\n", " clean_delta_table(env_code='dev', delta_lake='landing', table_name='test_table')\r\n", "\r\n", " # evaluate\r\n", " # validate expected error thrown\r\n", " self.assertequal(str(error.exception), expected_error)\r\n", "\r\n", "\r\n", " def test_clean_delta_table_invalid_environment(self):\r\n", " # preprocess\r\n", " # expect error thrown\r\n", " expected_error= \"not allowed clean containers environment tst. dev int cleanups allowed\"\r\n", "\r\n", " # execute\r\n", " # run function invalid value env_code argument\r\n", " self.assertraises(valueerror) error:\r\n", " clean_delta_table(env_code='tst', delta_lake='silver', table_name='test_table')\r\n", "\r\n", " # evaluate\r\n", " # validate expected error thrown\r\n", " self.assertequal(str(error.exception), expected_error)\r\n", "\r\n", "\r\n", " def test_clean_delta_table_patherror(self):\r\n", " # preprocess\r\n", " # mock output mssparkutils.fs.rm function\r\n", " # returned output filenotfounderror text-output \"pathnotfoundexception\"\r\n", " # purpose: file exists deleted, clean_delta_table function return print-statement 'delta table exist' instead throwing error\r\n", " self.mock_mssparkutils_fs_rm.side_effect = filenotfounderror(\"pathnotfoundexception\")\r\n", "\r\n", " # execute\r\n", " clean_delta_table(env_code='dev', delta_lake='silver', table_name='test_table')\r\n", " \r\n", " # evaluate\r\n", " # validate expected print statement returned\r\n", " self.mock_print.assert_called_with(\"delta table exist\")\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_cleandeltatable)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error clean delta table test, something went wrong!\")" ], "execution_count": 7 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### function: clean_table" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: cleantable\r\n", "# class run set methods clean_delta_table function \r\n", "class test_function_cleantable(unittest.testcase):\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " # patch python functions: clean_table function needs run isolation. \r\n", " # function-calls need mocked avoid getting errors things need tested.\r\n", " def setup(self):\r\n", "\r\n", " ## method coming library pyspark.sql\r\n", " self.mock_spark_sql_patcher = patch.object(spark, 'sql')\r\n", " self.mock_spark_sql = self.mock_spark_sql_patcher.start()\r\n", "\r\n", " def teardown(self):\r\n", " self.mock_spark_sql_patcher.stop()\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests\r\n", " # define set functions test successful unsuccesful calls clean_table function\r\n", "\r\n", " # success: call function spark.sql table 'mock' \r\n", " def test_clean_table_success(self):\r\n", " clean_table(\"mock\")\r\n", " self.mock_spark_sql.assert_called_once_with(\"drop table exists silver.mock\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_cleantable)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error clean table test, something went wrong!\")" ], "execution_count": 8 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### function: clean_folder" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_function_cleanfolder(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " def setup(self):\r\n", " return\r\n", " def teardown(self):\r\n", " return\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests\r\n", " # define set functions test successful unsuccesful calls clean_folder function\r\n", "\r\n", " # success:the folder deleted\r\n", " def test_cleanfolder_success_delete_empty_folder(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path_folder_empty=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/cleanworkspace/folder_empty'\r\n", " mssparkutils.fs.rm(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/cleanworkspace/folder_empty/file_to_be_deleted')\r\n", " # execute\r\n", " # run method test\r\n", " path=clean_folder(path_folder_empty)\r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " self.asserttrue(not os.path.exists(path_folder_empty), f\"[classes:file:landingtoarchive] folder still exist landing, expected.\")\r\n", " #success:the folder deleted\r\n", " def test_cleanfolder_failed_delete_not_empty_folder(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path_folder_not_empty=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/cleanworkspace/folder_not_empty'\r\n", " mock_mssparkutils_fs = magicmock()\r\n", " mock_mssparkutils_fs.rm.side_effect = exception(\"the recursive query parameter value must true delete non-empty directory.\")\r\n", " # execute\r\n", " # run method test\r\n", " patch.dict('sys.modules', {'mssparkutils': magicmock(fs=mock_mssparkutils_fs)}):\r\n", " self.assertraises(exception) error:\r\n", " clean_folder(path_folder_not_empty)\r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " self.assertequal(str(error.exception), f\"the path could deleted {path_folder_not_empty}\")\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_cleanfolder)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error clean folder test, something went wrong!\")" ], "execution_count": 10 } ] } }</file><file name="src\synapse\studio\notebook\Test_Dataframes.json">{ "name": "test_dataframes", "properties": { "description": "this contains tests notebook \"dataframes\" inside modules folder", "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "c04fe967-3236-4b38-9af2-984f809c687d" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "code", "source": [ "import unittest\r\n", "from unittest.mock import patch, magicmock, mock, any\r\n", "\r\n", "from types import simplenamespace # used mimic json-object returned object\r\n", "\r\n", "import json\r\n", "import datetime\r\n", "import delta.tables dt\r\n", "from delta.tables import deltaoptimizebuilder, deltatable" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\r\n", "environment_code = ''" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\r\n", "\r\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\r\n", "# according ci/cd, notebook never even deployed there, case ;)\r\n", "if env_code ['dev', 'int']:\r\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")\r\n", "" ], "execution_count": 3 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/dataframes" ], "execution_count": 4 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class testcalculatedfieldheaders(unittest.testcase):\r\n", " # dev-note: tests currently pretty basic (input checks, ...), expand tests that\r\n", " def setup(self):\r\n", " self.mock_filter_directory_content_patcher = patch(\"__main__.filter_directory_content\")\r\n", " self.mock_filter_directory_content = self.mock_filter_directory_content_patcher.start()\r\n", "\r\n", " def teardown(self):\r\n", " self.mock_filter_directory_content_patcher.stop()\r\n", "\r\n", " @patch.object(calculatedfieldheaders, \"object_checks\")\r\n", " def test_init_pass(self, mock_objects_checks):\r\n", " # test: mandatory params set correctly, optional parameters (= end_index) set none, validation function called\r\n", "\r\n", " all_columns = ['mock_col1', 'mock_col2']\r\n", " path = f\"abfss://landing@{env_code}dapstdala1.dfs.core.windows.net/mock\"\r\n", " df_preproc_object = calculatedfieldheaders(path, \"mock\", \"mock/test\",\"false\", \"\\t\", \"csv\", all_columns, 0)\r\n", "\r\n", " self.assertequal(f\"abfss://landing@{env_code}dapstdala1.dfs.core.windows.net/mock\", df_preproc_object.path)\r\n", " self.assertequal(f\"abfss://landing@{env_code}dapstdala1.dfs.core.windows.net/mock/test\", df_preproc_object.preprocess_path)\r\n", " self.assertequal(\"false\", df_preproc_object.header)\r\n", " self.assertequal(\"\\t\", df_preproc_object.column_delimiter)\r\n", " self.assertequal(\"csv\", df_preproc_object.file_extension)\r\n", " self.assertequal(all_columns, df_preproc_object.columns)\r\n", " self.assertequal(0, df_preproc_object.start_index)\r\n", " self.assertequal(none, df_preproc_object.end_index)\r\n", "\r\n", " mock_objects_checks.assert_called_once()\r\n", "\r\n", " def test_init_validation_header_error(self):\r\n", " # test: header 'false'\r\n", " all_columns = ['mock_col1', 'mock_col2']\r\n", " \r\n", " self.assertraisesregex(exception, \"^calculatedfieldheaders.*[dd]ataframe.*header.*expected.*false.*\"):\r\n", " df_preproc_object = calculatedfieldheaders(\"test_path.csv\", \"mock\", \"mock/test\", \"true\", mock(), mock(), all_columns, 0)\r\n", "\r\n", " def test_init_validation_end_index_pass(self):\r\n", " # test: header set properly, end_index provided, set len(columns) - 1 -&gt; (last index list)\r\n", " all_columns = ['mock_col1', 'mock_col2']\r\n", " df_preproc_object = calculatedfieldheaders(mock(), mock(), mock(), \"false\", mock(), mock(), all_columns, 0)\r\n", "\r\n", " self.assertequal(1, df_preproc_object.end_index)\r\n", "\r\n", " def test_init_validation_end_index_error1(self):\r\n", " # test: header set properly, end_index &gt; (len(self.columns) - 1), expect error\r\n", " all_columns = ['mock_col1', 'mock_col2']\r\n", "\r\n", " self.assertraisesregex(indexerror, \"^calculatedfieldheaders.*end.*index.*2.*cannot.*be.*higher.*than.*the.*last.*index.*1.*\"):\r\n", " df_preproc_object = calculatedfieldheaders(mock(), \"mock\", \"mock/test\", \"false\", mock(), mock(), all_columns, 0, 2)\r\n", "\r\n", " def test_init_end_index_error2(self):\r\n", " # test: header set properly, end_index &lt; self.start_index, expect error\r\n", " all_columns = ['mock_col1', 'mock_col2']\r\n", "\r\n", " self.assertraisesregex(exception, \"^calculatedfieldheaders.*end.*index.*-1.*cannot.*be.*smaller.*than.*the.*starting.*index.*0.*\"):\r\n", " df_preproc_object = calculatedfieldheaders(mock(), \"mock\", \"mock/test\", \"false\", mock(), mock(), all_columns, 0, -1)\r\n", "\r\n", " def test_init_start_index_error(self):\r\n", " # test: header set properly, self.start_index &lt; 0, expect error\r\n", " all_columns = ['mock_col1', 'mock_col2']\r\n", "\r\n", " self.assertraisesregex(exception, \"^calculatedfieldheaders.*start.*index.*-1.*cannot.*be.*lower.*than.*0.*\"):\r\n", " df_preproc_object = calculatedfieldheaders(mock(), \"mock\", \"mock/test\", \"false\", mock(), mock(), all_columns, -1, 1)\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(testcalculatedfieldheaders)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 5 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: initialisationdeltatable\r\n", "# class test __init__ function deltatable-class defined deploydeltatable_v3 notebook\r\n", "class testdataframeclass(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " def setup(self):\r\n", " # define dataframe used testing\r\n", " self.test_dataframe = spark.createdataframe(\r\n", " data=[(1, \"dev\"), (2, \"int\"),(3, \"tst\")],\r\n", " schema=[\"id\", \"label\"]\r\n", " )\r\n", "\r\n", "\r\n", " def teardown(self):\r\n", " # functions teardown. keep method consistency potential future use\r\n", " return\r\n", "\r\n", "\r\n", " def test_dataframeclass_init(self):\r\n", " # run\r\n", " # create class-object dataframeclass \r\n", " dataframe_object = dataframeclass(self.test_dataframe)\r\n", "\r\n", " # evaluate\r\n", " # validate object dataframe-argument initialised\r\n", " self.assertisnotnone(dataframe_object.dataframe)\r\n", " \r\n", " def test_add_logging_columns(self):\r\n", " # prepare\r\n", " # create class-object dataframeclass\r\n", " dataframe_object = dataframeclass(self.test_dataframe)\r\n", " # define list expected columns \r\n", " expected_columns = ['id', 'label', 't_load_date_raw', 't_load_date_silver', 't_plan_id', 't_task_id', 't_file_id', 't_file_name']\r\n", "\r\n", " # run\r\n", " # execute method add_logging_columns() \r\n", " returned_dataframe = dataframe_object.add_logging_columns(1, 1, 1, 'test')\r\n", "\r\n", " # evaluate\r\n", " # validate actual columns match expected columns\r\n", " self.assertequal(expected_columns, returned_dataframe.columns, f'the actual columns ({returned_dataframe.columns}) expected columns ({expected_columns})')\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(testdataframeclass)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\") " ], "execution_count": 6 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deltatableoptimizeclass_initialisation\r\n", "# class test __init__ function deltatableoptimizeclass-class defined dataframe notebook\r\n", "class test_deltatableoptimizeclass_initialisation(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " def setup(self):\r\n", " # create empty dataframe used test function\r\n", " self.table_name = 'initialize_deltatable_test'\r\n", " self.env_code = env_code\r\n", " self.delta_table_path = f'abfss://silver@{env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\r\n", " dataframe = spark.createdataframe(\r\n", " data=[],\r\n", " schema=\"test int, schema string\"\r\n", " )\r\n", "\r\n", " dataframe.write.mode('overwrite').format('delta').save(self.delta_table_path)\r\n", " return\r\n", "\r\n", "\r\n", " def teardown(self):\r\n", " # functions teardown. keep method consistency potential future use\r\n", " return\r\n", "\r\n", " def test_initization_deltatableoptimizeclass_success(self):\r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass] run method test\r\n", " deltatable_object = deltatableoptimizeclass(table_name=self.table_name, env_code=self.env_code)\r\n", "\r\n", " # evaluate\r\n", " # validate object dataframe-argument initialised\r\n", " self.assertisnotnone(deltatable_object.delta_table_path)\r\n", " self.assertequal(self.delta_table_path, deltatable_object.delta_table_path)\r\n", " self.assertisinstance(deltatable_object.delta_table, dt.deltatable)\r\n", "\r\n", " def test_initialization_deltatableoptimizeclass_function_calls(self):\r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass] run method test\r\n", " patch.object(dt.deltatable, 'forpath') mock_forpath:\r\n", " deltatable_object = deltatableoptimizeclass(table_name=self.table_name, env_code=self.env_code)\r\n", "\r\n", " # evaluate\r\n", " # validate expected functions called expected arguments\r\n", " mock_forpath.assert_called_once()\r\n", " mock_forpath.assert_called_once_with(any, self.delta_table_path)\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deltatableoptimizeclass_initialisation)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\") " ], "execution_count": 7 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deltatableoptimizeclass_initialisation\r\n", "# class test optimize_table_storage function (and related functions) deltatableoptimizeclass defined dataframes notebook\r\n", "class test_deltatableoptimizeclass_optimizetablestorage(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " def setup(self):\r\n", " # create empty dataframe used test function\r\n", " self.table_name = 'optimize_deltatable_test'\r\n", " self.env_code = env_code\r\n", " self.delta_table_path = f'abfss://silver@{env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\r\n", " dataframe = spark.createdataframe(\r\n", " data=[],\r\n", " schema=\"test int, schema string\"\r\n", " )\r\n", "\r\n", " dataframe.write.mode('overwrite').format('delta').save(self.delta_table_path)\r\n", "\r\n", " # [dataframes:deltatableoptimizeclass] create instance deltatableoptimizeclass use testing class-methods\r\n", " self.delta_table_object = deltatableoptimizeclass(self.table_name, self.env_code)\r\n", " return\r\n", "\r\n", "\r\n", " def teardown(self):\r\n", " # functions teardown. keep method consistency potential future use\r\n", " return\r\n", "\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls optimize_table_storage() method\r\n", "\r\n", " def test_optimizetablestorage_deltatableoptimizeclass_success(self):\r\n", " '''\r\n", " test logic:\r\n", " run 'orchestrator' method optimize_table_storage without patched objects\r\n", " expect object created class deltaoptimizebuilder\r\n", " '''\r\n", "\r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass] execute method test\r\n", " self.delta_table_object.optimize_table_storage()\r\n", "\r\n", " # evaluate\r\n", " # [dataframes:deltatableoptimizeclass] validate creation deltaoptimizebuilder-class instance\r\n", " self.assertisinstance(self.delta_table_object.delta_table_optimized, deltaoptimizebuilder)\r\n", "\r\n", "\r\n", " def test_optimizetablestorage_deltatableoptimizeclass_function_calls(self):\r\n", " '''\r\n", " test logic:\r\n", " run 'orchestrator' method optimize_table_storage patching deltaoptimizebuilder-class methods\r\n", " expect class methods (not) called based given arguments\r\n", " '''\r\n", "\r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass]: execute method test\r\n", " patch.object(delta.tables.deltaoptimizebuilder, 'executecompaction') mock_compaction, \\\r\n", " patch.object(delta.tables.deltaoptimizebuilder, 'executezorderby') mock_zorderby:\r\n", " self.delta_table_object.optimize_table_storage()\r\n", " \r\n", " # evaluate\r\n", " # [dataframes:deltatableoptimizeclass] validate underlying functions called expected\r\n", " mock_compaction.assert_called_once()\r\n", " mock_zorderby.assert_not_called()\r\n", "\r\n", "\r\n", " def test_optimizetablestorage_deltatableoptimizeclass_function_calls(self):\r\n", " '''\r\n", " test logic:\r\n", " run 'orchestrator' method optimize_table_storage patching deltaoptimizebuilder-class methods\r\n", " expect class methods (not) called based given arguments\r\n", " '''\r\n", "\r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass] execute method test\r\n", " patch.object(delta.tables.deltaoptimizebuilder, 'executecompaction') mock_compaction, \\\r\n", " patch.object(delta.tables.deltaoptimizebuilder, 'executezorderby') mock_zorderby:\r\n", " self.delta_table_object.optimize_table_storage(['test'])\r\n", " \r\n", " # evaluate\r\n", " # [dataframes:deltatableoptimizeclass] validate underlying functions called expected\r\n", " mock_compaction.assert_called_once()\r\n", " mock_zorderby.assert_called_once_with(z_order_columns=['test'])\r\n", "\r\n", " def test_compactdeltatable_deltatableoptimizeclass_nonexist_deltaoptimizebuilder(self):\r\n", " '''\r\n", " test logic:\r\n", " run method compact_deltatable(), deltatableoptimizeclass-member 'delta_table_optimized' set none\r\n", " expect error thrown: compaction cannot executed deltaoptimizebuilder-instance exist\r\n", " '''\r\n", "\r\n", " # preprocess\r\n", " # set delta_table_optimized class-member none\r\n", " self.delta_table_object.delta_table_optimized = none\r\n", " # define error expected thrown \r\n", " expected_error = \"[dataframes:deltatableoptimizeclass] delta_table_optimized member class deltaoptimizebuilder compaction\"\r\n", " \r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass] execute method test\r\n", " self.assertraises(typeerror) error:\r\n", " self.delta_table_object.compact_deltatable()\r\n", " \r\n", " # evaluate\r\n", " # validate expected error matches actual\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:deltatableoptimizeclass] expected error match actual: {expected_error} versus {actual_error}\")\r\n", "\r\n", " def test_zorderdeltatable_deltatableoptimizeclass_nonexist_deltaoptimizebuilder(self):\r\n", " '''\r\n", " test logic:\r\n", " run method z_order_deltatable(), deltatableoptimizeclass-member 'delta_table_optimized' set none\r\n", " expect error thrown: z-orderby cannot executed deltaoptimizebuilder-instance exist\r\n", " '''\r\n", "\r\n", " # preprocess\r\n", " # set delta_table_optimized class-member none\r\n", " self.delta_table_object.delta_table_optimized = none\r\n", " # define error expected thrown \r\n", " expected_error = \"[dataframes:deltatableoptimizeclass] delta_table_optimized member class deltaoptimizebuilder z-order\"\r\n", " \r\n", " # execute\r\n", " # [dataframes:deltatableoptimizeclass] execute method test\r\n", " self.assertraises(typeerror) error:\r\n", " self.delta_table_object.z_order_deltatable(z_order_columns=list())\r\n", " \r\n", " # evaluate\r\n", " # validate expected error matches actual\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:deltatableoptimizeclass] expected error match actual: {expected_error} versus {actual_error}\")\r\n", "\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deltatableoptimizeclass_optimizetablestorage)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\") " ], "execution_count": 37 } ] } }</file><file name="src\synapse\studio\notebook\Test_Dataframes_v2.json">{ "name": "test_dataframes_v2", "properties": { "folder": { "name": "modules/test_modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "45c805c9-4ff0-45b8-9ad1-c41617e95f25" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test dataframes_v2\n", "this notebook test classes defined notebook dataframes_v2.\n", "the structure notebook looks follows:\n", " \n", "- section: markdown cells used distinguish different classes tested.\n", "- code cells: code cell tests one specific method class tested section.\n", "\n", "**notes**\n", "1. end cell, piece commented code found. code uncommented tests specific cell need run. useful debugging. make sure that, completing pull request, pieces code commented again, tests otherwise executed multiple times test-job ci pipeline.\n", "2. approach taken always define setup teardown method beginning test case. done keep code consistent different classes. methods used entire test case use set variables function/method needs mocked/patched entire test case.\n", "3. test method uses similar structure, preprocess, execute, evaluate step. keep tests readable consistent." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary methods unittest library" ] }, { "cell_type": "code", "source": [ "# import unittest-library \n", "import unittest\n", "# import specific functions library defined mock-method\n", "from unittest.mock import patch, magicmock, mock, any, call\n", "\n", "# import dataframe class\n", "# reason: dataframes_v2 notebook occasions create class-instances dataframe. assertion executed validate whether returned objects actualy class.\n", "from pyspark.sql import dataframe\n", "from pyspark.sql.functions import lit\n", "\n", "# import functions used testing\n", "from pyspark.sql.types import structfield, structtype\n", "import delta.tables dt\n", "import json\n", "import ast" ], "execution_count": 2 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## initialize environment_code parameter\n", "in almost notebooks, reference parameter found. ci/cd pipelines overwrite specific environment-references defined notebooks. since notebooks deployed several environments, use environment_code parameter chosen. \n", "the environment_code used spark session environment_code-argument defined. argument linked core_configuration, found manage -&gt; apache spark configurations\n", "the use parameters arguments that, deployment, 1 place references need overwritten." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\n", "environment_code = ''" ], "execution_count": 3 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\n", "\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")\n", "" ], "execution_count": 4 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# define global parameters use entire notebook\n", "container_name = 'unittest'\n", "storage_account = f'{env_code}dapstdala1'" ], "execution_count": 5 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" dataframes_v2 notebook" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/dataframes_v2" ], "execution_count": 6 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 7 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": 8 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test load_dataframe() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_loaddataframe(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls load_dataframe() method\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------ \n", " # --------------------------------------------- test successfull calls --------------------------------------------- \n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " # success: call load_dataframe csv-file_kind \n", " def test_loaddataframe_sparkdataframeclass_else_case(self):\n", " # preprocess\n", " ## define arguments passed load_dataframe() method\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/load_dataframe/load_csv_test/csv_test.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " skip_lines = none\n", " \n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " ## define set expected values\n", " ### 1. expect total number rows dataframe 3\n", " expected_row_count = 3\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call load_dataframe() method valid set arguments\n", " class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n", "\n", " # evaluate\n", " ## 1. dataframe class-member instance class pyspark.sql.dataframe\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe initialized correctly dataframe\")\n", " ## 2. dataframe contain total 3 rows\n", " actual_row_count = class_object.dataframe.count()\n", " self.assertequal(expected_row_count, actual_row_count, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe contain expected rowcount {expected_row_count} initialization\")\n", "\n", " # success: call load_dataframe json-file_kind \n", " def test_loaddataframe_sparkdataframeclass_json(self):\n", " # preprocess\n", " ## define arguments passed load_dataframe() method\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/load_dataframe/load_json_test/json_test_multiobject.json'\n", " header = true\n", " separator = ','\n", " file_kind = 'json'\n", " skip_lines = none\n", "\n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " ## define set expected values\n", " ### 1. expect total number rows dataframe 3\n", " expected_row_count = 3\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call load_dataframe() method valid set arguments\n", " class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n", "\n", " # evaluate\n", " ## 1. dataframe class-member instance class pyspark.sql.dataframe\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe initialized correctly dataframe\")\n", " ## 2. dataframe contain total 3 rows\n", " actual_row_count = class_object.dataframe.count()\n", " self.assertequal(expected_row_count, actual_row_count, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe contain expected rowcount '{expected_row_count}' initialization\")\n", "\n", " # success: call load_dataframe skip_lines\n", " def test_loaddataframe_sparkdataframeclass_skipfirstlines(self):\n", " # preprocess\n", " ## define arguments passed load_dataframe() method\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/load_dataframe/load_skipfirstlines_test/skipfirstlines_test.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " skip_lines = 4\n", "\n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " ## define set expected values\n", " ### 1. expect total number rows dataframe 3\n", " expected_row_count = 3\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call load_dataframe() method valid set arguments\n", " class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n", "\n", " # evaluate\n", " ## 1. dataframe class-member instance class pyspark.sql.dataframe\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe initialized correctly dataframe\")\n", " \n", " ## 2. dataframe contain total 3 rows\n", " actual_row_count = class_object.dataframe.count()\n", " self.assertequal(expected_row_count, actual_row_count, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe contain expected rowcount '{expected_row_count}' initialization\")\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------ \n", " # ----------------------------------------------- test function calls ---------------------------------------------- \n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " # success: call load_dataframe skip_lines \n", " def test_loaddataframe_sparkdataframeclass_skipfirstlines_functioncall_called(self):\n", " # preprocess\n", " ## define arguments passed load_dataframe() method\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/load_dataframe/load_skipfirstlines_test/skipfirstlines_test.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " skip_lines = 3\n", "\n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call load_dataframe() method valid set arguments\n", " patch(\"__main__.skip_first_lines\") mock_skip_first_lines:\n", " class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n", "\n", " # evaluate\n", " ## 1. validate skip_first_lines function called\n", " mock_skip_first_lines.assert_called_once()\n", "\n", " # success: call load_dataframe without skip_first_lines \n", " def test_loaddataframe_sparkdataframeclass_skipfirstlines_functioncall_notcalled(self):\n", " # preprocess\n", " ## define arguments passed load_dataframe() method\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/load_dataframe/load_csv_test/csv_test.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'json'\n", " skip_lines = none\n", "\n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call load_dataframe() method valid set arguments\n", " patch(\"__main__.skip_first_lines\") mock_skip_first_lines:\n", " class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n", "\n", " # evaluate\n", " ## 1. validate skip_first_lines function called\n", " mock_skip_first_lines.assert_not_called()\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_loaddataframe)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test create_dataframe() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_createdataframe(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown testcass class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls create_dataframe() method\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_createdataframe_sparkdataframeclass_default_datatypes(self):\n", " # preprocess\n", " ## define arguments passed sparkdataframeclass object\n", " column_dict = [\n", " {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"scd2\", \"data_type\": \"array\"},\n", " {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"binary\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"boolean\"},\n", " {\"column_sequence\": 4, \"column_name\": \"column_4\", \"dimension\": \"scd2\", \"data_type\": \"date\"},\n", " {\"column_sequence\": 5, \"column_name\": \"column_5\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_sequence\": 6, \"column_name\": \"column_6\", \"dimension\": \"scd2\", \"data_type\": \"varchar(max)\"},\n", " {\"column_sequence\": 7, \"column_name\": \"column_7\", \"dimension\": \"scd2\", \"data_type\": \"timestamp\"},\n", " {\"column_sequence\": 8, \"column_name\": \"column_8\", \"dimension\": \"scd2\", \"data_type\": \"decimal\"},\n", " {\"column_sequence\": 9, \"column_name\": \"column_9\", \"dimension\": \"scd2\", \"data_type\": \"float\"},\n", " {\"column_sequence\": 10, \"column_name\": \"column_10\", \"dimension\": \"scd2\", \"data_type\": \"byte\"},\n", " {\"column_sequence\": 11, \"column_name\": \"column_11\", \"dimension\": \"scd2\", \"data_type\": \"integer\"},\n", " {\"column_sequence\": 12, \"column_name\": \"column_12\", \"dimension\": \"scd2\", \"data_type\": \"int\"},\n", " {\"column_sequence\": 13, \"column_name\": \"column_13\", \"dimension\": \"scd2\", \"data_type\": \"long_integer\"}\n", " ]\n", " table_description = \"table test basic functionalities create table function\"\n", " \n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " ## define set expected values:\n", " ### 1. expected column_names final dataframe creation -&gt; original fields technical fields \n", " expected_column_names = [\n", " \"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\", \"column_6\", \"column_7\", \n", " \"column_8\", \"column_9\", \"column_10\", \"column_11\", \"column_12\", \"column_13\",\n", " \"t_load_date_raw\", \"t_load_date_silver\", \"t_extract_date\", \"t_update_date\", \"t_insert_date\", \"t_file_name\", \"t_plan_id\", \"t_task_id\", \"t_file_id\"\n", " ]\n", " ### 2. expected data_types column dataframe -&gt; original fields technical fields\n", " expected_datatypes = [\n", " arraytype(stringtype()), binarytype(), booleantype(), datetype(), stringtype(), stringtype(), \n", " timestamptype(), decimaltype(38, 4), floattype(), bytetype(), integertype(), integertype(), longtype(),\n", " timestamptype(), timestamptype(), timestamptype(), timestamptype(), timestamptype(), stringtype(), integertype(), integertype(), integertype()\n", " ]\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call create_dataframe() method sparkdataframeclass valid set arguments\n", " class_object.create_dataframe(column_dict, table_description)\n", "\n", " # evaluate\n", " ## 1. dataframe class-member instance class pyspark.sql.dataframe\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe initialized correctly dataframe\")\n", " ## 2. column names dataframe match list expected column_names\n", " actual_column_names:list = class_object.dataframe.columns\n", " self.assertequal(expected_column_names, actual_column_names, f\"[dataframes:sparkdataframeclass:loaddataframe] expected column names match actuals: {expected_column_names} versus {actual_column_names}\")\n", " ## 3. datatypes dataframe match list expected datatypes\n", " actual_datatypes:list = [class_object.dataframe.select(column_name).schema[0].datatype column_name expected_column_names]\n", " self.assertequal(expected_datatypes, actual_datatypes, f\"[dataframes:sparkdataframeclass:loaddataframe] expected data types match actuals: {expected_datatypes} versus {actual_datatypes}\")\n", "\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_createdataframe_sparkdataframeclass_specialdatatypes(self):\n", " # preprocess\n", " # define arguments passed sparkdataframeclass object\n", " column_dict = [\n", " {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"varchar(4)\"},\n", " {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"varchar(10)\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"decimal(12,4)\"},\n", " ]\n", " table_description = \"table test basic functionalities create table function\"\n", " \n", " ## define class-object \n", " class_object = sparkdataframeclass()\n", "\n", " # define expected values class-instance members:\n", " # column_1: expectation metadata-field limits total number characters added\n", " expected_metadata_column1 = {\"datatype\": stringtype(), \"metadata\": {'column_1': 'metadata column column_1', '__char_varchar_type_string': 'varchar(4)'}}\n", " # column_2: expectation metadata-field limits total number characters added\n", " expected_metadata_column2 = {\"datatype\": stringtype(), \"metadata\": {'column_2': 'metadata column column_2', '__char_varchar_type_string': 'varchar(10)'}}\n", " # column_3: expectation metadata-field limits total number characters added\n", " expected_metadata_column3 = {\"datatype\": decimaltype(12,4), \"metadata\": {'column_3': 'metadata column column_3'}}\n", "\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: initiliaze sparkdataframeclass class-object valid set argument values\n", " class_object.create_dataframe(column_dict, table_description)\n", "\n", " # evaluate\n", " # validate initialised object members match expected values\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes:sparkdataframeclass:loaddataframe] class-member dataframe initialized correctly dataframe\")\n", " \n", " actual_metadata_column1 = class_object.dataframe.select(\"column_1\").schema[0]\n", " actual_metadata_column2 = class_object.dataframe.select(\"column_2\").schema[0]\n", " actual_metadata_column3 = class_object.dataframe.select(\"column_3\").schema[0]\n", "\n", " # validate match expected values\n", " self.assertequal(expected_metadata_column1[\"datatype\"], actual_metadata_column1.datatype)\n", " self.assertequal(expected_metadata_column1[\"metadata\"], actual_metadata_column1.metadata)\n", " self.assertequal(expected_metadata_column2[\"datatype\"], actual_metadata_column2.datatype)\n", " self.assertequal(expected_metadata_column2[\"metadata\"], actual_metadata_column2.metadata)\n", " self.assertequal(expected_metadata_column3[\"datatype\"], actual_metadata_column3.datatype)\n", " self.assertequal(expected_metadata_column3[\"metadata\"], actual_metadata_column3.metadata)\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_createdataframe)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test write_dataframe() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_writedataframe(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", "\n", " self.column_dict = [\n", " {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"binary\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"boolean\"}\n", " ]\n", " \n", " return\n", "\n", " def teardown(self):\n", " # teardown testcass class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls write_dataframe() method\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_writedataframe_sparkdataframeclass_partitioning(self):\n", " # preprocess\n", " ## define arguments passed sparkdataframeclass object\n", " table_description = \"table test write_dataframe method using partitioning\"\n", " class_object = sparkdataframeclass()\n", " class_object.create_dataframe(self.column_dict, table_description)\n", "\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/write_dataframe/partitioning_test'\n", " write_format = 'delta'\n", " write_mode = 'overwrite'\n", " partitioning_columns = ['column_1']\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call write_dataframe() method sparkdataframeclass valid set arguments\n", " class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n", "\n", " # evaluate\n", " detaildf = dt.deltatable.forpath(spark, destination_path).detail()\n", " actual_partitioning_columns = detaildf.select(\"partitioncolumns\").collect()[0][0] \n", " self.assertequal(partitioning_columns,actual_partitioning_columns, f\"[dataframes:sparkdataframeclass:writedataframe] expected partitioning columns found delta table metadata: expected {partitioning_columns}; actual {actual_partitioning_columns}\" )\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_writedataframe_sparkdataframeclass_no_partitioning(self):\n", " # preprocess\n", " ## define arguments passed sparkdataframeclass object\n", " table_description = \"table test write_dataframe method using partitioning\"\n", " class_object = sparkdataframeclass()\n", " class_object.create_dataframe(self.column_dict, table_description)\n", "\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/write_dataframe/no_partitioning_test'\n", " write_format = 'delta'\n", " write_mode = 'overwrite'\n", " partitioning_columns = none\n", " expected_partitioning_columns = list()\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call write_dataframe() method sparkdataframeclass valid set arguments\n", " class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n", "\n", " # evaluate\n", " detaildf = dt.deltatable.forpath(spark, destination_path).detail()\n", " actual_partitioning_columns = detaildf.select(\"partitioncolumns\").collect()[0][0] \n", " self.assertequal(expected_partitioning_columns,actual_partitioning_columns, f\"[dataframes:sparkdataframeclass:writedataframe] expected partitioning columns found delta table metadata: expected {expected_partitioning_columns}; actual {actual_partitioning_columns}\" )\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_writedataframe_sparkdataframeclass_multiplepartitioning(self):\n", " # preprocess\n", " ## define arguments passed sparkdataframeclass object\n", " table_description = \"table test write_dataframe method using partitioning\"\n", " class_object = sparkdataframeclass()\n", " class_object.create_dataframe(self.column_dict, table_description)\n", "\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/write_dataframe/multiple_partitioning_test'\n", " write_format = 'delta'\n", " write_mode = 'overwrite'\n", " partitioning_columns = ['column_1', 'column_2']\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call write_dataframe() method sparkdataframeclass valid set arguments\n", " class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n", "\n", " # evaluate\n", " detaildf = dt.deltatable.forpath(spark, destination_path).detail()\n", " actual_partitioning_columns = detaildf.select(\"partitioncolumns\").collect()[0][0] \n", " self.assertequal(partitioning_columns,actual_partitioning_columns, f\"[dataframes:sparkdataframeclass:writedataframe] expected partitioning columns found delta table metadata: expected {partitioning_columns}; actual {actual_partitioning_columns}\" )\n", "\n", " # failure: throw error dataframe-attribute exist\n", " def test_writedataframe_sparkdataframeclass_throwerror(self):\n", " # preprocess\n", " destination_path = 'test_path'\n", " write_format = ''\n", " write_mode = ''\n", " partitioning_columns = none\n", " expected_error = f\"[sparkdataframeclass] dataframe attribute defined. cannot write dataframe destination {destination_path}. failing task...\"\n", " class_object = sparkdataframeclass()\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call write_dataframe() method sparkdataframeclass valid set arguments\n", " self.assertraises(attributeerror) error:\n", " class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n", "\n", " # evaluate\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:sparkdataframeclass:writedataframe] expected error thrown dataframe exist writing dataframe. got {actual_error}\")\n", "\n", "\n", " # failure: throw error trying merge_schema using overwrite_method 'overwrite'\n", " def test_writedataframe_sparkdataframeclass_failure_overwriteandmerge(self):\n", " # preprocess\n", " # set arguments used call function with\n", " destination_path = 'test_path'\n", " write_format = ''\n", " write_mode = 'overwrite'\n", " merge_schema = true\n", " partitioning_columns = none\n", " expected_error = f\"[dataframes_v2:sparkdataframeclass] allowed mergeschema write_mode 'append'. overwrite entire dataframe\"\n", " class_object = sparkdataframeclass()\n", "\n", " # execute\n", " # run method test\n", " self.assertraises(valueerror) error:\n", " class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns, merge_schema=merge_schema)\n", "\n", "\n", " # evaluate\n", " # validate thrown error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:sparkdataframeclass:writedataframe] expected error thrown dataframe exist writing dataframe. got {actual_error}\")\n", "\n", " # success: append new column existing dataframe\n", " def test_writedataframe_sparkdataframeclass_success_appendandmerge_nopartitioning(self):\n", " # preprocess\n", " # create empty delta table\n", " delta_table_object = deltatableclass_v2()\n", "\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/schema_evolution'\n", " table_description = 'test schema evolution'\n", " column_objects = [\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\n", " ]\n", " partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\n", "\n", " delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\n", "\n", " # create empty dataframe\n", " class_object = sparkdataframeclass()\n", " column_objects = [\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\n", " ]\n", " class_object.create_dataframe(column_information=column_objects)\n", "\n", " # set arguments used call function with\n", " write_format = 'delta'\n", " write_mode = 'append'\n", " partitioning_columns = none\n", " expected_error = f\"[dataframes_v2:sparkdataframeclass] allowed mergeschema write_mode 'append'. overwrite entire dataframe\"\n", "\n", " # define expectations\n", " expected_columns = ['column_1', 'column_2', 'column_3']\n", " # execute\n", " # run method test\n", " class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns, merge_schema=true)\n", "\n", " # evaluate\n", " # validate delta table contains 3 columns (excluding technical fields)\n", " delta_table = delta_table_object.load_delta_table(source_path=destination_path)\n", " table_columns = delta_table.todf().columns\n", " \n", " actual_columns = []\n", " expected_column expected_columns:\n", " expected_column table_columns:\n", " actual_columns.append(expected_column)\n", "\n", " self.assertequal(expected_columns, actual_columns, f'[dataframes:sparkdataframeclass:writedataframe] expected columns appended delta table, missing columns: \\nexpected {expected_columns} \\nmissing {actual_columns}')\n", " \n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_writedataframe)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\") " ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test add_logging_columns_to_dataframe() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_addloggingcolumnstodataframe(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", "\n", " self.column_dict = [\n", " {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"binary\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"boolean\"}\n", " ]\n", " \n", " return\n", "\n", " def teardown(self):\n", " # teardown testcass class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls add_logging_columns_to_dataframe() method\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_addloggingcolumnstodataframe_sparkdataframeclass_success(self):\n", " # preprocess\n", "\n", " table_description = \"table test add_logging_columns_to_dataframe method\"\n", " class_object = sparkdataframeclass()\n", " class_object.create_dataframe(self.column_dict, table_description)\n", "\n", " task_id = -1\n", " plan_id = -1\n", " file_id = -1\n", " file_name = 'unittest'\n", "\n", " expected_columns = ['column_1', 'column_2', 'column_3', 't_load_date_raw', 't_load_date_silver', 't_extract_date', 't_update_date', 't_insert_date', 't_file_name', 't_plan_id', 't_task_id', 't_file_id']\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call add_logging_columns_to_dataframe() method sparkdataframeclass valid set arguments\n", " class_object.add_logging_columns_to_dataframe(task_id=task_id, plan_id=plan_id, file_id=file_id, file_name=file_name)\n", "\n", "\n", " # evaluate\n", " actual_columns = class_object.dataframe.columns\n", " self.assertequal(expected_columns, actual_columns, f\"[dataframes:sparkdataframeclass:addloggingcolumnstodataframe] dataframe contain expected column names: expected {expected_columns} ; actual {actual_columns}\")\n", "\n", "\n", " # failure: throw error dataframe-attribute exist\n", " def test_addloggingcolumnstodataframe_sparkdataframeclass_throwerror(self):\n", " # preprocess\n", " class_object = sparkdataframeclass()\n", " expected_error = \"[sparkdataframeclass] dataframe attribute defined. cannot add logging columns. failing task...\"\n", " task_id = -1\n", " plan_id = -1\n", " file_id = -1\n", " file_name = 'unittest'\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call write_dataframe() method sparkdataframeclass valid set arguments\n", " self.assertraises(attributeerror) error:\n", " class_object.add_logging_columns_to_dataframe(task_id=task_id, plan_id=plan_id, file_id=file_id, file_name=file_name)\n", "\n", " # evaluate\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:sparkdataframeclass:addloggingcolumnstodataframe] expected error thrown dataframe exist adding logging columns. got {actual_error}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_addloggingcolumnstodataframe)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test add_logging_columns_to_schema() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_addloggingcolumnstoschema(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self): \n", " return\n", "\n", " def teardown(self):\n", " # teardown testcass class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls add_logging_columns_to_schema() method\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_addloggingcolumnstoschema_sparkdataframeclass_emptylist(self):\n", " # preprocess\n", " class_object = sparkdataframeclass()\n", " expected_column_names = ['t_load_date_raw', 't_load_date_silver', 't_extract_date', 't_update_date', 't_insert_date', 't_file_name', 't_plan_id', 't_task_id', 't_file_id']\n", " expected_datatypes = [timestamptype(), timestamptype(), timestamptype(), timestamptype(), timestamptype(), stringtype(), integertype(), integertype(), integertype()]\n", " \n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call add_logging_columns_to_schema() method sparkdataframeclass valid set arguments\n", " structfield_objects:list = class_object.add_logging_columns_to_schema(structfields=list())\n", "\n", " # evaluate\n", " ## validate instance instance class structfield\n", " [self.assertisinstance(structfield, structfield) structfield structfield_objects]\n", "\n", " ## get list column_names datatypes\n", " actual_column_names = [structfield.name structfield structfield_objects]\n", " actual_datatypes = [structfield.datatype structfield structfield_objects]\n", " ## compare lists expected values\n", " self.assertequal(expected_column_names, actual_column_names, f\"[dataframes:sparkdataframeclass:addloggingcolumnstoschema] names columns match expectations: expected {expected_column_names} versus actual {actual_column_names}\")\n", " self.assertequal(expected_datatypes, actual_datatypes, f\"[dataframes:sparkdataframeclass:addloggingcolumnstoschema] datatypes columns match expectations: expected {expected_datatypes} versus actual {actual_datatypes}\")\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_addloggingcolumnstoschema_sparkdataframeclass_nonemptylist(self):\n", " # preprocess\n", " class_object = sparkdataframeclass()\n", " structfields = [structfield(name=\"unittest\", datatype=stringtype())]\n", " expected_column_names = [\"unittest\", 't_load_date_raw', 't_load_date_silver', 't_extract_date', 't_update_date', 't_insert_date', 't_file_name', 't_plan_id', 't_task_id', 't_file_id']\n", " expected_datatypes = [stringtype(), timestamptype(), timestamptype(), timestamptype(), timestamptype(), timestamptype(), stringtype(), integertype(), integertype(), integertype()]\n", " \n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call add_logging_columns_to_schema() method sparkdataframeclass valid set arguments\n", " structfield_objects:list = class_object.add_logging_columns_to_schema(structfields=structfields)\n", "\n", " # evaluate\n", " ## validate instance instance class structfield\n", " [self.assertisinstance(structfield, structfield) structfield structfield_objects]\n", "\n", " ## get list column_names datatypes\n", " actual_column_names = [structfield.name structfield structfield_objects]\n", " actual_datatypes = [structfield.datatype structfield structfield_objects]\n", " ## compare lists expected values\n", " self.assertequal(expected_column_names, actual_column_names, f\"[dataframes:sparkdataframeclass:addloggingcolumnstoschema] names columns match expectations: expected {expected_column_names} versus actual {actual_column_names}\")\n", " self.assertequal(expected_datatypes, actual_datatypes, f\"[dataframes:sparkdataframeclass:addloggingcolumnstoschema] datatypes columns match expectations: expected {expected_datatypes} versus actual {actual_datatypes}\")\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_addloggingcolumnstoschema)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test create_structfield_objects() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_createstructfieldobjects(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " self.configure_datatypes_patcher = patch.object(sparkdataframeclass, 'configure_datatypes')\n", " self.mock_configure_datatypes = self.configure_datatypes_patcher.start()\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " self.configure_datatypes_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls create_structfield_objects() method\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_createstructfieldobjects_sparkdataframeclass_basicfields(self):\n", " # preprocess\n", " column_information = [\n", " {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"binary\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"boolean\"}\n", " ]\n", "\n", " self.mock_configure_datatypes.return_value = {\"binary\": binarytype(), \"boolean\": booleantype(),\"string\": stringtype()}\n", "\n", " class_object = sparkdataframeclass()\n", " expected_column_names = [\"column_1\", \"column_2\", \"column_3\"]\n", " expected_datatypes = [stringtype(), binarytype(), booleantype()]\n", " \n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call create_structfield_objects() method sparkdataframeclass valid set arguments\n", " structfield_objects:list = class_object.create_structfield_objects(column_information=column_information)\n", "\n", " # evaluate\n", " ## validate instance instance class structfield\n", " [self.assertisinstance(structfield, structfield) structfield structfield_objects]\n", "\n", " ## get list column_names datatypes\n", " actual_column_names = [structfield.name structfield structfield_objects]\n", " actual_datatypes = [structfield.datatype structfield structfield_objects]\n", " ## compare lists expected values\n", " self.assertequal(expected_column_names, actual_column_names, f\"[dataframes:sparkdataframeclass:createstructfieldobjects] names columns match expectations: expected {expected_column_names} versus actual {actual_column_names}\")\n", " self.assertequal(expected_datatypes, actual_datatypes, f\"[dataframes:sparkdataframeclass:createstructfieldobjects] datatypes columns match expectations: expected {expected_datatypes} versus actual {actual_datatypes}\")\n", " \n", " self.mock_configure_datatypes.assert_called_once_with(table_data_types=[\"string\", \"binary\", \"boolean\"])\n", "\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_createstructfieldobjects_sparkdataframeclass_specialcasefields(self):\n", " # preprocess\n", " column_information = [\n", " {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\n", " {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"varchar(4)\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"varchar(10)\"},\n", " {\"column_sequence\": 3, \"column_name\": \"column_4\", \"dimension\": \"scd2\", \"data_type\": \"decimal(12,4)\"}\n", " ]\n", "\n", " self.mock_configure_datatypes.return_value = {\"string\": stringtype(), \"varchar(4)\": stringtype(),\"varchar(10)\": stringtype(), \"decimal(12,4)\": decimaltype(12,4)}\n", "\n", " class_object = sparkdataframeclass()\n", " expected_column_names = [\"column_1\", \"column_2\", \"column_3\", \"column_4\"]\n", " expected_datatypes = [stringtype(), stringtype(), stringtype(), decimaltype(12,4)]\n", " expected_metadata = [\n", " {\"column_1\": f\"metadata column column_1\"},\n", " {\"column_2\": f\"metadata column column_2\", '__char_varchar_type_string': \"varchar(4)\"},\n", " {\"column_3\": f\"metadata column column_3\", '__char_varchar_type_string': \"varchar(10)\"},\n", " {\"column_4\": f\"metadata column column_4\"}\n", " ]\n", " \n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call create_structfield_objects() method sparkdataframeclass valid set arguments\n", " structfield_objects:list = class_object.create_structfield_objects(column_information=column_information)\n", "\n", " # evaluate\n", " ## validate instance instance class structfield\n", " [self.assertisinstance(structfield, structfield) structfield structfield_objects]\n", "\n", " ## get list column_names datatypes\n", " actual_column_names = [structfield.name structfield structfield_objects]\n", " actual_datatypes = [structfield.datatype structfield structfield_objects]\n", " actual_metadata = [structfield.metadata structfield structfield_objects]\n", "\n", " ## compare lists expected values\n", " self.assertequal(expected_column_names, actual_column_names, f\"[dataframes:sparkdataframeclass:createstructfieldobjects] names columns match expectations: expected {expected_column_names} versus actual {actual_column_names}\")\n", " self.assertequal(expected_datatypes, actual_datatypes, f\"[dataframes:sparkdataframeclass:createstructfieldobjects] datatypes columns match expectations: expected {expected_datatypes} versus actual {actual_datatypes}\")\n", " self.assertequal(expected_metadata, actual_metadata, f\"[dataframes:sparkdataframeclass:createstructfieldobjects] metadata columns match expectations: expected {expected_metadata} versus actual {actual_metadata}\")\n", " \n", " self.mock_configure_datatypes.assert_called_once_with(table_data_types=[\"string\", \"varchar(4)\", \"varchar(10)\", \"decimal(12,4)\"])\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_createstructfieldobjects)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test add_decimal_datatype() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_adddecimaldatatype(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", " def setup(self):\n", " return\n", "\n", " def teardown(self):\n", " # tear-down needed. keeping function potential future use\n", " return\n", "\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_decimal_datatype() function\n", "\n", " # success: add decimal-datatype reference types-dictionary\n", " def test_add_decimal_datatype(self):\n", " # preprocess\n", " ## define arguments used call function with\n", " types = dict()\n", " value = 'decimal(12,5)'\n", " \n", " ## define object-instance class sparkdataframeclass\n", " class_object = sparkdataframeclass()\n", "\n", " ## set expected results\n", " expected_dictionary = {\"decimal(12,5)\": sql.types.decimaltype(12,5)}\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call add_decimal_datatype() method sparkdataframeclass valid set arguments\n", " actual_dictionary = class_object.add_decimal_datatype(value, types)\n", "\n", " # evaluate\n", " ## validate returned variables match expectations\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\n", "\n", " # failure: deny additional reference invalid decimal configuration types-dictionary \n", " def test_invalid_value(self):\n", " # preprocess\n", " ## define arguments used call function with\n", " types = dict()\n", " value = 'decimal(12)'\n", " \n", " ## define object-instance class sparkdataframeclass\n", " class_object = sparkdataframeclass()\n", "\n", " ## set expected results\n", " expected_error = f\"the decimal value decimal(12) valid datatype value cannot configured correctly. aborting delta table deployment...\"\n", " \n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call add_decimal_datatype() method sparkdataframeclass invalid set arguments\n", " self.assertraises(valueerror) error:\n", " class_object.add_decimal_datatype(value, types)\n", " actual_error = str(error.exception)\n", "\n", " # evaluate\n", " # validate expected error actually returned\n", " self.assertequal(expected_error, actual_error, f'the returned error add_decimal_datatype \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_adddecimaldatatype)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test add_varchar_datatype() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_addvarchardatatype(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_varchar_datatype() function\n", "\n", " # success: add varchar-datatype reference types-dictionary\n", " def test_addvarchardatatype_sparkdataframeclass_success(self):\n", " # preprocess\n", " ## define arguments used call function with\n", " types = dict()\n", " value = 'varchar(12)'\n", " \n", " ## define object-instance class sparkdataframeclass\n", " class_object = sparkdataframeclass()\n", "\n", " ## set expected results\n", " expected_dictionary = {\"varchar(12)\": stringtype()}\n", " \n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call add_varchar_datatype() method sparkdataframeclass valid set arguments\n", " actual_dictionary = class_object.add_varchar_datatype(varchar_value=value, datatypes=types)\n", "\n", " # evaluate\n", " ## validate returned variables match expectations\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"[dataframes_v2:sparkdataframeclass:addvarchardatatype]the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\n", "\n", " # failure: deny additional reference invalid varchar configuration types-dictionary \n", " def test_addvarchardatatype_sparkdataframeclass_invalidargument(self):\n", " # preprocess\n", " ## define arguments used call function with\n", " types = dict()\n", " value = 'varchar'\n", " \n", " ## define object-instance class sparkdataframeclass\n", " class_object = sparkdataframeclass()\n", "\n", " ## set expected results\n", " expected_error = f\"the varchar value varchar valid datatype value cannot configured correctly. aborting delta table deployment...\"\n", " \n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call add_varchar_datatype() method sparkdataframeclass invalid set arguments\n", " self.assertraises(valueerror) error:\n", " class_object.add_varchar_datatype(varchar_value=value, datatypes=types)\n", "\n", " # evaluate\n", " # validate expected error returned\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f'the returned error add_varchar_datatype \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_addvarchardatatype)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test configure_datatypes() method defined sparkdataframeclass notebook\n", "class test_sparkdataframeclass_configuredatatypes(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # patch functions called validate_table-method deltatable-class\n", " # patched functions self-made functions functions coming external libaries.\n", " # self-made functions tested separately, \"expected\" work called functions\n", "\n", " self.mock_add_decimal_datatype_patcher = patch.object(sparkdataframeclass, 'add_decimal_datatype')\n", " self.mock_add_decimal_datatype = self.mock_add_decimal_datatype_patcher.start()\n", "\n", " self.mock_add_varchar_datatype_patcher = patch.object(sparkdataframeclass, 'add_varchar_datatype')\n", " self.mock_add_varchar_datatype = self.mock_add_varchar_datatype_patcher.start()\n", "\n", " ## define object-instance class sparkdataframeclass -&gt; reuse object throughout test case\n", " self.class_object = sparkdataframeclass()\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " self.mock_add_decimal_datatype_patcher.stop()\n", " self.mock_add_varchar_datatype_patcher.stop()\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls configure_datatypes function\n", "\n", "\n", " # success: pass empty dictionary return dictionary predefined datatypes\n", " def test_configuredatatypes_sparkdataframeclass_returngenericdatatypes(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = list()\n", " ## define expected return values\n", " expected_dictionary = {\n", " \"array\": arraytype(stringtype()), \"binary\": binarytype(), \"boolean\": booleantype(), \"date\": datetype(), \"string\": stringtype(), \n", " \"varchar(max)\": stringtype(), \"timestamp\": timestamptype(), \"decimal\": decimaltype(38, 4), \"float\": floattype(), \"byte\": bytetype(), \n", " \"integer\": integertype(), \"int\": integertype(), \"long_integer\": longtype()\n", " }\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " actual_dictionary = self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate returned values match expectations\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"[dataframes_v2:sparkdataframeclass:configuredatatypes] returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\n", " \n", " # success: validate datatype-specific conversion functions (add_decimal_datatype add_varchar_datatype) invoked value-matches dictionary\n", " def test_configuredatatypes_sparkdataframeclass_functioncalls_nocalls(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['string']\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate datatype-specific functions called\n", " self.mock_add_decimal_datatype.assert_not_called()\n", " self.mock_add_varchar_datatype.assert_not_called()\n", "\n", "\n", " # success: validate dictionary passed contains value 'decimal(&lt;xx&gt;,&lt;yy&gt;', function add_decimal_datatype called\n", " def test_configuredatatypes_sparkdataframeclass_functioncalls_decimalfunction(self): \n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['decimal(12,5)']\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate function called exactly once\n", " self.mock_add_decimal_datatype.assert_called_with(decimal_value='decimal(12,5)', datatypes=any)\n", "\n", " # success: validate dictionary passed contains generic value 'decimal' specific 'decimal(&lt;xx&gt;,&lt;yy&gt;), \n", " # function add_decimal_datatype called reference\n", " def test_configuredatatypes_sparkdataframeclass_functioncalls_decimalfunctiontwice(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['decimal', 'decimal(12,5)']\n", " ## define list expected function-calls add_decimal_datatype\n", " expected_calls = [call(decimal_value='decimal', datatypes=any), call(decimal_value='decimal(12,5)', datatypes=any)]\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate function called instance 'decimal' \n", " self.mock_add_decimal_datatype.assert_has_calls(expected_calls, any_order=true)\n", "\n", "\n", " # success: validate dictionary passed contains generic value 'varchar(max)', function add_varchar_datatype called\n", " def test_configuredatatypes_sparkdataframeclass_functioncalls_varcharfunction(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['varchar(max)']\n", "\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate function add_varchar_datatype() called exactly once\n", " self.mock_add_varchar_datatype.assert_called_once()\n", "\n", " # success: validate dictionary passed contains specific value 'varchar(4)', function add_varchar_datatype called\n", " def test_configuredatatypes_sparkdataframeclass_functioncalls_varcharfunction_2(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['varchar(4)']\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate function called exactly once\n", " self.mock_add_varchar_datatype.assert_called_with(varchar_value='varchar(4)', datatypes=any)\n", "\n", " # success: validate dictionary passed contains generic value 'varchar(max)' specific value 'varchar(4)', \n", " # function add_varchar_datatype called reference\n", " def test_configuredatatypes_sparkdataframeclass_functioncalls_varcharfunctiontwice(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['varchar(max)', 'varchar(4)']\n", "\n", " # define list expected function-calls add_varchar_datatype\n", " expected_calls = [call(varchar_value='varchar(max)', datatypes=any), call(varchar_value='varchar(4)', datatypes=any)]\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass valid set arguments\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", "\n", " # evaluate\n", " ## validate function called instance 'varchar'\n", " self.mock_add_varchar_datatype.assert_has_calls(expected_calls, any_order=true)\n", "\n", "\n", " # failure: test invalid (or non-configured) datatype passed, error thrown\n", " def test_configuredatatypes_sparkdataframeclass_failure_invaliddatatype(self):\n", " # preprocess\n", " ## define argument call configure_datatypes method with\n", " table_data_types = ['string', 'invalid_datatype']\n", "\n", " # expect following error thrown:\n", " expected_error = f\"the datatype 'invalid_datatype' configured datatype current set-up.\"\n", "\n", " # execute\n", " ## [dataframes_v2:sparkdataframeclass]: call configure_datatypes() method sparkdataframeclass invalid set arguments\n", " self.assertraises(valueerror) error:\n", " self.class_object.configure_datatypes(table_data_types=table_data_types)\n", " actual_error = str(error.exception)\n", "\n", " # evaluate\n", " ## validate thrown error equal expected thrown error\n", " self.assertequal(expected_error, actual_error, f'[dataframes_v2:sparkdataframeclass:configuredatatypes] returned error add_decimal_datatype \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_configuredatatypes)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\n", "# class test convert_firstrow_to_columns() sparkdataframeclass-class defined dataframes_v2 notebook\n", "class test_sparkdataframeclass_convertfirstrowtocolumns(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", "\n", " self.validate_column_indexes_patcher = patch.object(sparkdataframeclass, 'validate_column_indexes')\n", " self.mock_validate_column_indexes = self.validate_column_indexes_patcher.start()\n", "\n", " self.write_dataframe_patcher = patch.object(sparkdataframeclass, 'write_dataframe')\n", " self.mock_write_dataframe = self.write_dataframe_patcher.start()\n", "\n", " self.rename_dataframe_file_patcher = patch.object(sparkdataframeclass, 'rename_dataframe_file')\n", " self.mock_rename_dataframe_file = self.rename_dataframe_file_patcher.start()\n", "\n", " self.source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/convert_firstrow_to_columns/firstrow_to_headers.fil'\n", " \n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " self.validate_column_indexes_patcher.stop()\n", " self.rename_dataframe_file_patcher.stop()\n", " self.write_dataframe_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls convert_firstrow_to_columns() method\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_convertfirstrowtocolumns_sparkdataframeclass_success(self):\n", " # preprocess\n", " start_index = 1\n", " end_index = 2\n", " column_names = [\"name\", \"type\", \"hp\", \"attack\", \"catch_date\", \"state\"]\n", " header = false\n", " destination_path = 'destination_path.csv'\n", "\n", " expected_catch_date = '2024-08-19'\n", " expected_status = 'captured'\n", "\n", " class_object = sparkdataframeclass()\n", " class_object.load_dataframe(source_path=self.source_path, header=header, separator='\\t', file_kind='csv')\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call convert_firstrow_to_columns() method sparkdataframeclass valid set arguments\n", " class_object.convert_firstrow_to_columns(start_index=start_index, end_index=end_index, column_names=column_names, header=header, destination_path=destination_path, file_extension='csv', column_delimiter=mock())\n", "\n", " # evaluate\n", " actual_column_names = class_object.dataframe.columns\n", " actual_catch_date = class_object.dataframe.select(\"catch_date\").distinct().rdd.map(lambda x: x.catch_date).collect()\n", " actual_status = class_object.dataframe.select(\"state\").distinct().rdd.map(lambda x: x.state).collect()\n", "\n", " self.assertequal(column_names, actual_column_names, f\"[dataframes_v2:sparkdataframeclass:convertfirstrowtocolumns] expected column names found final dataframe: expected {column_names} versus actual {actual_column_names}\")\n", " self.assertequal(1, len(actual_catch_date), f\"[dataframes_v2:sparkdataframeclass:convertfirstrowtocolumns] number distinct values expected literal column 'catch_date' 1: {actual_catch_date}\")\n", " self.assertequal(expected_catch_date, actual_catch_date[0], f\"[dataframes_v2:sparkdataframeclass:convertfirstrowtocolumns] expected literal 'catch_date' match expectations: {actual_catch_date}\")\n", " self.assertequal(1, len(actual_status), f\"[dataframes_v2:sparkdataframeclass:convertfirstrowtocolumns] number distinct values expected literal column 'status' 1: {actual_status}\")\n", " self.assertequal(expected_status, actual_status[0], f\"[dataframes_v2:sparkdataframeclass:convertfirstrowtocolumns] expected literal 'status' match expectations: {actual_status}\")\n", "\n", " # success: initialize sparkdataframeclass class-object valid arguments \n", " def test_convertfirstrowtocolumns_sparkdataframeclass_functioncalls(self):\n", " # preprocess\n", " start_index = 1\n", " end_index = 2\n", " column_names = [\"name\", \"type\", \"hp\", \"attack\", \"catch_date\", \"state\"]\n", " header = false\n", " destination_path = 'destination_path.csv'\n", " file_extension = 'csv'\n", " column_delimiter =mock()\n", "\n", " expected_catch_date = '2024-08-19'\n", " expected_status = 'captured'\n", "\n", " class_object = sparkdataframeclass()\n", " class_object.load_dataframe(source_path=self.source_path, header=header, separator='\\t', file_kind='csv')\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call convert_firstrow_to_columns() method sparkdataframeclass valid set arguments\n", " class_object.convert_firstrow_to_columns(start_index=start_index, end_index=end_index, column_names=column_names, header=header, destination_path=destination_path, file_extension=file_extension, column_delimiter=column_delimiter)\n", "\n", " # evaluate\n", " ## validate mocked functions called\n", " self.mock_validate_column_indexes.assert_called_once_with(start_index=start_index, end_index=end_index, column_names=column_names, header=header)\n", " self.mock_write_dataframe.assert_called_once_with(write_format=any, write_mode='overwrite', destination_path='destination_path', column_delimiter=column_delimiter)\n", " self.mock_rename_dataframe_file.assert_called_once_with(source_path='destination_path', file_extension=file_extension)\n", "\n", "\n", " def test_convertfirstrowtocolumns_sparkdataframeclass_raiseerror(self):\n", " # preprocess\n", " class_object = sparkdataframeclass()\n", " expected_error = \"[sparkdataframeclass] dataframe attribute defined. cannot convert first row columns. failing task...\"\n", " start_index = 1\n", " end_index = 2\n", " column_names = [\"name\", \"type\", \"hp\", \"attack\", \"catch_date\", \"state\"]\n", " header = false\n", " destination_path = 'destination_path.csv'\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframeclass]: call convert_firstrow_to_columns() method sparkdataframeclass valid set arguments\n", " self.assertraises(attributeerror) error:\n", " class_object.convert_firstrow_to_columns(start_index=start_index, end_index=end_index, column_names=column_names, header=header, destination_path=destination_path, file_extension=mock(), column_delimiter=mock())\n", "\n", " # evaluate\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:sparkdataframeclass:addloggingcolumnstodataframe] expected error thrown dataframe exist adding logging columns. got {actual_error}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_convertfirstrowtocolumns)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\r\n", "# class test load_dataframe_with_schema() sparkdataframeclass-class defined dataframes_v2 notebook\r\n", "class test_sparkdataframeclass_loaddataframewithschema(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " self.source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/load_dataframe_with_schema/load_dataframe_with_schema.csv'\r\n", " self.header = true\r\n", "\r\n", " self.column_info = [\r\n", " {\"column_name\": \"decimalus\", \"data_type\": \"decimal(5,3)\"},\r\n", " {\"column_name\": \"decimalfr\", \"data_type\": \"decimal(5,3)\"},\r\n", " {\"column_name\": \"timestampus\", \"data_type\": \"decimal(5,3)\"},\r\n", " {\"column_name\": \"timestampfr\", \"data_type\": \"decimal(5,3)\"},\r\n", " {\"column_name\": \"dateus\", \"data_type\": \"decimal(5,3)\"},\r\n", " {\"column_name\": \"datefr\", \"data_type\": \"decimal(5,3)\"},\r\n", " {\"column_name\": \"datenoseparator\", \"data_type\": \"decimal(5,3)\"}\r\n", " ] \r\n", "\r\n", " self.separator:str=';'\r\n", " \r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown test case class\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful expected_validate_argument_calls load_dataframe_with_schema() method\r\n", "\r\n", " # success: initialize sparkdataframeclass class-object valid arguments \r\n", " def test_loaddataframewithschema_sparkdataframeclass_success_matchingfrlocale(self):\r\n", " # preprocess\r\n", " # set class_object arguments\r\n", " class_object = sparkdataframeclass()\r\n", " structfield_objects = class_object.create_structfield_objects(self.column_info)\r\n", " schema = structtype(structfield_objects)\r\n", " column_to_select = \"decimalfr\"\r\n", " column_selection = [column_to_select]\r\n", " locale = 'fr-fr'\r\n", "\r\n", " # set expected values\r\n", " expected_corrupt_records = 1\r\n", " expected_valid_records = 2\r\n", "\r\n", "\r\n", " # execute\r\n", " # [dataframes_v2:sparkdataframeclass]: call load_dataframe_with_schema() method sparkdataframeclass valid set arguments\r\n", " dataframe = class_object.load_dataframe_with_schema(source_path=self.source_path, header=self.header,schema=schema,column_selection=column_selection,separator=self.separator,locale=locale)\r\n", " \r\n", " # evaluate\r\n", " # get actual number corrupt non-corrupt records\r\n", " dataframe.cache()\r\n", " actual_corrupt_records = dataframe.filter(col('_corrupt_record').isnotnull()).count()\r\n", " actual_valid_records = dataframe.filter(col(column_to_select).isnotnull()).count()\r\n", " dataframe.unpersist()\r\n", " # compare actuals expectations\r\n", " self.assertequal(expected_corrupt_records, actual_corrupt_records, f\"[dataframes:sparkdataframeclass:loaddataframewithschema] 1. expected number corrupt records match actuals\")\r\n", " self.assertequal(expected_valid_records, actual_valid_records, f\"[dataframes:sparkdataframeclass:loaddataframewithschema] 1. expected number valid records match actuals\")\r\n", "\r\n", " def test_loaddataframewithschema_sparkdataframeclass_success_notmatchinglocale(self):\r\n", " # preprocess\r\n", " # set class_object arguments\r\n", " class_object = sparkdataframeclass()\r\n", " structfield_objects = class_object.create_structfield_objects(self.column_info)\r\n", " schema = structtype(structfield_objects)\r\n", " column_to_select = \"decimalfr\"\r\n", " column_selection = [column_to_select]\r\n", " locale = 'en-us'\r\n", "\r\n", " # set expected values\r\n", " expected_corrupt_records = 2\r\n", " expected_valid_records = 1\r\n", "\r\n", "\r\n", " # execute\r\n", " # [dataframes_v2:sparkdataframeclass]: call load_dataframe_with_schema() method sparkdataframeclass valid set arguments\r\n", " dataframe = class_object.load_dataframe_with_schema(source_path=self.source_path, header=self.header,schema=schema,column_selection=column_selection,separator=self.separator,locale=locale)\r\n", " \r\n", " # evaluate\r\n", " # get actual number corrupt non-corrupt records\r\n", " dataframe.cache()\r\n", " actual_corrupt_records = dataframe.filter(col('_corrupt_record').isnotnull()).count()\r\n", " actual_valid_records = dataframe.filter(col(column_to_select).isnotnull()).count()\r\n", " dataframe.unpersist()\r\n", "\r\n", " # compare actuals expectations\r\n", " self.assertequal(expected_corrupt_records, actual_corrupt_records, f\"[dataframes:sparkdataframeclass:loaddataframewithschema] 2. expected number corrupt records match actuals\")\r\n", " self.assertequal(expected_valid_records, actual_valid_records, f\"[dataframes:sparkdataframeclass:loaddataframewithschema] 2. expected number valid records match actuals\")\r\n", "\r\n", " def test_loaddataframewithschema_sparkdataframeclass_success_matchinguslocale(self):\r\n", " # preprocess\r\n", " # set class_object arguments\r\n", " class_object = sparkdataframeclass()\r\n", " structfield_objects = class_object.create_structfield_objects(self.column_info)\r\n", " schema = structtype(structfield_objects)\r\n", " column_to_select = \"decimalus\"\r\n", " column_selection = [column_to_select]\r\n", " locale = 'en-us'\r\n", " \r\n", " # set expected values\r\n", " expected_corrupt_records = 1\r\n", " expected_valid_records = 2\r\n", "\r\n", "\r\n", " # execute\r\n", " # [dataframes_v2:sparkdataframeclass]: call load_dataframe_with_schema() method sparkdataframeclass valid set arguments\r\n", " dataframe = class_object.load_dataframe_with_schema(source_path=self.source_path, header=self.header,schema=schema,column_selection=column_selection,separator=self.separator,locale=locale)\r\n", " \r\n", " # evaluate\r\n", " # get actual number corrupt non-corrupt records\r\n", " dataframe.cache()\r\n", " actual_corrupt_records = dataframe.filter(col('_corrupt_record').isnotnull()).count()\r\n", " actual_valid_records = dataframe.filter(col(column_to_select).isnotnull()).count()\r\n", " dataframe.unpersist()\r\n", "\r\n", " # compare actuals expectations\r\n", " self.assertequal(expected_corrupt_records, actual_corrupt_records, f\"[dataframes:sparkdataframeclass:loaddataframewithschema] 3. expected number corrupt records match actuals\")\r\n", " self.assertequal(expected_valid_records, actual_valid_records, f\"[dataframes:sparkdataframeclass:loaddataframewithschema] 3. expected number valid records match actuals\")\r\n", "\r\n", " \r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_loaddataframewithschema)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframeclass\r\n", "# class test replace_dataframe_columns() sparkdataframeclass-class defined dataframes_v2 notebook\r\n", "class test_sparkdataframeclass_replacedataframecolumns(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " self.source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframeclass/replace_dataframe_columns/replace_dataframe_columns.csv'\r\n", " self.header = true\r\n", " self.separator:str=';'\r\n", " self.kind = 'csv'\r\n", " \r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown test case class\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful expected_validate_argument_calls replace_dataframe_columns() method\r\n", "\r\n", " # success: initialize sparkdataframeclass class-object valid arguments \r\n", " def test_replacedataframecolumns_sparkdataframeclass_success_replaceonecolumn(self):\r\n", " # preprocess\r\n", " # set class_object arguments\r\n", " class_object = sparkdataframeclass()\r\n", " class_object.load_dataframe(source_path=self.source_path, header=self.header, separator=self.separator, file_kind=self.kind)\r\n", " dataframe = class_object.dataframe\r\n", "\r\n", " columns_to_replace= ['column1']\r\n", " dataframe = dataframe.withcolumn('column1', lit(\"default\"))\r\n", " dataframe = dataframe.withcolumn('column2', lit(2))\r\n", "\r\n", " # set expected values\r\n", " expected_total_rows = 4\r\n", " expected_default_occurences = 4\r\n", " expected_2_occurences = 0\r\n", "\r\n", " # execute\r\n", " # [dataframes_v2:sparkdataframeclass]: call replace_dataframe_columns() method sparkdataframeclass valid set arguments\r\n", " class_object.replace_dataframe_columns(replacing_dataframe=dataframe, columns_to_replace=columns_to_replace)\r\n", " \r\n", " # evaluate\r\n", " # get actual number value occurances table\r\n", " actual_total_rows = class_object.dataframe.count()\r\n", " actual_count_default_occurences = class_object.dataframe.filter(col('column1') == 'default').count()\r\n", " actual_count_2_occurences = class_object.dataframe.filter(col('column2') == 2).count()\r\n", "\r\n", " # compare actuals expectations\r\n", " self.assertequal(expected_total_rows, actual_total_rows, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 1. expected number total rows match actuals\")\r\n", " self.assertequal(expected_default_occurences, actual_count_default_occurences, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 1. expected number default values match actuals\")\r\n", " self.assertequal(expected_2_occurences, actual_count_2_occurences, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 1. expected number 2 values match actuals\")\r\n", "\r\n", "\r\n", " def test_replacedataframecolumns_sparkdataframeclass_success_replacetwocolumns(self):\r\n", " # preprocess\r\n", " # set class_object arguments\r\n", " class_object = sparkdataframeclass()\r\n", " class_object.load_dataframe(source_path=self.source_path, header=self.header, separator=self.separator, file_kind=self.kind)\r\n", " dataframe = class_object.dataframe\r\n", "\r\n", " columns_to_replace= ['column1', 'column2']\r\n", " dataframe = dataframe.withcolumn('column1', lit(\"default\"))\r\n", " dataframe = dataframe.withcolumn('column2', lit(2))\r\n", "\r\n", " # set expected values\r\n", " expected_total_rows = 4\r\n", " expected_default_occurences = 4\r\n", " expected_2_occurences = 4\r\n", "\r\n", " # execute\r\n", " # [dataframes_v2:sparkdataframeclass]: call replace_dataframe_columns() method sparkdataframeclass valid set arguments\r\n", " class_object.replace_dataframe_columns(replacing_dataframe=dataframe, columns_to_replace=columns_to_replace)\r\n", " \r\n", " # evaluate\r\n", " # get actual number value occurances table\r\n", " actual_total_rows = class_object.dataframe.count()\r\n", " actual_count_default_occurences = class_object.dataframe.filter(col('column1') == 'default').count()\r\n", " actual_count_2_occurences = class_object.dataframe.filter(col('column2') == 2).count()\r\n", "\r\n", " # compare actuals expectations\r\n", " self.assertequal(expected_total_rows, actual_total_rows, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 2. expected number total rows match actuals\")\r\n", " self.assertequal(expected_default_occurences, actual_count_default_occurences, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 2. expected number default values match actuals\")\r\n", " self.assertequal(expected_2_occurences, actual_count_2_occurences, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 2. expected number 2 values match actuals\")\r\n", "\r\n", " def test_replacedataframecolumns_sparkdataframeclass_success_replacenocolumns(self):\r\n", " # preprocess\r\n", " # set class_object arguments\r\n", " class_object = sparkdataframeclass()\r\n", " class_object.load_dataframe(source_path=self.source_path, header=self.header, separator=self.separator, file_kind=self.kind)\r\n", " dataframe = class_object.dataframe\r\n", "\r\n", " columns_to_replace= []\r\n", " dataframe = dataframe.withcolumn('column1', lit(\"default\"))\r\n", " dataframe = dataframe.withcolumn('column2', lit(2))\r\n", "\r\n", " # set expected values\r\n", " expected_total_rows = 4\r\n", " expected_default_occurences = 0\r\n", " expected_2_occurences = 0\r\n", "\r\n", " # execute\r\n", " # [dataframes_v2:sparkdataframeclass]: call replace_dataframe_columns() method sparkdataframeclass valid set arguments\r\n", " class_object.replace_dataframe_columns(replacing_dataframe=dataframe, columns_to_replace=columns_to_replace)\r\n", " \r\n", " # evaluate\r\n", " # get actual number value occurances table\r\n", " actual_total_rows = class_object.dataframe.count()\r\n", " actual_count_default_occurences = class_object.dataframe.filter(col('column1') == 'default').count()\r\n", " actual_count_2_occurences = class_object.dataframe.filter(col('column2') == 2).count()\r\n", "\r\n", " # compare actuals expectations\r\n", " self.assertequal(expected_total_rows, actual_total_rows, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 3. expected number total rows match actuals\")\r\n", " self.assertequal(expected_default_occurences, actual_count_default_occurences, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 3. expected number default values match actuals\")\r\n", " self.assertequal(expected_2_occurences, actual_count_2_occurences, f\"[dataframes:sparkdataframeclass:replacedataframecolumns] 3. expected number 2 values match actuals\")\r\n", " \r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframeclass_replacedataframecolumns)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error sparkdataframeclass loaddataframe tests, something went wrong!\")" ], "execution_count": 19 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## test class: sparkdataframechecks" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test __init__() sparkdataframechecks-class defined dataframes_v2 notebook\n", "class test_sparkdataframechecks_initialization(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self): \n", " return\n", "\n", " def teardown(self):\n", " # teardown testcass class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls __init__() method\n", "\n", " # success: initialize sparkdataframechecks class-object valid arguments \n", " def test_initialization_sparkdataframechecks_emptychecklist(self):\n", " # preprocess\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframechecks/initialization/initialize_checks_object.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " column_names = ['date', 'string']\n", " data_types = ['timestamp', 'string']\n", " checks = []\n", " column_information = [{\"locale\": \"nl-nl\"}, {\"format\": \"yyyy-mm-dd\"}]\n", "\n", "\n", " expected_checks = ['header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n", " expected_row_count = 3\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call __init__() method sparkdataframechecks valid set arguments\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # evaluate\n", " ## validate instance instance class structfield\n", " actual_checks = class_object.checks\n", " actual_column_names = class_object.dataframe.columns\n", " actual_row_count = class_object.dataframe.count()\n", " \n", " self.assertequal(expected_checks, actual_checks, f\"[dataframes_v2:sparkdataframechecks:initialization] class member 'checks' contain expected values: expected {expected_checks} vs actual {actual_checks}\")\n", " self.assertequal(column_names, actual_column_names, f\"[dataframes_v2:sparkdataframechecks:initialization] loaded dataframe contain expected column names: expected {column_names} vs actual {actual_column_names}\")\n", " self.assertequal(expected_row_count, actual_row_count, f\"[dataframes_v2:sparkdataframechecks:initialization] loaded dataframe contain expected row count: expected {expected_row_count} vs actual {actual_row_count}\")\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes_v2:sparkdataframechecks:initialization] loaded dataframe class 'pyspark.sql.dataframe'\")\n", "\n", "\n", " # success: initialize sparkdataframechecks class-object valid arguments \n", " def test_initialization_sparkdataframechecks_nonemptychecklist(self):\n", " # preprocess\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframechecks/initialization/initialize_checks_object.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " column_names = ['date', 'string']\n", " data_types = ['timestamp', 'string']\n", " checks = ['landing_rows', 'header']\n", " column_information = [{\"locale\": \"nl-nl\"}, {\"format\": \"yyyy-mm-dd\"}]\n", "\n", " expected_checks = ['landing_rows', 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n", " expected_row_count = 3\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call __init__() method sparkdataframechecks valid set arguments\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # evaluate\n", " ## validate instance instance class structfield\n", " actual_checks = class_object.checks\n", " actual_column_names = class_object.dataframe.columns\n", " actual_row_count = class_object.dataframe.count()\n", " \n", " self.assertequal(expected_checks, actual_checks, f\"[dataframes_v2:sparkdataframechecks:initialization] class member 'checks' contain expected values: expected {expected_checks} vs actual {actual_checks}\")\n", " self.assertequal(column_names, actual_column_names, f\"[dataframes_v2:sparkdataframechecks:initialization] loaded dataframe contain expected column names: expected {column_names} vs actual {actual_column_names}\")\n", " self.assertequal(expected_row_count, actual_row_count, f\"[dataframes_v2:sparkdataframechecks:initialization] loaded dataframe contain expected row count: expected {expected_row_count} vs actual {actual_row_count}\")\n", " self.assertisinstance(class_object.dataframe, dataframe, f\"[dataframes_v2:sparkdataframechecks:initialization] loaded dataframe class 'pyspark.sql.dataframe'\")\n", " \n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_initialization)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 12 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\r\n", "# class test replace_value function sparkdataframechecks-class defined dataframes_v2 notebook\r\n", "class test_sparkdataframechecks_replace_value(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self): \r\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\r\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown testcass class\r\n", " self.load_dataframe_patcher.stop()\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful replace_value function\r\n", " # success:replace null pk value dataframe \r\n", " def test_check_pk_values_to_replace_with_a_integer(self):\r\n", " # preprocess\r\n", " source_path = mock()\r\n", " header=true\r\n", " separator=';'\r\n", " file_kind='csv'\r\n", " column_names=['primary_key','column_1']\r\n", " data_types=['integer','string']\r\n", " checks=[]\r\n", " expected_replace_value=0\r\n", " column_information=[\r\n", " {\r\n", " \"replace_value\": expected_replace_value\r\n", " },\r\n", " {\r\n", " none\r\n", " }]\r\n", " self.schema = sql.types.structtype([\r\n", " sql.types.structfield(\"primary_key\", sql.types.integertype(), true),\r\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true)\r\n", " ])\r\n", "\r\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n", " data_types=data_types, checks=checks, column_information=column_information)\r\n", " class_object.dataframe = spark.createdataframe(data=[(none,'test'),(1,'test1')], schema=self.schema)\r\n", " # # execute\r\n", " actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n", " first_row_value = class_object.dataframe.first()\r\n", " actual_replace_value = first_row_value.primary_key\r\n", " # evaluate\r\n", " ## validate null pk replaced\r\n", " self.assertequal(expected_replace_value,actual_replace_value,f\"[dataframes_v2:sparkdataframechecks:pk_values_to_replace] dataframe change expected integer value\")\r\n", " \r\n", " # success:replace null pk value dataframe \r\n", " def test_check_pk_values_to_replace_with_a_string(self):\r\n", " # preprocess\r\n", " source_path = mock()\r\n", " header=true\r\n", " separator=';'\r\n", " file_kind='csv'\r\n", " column_names=['primary_key','column_1']\r\n", " data_types=['string','string']\r\n", " checks=[]\r\n", " expected_replace_value=\"angry_bird\"\r\n", " column_information=[\r\n", " {\r\n", " \"replace_value\": expected_replace_value\r\n", " },\r\n", " {\r\n", " none\r\n", " }]\r\n", " self.schema = sql.types.structtype([\r\n", " sql.types.structfield(\"primary_key\", sql.types.stringtype(), true),\r\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true)\r\n", " ])\r\n", "\r\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n", " data_types=data_types, checks=checks, column_information=column_information)\r\n", " class_object.dataframe = spark.createdataframe(data=[(none,'test'),(1,'test1')], schema=self.schema)\r\n", " # # execute\r\n", " actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n", " first_row_value = class_object.dataframe.first()\r\n", " actual_replace_value = first_row_value.primary_key\r\n", " # evaluate\r\n", " ## validate null pk replaced\r\n", " self.assertequal(expected_replace_value,actual_replace_value,f\"[dataframes_v2:sparkdataframechecks:pk_values_to_replace] dataframe change expected string value\")\r\n", " \r\n", " # success:replace null pk value dataframe return 0\r\n", " def test_check_pk_values_to_replace_with_the_option(self):\r\n", " # preprocess\r\n", " source_path = mock()\r\n", " header=true\r\n", " separator=';'\r\n", " file_kind='csv'\r\n", " column_names=['primary_key','column_1']\r\n", " data_types=['integer','string']\r\n", " checks=[]\r\n", " column_information=[\r\n", " {\r\n", " \"replace_value\": \"0\"\r\n", " },\r\n", " {\r\n", " none\r\n", " }]\r\n", "\r\n", " expected_result=0\r\n", "\r\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n", " data_types=data_types, checks=checks, column_information=column_information)\r\n", " # execute\r\n", " actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n", " # evaluate\r\n", " ## validate null pk replaced\r\n", " self.assertequal(expected_result,actual_result,f\"[dataframes_v2:sparkdataframechecks:pk_values_to_replace] primary value replace 'checks' work expected\")\r\n", " \r\n", " # failure:the 'column_information' dict. return 1\r\n", " def test_check_pk_values_to_replace_with_set(self):\r\n", " # preprocess\r\n", " source_path = mock()\r\n", " header=true\r\n", " separator=';'\r\n", " file_kind='csv'\r\n", " column_names=['primary_key','column_1']\r\n", " data_types=['integer','string']\r\n", " checks=[]\r\n", " column_information=[\r\n", " {\r\n", " none\r\n", " },\r\n", " {\r\n", " none\r\n", " }]\r\n", "\r\n", " expected_result=1\r\n", "\r\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n", " data_types=data_types, checks=checks, column_information=column_information)\r\n", " # execute\r\n", " actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n", " # evaluate\r\n", " ## validate null pk replaced\r\n", " self.assertequal(expected_result,actual_result,f\"[dataframes_v2:sparkdataframechecks:pk_values_to_replace] primary value replace 'checks' work expected\")\r\n", " \r\n", " # failure:the 'column_information' contain 'replace_value' key. return 1\r\n", " def test_check_pk_values_to_replace_with_the_option(self):\r\n", " # preprocess\r\n", " source_path = mock()\r\n", " header=true\r\n", " separator=';'\r\n", " file_kind='csv'\r\n", " column_names=['primary_key','column_1']\r\n", " data_types=['integer','string']\r\n", " checks=[]\r\n", " column_information=[\r\n", " {\r\n", " \"optional\": \"0\"\r\n", " },\r\n", " {\r\n", " none\r\n", " }]\r\n", "\r\n", " expected_result=1\r\n", "\r\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n", " data_types=data_types, checks=checks, column_information=column_information)\r\n", " # execute\r\n", " actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n", " # evaluate\r\n", " ## validate null pk replaced\r\n", " self.assertequal(expected_result,actual_result,f\"[dataframes_v2:sparkdataframechecks:pk_values_to_replace] primary value replace 'checks' work expected\")\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_replace_value)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 11 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "**dev-info**: test_sparkdataframechecks_startchecks class implicitly test get_checks_for_phase filter_checks. therefore, reparate test case functions implemented." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test start_checks() sparkdataframechecks-class defined dataframes_v2 notebook\n", "class test_sparkdataframechecks_startchecks(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # mock check-methods -&gt; check functions called\n", " self.check_minimum_rowcount_patcher = patch.object(sparkdataframechecks, 'check_minimum_rowcount')\n", " self.mock_check_minimum_rowcount = self.check_minimum_rowcount_patcher.start()\n", " \n", " self.check_dataframe_headers_patcher = patch.object(sparkdataframechecks, 'check_dataframe_headers')\n", " self.mock_check_dataframe_headers = self.check_dataframe_headers_patcher.start()\n", "\n", " self.check_primary_keys_patcher = patch.object(sparkdataframechecks, 'check_primary_keys')\n", " self.mock_check_primary_keys = self.check_primary_keys_patcher.start()\n", "\n", " self.check_duplicate_primary_keys_patcher = patch.object(sparkdataframechecks, 'check_duplicate_primary_keys')\n", " self.mock_check_duplicate_primary_keys = self.check_duplicate_primary_keys_patcher.start()\n", "\n", " self.check_dataframe_datatypes_patcher = patch.object(sparkdataframechecks, 'check_dataframe_datatypes')\n", " self.mock_check_dataframe_datatypes = self.check_dataframe_datatypes_patcher.start()\n", "\n", " # mock validate_parameters_keys -&gt; since checks executed, need validate parameter-argument\n", " self.validate_parameter_keys_patcher = patch.object(sparkdataframechecks, 'validate_parameter_keys')\n", " self.mock_validate_parameter_keys = self.validate_parameter_keys_patcher.start() \n", "\n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \n", "\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " self.class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " # stop patcher-functions\n", " self.check_minimum_rowcount_patcher.stop()\n", " self.check_dataframe_headers_patcher.stop()\n", " self.check_primary_keys_patcher.stop()\n", " self.check_dataframe_datatypes_patcher.stop()\n", " self.check_duplicate_primary_keys_patcher.stop()\n", " self.load_dataframe_patcher.stop()\n", " self.validate_parameter_keys_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls start_checks() method\n", "\n", " # success: call start_checks() non-empty list checks phase 'landing'\n", " def test_startchecks_sparkdataframechecks_executelandingchecks(self):\n", " # preprocess\n", " # # set checks-argument non-empty list checks\n", " self.class_object.checks = ['landing_rows', 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n", " # # define arguments use calling start_checks() method\n", " phase = 'landing'\n", " parameters = 'parameter_object'\n", "\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call start_checks() method sparkdataframechecks valid set arguments\n", " self.class_object.start_checks(phase=phase, parameters=parameters)\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " self.mock_check_minimum_rowcount.assert_called_once()\n", " self.mock_check_dataframe_headers.assert_not_called()\n", " self.mock_check_primary_keys.assert_not_called()\n", " self.mock_check_dataframe_datatypes.assert_not_called()\n", " self.mock_check_duplicate_primary_keys.assert_not_called()\n", " self.mock_validate_parameter_keys.assert_called_once_with(parameters=parameters)\n", "\n", " # success: call start_checks() empty list checks phase 'landing'\n", " def test_startchecks_sparkdataframechecks_executelandingchecks_emptychecklist(self):\n", " # preprocess\n", " # # set checks-argument empty list checks\n", " self.class_object.checks = []\n", " # # define arguments use calling start_checks() method\n", " phase = 'landing'\n", " parameters = 'parameter_object'\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call start_checks() method sparkdataframechecks valid set arguments\n", " self.class_object.start_checks(phase=phase, parameters=parameters)\n", "\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " self.mock_check_minimum_rowcount.assert_not_called()\n", " self.mock_check_dataframe_headers.assert_not_called()\n", " self.mock_check_primary_keys.assert_not_called()\n", " self.mock_check_duplicate_primary_keys.assert_not_called()\n", " self.mock_check_dataframe_datatypes.assert_not_called()\n", " self.mock_validate_parameter_keys.assert_not_called()\n", "\n", " # success: call start_checks() clean class-instance empty list checks phase 'landing'\n", " def test_startchecks_sparkdataframechecks_executelandingchecks_localclassobject(self):\n", " # preprocess\n", " # # initialize clean class-instance use self.checks-argument defined initialisation\n", " # # dev-info: expect self.checks = [ 'header', 'data_type', 'primary_keys']\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " local_class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # # define arguments use calling start_checks() method\n", " phase = 'landing'\n", " parameters = 'parameter_object'\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call start_checks() method sparkdataframechecks valid set arguments\n", " self.class_object.start_checks(phase=phase, parameters=parameters)\n", "\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " self.mock_check_minimum_rowcount.assert_not_called()\n", " self.mock_check_dataframe_headers.assert_not_called()\n", " self.mock_check_primary_keys.assert_not_called()\n", " self.mock_check_duplicate_primary_keys.assert_not_called()\n", " self.mock_check_dataframe_datatypes.assert_not_called()\n", " self.mock_validate_parameter_keys.assert_not_called()\n", "\n", "\n", " # success: call start_checks() non-empty list checks phase 'landing'\n", " def test_startchecks_sparkdataframechecks_executerawchecks(self):\n", " # preprocess\n", " self.class_object.checks = ['landing_rows', 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n", " # # define arguments use calling start_checks() method\n", " phase = 'raw'\n", " parameters = 'parameter_object'\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call start_checks() method sparkdataframechecks valid set arguments\n", " self.class_object.start_checks(phase=phase, parameters=parameters)\n", "\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " self.mock_check_minimum_rowcount.assert_not_called()\n", " self.mock_check_dataframe_headers.assert_called_once()\n", " self.mock_check_primary_keys.assert_called_once()\n", " self.mock_check_duplicate_primary_keys.assert_called_once()\n", " self.mock_check_dataframe_datatypes.assert_called_once()\n", " self.mock_validate_parameter_keys.assert_called_once_with(parameters=parameters)\n", "\n", "\n", " # success: call start_checks() clean class-instance empty list checks phase 'landing'\n", " def test_startchecks_sparkdataframechecks_executerawchecks_localclassobject(self):\n", " # preprocess\n", " # # initialize clean class-instance use self.checks-argument defined initialisation\n", " # # dev-info: expect self.checks = [ 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys]\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " local_class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # # define arguments use calling start_checks() method\n", " phase = 'raw'\n", " parameters = 'parameter_object'\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call start_checks() method sparkdataframechecks valid set arguments\n", " local_class_object.start_checks(phase=phase, parameters=parameters)\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " self.mock_check_minimum_rowcount.assert_not_called()\n", " self.mock_check_dataframe_headers.assert_called_once()\n", " self.mock_check_primary_keys.assert_called_once()\n", " self.mock_check_duplicate_primary_keys.assert_called_once()\n", " self.mock_check_dataframe_datatypes.assert_called_once()\n", " self.mock_validate_parameter_keys.assert_called_once_with(parameters=parameters)\n", "\n", " # success: call start_checks() expect validate_parameter_keys() throw error invalid phase\n", " # test guarantees validate-function called inexplicitly\n", " def test_startchecks_sparkdataframechecks_failure_invalidphase(self):\n", " # preprocess\n", " # # define arguments use calling start_checks() method\n", " phase = 'invalid_phase'\n", " parameters = 'parameter_object'\n", "\n", " # # define expected return values\n", " expected_error = f\"the phase-argument '{str(phase)}' listed allowed phase list: ['landing', 'raw', 'silver']\"\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call start_checks() method sparkdataframechecks valid set arguments\n", " self.assertraises(valueerror) error:\n", " self.class_object.start_checks(phase=phase, parameters=parameters)\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes_v2:sparkdataframechecks:startchecks] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_error}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_startchecks)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 21 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test validate_phase() sparkdataframechecks-class defined dataframes_v2 notebook\n", "class test_sparkdataframechecks_validatephase(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \n", "\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " self.class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " # stop patcher-functions\n", " self.load_dataframe_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls validate_phase() method\n", "\n", " # success: call validate_phase() expect error thrown\n", " def test_validatephase_sparkdataframechecks_landingphase(self):\n", " # preprocess\n", " # # set checks-argument non-empty list checks\n", " # # define arguments use calling validate_phase() method\n", " phase = 'landing'\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_phase() method sparkdataframechecks valid set arguments\n", " self.class_object.validate_phase(phase=phase)\n", "\n", " # success: call validate_phase() expect error thrown\n", " def test_validatephase_sparkdataframechecks_rawphase(self):\n", " # preprocess\n", " # # set checks-argument non-empty list checks\n", " # # define arguments use calling validate_phase() method\n", " phase = 'raw'\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_phase() method sparkdataframechecks valid set arguments\n", " self.class_object.validate_phase(phase=phase)\n", "\n", " # success: call validate_phase() expect error thrown\n", " def test_validatephase_sparkdataframechecks_silverphase(self):\n", " # preprocess\n", " # # set checks-argument non-empty list checks\n", " # # define arguments use calling validate_phase() method\n", " phase = 'silver'\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_phase() method sparkdataframechecks valid set arguments\n", " self.class_object.validate_phase(phase=phase)\n", "\n", " # success: call validate_phase() expect error thrown\n", " def test_validatephase_sparkdataframechecks_invalidphase(self):\n", " # preprocess\n", " # # set checks-argument non-empty list checks\n", " # # define arguments use calling validate_phase() method\n", " phase = 'invalid_phase'\n", "\n", " # # define expected return values\n", " expected_error = f\"the phase-argument '{str(phase)}' listed allowed phase list: ['landing', 'raw', 'silver']\"\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call validate_phase() method sparkdataframechecks valid set arguments\n", " self.assertraises(valueerror) error:\n", " self.class_object.validate_phase(phase=phase)\n", "\n", " # evaluate\n", " # # check check functions (not) called\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes_v2:sparkdataframechecks:startchecks] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_error}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_validatephase)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 22 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test validate_parameter_keys() sparkdataframechecks-class defined dataframes_v2 notebook\n", "class test_sparkdataframechecks_validateparameterkeys(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " # stop patcher-functions\n", " self.load_dataframe_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls validate_parameter_keys() method\n", "\n", " # success: call validate_parameter_keys() expect error thrown\n", " def test_validatephase_sparkdataframechecks_emptyobject(self):\n", " # preprocess\n", " # # define arguments use calling validate_parameter_keys() method\n", " parameters = dict()\n", "\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_parameter_keys() method sparkdataframechecks valid set arguments\n", " class_object.validate_parameter_keys(parameters=parameters)\n", " \n", " # evaluate\n", " # validate parameters-attribute set\n", " self.asserttrue(hasattr(class_object, 'parameters'))\n", "\n", "\n", " # success: call validate_parameter_keys() expect error thrown\n", " def test_validatephase_sparkdataframechecks_validobject(self):\n", " # preprocess\n", " # # define arguments use calling validate_parameter_keys() method\n", " parameters = {'landing_rows_expected': -1, 'primary_key_columns': ['column_name']}\n", " \n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_parameter_keys() method sparkdataframechecks valid set arguments\n", " class_object.validate_parameter_keys(parameters=parameters)\n", " \n", " # evaluate\n", " # # validate parameter-attribute set\n", " self.asserttrue(hasattr(class_object, 'parameters'))\n", " actual_parameters = class_object.parameters\n", " self.assertequal(parameters, actual_parameters)\n", "\n", " # failure: call validate_parameter_keys() expect error thrown\n", " def test_validatephase_sparkdataframechecks_invalidobject(self):\n", " # preprocess\n", " # # define arguments use calling validate_parameter_keys() method\n", " parameters = {'invalid_key': -1, 'other_invalid_key': -1}\n", " expected_error = \"[genericfunctions] check_parameters-argument take following list give parameters: 'invalid_key, other_invalid_key'; allowed values: landing_rows_expected, primary_key_columns\"\n", " \n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_parameter_keys() method sparkdataframechecks valid set arguments\n", " self.assertraises(valueerror) error:\n", " class_object.validate_parameter_keys(parameters=parameters)\n", "\n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes_v2:sparkdataframechecks:startchecks] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_error}\")\n", "\n", "\n", " # failure: call validate_parameter_keys() expect error thrown\n", " def test_validatephase_sparkdataframechecks_invalidobject_2(self):\n", " # preprocess\n", " # # define arguments use calling validate_parameter_keys() method\n", " parameters = {'invalid_key': -1, 'landing_rows_expected': -1}\n", " expected_error = \"[genericfunctions] check_parameters-argument take following list give parameters: 'invalid_key'; allowed values: landing_rows_expected, primary_key_columns\"\n", " \n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call validate_parameter_keys() method sparkdataframechecks valid set arguments\n", " self.assertraises(valueerror) error:\n", " class_object.validate_parameter_keys(parameters=parameters)\n", "\n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes_v2:sparkdataframechecks:startchecks] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_error}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_validateparameterkeys)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 23 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test check_minimum_rowcount() sparkdataframechecks-class defined dataframes_v2 notebook\n", "class test_sparkdataframechecks_checkminimumrowcount(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " # preprocess\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframechecks/initialization/initialize_checks_object.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " column_names = ['date', 'string']\n", " data_types = ['timestamp', 'string']\n", " column_information = [none, none]\n", " checks = ['landing_rows']\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call __init__() method sparkdataframechecks valid set arguments\n", " self.class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls check_minimum_rowcount() method\n", "\n", " # success: call check_minimum_rowcount() expect error thrown\n", " def test_checkminimumrowcount_sparkdataframechecks_success(self):\n", " # preprocess\n", " # # define arguments use calling check_minimum_rowcount() method\n", " self.class_object.parameters = {'landing_rows_expected': 2}\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_minimum_rowcount() method sparkdataframechecks valid set arguments\n", " result = self.class_object.check_minimum_rowcount()\n", " \n", " # evaluate\n", " # validate parameters-attribute set\n", " self.assertequal(0, result)\n", "\n", " # failure: call check_minimum_rowcount() expect assertionerror thrown\n", " def test_checkminimumrowcount_sparkdataframechecks_failure_assertionerror(self):\n", " # preprocess\n", " # # define arguments use calling check_minimum_rowcount() method\n", " self.class_object.parameters = {'landing_rows_expected': 4}\n", " expected_error = f\"[dataframes_v2:sparkdataframechecks] minimum rows {self.class_object.parameters['landing_rows_expected']}, got 3\"\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_minimum_rowcount() method sparkdataframechecks valid set arguments\n", " self.assertraises(assertionerror) error:\n", " self.class_object.check_minimum_rowcount()\n", "\n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes_v2:sparkdataframechecks:startchecks] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_error}\")\n", "\n", " # failure: call check_minimum_rowcount() expect valueerror thrown\n", " def test_checkminimumrowcount_sparkdataframechecks_failure_valueerror(self):\n", " # preprocess\n", " # # define arguments use calling check_minimum_rowcount() method\n", " self.class_object.parameters = dict()\n", " expected_error = f\"[dataframes_v2:sparkdataframechecks] cannot check minimum rowcount without landing_rows_expected parameter\"\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_minimum_rowcount() method sparkdataframechecks valid set arguments\n", " self.assertraises(valueerror) error:\n", " self.class_object.check_minimum_rowcount()\n", "\n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes_v2:sparkdataframechecks:startchecks] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_error}\")\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_checkminimumrowcount)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 24 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test check_dataframe_headers() sparkdataframechecks-class defined dataframes_v2 notebook\n", "class test_sparkdataframechecks_checkdataframeheaders(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframechecks/initialization/initialize_checks_object.csv'\n", " header = true\n", " separator = ';'\n", " file_kind = 'csv'\n", " column_names = list()\n", " data_types = list()\n", " column_information = list()\n", " checks = ['headers']\n", "\n", " self.class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " source_path_c0 = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/sparkdataframechecks/initialization/initialize_checks_object_c0.csv'\n", " self.class_object_c0 = sparkdataframechecks(source_path=source_path_c0, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " self.class_object_headerless = sparkdataframechecks(source_path=source_path, header=false, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful expected_validate_argument_calls check_dataframe_headers() method\n", "\n", " # success: call check_dataframe_headers() correct column sequence\n", " def test_checkdataframeheaders_sparkdataframechecks_success(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['date', 'string']\n", " self.class_object.header = true\n", " self.class_object.file_kind = 'csv'\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " result = self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " \n", " self.assertequal(0, result)\n", "\n", " # success: call check_dataframe_headers() incorrect column sequence\n", " def test_checkdataframeheaders_sparkdataframechecks_failonsequence(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['string', 'date']\n", " self.class_object.header = true\n", " self.class_object.file_kind = 'csv'\n", " \n", " expected_error = f\"column sequence configuration ([string, date]) match column sequence dataframe ([date, string])\"\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " self.assertraises(configurationerror) error:\n", " self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.assertequal(expected_error, actual_message, f\"[dataframes_v2:sparkdataframechecks:checkdataframeheaders] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_message}\")\n", "\n", "\n", " # success: call check_dataframe_headers() incorrect sequence 'json' file_kind\n", " def test_checkdataframeheaders_sparkdataframechecks_jsonsequence(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['string', 'date']\n", " self.class_object.header = true\n", " self.class_object.source_kind = 'json'\n", " self.class_object.file_kind = 'parquet'\n", " \n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " result = self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " \n", " self.assertequal(0, result)\n", "\n", " # success: call check_dataframe_headers() invalid column names\n", " def test_checkdataframeheaders_sparkdataframechecks_failoncolumns(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['invalid', 'columnname']\n", " self.class_object.header = true\n", " self.class_object.file_kind = 'csv'\n", " \n", " expected_error = f\"header mismatch: actual ([date, string]) vs expected ([invalid, columnname]).\"\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " self.assertraises(configurationerror) error:\n", " self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.assertequal(expected_error, actual_message, f\"[dataframes_v2:sparkdataframechecks:checkdataframeheaders] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_message}\")\n", "\n", "\n", " # success: call check_dataframe_headers() column c0 correct sequence\n", " def test_checkdataframeheaders_sparkdataframechecks_c0succes(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object_c0.column_names = ['date', 'string']\n", " self.class_object_c0.header = true\n", " self.class_object_c0.file_kind = 'csv'\n", " \n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " result = self.class_object_c0.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " self.assertequal(result, 0)\n", " \n", " \n", " # success: call check_dataframe_headers() column c0 incorrect sequence\n", " def test_checkdataframeheaders_sparkdataframechecks_c0failonsequence(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object_c0.column_names = ['string', 'date']\n", " self.class_object_c0.header = true\n", " self.class_object_c0.file_kind = 'csv'\n", "\n", " expected_error = f\"column sequence configuration ([string, date]) match column sequence dataframe ([date, string]) (case: index column _c0)\"\n", "\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " self.assertraises(configurationerror) error:\n", " self.class_object_c0.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.assertequal(expected_error, actual_message, f\"[dataframes_v2:sparkdataframechecks:checkdataframeheaders] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_message}\")\n", "\n", " # success: call check_dataframe_headers() headerless file column names first row\n", " def test_checkdataframeheaders_sparkdataframechecks_failheaderless(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object_headerless.column_names = ['date', 'string']\n", " self.class_object_headerless.header = false\n", " self.class_object_headerless.file_kind = 'csv'\n", "\n", " expected_error = f\"file {self.class_object_headerless.source_path} expected contain header-line does\"\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " self.assertraises(configurationerror) error:\n", " self.class_object_headerless.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.assertequal(expected_error, actual_message, f\"[dataframes_v2:sparkdataframechecks:checkdataframeheaders] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_message}\")\n", "\n", " # success: call check_dataframe_headers() headerless file column names first row\n", " def test_checkdataframeheaders_sparkdataframechecks_successheaderless(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object_headerless.column_names = ['different', 'columnnames']\n", " self.class_object_headerless.header = false\n", " self.class_object_headerless.file_kind = 'csv'\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " result = self.class_object_headerless.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " self.assertequal(0, result)\n", "\n", " # success: call check_dataframe_headers() file columns optional\n", " def test_checkdataframeheaders_sparkdataframechecks_success_optionalcolumn(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['date', 'string', 'test']\n", " self.class_object.header = true\n", " self.class_object.file_kind = 'csv'\n", " self.class_object.column_information = [\n", " {\n", " none\n", " },\n", " {\n", " \"optional\": false\n", " },\n", " {\n", " \"optional\": true\n", " } \n", " ]\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " result = self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " self.assertequal(0, result)\n", "\n", "\n", " # failure: columns dataframe configuration\n", " def test_checkdataframeheaders_sparkdataframechecks_failure_missingconfig(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['date']\n", " self.class_object.header = true\n", " self.class_object.file_kind = 'csv'\n", " self.class_object.column_information = [\n", " {\n", " \"optional\": false\n", " }\n", " ]\n", "\n", "\n", " # execute\n", " # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " expected_error = f\"the following columns found source file configuration: [string]\"\n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", "\n", " self.assertraises(configurationerror) error:\n", " self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " # validate returned error expected error\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.assertequal(expected_error, actual_message, f\"[dataframes_v2:sparkdataframechecks:checkdataframeheaders] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_message}\")\n", "\n", "\n", " # failure: mandatory (non-optional) columns dataframe\n", " def test_checkdataframeheaders_sparkdataframechecks_failure_missingmandatorycolumn(self):\n", " # preprocess\n", " # # define arguments use calling check_dataframe_headers() method\n", " self.class_object.column_names = ['date', 'string', 'test', 'mandatory']\n", " self.class_object.header = true\n", " self.class_object.file_kind = 'csv'\n", " self.class_object.column_information = [\n", " {\n", " none\n", " },\n", " {\n", " \"optional\": false\n", " },\n", " {\n", " \"optional\": true\n", " },\n", " {\n", " \"optional\": false\n", " } \n", " ]\n", "\n", " # define expected error message\n", " expected_error = f\"the following columns non-optional configuration found source file: [mandatory]\"\n", " \n", " # execute\n", " # # [dataframes_v2:sparkdataframechecks]: call check_dataframe_headers() method sparkdataframechecks\n", " self.assertraises(configurationerror) error:\n", " self.class_object.check_dataframe_headers()\n", " \n", " # evaluate\n", " # # validate returned error expected error\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.assertequal(expected_error, actual_message, f\"[dataframes_v2:sparkdataframechecks:checkdataframeheaders] expected error match actual error: \\nexpected {expected_error} ; \\nactual {actual_message}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_checkdataframeheaders)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframechecks loaddataframe tests, something went wrong!\")" ], "execution_count": 25 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test check_pirmary_keys() method deltatable-class defined deploydeltatable_v3 notebook\n", "class test_sparkdataframechecks_checkprimarykeys(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # preprocess:\n", " # define expected schema metadata values\n", " # following schema expected returned\n", " self.schema = sql.types.structtype([\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"column_2\", sql.types.integertype(), true)\n", " ])\n", " \n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \n", " self.replace_value_patcher = patch.object(sparkdataframechecks, 'check_pk_values_to_replace')\n", " self.mock_replace_value = self.replace_value_patcher.start() \n", " self.mock_replace_value.return_value = 1\n", "\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\n", " checks = list()\n", " self.class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", "\n", "\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " # stop patcher-functions\n", " self.load_dataframe_patcher.stop()\n", " self.replace_value_patcher.stop()\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_pirmary_keys method\n", "\n", " # success: create table certain schema varchar(4) datatype\n", " # varchar(4) datatype expected part metadata column \n", " def test_checkprimarykey_sparkdataframechecks_successinteger(self):\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', 1),\n", " (none, 2),\n", " ('test', 3)\n", " ], schema=self.schema)\n", "\n", " self.class_object.dataframe = valid_dataframe\n", " self.class_object.parameters = {'primary_key_columns': ['column_2']}\n", "\n", " result = self.class_object.check_primary_keys()\n", " self.assertequal(0, result, \"[classes:checks:checkprimarykeys]: expected errors return value match expected value 0\")\n", "\n", " def test_checkprimarykey_sparkdataframechecks_successstring(self):\n", " expected_error = \"[checks:primarykeys] primary keys contain null values. number null values pk columns: {'column_1': 1}\"\n", " \n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', 1),\n", " ('test', none),\n", " ('test', 3)\n", " ], schema=self.schema)\n", "\n", " self.class_object.dataframe = valid_dataframe\n", " self.class_object.parameters = {'primary_key_columns': ['column_1']}\n", " \n", " result = self.class_object.check_primary_keys()\n", " self.assertequal(0, result, \"[classes:checks:checkprimarykeys]: expected errors return value match expected value 0\")\n", "\n", " def test_checkprimarykey_sparkdataframechecks_failinteger(self):\n", " expected_error = \"[checks:primarykeys] primary keys contain null values. number null values pk columns: {'column_2': 1}\"\n", " \n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', 1),\n", " (none, none),\n", " ('test', 3)\n", " ], schema=self.schema)\n", "\n", " self.class_object.dataframe = valid_dataframe\n", " self.class_object.parameters = {'primary_key_columns': ['column_2']}\n", "\n", " self.assertraises(assertionerror) error:\n", " self.class_object.check_primary_keys()\n", "\n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:checks:checkprimarykeys] expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", " def test_checkprimarykey_sparkdataframechecks_failstringcolumn(self):\n", " expected_error = \"[checks:primarykeys] primary keys contain null values. number null values pk columns: {'column_1': 1}\"\n", " \n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', 1),\n", " (none, none),\n", " ('test', 3)\n", " ], schema=self.schema)\n", "\n", " self.class_object.dataframe = valid_dataframe\n", " self.class_object.parameters = {'primary_key_columns': ['column_1']}\n", "\n", " self.assertraises(assertionerror) error:\n", " self.class_object.check_primary_keys()\n", "\n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:checks:checkprimarykeys] expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", "\n", " def test_checkprimarykey_sparkdataframechecks_noparameters(self):\n", " expected_error = \"[checks:primarykeys]: cannot execute primary_key check without list primary keys\"\n", " \n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', 1),\n", " ('test', none),\n", " ('test', 3)\n", " ], schema=self.schema)\n", "\n", " self.class_object.dataframe = valid_dataframe\n", " self.class_object.parameters = {}\n", "\n", " self.assertraises(valueerror) error:\n", " self.class_object.check_primary_keys()\n", "\n", " # evaluate\n", " # validate returned error matches expected error\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[classes:checks:checkprimarykeys] expected error match actual error: {expected_error} versus {actual_error} \")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_checkprimarykeys)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)" ], "execution_count": 10 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/dataframes_v2" ], "execution_count": 31 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\r\n", "# class test check_duplicate_primary_keys() method deltatable-class defined deploydeltatable_v3 notebook\r\n", "class test_sparkdataframechecks_checkduplicateprimarykeys(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define reusable schema metadata values\r\n", " self.schema = sql.types.structtype([\r\n", " sql.types.structfield(\"pk_column\", sql.types.stringtype(), true),\r\n", " sql.types.structfield(\"spare_column\", sql.types.stringtype(), true)\r\n", " ])\r\n", " \r\n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\r\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\r\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \r\n", "\r\n", " # mock reusable class-object \r\n", " source_path, header, separator, file_kind, column_names, data_types, column_information = mock(), mock(), mock(), mock(), mock(), mock(), mock()\r\n", " checks = list()\r\n", " self.class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\r\n", "\r\n", "\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown test case class\r\n", " # stop patcher-functions\r\n", " self.load_dataframe_patcher.stop()\r\n", " return\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls check_duplicate_primary_keys method\r\n", "\r\n", " # success: raise error duplicates single pk instance\r\n", " def test_checkduplicateprimarykey_sparkdataframechecks_success_singlepk(self):\r\n", " # preprocess\r\n", " # initialize dataframe duplicate values\r\n", " valid_dataframe = spark.createdataframe(data=[\r\n", " (12345, \"spare_value\"),\r\n", " (67890, \"spare_value\"),\r\n", " (54321, \"spare_value\")\r\n", " ], schema=self.schema)\r\n", "\r\n", " # define class-arguments\r\n", " self.class_object.dataframe = valid_dataframe\r\n", " self.class_object.parameters = {'primary_key_columns': ['pk_column']}\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " result = self.class_object.check_duplicate_primary_keys()\r\n", "\r\n", " # evaluate\r\n", " # expect function return 0\r\n", " self.assertequal(0, result, \"[classes:checks:checkduplicateprimarykeys]: 1. expected errors return value match expected value 0\")\r\n", "\r\n", " # success: raise error duplicates combined pk instance\r\n", " def test_checkduplicateprimarykey_sparkdataframechecks_success_combinedpk(self):\r\n", " # preprocess\r\n", " # initialize dataframe duplicate values\r\n", " valid_dataframe = spark.createdataframe(data=[\r\n", " (12345, \"spare_value\"),\r\n", " (67890, \"spare_value\"),\r\n", " (54321, \"spare_value\")\r\n", " ], schema=self.schema)\r\n", "\r\n", " # define class-arguments\r\n", " self.class_object.dataframe = valid_dataframe\r\n", " self.class_object.parameters = {'primary_key_columns': ['pk_column', 'spare_column']}\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " result = self.class_object.check_duplicate_primary_keys()\r\n", "\r\n", " # evaluate\r\n", " # expect function return 0\r\n", " self.assertequal(0, result, \"[classes:checks:checkduplicateprimarykeys]: 2. expected errors return value match expected value 0\")\r\n", "\r\n", "\r\n", " # failure: raise error duplicate pk values single pk instance\r\n", " def test_checkduplicateprimarykey_sparkdataframechecks_failure_duplicatesinglepk(self):\r\n", " # preprocess\r\n", " # initialize dataframe duplicate values\r\n", " valid_dataframe = spark.createdataframe(data=[\r\n", " (12345, \"spare_value\"),\r\n", " (67890, \"spare_value\"),\r\n", " (12345, \"spare_value\")\r\n", " ], schema=self.schema)\r\n", "\r\n", " # define expected error message\r\n", " expected_message = \"[checks:duplicatprimarykeys] duplicate primary keys found source file 1 instances\"\r\n", "\r\n", " # define class-arguments\r\n", " self.class_object.dataframe = valid_dataframe\r\n", " self.class_object.parameters = {'primary_key_columns': ['pk_column']}\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " self.assertraises(middlewareerror) error:\r\n", " self.class_object.check_duplicate_primary_keys()\r\n", " \r\n", " # evaluate\r\n", " # validate returned error expected error\r\n", " actual_error = str(error.exception)\r\n", " actual_error = ast.literal_eval(actual_error)\r\n", " actual_message = actual_error[\"custom_message\"]\r\n", " \r\n", " self.assertequal(actual_message, expected_message, f\"[classes:checks:checkduplicateprimarykeys]: 3. expected error message match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", "\r\n", " # failure: raise error duplicate pk values combined pk instance \r\n", " def test_checkduplicateprimarykey_sparkdataframechecks_failure_duplicatecombinedpk(self):\r\n", " # preprocess\r\n", " # initialize dataframe duplicate values\r\n", " valid_dataframe = spark.createdataframe(data=[\r\n", " (12345, \"spare_value\"),\r\n", " (12345, \"spare_value\"),\r\n", " (12345, \"spare_value2\"),\r\n", " (12345, \"spare_value3\"),\r\n", " (67890, \"spare_value\"),\r\n", " (67890, \"spare_value\"),\r\n", " (67890, \"spare_value2\"),\r\n", " (67890, \"spare_value3\")\r\n", " ], schema=self.schema)\r\n", "\r\n", " # define expected error message\r\n", " expected_message = \"[checks:duplicatprimarykeys] duplicate primary keys found source file 2 instances\"\r\n", "\r\n", " # define class-arguments\r\n", " self.class_object.dataframe = valid_dataframe\r\n", " self.class_object.parameters = {'primary_key_columns': ['pk_column', 'spare_column']}\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " self.assertraises(middlewareerror) error:\r\n", " self.class_object.check_duplicate_primary_keys()\r\n", " \r\n", " # evaluate\r\n", " # validate returned error expected error\r\n", " actual_error = str(error.exception)\r\n", " actual_error = ast.literal_eval(actual_error)\r\n", " actual_message = actual_error[\"custom_message\"]\r\n", " \r\n", " self.assertequal(actual_message, expected_message, f\"[classes:checks:checkduplicateprimarykeys]: 4. expected error message match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_checkduplicateprimarykeys)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error create delta table tests, something went wrong!\")\r\n", "# print(test_resuts)" ], "execution_count": 32 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: checks\n", "# class test check_values_to_replace() method deltatable-class defined checks_v3 notebook\n", "class test_checks_checkvaluestoreplace(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # preprocess:\n", " # define expected schema metadata values\n", " # following schema expected returned\n", " self.schema = sql.types.structtype([\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"column_2\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"column_3\", sql.types.stringtype(), true)\n", " ])\n", " \n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \n", " \n", " return\n", "\n", " def teardown(self):\n", " self.load_dataframe_patcher.stop()\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_values_to_replace method\n", "\n", " # success: create table certain schema varchar(4) datatype\n", " # varchar(4) datatype expected part metadata column \n", " def test_checkvaluestoreplace_checks_noreplace(self):\n", "\n", " source_path, header, separator, file_kind= mock(), mock(), mock(), mock()\n", " column_names = ['column_1', 'column_2', 'column_3'] \n", " data_types = ['string', 'timestamp', 'date']\n", " column_information = [none, none, none]\n", " checks = list()\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " \n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', '2024-08-22 00:00:00', '2024-08-22'),\n", " ('test', '2024-08-22 00:00:00', '2024-08-22'),\n", " ('test', '2024-08-22 00:00:00', '2024-08-22')\n", " ], schema=self.schema)\n", "\n", " class_object.dataframe = valid_dataframe\n", "\n", " class_object.check_values_to_replace()\n", "\n", " def test_checkvaluestoreplace_checks_invaliddatatype(self):\n", " source_path, header, separator, file_kind= mock(), mock(), mock(), mock()\n", " column_names = ['column_1', 'column_2', 'column_3'] \n", " data_types = ['string', 'timestamp', 'date']\n", " column_information = [none, none, none]\n", "\n", " checks = list()\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " \n", " \n", " valid_dataframe = spark.createdataframe(data=[\n", " ('test', '0000-01-01 00:00:00', '2024-08-22'),\n", " ('test', '2024-08-22 00:00:00', '0000-01-01'),\n", " ('test', '2024-08-22 00:00:00', '2024-08-22')\n", " ], schema=self.schema)\n", "\n", " class_object.dataframe = valid_dataframe\n", "\n", " result = class_object.check_values_to_replace()\n", " expected_result = spark.createdataframe(data=[\n", " ('test', none, '2024-08-22'),\n", " ('test', '2024-08-22 00:00:00', none),\n", " ('test', '2024-08-22 00:00:00', '2024-08-22')\n", " ],\n", " schema=['column_1', 'column_2', 'column_3']\n", " )\n", "\n", " self.assertequal([row row class_object.dataframe.collect()], expected_result.collect())\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_checks_checkvaluestoreplace)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "**dev-info**: test_sparkdataframechecks_checkdataframedatatypes class implicitly test cast_column count_miscasted_column_values. therefore, reparate test case functions implemented." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: sparkdataframechecks\n", "# class test check_dataframe_datatypes() method deltatable-class defined deploydeltatable_v3 notebook\n", "class test_sparkdataframechecks_checkdataframedatatypes(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # preprocess:\n", " # define expected schema metadata values\n", " # following schema expected returned\n", " self.schema = sql.types.structtype([\n", " sql.types.structfield(\"string_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"integer_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"varchar_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"timestamp_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"date_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"decimal_column\", sql.types.stringtype(), true)\n", " ])\n", " \n", " # mock load_dataframe() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_patcher = patch.object(sparkdataframechecks, 'load_dataframe')\n", " self.mock_load_dataframe = self.load_dataframe_patcher.start() \n", "\n", " # mock load_dataframe_with_schema() -&gt; need create dataframe object checks executed\n", " self.load_dataframe_with_schema_patcher = patch.object(sparkdataframechecks, 'load_dataframe_with_schema')\n", " self.mock_load_dataframe_with_schema = self.load_dataframe_with_schema_patcher.start() \n", "\n", " # mock replace_dataframe_columns() -&gt; need execute funtion replace columns\n", " self.replace_dataframecolumns__patcher = patch.object(sparkdataframechecks, 'replace_dataframe_columns')\n", " self.mock_replace_dataframe_columns = self.replace_dataframecolumns__patcher.start() \n", " return\n", "\n", " def teardown(self):\n", " # teardown test case class\n", " # stop patcher-functions\n", " self.load_dataframe_patcher.stop()\n", " self.load_dataframe_with_schema_patcher.stop()\n", " self.replace_dataframecolumns__patcher.stop()\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls check_dataframe_datatypes method\n", "\n", " # success: create table certain schema varchar(4) datatype\n", " # varchar(4) datatype expected part metadata column \n", " def test_checkdataframedatatypes_sparkdataframechecks_successfulcasting(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 2, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 3, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14)\n", " ], schema=self.schema)\n", "\n", " expected_result = {\"string_column\": 0, \"integer_column\": 0, \"varchar_column\": 0, \"timestamp_column\": 0, \"date_column\": 0, \"decimal_column\": 0}\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n", " data_types = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n", " column_information = [none, none, none, {\"format\": \"yyyy-mm-dd hh:mm:ss\"}, {\"format\": \"yyyy-mm-dd\"}, {\"locale\": \"en-us\"}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " class_object.dataframe = dataframe\n", "\n", " # execute\n", " actual_result = class_object.check_dataframe_datatypes()\n", "\n", " # evaluate\n", " self.assertequal(expected_result, actual_result)\n", "\n", " def test_checkdataframedatatypes_sparkdataframechecks_unsuccessfulcasting(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'values', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 'string', 'value', '1900-01-01 00:00:00', '1900-01-01', 3.1434),\n", " ('test', 3, 'values', '1900-01-01 00:00:00', '1900-01-01', 3.1234)\n", " ], schema=self.schema)\n", "\n", " expected_result = {\n", " 'string_column': 0, \n", " 'integer_column': assertionerror('casting error: null values column integer_column casting: could convert 1 columns'), \n", " 'varchar_column': assertionerror('casting error: null values column varchar_column casting: could convert 2 columns'), \n", " 'timestamp_column': 0, \n", " 'date_column': 0,\n", " 'decimal_column': 0\n", " }\n", " expected_error = f\"error casting: {expected_result}\"\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n", " data_types = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n", " column_information = [none, none, none, {\"format\": \"yyyy-mm-dd hh:mm:ss\"}, {\"format\": \"yyyy-mm-dd\"}, {\"locale\": \"en-us\"}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " class_object.dataframe = dataframe\n", "\n", " # execute\n", " self.assertraises(valueerror) error:\n", " actual_result = class_object.check_dataframe_datatypes()\n", "\n", "\n", " # evaluate\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error)\n", "\n", " def test_checkdataframedatatypes_sparkdataframechecks_optionalcolumn(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 2, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.1434),\n", " ('test', 3, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.1234)\n", " ], schema=self.schema)\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"optional_column\"]\n", " data_types = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n", " column_information = [none, none, none, {\"format\": \"yyyy-mm-dd hh:mm:ss\"}, {\"format\": \"yyyy-mm-dd\"}, {\"locale\": \"en-us\"}, {\"optional\": true}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " class_object.dataframe = dataframe\n", "\n", " class_object.column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"optional_column\"]\n", " expected_result = {\"string_column\": 0, \"integer_column\": 0, \"varchar_column\": 0, \"timestamp_column\": 0, \"date_column\": 0, \"decimal_column\": 0, \"optional_column\": 'optional_skip'}\n", "\n", " # execute\n", " actual_result = class_object.check_dataframe_datatypes()\n", "\n", " # evaluate\n", " self.assertequal(expected_result, actual_result)\n", "\n", " def test_checkdataframedatatypes_sparkdataframechecks_nonexistingdatatype(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 2, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 3, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.12)\n", " ], schema=self.schema)\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n", " data_types = [\"non_existing_datatype\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n", " column_information = [none, none, none, {\"format\": \"yyyy-mm-dd hh:mm:ss\"}, {\"format\": \"yyyy-mm-dd\"}, {\"locale\": \"en-us\"}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " class_object.dataframe = dataframe\n", "\n", " expected_error = f\"casting error casting column string_column data type non_existing_datatype: .*\"\n", " \n", " # execute &amp; evaluate\n", " self.assertraisesregex(valueerror, expected_error) error:\n", " actual_result = class_object.check_dataframe_datatypes()\n", "\n", "\n", " def test_checkdataframedatatypes_sparkdataframechecks_datatype_functioncalls_datetimestamp(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 2, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 3, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.12)\n", " ], schema=self.schema)\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n", " data_types = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n", " column_information = [none, none, none,{\"format\": \"yyyy-mm-dd hh:mm:ss\"}, {\"format\": \"yyyy-mm-dd\"}, {\"locale\": \"en-us\"}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information, debug=false)\n", " # set dataframe-argument object dataframe\n", " class_object.dataframe = dataframe\n", " \n", " # execute\n", " # run function test mocked functions to_timestamp, to_date, count_miscasted_column_values\n", " # patch.object(pyspark.sql.functions, \"to_timestamp\") mock_to_timestamp, \\\n", " # patch.object(pyspark.sql.functions, \"to_date\") mock_to_date, \\\n", " patch.object(sparkdataframechecks, \"count_miscasted_column_values\") mock_mismatch_count:\n", " class_object.check_dataframe_datatypes()\n", "\n", " # evaluate\n", " # mock_to_timestamp.assert_called_once()\n", " # mock_to_date.assert_called_once()\n", " self.mock_load_dataframe_with_schema.assert_not_called() # decommissioned function\n", "\n", " def test_checkdataframedatatypes_sparkdataframechecks_datatype_functioncalls_decimal(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks\n", " schema = sql.types.structtype([\n", " sql.types.structfield(\"string_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"integer_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"varchar_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"timestamp_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"date_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"decimal_column\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"decimal_column_dot\", sql.types.stringtype(), true)\n", " ])\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'value', '01-01-1999 00:00:00', '01-01-1999', 3.14, \"3,23\"),\n", " ('test', 2, 'value', '01-01-1999 00:00:00', '01-01-1999', 3.14, \"3,23\"),\n", " ('test', 3, 'value', '01-01-1999 00:00:00', '01-01-1999', 3.12, \"3,23\")\n", " ], schema=schema)\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"decimal_column_dot\"]\n", " data_types = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\", \"decimal(3,2)\"]\n", " column_information = [none, none, none,{\"format\": \"dd-mm-yyyy hh:mm:ss\"}, {\"format\": \"dd-mm-yyyy\"}, {\"locale\": \"en-us\"}, {\"locale\": \"fr-fr\"}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information, debug=false)\n", " class_object.dataframe = dataframe\n", " \n", " # execute\n", " # patch.object(pyspark.sql.functions, \"to_timestamp\") mock_to_timestamp, \\\n", " # patch.object(pyspark.sql.functions, \"to_date\") mock_to_date, \\\n", " patch.object(sparkdataframechecks, \"count_miscasted_column_values\") mock_mismatch_count:\n", " class_object.check_dataframe_datatypes()\n", "\n", " # evaluate\n", " # mock_to_timestamp.assert_called_once()\n", " # mock_to_date.assert_called_once()\n", " self.mock_load_dataframe_with_schema.assert_not_called() # decommissioned function\n", " self.mock_replace_dataframe_columns.assert_not_called() # decommissioned function\n", "\n", " def test_checkdataframedatatypes_sparkdataframechecks_columnmismatch(self):\n", " # preprocess\n", " # create reusable instance sparkdataframechecks -&gt; mock arguments except checks (needed add_mandatory_checks)\n", " dataframe = spark.createdataframe(data=[\n", " ('test', 1, 'values', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n", " ('test', 'string', 'value', '1900-01-01 00:00:00', '1900-01-01', 3.1434),\n", " ('test', 3, 'values', '1900-01-01 00:00:00', '1900-01-01', 3.1234)\n", " ], schema=self.schema)\n", "\n", " source_path, header, separator, file_kind = mock(), mock(), mock(), mock()\n", " column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n", " data_types = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n", " column_information = [none, none, none, {\"format\": \"yyyy-mm-dd hh:mm:ss\"}, {\"format\": \"yyyy-mm-dd\"}, {\"locale\": \"en-us\"}]\n", " checks = ['header']\n", " class_object = sparkdataframechecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n", " class_object.dataframe = dataframe\n", "\n", " class_object.column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"additional_column\"]\n", " expected_error = (f\"total number configured columns match total number source file columns: \\n{str(class_object.column_names)} ({len(class_object.column_names)}) versus {str(column_names)} ({len(column_names)})\").replace(\"'\", \"\")\n", "\n", " # execute\n", " self.assertraises(configurationerror) error:\n", " actual_result = class_object.check_dataframe_datatypes()\n", "\n", " # evaluate\n", " actual_error = str(error.exception)\n", " actual_error = ast.literal_eval(actual_error)\n", " actual_message = actual_error[\"custom_message\"]\n", " self.maxdiff = none\n", " self.assertequal(expected_error, actual_message)\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_sparkdataframechecks_checkdataframedatatypes)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\Test_DeltaTables.json">{ "name": "test_deltatables", "properties": { "folder": { "name": "modules/test_modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "34f4f246-8ea3-417b-8a8b-cb4d7a042eb0" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test deltatables\n", "this notebook test classes defined notebook deltatables.\n", "the structure notebook looks follows:\n", " \n", "- section: markdown cells used distinguish different classes tested.\n", "- code cells: code cell tests one specific method class tested section.\n", "\n", "**notes**\n", "1. end cell, piece commented code found. code uncommented tests specific cell need run. useful debugging. make sure that, completing pull request, pieces code commented again, tests otherwise executed multiple times test-job ci pipeline.\n", "2. approach taken always define setup teardown method beginning test case. done keep code consistent different classes. methods used entire test case use set variables function/method needs mocked/patched entire test case.\n", "3. test method uses similar structure, preprocess, execute, evaluate step. keep tests readable consistent." ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary methods unittest library" ] }, { "cell_type": "code", "source": [ "# import unittest-library \n", "import unittest\n", "# import specific functions library defined mock-method\n", "from unittest.mock import patch, magicmock, mock, any, call\n", "\n", "# import deltatable deltaoptimizebuilder class\n", "# reason: deltatables notebook occasions create class-instances deltatable/deltaoptimizebuilder. assertion executed validate whether returned objects actualy class.\n", "from delta.tables import deltatable, deltaoptimizebuilder\n", "import delta.tables dt" ], "execution_count": 1 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## initialize environment_code parameter\n", "in almost notebooks, reference parameter found. ci/cd pipelines overwrite specific environment-references defined notebooks. since notebooks deployed several environments, use environment_code parameter chosen. \n", "the environment_code used spark session environment_code-argument defined. argument linked core_configuration, found manage -&gt; apache spark configurations\n", "the use parameters arguments that, deployment, 1 place references need overwritten." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\n", "environment_code = ''" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\n", "\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 3 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# define global parameters use entire notebook\n", "container_name = 'unittest'\n", "storage_account = f'{env_code}dapstdala1'" ], "execution_count": 4 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" deltatables notebook" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 5 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 6 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## test class: deltatableclass_v2" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deltatableclassv2\n", "# class test __init__() deltatableclassv2-class defined deltatables notebook\n", "class test_deltatableclassv2_initialization(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls __init__() method\n", "\n", " # success: create instance class deltatableclassv2\n", " def test_inititalization_deltatableclass_objectcreation(self):\n", " # preprocess\n", "\n", " # execute\n", " # [deltatables:deltatableclassv2:__init__] run method test\n", " class_object = deltatableclass_v2(debug=true)\n", "\n", " # evaluate\n", " # validate debug-property set\n", " self.asserttrue(class_object.debug)\n", " # validate class_object instance class sparkdataframeclass\n", " self.assertisinstance(class_object, sparkdataframeclass)\n", "\n", " # success: mock init function validate function called initializing deltatableclass_v2 class-instance\n", " def test_inititalization_deltatableclass_functioncalls(self):\n", " # preprocess\n", " debug = false\n", "\n", " # execute\n", " # [deltatables:deltatableclassv2:__init__] run method test\n", " unittest.mock.patch.object(deltatableclass_v2, '__init__', return_value = none) mock_init:\n", " deltatableclass_v2(debug=debug)\n", " \n", " # evaluate\n", " # validate init function called debug-argument false\n", " mock_init.assert_called_once_with(debug=debug)\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deltatableclassv2_initialization)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass initialization tests, something went wrong!\")" ], "execution_count": 7 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "**dev-info**: test case tests method create_delta_table. load_delta_table tested, calls spark-native function real point writing test method. however, test case uses load_delta_table method validation-steps, tested implicitly case." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deltatableclassv2\n", "# class test create_delta_table() deltatableclassv2-class defined deltatables notebook\n", "class test_deltatableclassv2_createdeltatable(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls create_delta_table() method\n", "\n", " # success: create new table validate deltatable-object\n", " def test_createdeltatable_deltatableclass_writenewtable(self):\n", " # preprocess\n", " # define arguments used call method with\n", " table_name = 'deltatableclassv2/create_delta_table/new_table'\n", " column_objects = [\n", " { 'column_name': 'column_1', 'data_type': 'string', 'dimension':'pk' }, \n", " { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }, \n", " { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimension':'pk' }, \n", " { 'column_name': 'column_4', 'data_type': 'varchar(4)', 'dimension':'scd2' }\n", " ]\n", " table_description = 'test method create_delta_table'\n", " class_object = deltatableclass_v2()\n", "\n", " # catch: make sure table trying created exist already \n", " delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n", " mssparkutils.fs.exists(delta_table_path):\n", " clean_delta_table(env_code=self.env_code, delta_lake=container_name, table_name=table_name)\n", " \n", " # define expectations values calling method\n", " expected_partitioning_columns = list()\n", "\n", " # run\n", " # [deltatables:deltatableclassv2:create_delta_table] run method test\n", " class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects)\n", "\n", " # evaluate\n", " # validate new table indeed deltatable\n", " delta_table = class_object.load_delta_table(source_path=delta_table_path)\n", " self.assertisinstance(delta_table, deltatable, f\"[deltatables:deltatableclassv2:createdeltatable]: new table {table_name} class 'deltatable'...\")\n", " # validate partitioning done table\n", " actual_partitioning_columns = delta_table.detail().select(\"partitioncolumns\").collect()[0][0]\n", " self.assertequal(expected_partitioning_columns, actual_partitioning_columns, f\"[deltatables:deltatableclassv2:createdeltatable]: partitioning columns expected defined {table_name}\")\n", "\n", " # success: create new table partitioning_columns validate metadata deltatable contains partitioning-columns\n", " # additional check: write column_4 varchar validate metadata column contains __char_varchar_type_string-constraint\n", " def test_createdeltatable_deltatableclass_writepartitionedtable(self):\n", " # preprocess\n", " # define arguments used call method with\n", " table_name = 'deltatableclassv2/create_delta_table/partitioned_table'\n", " column_objects = [\n", " { 'column_name': 'column_1', 'data_type': 'string', 'dimension':'pk' }, \n", " { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }, \n", " { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimension':'pk' }, \n", " { 'column_name': 'column_4', 'data_type': 'varchar(4)', 'dimension':'scd2' }\n", " ]\n", " table_description = 'test method create_delta_table'\n", " partition_objects = [\n", " {'name': 'column_1', 'sequence': 1}\n", " ]\n", "\n", " class_object = deltatableclass_v2()\n", " delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n", " \n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(delta_table_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(env_code=self.env_code, delta_lake=container_name, table_name=table_name)\n", " \n", " # define expectations values calling method\n", " expected_metadata_column4 = {'column_4': 'metadata column column_4', '__char_varchar_type_string': 'varchar(4)'}\n", " expected_partitioning_columns = ['column_1']\n", "\n", " # execute\n", " # [deltatables:deltatableclassv2:create_delta_table] run method test\n", " class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\n", "\n", " # evaluate\n", " # validate new table indeed deltatable\n", " delta_table = class_object.load_delta_table(source_path=delta_table_path)\n", " self.assertisinstance(delta_table, deltatable, f\"[deltatables:deltatableclassv2:createdeltatable]: new table {table_name} class 'deltatable'...\")\n", " # validate table contains expected partitioning columns\n", " actual_partitioning_columns = delta_table.detail().select(\"partitioncolumns\").collect()[0][0]\n", " self.assertequal(expected_partitioning_columns, actual_partitioning_columns, f\"[deltatables:deltatableclassv2:createdeltatable]: partitioning columns match expectations: \\nexpected: {expected_partitioning_columns} \\nactual: {actual_partitioning_columns}\")\n", " # validate metadata column 4 contains __char_varchar_type_string-constraint\n", " actual_metadata_column4 = delta_table.todf().select(\"column_4\").schema[0].metadata\n", " self.assertequal(expected_metadata_column4, actual_metadata_column4, f\"[deltatables:deltatableclassv2:createdeltatable]: metadata column_4 match expectations: \\nexpected: {expected_metadata_column4} \\nactual: {actual_metadata_column4}\")\n", "\n", " # success: create new table partitioning_columns dateparts validate metadata deltatable contains datepart partitioning-columns\n", " # additional check: write column_4 varchar validate metadata column contains __char_varchar_type_string-constraint\n", " def test_createdeltatable_deltatableclass_writepartitionedtable_datepart(self):\n", " # preprocess\n", " # define arguments used call method with\n", " table_name = 'deltatableclassv2/create_delta_table/partitioned_table_datepart'\n", " column_objects = [\n", " { 'column_name': 'column_1', 'data_type': 'timestamp', 'dimension':'pk' }, \n", " { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }, \n", " { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimension':'pk' }, \n", " { 'column_name': 'column_4', 'data_type': 'varchar(4)', 'dimension':'scd2' }\n", " ]\n", " table_description = 'test method create_delta_table' \n", " partition_objects = [\n", " {'name': 'column_1', 'sequence': 1, 'datepart': 'year'},\n", " {'name': 'column_1', 'sequence': 2, 'datepart': 'month'}\n", "\n", " ]\n", " class_object = deltatableclass_v2()\n", " delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n", " \n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(delta_table_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(env_code=self.env_code, delta_lake=container_name, table_name=table_name)\n", "\n", " # define expectations values calling method\n", " expected_metadata_column4 = {'column_4': 'metadata column column_4', '__char_varchar_type_string': 'varchar(4)'}\n", " expected_partitioning_columns = ['p_year', 'p_month']\n", "\n", " # execute\n", " # [deltatables:deltatableclassv2:create_delta_table] run method test\n", " class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\n", "\n", " # evaluate\n", " ## check table created location\n", " deltatable = class_object.load_delta_table(source_path=delta_table_path)\n", " self.assertisinstance(dt.deltatable.forpath(spark, delta_table_path), deltatable, f\"[deltatables:deltatableclassv2:createdeltatable]: new table {table_name} class 'deltatable'...\")\n", " # validate table contains expected partitioning columns\n", " actual_partitioning_columns = deltatable.detail().select(\"partitioncolumns\").collect()[0][0]\n", " self.assertequal(expected_partitioning_columns, actual_partitioning_columns, f\"[deltatables:deltatableclassv2:createdeltatable]: partitioning columns match expectations: \\nexpected: {expected_partitioning_columns} \\nactual: {actual_partitioning_columns}\")\n", " # validate metadata column 4 contains __char_varchar_type_string-constraint\n", " actual_metadata_column4 = deltatable.todf().select(\"column_4\").schema[0].metadata\n", " self.assertequal(expected_metadata_column4, actual_metadata_column4, f\"[deltatables:deltatableclassv2:createdeltatable]: metadata column_4 match expectations: \\nexpected: {expected_metadata_column4} \\nactual: {actual_metadata_column4}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deltatableclassv2_createdeltatable)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass createdeltatable tests, something went wrong!\")" ], "execution_count": 8 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deltatableclassv2\n", "# class test add_notnull_constraints() deltatableclassv2-class defined deltatables notebook\n", "class test_deltatableclassv2_addnontnullconstraints(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_notnull_constraints() method\n", " \n", " # success: add not_null constraints delta table\n", " def test_addnontnullconstraints_deltatableclass_addconstraintstotable(self):\n", " # preprocess\n", " # define arguments used call method with\n", " # note: new table created first make sure table actually exist adding constraints\n", " table_name = 'deltatableclassv2/add_notnull_constraints/new_table'\n", " column_objects = [\n", " { 'column_name': 'column_1', 'data_type': 'string', 'dimension':'pk' }, \n", " { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }, \n", " { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimension':'pk' }, \n", " { 'column_name': 'column_4', 'data_type': 'varchar(4)', 'dimension':'scd2' }\n", " ]\n", " table_description = 'test method add_notnull_constraints'\n", "\n", " column_dimensions = {'column_1': 'pk', 'column_2' :'scd2', 'column_3': 'pk', 'column_4': 'scd2'}\n", " class_object = deltatableclass_v2()\n", " delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n", " \n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(delta_table_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(self.env_code, container_name, table_name)\n", "\n", " # create empty delta table first adding constraints\n", " class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects)\n", "\n", " # define expectations values calling method\n", " expected_constraint_names = ['delta.constraints.pk__notnull_column_1', 'delta.constraints.pk__notnull_column_3']\n", " expected_constraint_values = ['column_1 null', 'column_3 null']\n", "\n", " # execute\n", " # [deltatables:deltatableclassv2:add_notnull_constraints] run method test\n", " class_object.add_notnull_constraints(source_path=delta_table_path, column_dimensions=column_dimensions)\n", "\n", "\n", " # evaluate\n", " # get list constraints added table\n", " constraints = spark.sql(f\"describe detail delta.`{delta_table_path}`\").select('properties').collect()[0][0]\n", " actual_constraint_names = list(constraints.keys())\n", " actual_constraint_values = list(constraints.values())\n", " # validate names values constraints match expectations\n", " self.assertequal(expected_constraint_names, actual_constraint_names, f\"[deploydeltatables:addnotnullconstraints] constraints initialised expected: actual = {actual_constraint_names} versus expected = {expected_constraint_names}\")\n", " self.assertequal(expected_constraint_values, actual_constraint_values, f\"[deploydeltatables:addnotnullconstraints] constraints initialised expected: actual = {actual_constraint_values} versus expected = {expected_constraint_values}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deltatableclassv2_addnontnullconstraints)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass addnontnullconstraints tests, something went wrong!\")" ], "execution_count": 9 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/deltatables" ], "execution_count": 10 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deltatableclassv2\r\n", "# class test validate_columnnames() deltatableclassv2-class defined deltatables notebook\r\n", "class test_deltatableclassv2_validatecolumnnames(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " # define env_code parameter use throughout class\r\n", " self.env_code = env_code\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown test case\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls validate_columnnames() method\r\n", " \r\n", " # success: add columns non-partitioned table\r\n", " def test_validatecolumnnames_deltatableclass_addcolumnstononpartitionedtable(self):\r\n", " # preprocess\r\n", " # create empty delta table\r\n", " delta_table_object = deltatableclass_v2()\r\n", "\r\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_unpartitioned_table'\r\n", " table_description = 'test schema evolution'\r\n", " column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", "\r\n", " delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects)\r\n", " \r\n", " new_column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", " # define expectations\r\n", " expected_columns = ['column_1', 'column_2', 'column_3']\r\n", " # execute\r\n", " # run method test\r\n", " delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=new_column_objects)\r\n", " # evaluate\r\n", " # validate delta table contains 3 columns (excluding technical fields)\r\n", " delta_table = delta_table_object.load_delta_table(source_path=destination_path)\r\n", " table_columns = delta_table.todf().columns\r\n", " \r\n", " actual_columns = []\r\n", " expected_column expected_columns:\r\n", " expected_column table_columns:\r\n", " actual_columns.append(expected_column)\r\n", "\r\n", " self.assertequal(expected_columns, actual_columns, f'[dataframes:sparkdataframeclass:writedataframe] expected columns appended delta table, missing columns: \\nexpected {expected_columns} \\nmissing {actual_columns}')\r\n", " \r\n", " # success: add columns partitioned table\r\n", " def test_validatecolumnnames_deltatableclass_addcolumnstopartitionedtable(self):\r\n", " # preprocess\r\n", " # create empty delta table\r\n", " delta_table_object = deltatableclass_v2()\r\n", "\r\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_partitioned_table'\r\n", " table_description = 'test schema evolution'\r\n", " column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", " partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\r\n", "\r\n", " delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\r\n", " \r\n", " new_column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", " # define expectations\r\n", " expected_columns = ['column_1', 'column_2', 'column_3']\r\n", " # execute\r\n", " # run method test\r\n", " delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=new_column_objects)\r\n", " # evaluate\r\n", " # validate delta table contains 3 columns (excluding technical fields)\r\n", " delta_table = delta_table_object.load_delta_table(source_path=destination_path)\r\n", " table_columns = delta_table.todf().columns\r\n", " \r\n", " actual_columns = []\r\n", " expected_column expected_columns:\r\n", " expected_column table_columns:\r\n", " actual_columns.append(expected_column)\r\n", "\r\n", " self.assertequal(expected_columns, actual_columns, f'[dataframes:sparkdataframeclass:writedataframe] expected columns appended delta table, missing columns: \\nexpected {expected_columns} \\nmissing {actual_columns}')\r\n", " \r\n", " # success: add columns non-empty table\r\n", " def test_validatecolumnnames_deltatableclass_addcolumnstononemptytable(self):\r\n", " # preprocess\r\n", " # create empty delta table\r\n", " delta_table_object = deltatableclass_v2()\r\n", "\r\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_nonempty_table'\r\n", " table_description = 'test schema evolution'\r\n", " column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", " partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\r\n", "\r\n", " delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\r\n", " delta_table = delta_table_object.load_delta_table(destination_path)\r\n", " # add data delta table\r\n", " dataframe = spark.createdataframe(\r\n", " data=[\r\n", " [\"foo\", \"foo\", none, none, none, none, none, none, 1, 1, 1],\r\n", " [\"bar\", \"bar\", none, none, none, none, none, none, 1, 1, 1]\r\n", " ],\r\n", " schema=\"column_1 string, column_2 string, t_load_date_raw string, t_load_date_silver string, t_extract_date string, t_update_date string, t_insert_date string, t_file_name string, t_plan_id string, t_task_id string, t_file_id string\"\r\n", " )\r\n", "\r\n", " pk_columns_dict = {\"column_1\": \"column_1\"}\r\n", " column_names_dict = {\r\n", " \"column_1\": \"column_1\", \r\n", " \"column_2\": \"column_2\",\r\n", " \"t_load_date_raw\": \"t_load_date_raw\", \r\n", " \"t_load_date_silver\": \"t_load_date_silver\", \r\n", " \"t_extract_date\": \"t_extract_date\", \r\n", " \"t_update_date\": \"t_update_date\", \r\n", " \"t_insert_date\": \"t_insert_date\", \r\n", " \"t_file_name\": \"t_file_name\", \r\n", " \"t_plan_id\":\"t_plan_id\" , \r\n", " \"t_task_id\":\"t_task_id\" , \r\n", " \"t_file_id\":\"t_file_id\"\r\n", " }\r\n", "\r\n", " mergefile(delta_table, dataframe, pk_columns_dict=pk_columns_dict, column_names_dict=column_names_dict, target_options={\"partitioning\": [{\"name\": \"column_1\"}]})\r\n", " \r\n", " # add new columns delta table\r\n", " new_column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_3\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", " # define expectations\r\n", " expected_columns = ['column_1', 'column_2', 'column_3']\r\n", " expected_count = 2\r\n", " # execute\r\n", " # run method test\r\n", " delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=new_column_objects)\r\n", " # evaluate\r\n", " # validate delta table contains 3 columns (excluding technical fields)\r\n", " delta_table = delta_table_object.load_delta_table(source_path=destination_path)\r\n", " table_columns = delta_table.todf().columns\r\n", " actual_count = delta_table.todf().count()\r\n", " \r\n", " actual_columns = []\r\n", " expected_column expected_columns:\r\n", " expected_column table_columns:\r\n", " actual_columns.append(expected_column)\r\n", "\r\n", " self.assertequal(expected_columns, actual_columns, f'[dataframes:sparkdataframeclass:writedataframe] expected columns appended delta table, missing columns: \\nexpected {expected_columns} \\nmissing {actual_columns}')\r\n", " self.assertequal(expected_count, actual_count, f'[dataframes:sparkdataframeclass:writedataframe] expected number columns match actual number columns: \\nexpected {expected_count} \\actual {actual_count}')\r\n", "\r\n", " # success: columns need added\r\n", " \r\n", " def test_validatecolumnnames_deltatableclass_functioncalls(self):\r\n", " # preprocess\r\n", " # create empty delta table\r\n", " delta_table_object = deltatableclass_v2()\r\n", "\r\n", " destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_nonempty_table'\r\n", " table_description = 'test schema evolution'\r\n", " column_objects = [\r\n", " {\"column_name\": \"column_1\", \"dimension\": \"pk\", \"data_type\": \"string\"},\r\n", " {\"column_name\": \"column_2\", \"dimension\": \"scd2\", \"data_type\": \"string\"}\r\n", " ]\r\n", " partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\r\n", "\r\n", " delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\r\n", " \r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " patch.object(sparkdataframeclass, 'create_dataframe') mock_create_dataframe, \\\r\n", " patch.object(sparkdataframeclass, 'write_dataframe') mock_write_dataframe:\r\n", " delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=column_objects)\r\n", "\r\n", " # evalute\r\n", " # validate mocked functions called\r\n", " mock_write_dataframe.assert_not_called()\r\n", " mock_create_dataframe.assert_not_called()\r\n", " \r\n", " def test_mergefile_count_amount_of_rows (self):\r\n", " # preprocess\r\n", " # initialize variables\r\n", " inserted_row_start_expected='3'\r\n", " updated_row_start_expected='0'\r\n", " source_row_start_expected='3'\r\n", " inserted_row_end_expected='1'\r\n", " updated_row_end_expected='2'\r\n", " source_row_end_expected='3'\r\n", " path=\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"count_test\")\r\n", " delta_schema = structtype([structfield('id', stringtype(),true), structfield('info', stringtype(), false)])\r\n", " old_alias = 'test'\r\n", " new_alias = 'debug'\r\n", " pk_columns_dict = {'id': 'id'}\r\n", " column_names_dict = {'id': 'id', \"info\": \"info\"} \r\n", " #create dataframe\r\n", " deltatable=spark.createdataframe([],delta_schema)\r\n", " deltatable.write.mode('overwrite').format(\"delta\").save(path)\r\n", " delta_table_class = deltatableclass_v2(debug=true)\r\n", " #loading first deltatable\r\n", " deltatable = delta_table_class.load_delta_table(source_path=path)\r\n", " dataframe=spark.createdataframe(\r\n", " data=[\r\n", " (\"column1\",\"valeur1\"),\r\n", " (\"column2\",\"valeur2\"),\r\n", " (\"column3\",\"valeur3\")\r\n", " ],\r\n", " schema=\"id string, info string\"\r\n", " )\r\n", " #merge result\r\n", " deltatable.alias(old_alias) \\\r\n", " .merge(\r\n", " dataframe.alias(new_alias), pk_match(old_alias, new_alias, pk_columns_dict)) \\\r\n", " .whenmatchedupdate(set=columnmatch(column_names_dict, new_alias)) \\\r\n", " .whennotmatchedinsert(values = nomatch(column_names_dict, new_alias)) \\\r\n", " .execute()\r\n", " # execute\r\n", " # run function test\r\n", " inserted_row_start_actual,updated_row_start_actual,source_row_start_actual=delta_table_class.count_delta_table()\r\n", " #loading second deltatable\r\n", " dataframe=spark.createdataframe(\r\n", " data=[\r\n", " (\"column2\",\"valeur1\"),\r\n", " (\"column3\",\"valeur2\"),\r\n", " (\"column4\",\"valeur3\")\r\n", " ],\r\n", " schema=\"id string, info string\"\r\n", " )\r\n", " #merge result\r\n", " deltatable.alias(old_alias) \\\r\n", " .merge(\r\n", " dataframe.alias(new_alias), pk_match(old_alias, new_alias, pk_columns_dict)) \\\r\n", " .whenmatchedupdate(set=columnmatch(column_names_dict, new_alias)) \\\r\n", " .whennotmatchedinsert(values = nomatch(column_names_dict, new_alias)) \\\r\n", " .execute()# execute\r\n", " # run function test\r\n", " inserted_row_end_actual,updated_row_end_actual,source_row_end_actual=delta_table_class.count_delta_table()\r\n", " # evalute\r\n", " self.assertequal(inserted_row_start_expected, inserted_row_start_actual, f\"[deltatables:deltatableclassv2:count_delta_table]: insert merge count match expectations: \\nexpected: {inserted_row_start_expected} \\nactual: {inserted_row_start_actual}\")\r\n", " self.assertequal(updated_row_start_expected, updated_row_start_actual, f\"[deltatables:deltatableclassv2:count_delta_table]: update merge count match expectations: \\nexpected: {updated_row_start_expected} \\nactual: {updated_row_start_actual}\")\r\n", " self.assertequal(source_row_start_expected, source_row_start_actual, f\"[deltatables:deltatableclassv2:count_delta_table]: merge count match expectations: \\nexpected: {source_row_start_expected} \\nactual: {source_row_start_actual}\")\r\n", " self.assertequal(inserted_row_end_expected, inserted_row_end_actual, f\"[deltatables:deltatableclassv2:count_delta_table]: insert merge count match expectations: \\nexpected: {inserted_row_end_expected} \\nactual: {inserted_row_end_actual}\")\r\n", " self.assertequal(updated_row_end_expected, updated_row_end_actual, f\"[deltatables:deltatableclassv2:count_delta_table]: update merge count match expectations: \\nexpected: {updated_row_end_expected} \\nactual: {updated_row_end_actual}\")\r\n", " self.assertequal(source_row_end_expected, source_row_end_actual, f\"[deltatables:deltatableclassv2:count_delta_table]: merge count match expectations: \\nexpected: {source_row_end_expected} \\nactual: {source_row_end_actual}\")\r\n", " \r\n", " \r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deltatableclassv2_validatecolumnnames)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error sparkdataframeclass validatecolumnnames tests, something went wrong!\")" ], "execution_count": 12 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## test class: optimizedeltatable" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: optimizedeltatable\n", "# class test __init__() optimizedeltatable-class defined deltatables notebook\n", "class test_optimizedeltatable_initialization(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls __init__() method\n", " \n", " # success: initialize optimizedeltatable class instance\n", " # expect creation deltatable deltaoptimizebuilder class instance class-arguments\n", " def test_initization_optimizedeltatable_success(self):\n", " # preprocess\n", " # define arguments used call method with\n", " table_name = 'optimizedeltatable/initialization/load_delta_table'\n", "\n", " # create empty delta table silver container -&gt; table loaded using load_delta_table function\n", " column_objects = [\n", " { 'column_name': 'column_1', 'data_type': 'timestamp', 'dimension':'pk' }, \n", " { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }\n", " ]\n", " table_description = 'test optimizedeltatable class initialization'\n", " class_object = deltatableclass_v2()\n", "\n", " delta_table_path = f'abfss://silver@{storage_account}.dfs.core.windows.net/{table_name}'\n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(delta_table_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(self.env_code, container_name, table_name)\n", "\n", " class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=[])\n", "\n", " # execute\n", " # [dataframes:optimizedeltatable:__init__] run method test\n", " class_object = optimizedeltatable(table_name=table_name, env_code=self.env_code)\n", "\n", " # evaluate\n", " # validate delta_table_path-argument initialised\n", " self.assertisnotnone(class_object.delta_table_path, f\"[deltatables:optimizedeltatable:__init__] argument delta_table_path initialized\")\n", " self.assertequal(delta_table_path, class_object.delta_table_path, f\"[deltatables:optimizedeltatable:__init__] argument delta_table_path initialized correctly: \\nexpected {delta_table_path} \\nactual: {class_object.delta_table_path}\")\n", " # validate instance class deltatable initialised\n", " self.assertisnotnone(class_object.delta_table, f\"[deltatables:optimizedeltatable:__init__] argument delta_table initialized\")\n", " self.assertisinstance(class_object.delta_table, deltatable, f\"[deltatables:optimizedeltatable:__init__] argument delta_table initialized deltatable instance\")\n", " # validate instance class deltaoptimizebuilder initialised\n", " self.assertisnotnone(class_object.delta_table_optimized, f\"[deltatables:optimizedeltatable:__init__] argument delta_table_optimized initialized\")\n", " self.assertisinstance(class_object.delta_table_optimized, deltaoptimizebuilder, f\"[deltatables:optimizedeltatable:__init__] argument delta_table_optimized initialized deltaoptimizebuilder instance\")\n", "\n", " # success: validate correct functions called initialization\n", " def test_initialization_optimizedeltatable_function_calls(self):\n", " # preprocess\n", "\n", " # execute\n", " # [dataframes:optimizedeltatable:__init__] run method test mocked functions\n", " patch.object(deltatable, 'forpath') mock_forpath, \\\n", " patch.object(optimizedeltatable, 'optimize_deltatable') mock_optimize_deltatable:\n", " class_object = optimizedeltatable(table_name='test', env_code=self.env_code)\n", "\n", " # evaluate\n", " # validate expected functions called expected arguments\n", " mock_forpath.assert_called_once()\n", " mock_forpath.assert_called_once_with(any, class_object.delta_table_path)\n", " mock_optimize_deltatable.assert_called_once()\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_optimizedeltatable_initialization)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass initialization tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "**dev-info**: test case test optimize_table_storage function, calls compat_deltatable z_order_deltatable. last 2 methods, one additional test added validate error thrown deltaoptimizebuilder object present. separate test case defined methods, call internal spark functions necessarily contain logic test." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: optimizedeltatable_initialisation\n", "# class test optimize_table_storage function (and related functions) optimizedeltatable defined dataframes notebook\n", "class test_optimizedeltatable_optimizetablestorage(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", " def setup(self):\n", " self.env_code = env_code\n", " \n", " # create empty delta table silver container -&gt; table loaded using load_delta_table function\n", " self.table_name = 'optimizedeltatable/optimize_table_storage/load_delta_table'\n", " silver_container = 'silver'\n", " delta_table_path = f'abfss://{silver_container}@{storage_account}.dfs.core.windows.net/{self.table_name}'\n", " column_objects = [\n", " { 'column_name': 'column_1', 'data_type': 'timestamp', 'dimension':'pk' }, \n", " { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }\n", " ]\n", " table_description = 'test optimizedeltatable class optimize_table_storage'\n", "\n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(delta_table_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(self.env_code, silver_container, self.table_name)\n", " \n", " class_object = deltatableclass_v2()\n", " class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=[])\n", "\n", "\n", " return\n", "\n", "\n", " def teardown(self):\n", " return\n", "\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls optimize_table_storage() method\n", "\n", " # success: call method test without z-order list validate set functions called set arguments\n", " def test_optimizetablestorage_optimizedeltatable_function_calls(self):\n", " # preprocess\n", " # define class-object used call method with\n", " class_object = optimizedeltatable(table_name='test', env_code=self.env_code)\n", "\n", " # execute\n", " # [dataframes:optimizedeltatable:optimize_table_storage]: execute method test\n", " patch.object(deltaoptimizebuilder, 'executecompaction') mock_compaction, \\\n", " patch.object(deltaoptimizebuilder, 'executezorderby') mock_zorderby:\n", " class_object.optimize_table_storage()\n", " \n", " # evaluate\n", " # [dataframes:optimizedeltatable] validate underlying functions called expected\n", " mock_compaction.assert_called_once()\n", " mock_zorderby.assert_not_called()\n", "\n", " # success: call method test z-order list validate set functions called set arguments\n", " def test_optimizetablestorage_optimizedeltatable_function_calls(self):\n", " # preprocess\n", " class_object = optimizedeltatable(table_name=self.table_name, env_code=self.env_code)\n", "\n", " # execute\n", " # [dataframes:optimizedeltatable] execute method test\n", " patch.object(deltaoptimizebuilder, 'executecompaction') mock_compaction, \\\n", " patch.object(deltaoptimizebuilder, 'executezorderby') mock_zorderby:\n", " class_object.optimize_table_storage(['test'])\n", " \n", " # evaluate\n", " # [dataframes:optimizedeltatable] validate underlying functions called expected\n", " mock_compaction.assert_called_once()\n", " mock_zorderby.assert_called_once_with(z_order_columns=['test'])\n", "\n", " # failure: throw error class-object contain deltaoptimizebuilder object calling compact_deltatable()\n", " def test_compactdeltatable_optimizedeltatable_nonexist_deltaoptimizebuilder(self):\n", " # preprocess\n", " # create class_object set delta_table_optimized none\n", " class_object = optimizedeltatable(table_name=self.table_name, env_code=self.env_code)\n", " class_object.delta_table_optimized = none\n", "\n", " # define error expected thrown \n", " expected_error = \"[dataframes:optimizedeltatable] delta_table_optimized member class deltaoptimizebuilder compaction\"\n", " \n", " # execute\n", " # [dataframes:optimizedeltatable:compact_deltatable] execute method test\n", " self.assertraises(typeerror) error:\n", " class_object.compact_deltatable()\n", " \n", " # evaluate\n", " # validate expected error matches actual\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:optimizedeltatable:compact_deltatable] expected error match actual: {expected_error} versus {actual_error}\")\n", "\n", " # failure: throw error class-object contain deltaoptimizebuilder object calling z_order_deltatable()\n", " def test_zorderdeltatable_optimizedeltatable_nonexist_deltaoptimizebuilder(self):\n", " # preprocess\n", " # create class_object set delta_table_optimized none\n", " class_object = optimizedeltatable(table_name=self.table_name, env_code=self.env_code)\n", " class_object.delta_table_optimized = none\n", "\n", " # define error expected thrown \n", " expected_error = \"[dataframes:optimizedeltatable] delta_table_optimized member class deltaoptimizebuilder z-order\"\n", " \n", " # execute\n", " # [dataframes:optimizedeltatable:z_order_deltatable] execute method test\n", " self.assertraises(typeerror) error:\n", " class_object.z_order_deltatable(z_order_columns=list())\n", " \n", " # evaluate\n", " # validate expected error matches actual\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"[dataframes:optimizedeltatable:z_order_deltatable] expected error match actual: {expected_error} versus {actual_error}\")\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_optimizedeltatable_optimizetablestorage)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\") " ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## test class: deploydeltatable" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deploydeltatable\n", "# class test __init__() deploydeltatable-class defined deltatables notebook\n", "class test_deploydeltatable_initialization(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls __init__() method\n", " \n", " # success: initialize deploydeltatable class instance\n", " # expect creation deltatable deltaoptimizebuilder class instance class-arguments\n", " def test_initization_deploydeltatable_success(self):\n", " # preprocess\n", " # create empty delta table silver container -&gt; table loaded using load_delta_table function\n", " table_name = 'deploydeltatable/initialization/load_delta_table'\n", " table_description = 'initialize deploydeltatable class'\n", " column_info = \"\"\"[\n", " {'column_name': 'test', 'dimension': 'pk', 'data_type': 'string'},\n", " {'column_name': 'test2', 'dimension': 'scd2', 'data_type': 'string'},\n", " {'column_name': 'test3', 'dimension': 'scd2', 'data_type': 'string'}\n", " ]\"\"\"\n", " env_code = self.env_code\n", " partitioning = list() \n", "\n", " expected_silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, table_name) \n", "\n", " # execute\n", " # [dataframes:deploydeltatable] run method test\n", " class_object = deploydeltatable(\n", " table_name=table_name, \n", " storage_account=storage_account, \n", " container_name=container_name, \n", " table_description=table_description, \n", " column_info=column_info,\n", " partitioning=partitioning\n", " )\n", " \n", " # evaluate\n", " # validate silver_path matches expectations\n", " self.assertisnotnone(class_object.silver_path)\n", " self.assertequal(expected_silver_path, class_object.silver_path)\n", " # validate column_objects converted list\n", " self.asserttrue(isinstance(class_object.column_objects, list))\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deploydeltatable_initialization)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass initialization tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deploydeltatable\n", "# class test __init__() deploydeltatable-class defined deltatables notebook\n", "class test_deploydeltatable_initialization(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # ------------------------------------------------------------------------------------------------------------------\n", " \n", " def setup(self):\n", " # define env_code parameter use throughout class\n", " self.env_code = env_code\n", "\n", " # mock function mssparuktils.fs.exist\n", " # function check delta table already exists\n", " # test-function mock output function test functions called depending output (true/false)\n", " self.mssparkutils_fs_exists_patcher = patch.object(mssparkutils.fs, 'exists')\n", " self.mock_mssparkutils_fs_exists = self.mssparkutils_fs_exists_patcher.start()\n", "\n", " # mock functions related table creation validation\n", " # functions part deltatableclass_v2 already tested\n", " # depending output mssparkutils.fs.exist, certain methods need called whilst others be\n", " self.create_delta_table_patcher = patch.object(deltatableclass_v2, 'create_delta_table')\n", " self.mock_create_delta_table = self.create_delta_table_patcher.start()\n", "\n", " self.validate_delta_table_patcher = patch.object(deltatableclass_v2, 'validate_delta_table')\n", " self.mock_validate_delta_table = self.validate_delta_table_patcher.start()\n", "\n", " self.add_notnull_constraints_patcher = patch.object(deltatableclass_v2, 'add_notnull_constraints')\n", " self.mock_add_notnull_constraints = self.add_notnull_constraints_patcher.start()\n", " return\n", "\n", " def teardown(self):\n", " # teardown test case\n", " # stop patched functions\n", " self.mssparkutils_fs_exists_patcher.stop()\n", " self.add_notnull_constraints_patcher.stop()\n", " self.create_delta_table_patcher.stop()\n", " self.validate_delta_table_patcher.stop()\n", " return\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls __init__() method\n", " \n", " # success: mssparutils.fs.exists returns false, call create_delta_table add_notnull_constraints\n", " def test_initization_deploydeltatable_createtable(self):\n", " # preprocess\n", " # define return value mssparkutils.fs.exists\n", " self.mock_mssparkutils_fs_exists.return_value = false\n", "\n", " # create instance class deploydeltatable\n", " column_info =\"\"\"[\n", " {'column_name': 'test', 'dimension': 'pk', 'data_type': 'string'},\n", " {'column_name': 'test2', 'dimension': 'scd2', 'data_type': 'string'},\n", " {'column_name': 'test3', 'dimension': 'scd2', 'data_type': 'string'}\n", " ]\"\"\"\n", "\n", " class_object = deploydeltatable(\n", " table_name=mock(), \n", " storage_account=mock(), \n", " container_name=mock(), \n", " table_description=mock(), \n", " column_info=column_info, \n", " partitioning=mock()\n", " )\n", "\n", " # define expected return values\n", " expected_column_dimensions = {'test': 'pk', 'test2': 'scd2', 'test3': \"scd2\"}\n", "\n", " # execute\n", " # [dataframes:deploydeltatable] run method test\n", " class_object.deploy_delta_table()\n", "\n", " # evaluate\n", " # validate certain functions (not) called\n", " self.mock_create_delta_table.assert_called_once()\n", " self.mock_validate_delta_table.assert_not_called()\n", " self.mock_add_notnull_constraints.assert_called_once()\n", " # add_notnull_constraints: validate method called specific set arguments\n", " self.mock_add_notnull_constraints.assert_called_once_with(source_path=any, column_dimensions=expected_column_dimensions)\n", "\n", " # success: mssparutils.fs.exists returns true, call validate_delta_table\n", " def test_initization_deploydeltatable_validatetable(self):\n", " # preprocess\n", " # define return value mssparkutils.fs.exists\n", " self.mock_mssparkutils_fs_exists.return_value = true\n", "\n", " # create instance class deploydeltatable\n", " class_object = deploydeltatable(\n", " table_name='test_table', \n", " storage_account='test_storage', \n", " container_name='test_container', \n", " table_description='test_description', \n", " column_info='{\"test\": \"column\"}', \n", " partitioning=mock()\n", " )\n", "\n", " # execute\n", " # [dataframes:deploydeltatable] run method test\n", " class_object.deploy_delta_table()\n", "\n", " # evaluate\n", " # validate certain functions (not) called\n", " self.mock_create_delta_table.assert_not_called()\n", " self.mock_validate_delta_table.assert_called_once()\n", " self.mock_validate_delta_table.assert_called_once_with(table_name='test_table', storage_account='test_storage', container_name='test_container', column_objects={\"test\": \"column\"})\n", " self.mock_add_notnull_constraints.assert_not_called()\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deploydeltatable_initialization)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error sparkdataframeclass initialization tests, something went wrong!\")" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\Test_Deploy.json">{ "name": "test_deploy", "properties": { "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "f5537ed4-78d0-44b9-b63d-67f4c7efe1ed" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28 }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test deploy\r\n", "**purpose**: notebook use unittest library pyspark.testing.utils library python pyspark execute unit tests spark notebooks folder *deploy*.\r\n", "\r\n", "unit tests: test method expect do, untit test written. unit test isolates method overall environment executes method. aim able define expected outcome method see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute method successfully\r\n", "\r\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\r\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\r\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary libraries" ] }, { "cell_type": "code", "source": [ "import unittest\r\n", "from unittest.mock import patch, magicmock, mock\r\n", "from notebookutils import mssparkutils\r\n", "from io import stringio" ], "execution_count": 18 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## parameters\r\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "# parameters need given create delta table\r\n", "environment_code = ''" ], "execution_count": 19 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\r\n", "storage_account = f'{env_code}dapstdala1' # storage account delta table expected stored\r\n", "database_name = 'silver'" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" methods need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 20 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test classes" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_deploydeltatables(unittest.testcase):\r\n", " def setup(self):\r\n", " # everything else, remove tables silver database\r\n", " # save names use later teardown function\r\n", " self.tables = ['delta_mock_container', 'delta_mock', ]\r\n", "\r\n", " table self.tables:\r\n", " clean_table(table)\r\n", "\r\n", " # init basic variables\r\n", " self.table_name = 'delta_mock'\r\n", " self.database_name = database_name\r\n", " self.column_info = '[{\"column_name\": \"mock_test1\", \"data_type\": \"string\"}, {\"column_name\": \"mock_test2\", \"data_type\": \"string\"}]'\r\n", " self.env_code = env_code\r\n", "\r\n", " def teardown(self):\r\n", " # everything done, remove created tables again\r\n", " table self.tables:\r\n", " clean_table(table)\r\n", " \r\n", " # also clean silver container\r\n", " clean_containers(self.env_code,['silver'])\r\n", "\r\n", " def test_columninfo_error1(self):\r\n", " # test error thrown column_info argument right structure, \\n \\r white spaces it\r\n", " column_info = \"\\\"[\\\\r\\\\n {\\\\r\\\\n \\\\\\\"column_name\\\\\\\": \\\\\\\"id\\\\\\\",\\\\r\\\\n \\\\\\\"dimension\\\\\\\": \\\\\\\"pk\\\\\\\",\\\\r\\\\n \\\\\\\"data_type\\\\\\\": \\\\\\\"integer\\\\\\\"\\\\r\\\\n },\\\\r\\\\n {\\\\r\\\\n \\\\\\\"column_name\\\\\\\": \\\\\\\"employees\\\\\\\",\\\\r\\\\n \\\\\\\"dimension\\\\\\\": \\\\\\\"scd2\\\\\\\",\\\\r\\\\n \\\\\\\"data_type\\\\\\\": \\\\\\\"string\\\\\\\"\\\\r\\\\n },\\\\r\\\\n {\\\\r\\\\n \\\\\\\"column_name\\\\\\\": \\\\\\\"company\\\\\\\",\\\\r\\\\n \\\\\\\"dimension\\\\\\\": \\\\\\\"scd2\\\\\\\",\\\\r\\\\n \\\\\\\"data_type\\\\\\\": \\\\\\\"string\\\\\\\"\\\\r\\\\n }\\\\r\\\\n]\\\"\"\r\n", "\r\n", " self.assertraises(exception) error:\r\n", " mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': self.table_name, 'database_name': self.database_name, 'column_info': column_info, 'storage_account': storage_account})\r\n", "\r\n", " # error message bit dynamic, test substring\r\n", " self.assertin('string indices must integers', str(error.exception))\r\n", "\r\n", " def test_columninfo_error2(self):\r\n", " # test error thrown column_info argument wrong structure\r\n", " column_info = '[{\"column_names\": [\"id\", \"string_column\"], \"column_datatypes\": [\"integer\", \"string\", \"integer\"]}]'\r\n", "\r\n", " self.assertraises(exception) error:\r\n", " mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': self.table_name, 'database_name': self.database_name, 'column_info': column_info, 'storage_account': storage_account})\r\n", "\r\n", " # error message bit dynamic, test substring\r\n", " self.assertin('data_type', str(error.exception))\r\n", " \r\n", " def test_table_creation_success(self):\r\n", " # test successfully create new table exists\r\n", " mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': self.table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n", "\r\n", " spark.sql(f'select * {self.database_name}.delta_mock')\r\n", " \r\n", " def test_table_notexist_error(self):\r\n", " # test query returns error table exist\r\n", " # include name inside clean-up function, never created\r\n", " # created, test break needs happen\r\n", " self.assertraises(exception) error:\r\n", " spark.sql(f'select * {self.database_name}.delta_mock_error')\r\n", " \r\n", " self.assertin('table view found', str(error.exception))\r\n", "\r\n", " def test_containerempty_tableexists_success(self):\r\n", " # test creation delta table, silver container cleared return error\r\n", " # table exist silver\r\n", " table_name = 'delta_mock_container'\r\n", "\r\n", " # creating table also create folder inside silver container\r\n", " mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n", "\r\n", " # keep table, clean container\r\n", " clean_containers(self.env_code,['silver'])\r\n", "\r\n", " mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n", "\r\n", " # still return result, need test properly run\r\n", " spark.sql(f'select * {self.database_name}.delta_mock_container')\r\n", "\r\n", " # def test_containerempty_tableexists_emptycontainer(self):\r\n", " # # dev-note: https://stackoverflow.com/questions/73310653/python-unit-test-with-mocking-assert-called-for-function-outside-class\r\n", " # # dev-note: create class deploydeltatables test properly (can't use %run expose functions patch them, can't access sys.stdout notebook.run)\r\n", "\r\n", " # # test creation delta table, silver container cleared return specific error\r\n", " # table_name = 'delta_mock_container'\r\n", "\r\n", " # # creating table also create folder inside silver container\r\n", " # mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n", "\r\n", " # # keep table, clean container\r\n", " # clean_containers(self.env_code,['silver'])\r\n", "\r\n", " # patch('sys.stdout', new = stringio()) mock_stdout:\r\n", " # mssparkutils.notebook.run('deploy/deploydeltatable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n", "\r\n", " \r\n", " # # starting table deployment notebook silver.delta_mock_container\r\n", " # # ['simon_test', 'powershell_test', 'powershell_test_2', 'csv_integration', 'json_integration', 'delta_test', 'delta_mock_container']\r\n", " # # table silver.delta_mock_container exist, inside silver container.\r\n", " # # removing silver.delta_mock_container silver db, creating new table &amp; link\r\n", " # # creating table schema\r\n", " # # creating delta table\r\n", " \r\n", " # self.assertin(\"starting table deployment notebook silver.delta_mock_container\",mock_stdout.getvalue())\r\n", " \r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deploydeltatables)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 25 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\Test_DeployDeltaTable_v3.json">{ "name": "test_deploydeltatable_v3", "properties": { "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "6adce960-33c8-4e04-bf10-d6238ca62188" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28 }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test deploy v3\n", "**purpose**: notebook use unittest library pyspark.testing.utils library python pyspark execute unit tests spark notebooks folder *deploy*.\n", "\n", "unit tests: test method expect do, untit test written. unit test isolates method overall environment executes method. aim able define expected outcome method see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute method successfully\n", "\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary packages" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "import unittest\n", "from unittest.mock import patch, magicmock, mock, any, call\n", "from notebookutils import mssparkutils\n", "from io import stringio\n", "from pyspark.sql import sparksession\n", "import delta.tables dt\n", "import pyspark.sql sql\n", "" ], "execution_count": 41 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## parameters\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "# parameters need given create delta table\n", "environment_code = 'dev'" ], "execution_count": 40 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\n", "storage_account = f'{env_code}dapstdala1' # storage account delta table expected stored\n", "container_name = 'silver'\n", "\n", "\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 42 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" methods need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 43 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run deploy/deploydeltatable_v3" ], "execution_count": 44 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 45 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test classes" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: initialisationdeltatable\n", "# class test __init__ function deltatable-class defined deploydeltatable_v3 notebook\n", "class test_initialisationdeltatable(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # patch python functions: clean_containers function needs run isolation. \n", " # function-calls need mocked avoid getting errors things need tested.\n", " # set parameters used initialise deltatable-object\n", " self.table_name = 'unittest_table'\n", " self.container_name = 'unittest_database'\n", " self.storage_account = 'unittest'\n", " self.table_description = 'unittest created table'\n", " self.column_info = \"[{}]\" #\"[{ 'column_name': 'column_1', 'data_type': 'string', 'dimenion': 'bk' }, { 'column_name': 'column_2','data_type': 'integer', 'dimenion': 'scd2' }, { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimenion': 'bk' }]\"\n", " self.target_options = {}\n", "\n", "\n", " # patch functions called init-function deltatable-class\n", " # patched functions self-made functions functions coming external libaries.\n", " # self-made functions tested separately, \"expected\" work called functions\n", "\n", " ## function deploydeltatable notebook \n", " self.mock_process_column_info_string_patcher = patch('__main__.process_column_info_string')\n", " self.mock_process_column_info_string = self.mock_process_column_info_string_patcher.start()\n", " \n", " ## method deploydeltatable notebook \n", " self.mock_configure_datatypes_patcher = patch('__main__.configure_datatypes')\n", " self.mock_configure_datatypes = self.mock_configure_datatypes_patcher.start()\n", "\n", "\n", " def teardown(self):\n", " # stop patcher-functions \n", " self.mock_process_column_info_string_patcher.stop()\n", " self.mock_configure_datatypes_patcher.stop()\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls __init__ function\n", "\n", " # success: validate parameters full_name, silver_path, column_dict take expected values\n", " def test_delta_table_init(self):\n", " # preprocess\n", " # based function call execution-section, following parameter values expected initialised\n", " # expected_full_name = 'unittest_database.unittest_table'\n", " expected_silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.container_name, self.storage_account, self.table_name)\n", " expected_column_dict = { \"column_1\": \"string\", \"column_2\": \"integer\", \"column_3\": \"decimal(12,5)\", \"column_4\": \"varchar(4)\" }\n", " expected_column_dimensions = { \"column_1\": \"bk\", \"column_2\": \"scd2\", \"column_3\": \"bk\", \"column_4\": \"scd2\" }\n", "\n", "\n", " # since output process_column_info_string function called used __init__ function, output (= return_value) mocked\n", " # assuming process_column_info_string function works, following output would expected returned:\n", " self.mock_process_column_info_string.return_value = { \"column_name\": \"column_1\", \"data_type\": \"string\", \"dimension\": \"bk\" }, { \"column_name\": \"column_2\",\"data_type\": \"integer\", \"dimension\": \"scd2\" }, { \"column_name\": \"column_3\", \"data_type\": \"decimal(12,5)\", \"dimension\": \"bk\" }, { \"column_name\": \"column_4\", \"data_type\": \"varchar(4)\", \"dimension\": \"scd2\" }\n", "\n", " # execute\n", " # initialize deltatable-object, thereby calling to-be tested __init__ function\n", " delta_table = deltatableclass( self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info, self.target_options )\n", "\n", " # evaluate\n", " # initialization, following object-parameters created\n", " # actual_full_name = delta_table.full_name\n", " actual_silver_path = delta_table.silver_path\n", " actual_column_dict = delta_table.column_dict\n", " actual_column_dimensions = delta_table.column_dimensions\n", "\n", " # validate expected object-parameters equal actual object-parameters\n", " # self.assertequal(expected_full_name, actual_full_name, 'something went wrong initialisation full_name')\n", " self.assertequal(expected_silver_path, actual_silver_path, 'something went wrong initialisation silver_path')\n", " self.assertequal(expected_column_dict, actual_column_dict, 'something went wrong initialisation column_dict')\n", " self.assertequal(expected_column_dimensions, actual_column_dimensions, 'something went wrong initialisation column_dimensions')\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_initialisationdeltatable)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)\n", "" ], "execution_count": 16 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: deploydeltatable_v3\n", "# class test deploy_delta_table() method deltatable-class defined deploydeltatable_v3 notebook\n", "class test_deploydeltatable_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # create deltatable-object using parameters\n", " self.table_name = 'unittest_table'\n", " self.container_name = 'unittest_database'\n", " self.storage_account = 'unittest'\n", " self.table_description = 'unittest created table'\n", " self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string', 'dimension':'bk' }, { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }, { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimension':'bk' }, { 'column_name': 'column_4', 'data_type': 'varchar(4)', 'dimension':'scd2' }]\"\n", " \n", " self.delta_table_object = deltatableclass(self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info)\n", " \n", " # patch functions called deploy_delta_table-method deltatable-class\n", " # patched functions self-made functions functions coming external libaries.\n", " # self-made functions tested separately, \"expected\" work called functions\n", "\n", " ## method deltatable class\n", " self.mock_validate_table_patcher = patch.object(deltatableclass, 'validate_table')\n", " self.mock_validate_table = self.mock_validate_table_patcher.start()\n", " \n", " ## method deltatable class \n", " self.mock_create_delta_table_patcher = patch.object(deltatableclass, 'create_delta_table')\n", " self.mock_create_delta_table = self.mock_create_delta_table_patcher.start()\n", "\n", " ## function cleanworkspace notebook\n", " self.mock_clean_table_patcher = patch('__main__.clean_table')\n", " self.mock_clean_table = self.mock_clean_table_patcher.start()\n", "\n", " ## method coming library notebookutils.mssparkutils\n", " self.mock_mssparkutils_fs_patcher = patch.object(mssparkutils, 'fs')\n", " self.mock_mssparkutils_fs = self.mock_mssparkutils_fs_patcher.start()\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " self.mock_create_delta_table_patcher.stop()\n", " self.mock_validate_table_patcher.stop()\n", " self.mock_clean_table_patcher.stop()\n", " self.mock_mssparkutils_fs_patcher.stop()\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls deploy_delta_table method\n", "\n", " # success: table exists, call validate_table() function\n", " def test_deploy_delta_table_call_validate(self):\n", " # preprocess\n", " # table exist -&gt; call validate_table()\n", " self.mock_mssparkutils_fs.exists.return_value = true\n", "\n", " # execute\n", " self.delta_table_object.deploy_delta_table()\n", "\n", " # evaluate\n", " # assert whether validate_table called exactly once\n", " self.mock_validate_table.assert_called_once()\n", "\n", " # success: tbale exist, call create_table() function\n", " def test_deploy_delta_table_call_create(self):\n", " # preprocess\n", " # table exist -&gt; call validate_table()\n", " self.mock_mssparkutils_fs.exists.return_value = false\n", "\n", " # execute\n", " self.delta_table_object.deploy_delta_table()\n", "\n", " # evluate\n", " # assert whether create_delta_table called exactly once\n", " self.mock_create_delta_table.assert_called_once()\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_deploydeltatable_v3)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)" ], "execution_count": 46 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: createdeltatable_v3\n", "# class test deploy_delta_table() method deltatable-class defined deploydeltatable_v3 notebook\n", "class test_createdeltatable_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # create deltatable-object using parameters\n", " self.table_name = 'unittest_create_table'\n", " self.container_name = 'unittest'\n", " self.storage_account = storage_account\n", " self.table_description = 'unittest created table'\n", " self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string', 'dimension':'bk' }, { 'column_name': 'column_2','data_type': 'integer', 'dimension':'scd2' }, { 'column_name': 'column_3', 'data_type': 'decimal(12,5)', 'dimension':'bk' }, { 'column_name': 'column_4', 'data_type': 'varchar(4)', 'dimension':'scd2' }]\"\n", " self.partitioning = {}\n", " self.delta_table_object = deltatableclass(self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info, self.partitioning)\n", "\n", " ## method ingestionfunctions class\n", " self.mock_create_partioning_list_patcher = patch('__main__.create_partioning_list')\n", " self.mock_create_partioning_list = self.mock_create_partioning_list_patcher.start()\n", "\n", " ## method ingestionfunctions class\n", " self.mock_add_notnull_constraints_patcher = patch.object( deltatableclass, 'add_notnull_constraints')\n", " self.mock_add_notnull_constraints = self.mock_add_notnull_constraints_patcher.start()\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " self.mock_create_partioning_list_patcher.stop()\n", " self.mock_add_notnull_constraints_patcher.stop()\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls deploy_delta_table method\n", "\n", " # success: create table certain schema varchar(4) datatype\n", " # varchar(4) datatype expected part metadata column \n", " def test_create_delta_table_no_partitioning(self):\n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(self.delta_table_object.silver_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(env_code, self.container_name, self.table_name)\n", "\n", " # preprocess:\n", " # define expected schema metadata values\n", " # following schema expected returned\n", " mock_schema = sql.types.structtype([\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"column_2\", sql.types.integertype(), true),\n", " sql.types.structfield(\"column_3\", sql.types.decimaltype(12,5), true),\n", " sql.types.structfield(\"column_4\", sql.types.stringtype(), true)\n", " ])\n", " df = spark.createdataframe(data=[], schema=mock_schema)\n", "\n", " # define exepcted schema metadata definitions\n", " expected_schema = df.printschema()\n", " # column_4 especially, expectation metadata-field limits total number characters added\n", " expected_metadata_column4 = {'column_4': 'metadata column column_4', '__char_varchar_type_string': 'varchar(4)'}\n", "\n", "\n", " # execute\n", " self.delta_table_object.create_delta_table()\n", "\n", " # evaluate\n", " # get schema metadata table created\n", " deltatable = dt.deltatable.forpath(spark, self.delta_table_object.silver_path)\n", " actual_schema = deltatable.todf().printschema()\n", " actual_metadata = deltatable.todf().select(\"column_4\").schema[0].metadata\n", "\n", " # validate match expected values\n", " self.assertequal(expected_schema, actual_schema)\n", " self.assertequal(expected_metadata_column4, actual_metadata)\n", " self.mock_create_partioning_list.assert_not_called()\n", " self.mock_add_notnull_constraints.assert_called_once()\n", "\n", " def test_create_delta_table_partitioning(self):\n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(self.delta_table_object.silver_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(env_code, self.container_name, self.table_name)\n", "\n", " # preprocess:\n", " # define expected schema metadata values\n", " # following schema expected returned\n", " mock_schema = sql.types.structtype([\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"column_2\", sql.types.integertype(), true),\n", " sql.types.structfield(\"column_3\", sql.types.decimaltype(12,5), true),\n", " sql.types.structfield(\"column_4\", sql.types.stringtype(), true, metadata={'column_4': 'metadata column column_4', '__char_varchar_type_string': 'varchar(4)'})\n", " ])\n", " df = spark.createdataframe(data=[], schema=mock_schema)\n", "\n", " # define exepcted schema metadata definitions\n", " expected_schema = df.printschema()\n", " # column_4 especially, expectation metadata-field limits total number characters added\n", " expected_metadata_column4 = {'column_4': 'metadata column column_4', '__char_varchar_type_string': 'varchar(4)'}\n", " self.delta_table_object.partitioning = [{\"name\": \"column_1\", \"sequence\": 1}, {\"name\": \"column_2\", \"sequence\": 1}]\n", " self.mock_create_partioning_list.return_value = df, [\"column_1\", \"column_2\"]\n", "\n", " # execute\n", " self.delta_table_object.create_delta_table()\n", "\n", " # evaluate\n", " # get schema metadata table created\n", " deltatable = dt.deltatable.forpath(spark, self.delta_table_object.silver_path)\n", " actual_schema = deltatable.todf().printschema()\n", " actual_metadata = deltatable.todf().select(\"column_4\").schema[0].metadata\n", "\n", " # validate match expected values\n", " self.assertequal(expected_schema, actual_schema)\n", " self.assertequal(expected_metadata_column4, actual_metadata)\n", " self.mock_create_partioning_list.assert_called_once()\n", " self.mock_add_notnull_constraints.assert_called_once()\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_createdeltatable_v3)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)\n", "" ], "execution_count": 20 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: addnotnullconstraint_v3\n", "# class test add_notnull_constraint() method deltatable-class defined deploydeltatable_v3 notebook\n", "class test_addnotnullconstraint_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", "\n", " # preprocess:\n", " # define expected schema metadata values\n", " # following schema expected returned\n", " mock_schema = sql.types.structtype([\n", " sql.types.structfield(\"column_1\", sql.types.stringtype(), true),\n", " sql.types.structfield(\"column_2\", sql.types.integertype(), true),\n", " sql.types.structfield(\"column_3\", sql.types.decimaltype(12,5), true),\n", " sql.types.structfield(\"column_4\", sql.types.stringtype(), true)\n", " ])\n", " df = spark.createdataframe(data=[], schema=mock_schema)\n", "\n", " table_name = 'add_notnull_constraints'\n", " container_name = 'unittest'\n", " storage_account = f'{env_code}dapstdala1'\n", " table_description = ''\n", " column_info = \"[{ 'column_name': 'mock', 'data_type': 'string', 'dimension':'bk' }]\"\n", " partitioning = dict()\n", "\n", " self.silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, table_name)\n", " # catch: make sure table trying created exist already\n", " mssparkutils.fs.exists(self.silver_path):\n", " # raise exception(f\"table definition already exists. table {self.table_name} removed testing creation\")\n", " clean_delta_table(env_code, container_name, table_name)\n", " df.write.format('delta').save(self.silver_path)\n", "\n", " self.delta_table_object = deltatableclass(table_name, container_name, storage_account, table_description, column_info, partitioning)\n", "\n", " return\n", "\n", " def teardown(self):\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_notnull_constraint method\n", "\n", " # success: create table certain schema varchar(4) datatype\n", " # varchar(4) datatype expected part metadata column \n", " def test_addnotnullconstraint_deploydeltatable_success(self):\n", " # scd2\n", " self.delta_table_object.column_dimensions = {\"column_1\": \"pk\", \"column_2\": \"scd2\", \"column_3\": \"pk\", \"column_4\": \"scd2\"}\n", " expected_constraint_names = ['delta.constraints.pk__notnull_column_1', 'delta.constraints.pk__notnull_column_3']\n", " expected_constraint_values = ['column_1 null', 'column_3 null']\n", " # execute\n", " self.delta_table_object.add_notnull_constraints()\n", "\n", " # evaluate\n", " # get list constraints added table\n", " constraints = spark.sql(f\"describe detail delta.`{self.silver_path}`\").select('properties').collect()[0][0]\n", " actual_constraint_names = list(constraints.keys())\n", " actual_constraint_values = list(constraints.values())\n", "\n", " # compare actuals expectations\n", " self.assertequal(expected_constraint_names, actual_constraint_names, f\"[deploydeltatables:addnotnullconstraints] constraints initialised expected: actual = {actual_constraint_names} versus expected = {expected_constraint_names}\")\n", " self.assertequal(expected_constraint_values, actual_constraint_values, f\"[deploydeltatables:addnotnullconstraints] constraints initialised expected: actual = {actual_constraint_values} versus expected = {expected_constraint_values}\")\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_addnotnullconstraint_v3)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)\n", "" ], "execution_count": 38 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: createdeltatable_v3\n", "# class test add_structfield_logging_columns() method deltatable-class defined deploydeltatable_v3 notebook\n", "class test_addloggingcolumns_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", " return\n", "\n", " def teardown(self):\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_structfield_logging_columns() method\n", "\n", " # success: pass empty list expect logging-columns returned structfield format\n", " def test_add_structfield_logging_columns_empty_list(self):\n", " # preprocess\n", " expected_structfield_list = [\n", " sql.types.structfield('t_load_date_raw', sql.types.timestamptype(), true),\n", " sql.types.structfield('t_load_date_silver', sql.types.timestamptype(), true),\n", " sql.types.structfield('t_file_name', sql.types.stringtype(), true),\n", " sql.types.structfield('t_plan_id', sql.types.integertype(), false),\n", " sql.types.structfield('t_task_id', sql.types.integertype(), false),\n", " sql.types.structfield('t_file_id', sql.types.integertype(), false)\n", " ]\n", "\n", " # execute\n", " actual_structfield_list = add_structfield_logging_columns(structfield_list=[])\n", "\n", " # evaluate\n", " # check list contains necessary objects\n", " self.assertequal(len(actual_structfield_list), len(expected_structfield_list))\n", "\n", " # # check list contains correct values field\n", " # # dev-info: direct list-compare (using assertequal) returns error. unclear is, assuming comparing object definitions straightfoward\n", " field, expected_field zip(actual_structfield_list, expected_structfield_list):\n", " self.subtest(field=field):\n", " self.assertequal(field.name, expected_field.name)\n", " self.assertequal(field.datatype, expected_field.datatype)\n", " self.assertequal(field.nullable, expected_field.nullable)\n", "\n", " # success: pass non-empty list expect logging-columns returned structfield format\n", " def test_add_structfield_logging_columns_nonempty_list(self):\n", " # preprocess\n", " expected_structfield_list = [\n", " sql.types.structfield('test', sql.types.stringtype(), false),\n", " sql.types.structfield('t_load_date_raw', sql.types.timestamptype(), true),\n", " sql.types.structfield('t_load_date_silver', sql.types.timestamptype(), true),\n", " sql.types.structfield('t_file_name', sql.types.stringtype(), true),\n", " sql.types.structfield('t_plan_id', sql.types.integertype(), false),\n", " sql.types.structfield('t_task_id', sql.types.integertype(), false),\n", " sql.types.structfield('t_file_id', sql.types.integertype(), false)\n", " ] \n", "\n", " # execute\n", " structfield_list = [sql.types.structfield('test', sql.types.stringtype(), false)]\n", " actual_structfield_list = add_structfield_logging_columns(structfield_list=structfield_list)\n", "\n", " # evaluate\n", " # check list contains necessary objects\n", " self.assertequal(len(actual_structfield_list), len(expected_structfield_list), \n", " f\"the expected structfield list match actual list: {expected_structfield_list} versus {actual_structfield_list}\")\n", "\n", " # # check list contains correct values field\n", " # # dev-info: direct list-compare (using assertequal) returns error. unclear is, assuming comparing object definitions straightfoward\n", " field, expected_field zip(actual_structfield_list, expected_structfield_list):\n", " self.subtest(field=field):\n", " self.assertequal(field.name, expected_field.name, f\"field names match {field} {expected_field}\")\n", " self.assertequal(field.datatype, expected_field.datatype, f\"datatypes match {field} {expected_field}\")\n", " self.assertequal(field.nullable, expected_field.nullable, f\"nullables argument match {field} {expected_field}\")\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_addloggingcolumns_v3)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error create delta table tests, something went wrong!\")\n", "# print(test_resuts)" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: processcolumninfostring_v3\n", "# class test process_column_info_string() method defined deploydeltatable_v3 notebook\n", "class test_processcolumninfostring_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # test process_column_info_string function deploydeltatable_v2 notebook\n", " # process_column_info_string method converts string json-object\n", " def setup(self):\n", " # set-up needed. keeping function potential future use\n", " return\n", "\n", " def teardown(self):\n", " # tear-down needed. keeping function potential future use\n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls deploy_delta_table method\n", "\n", " # success: pass valid (simple) dictionary string expect conversion happen correctly\n", " def test_process_column_info_string_2(self):\n", " # preprocess\n", " # define expected return object\n", " expected_object = {\"key\": \"value\"}\n", "\n", " # execute\n", " # invoke function argument\n", " string_object = '{\"key\":\"value\"}'\n", " actual_object = process_column_info_string(string_object)\n", "\n", " # evaluate\n", " # validate actual return value equal expectated return value\n", " self.assertequal(expected_object, actual_object, f\"the column info string processed correctly. expected {expected_object}, got {actual_object}\")\n", "\n", "\n", " # success: pass valid list dictionaries string expect conversion happen correctly\n", " def test_process_column_info_string(self):\n", " # preprocess\n", " # define expected return object\n", " expected_object = [{'column_name':'column_1', 'data_type':'string'}, {'column_name':'column_2', 'data_type':'integer'}, {'column_name':'column_3', 'data_type':'decimal(12,5)'}, {'column_name':'column_4', 'data_type':'varchar(4)'}]\n", " \n", " # execute\n", " # run function argument\n", " string_object = '[{\"column_name\":\"column_1\", \"data_type\":\"string\"}, {\"column_name\":\"column_2\", \"data_type\":\"integer\"}, {\"column_name\":\"column_3\", \"data_type\":\"decimal(12,5)\"}, {\"column_name\":\"column_4\", \"data_type\":\"varchar(4)\"}]'\n", " actual_object = process_column_info_string(string_object=string_object)\n", "\n", " # evaluate\n", " # validate actual return value equal expectated return value\n", " self.assertequal(expected_object, actual_object, f\"the column info string processed correctly. expected {expected_object}, got {actual_object}\")\n", "\n", " # failure: test string object valid object, error gets thrown\n", " def test_process_column_info_string_invalid_json(self):\n", " # preprocess\n", " # define string_object argument\n", " string_object = '[invalid_json_object]'\n", " # expect following error returned\n", " expected_error = f\"the string_object passed cannot converted json object. passed string: {string_object}\"\n", "\n", " # execute\n", " self.assertraises(typeerror) error:\n", " process_column_info_string(string_object)\n", "\n", " # evaluate\n", " # get returned exception string\n", " actual_error = str(error.exception)\n", " # validate returned error equal expectated error\n", " self.assertequal(expected_error, actual_error, f'the returned error process_column_info_string \"{actual_error}\", expected error \"{expected_error}\".')\n", "" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: configuredatatypes_v3\n", "# class test configure_datatypes() method defined deploydeltatable_v3 notebook\n", "class test_configuredatatypes_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # test configurate_datatypes function deploydeltatable_v3 notebook\n", " # configurate_datatypes method converts string json-object\n", "\n", " def setup(self):\n", "\n", " # patch functions called validate_table-method deltatable-class\n", " # patched functions self-made functions functions coming external libaries.\n", " # self-made functions tested separately, \"expected\" work called functions\n", "\n", " ## function deploydeltatable_v3 notebook\n", " self.mock_add_decimal_datatype_patcher = patch('__main__.add_decimal_datatype')\n", " self.mock_add_decimal_datatype = self.mock_add_decimal_datatype_patcher.start()\n", "\n", " self.mock_add_varchar_datatype_patcher = patch('__main__.add_varchar_datatype')\n", " self.mock_add_varchar_datatype = self.mock_add_varchar_datatype_patcher.start()\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " self.mock_add_decimal_datatype_patcher.stop()\n", " self.mock_add_varchar_datatype_patcher.stop()\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls configure_datatypes function\n", "\n", "\n", " # success: pass empty dictionary return dictionary predefined datatypes\n", " def test_configure_datatypes_return_generic_datatypes(self):\n", " # preprocess\n", " # expect dictionary returned, containing generic datatypes\n", " expected_dictionary = {\"array\": sql.types.arraytype(sql.types.stringtype()), \"binary\": sql.types.binarytype(), \n", " \"boolean\": sql.types.booleantype(), \"date\": sql.types.datetype(), \"string\": sql.types.stringtype(), \n", " \"timestamp\": sql.types.timestamptype(), \"decimal\": sql.types.decimaltype(38,4), \"float\": sql.types.floattype(), \n", " \"byte\": sql.types.bytetype(), \"integer\": sql.types.integertype(), \"long_integer\": sql.types.longtype(), \n", " \"varchar(max)\": sql.types.stringtype(), \"int\": sql.types.integertype()\n", " }\n", "\n", " # execute\n", " # run function empty dictionary argument\n", " empty_dictionary = dict()\n", " actual_dictionary = configure_datatypes(empty_dictionary)\n", "\n", " # evaluate\n", " # validate returned dictionary matches expected dictionary\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\n", " \n", " # success: validate datatype-specific conversion functions (add_decimal_datatype add_varchar_datatype) invoked value-matches dictionary\n", " def test_configure_datatypes_dont_call_datatype_specific_functions(self):\n", " # preprocess\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing values referencing 'decimal' 'varchar'\n", " decimal_dictionary = {'key': 'string'}\n", " configure_datatypes(decimal_dictionary)\n", "\n", " # evaluate\n", " # validate datatype-specific functions called\n", " self.mock_add_decimal_datatype.assert_not_called()\n", " self.mock_add_varchar_datatype.assert_not_called()\n", " \n", " # success: validate dictionary passed contains value 'decimal', function add_decimal_datatype called\n", " def test_configure_datatypes_decimal(self):\n", " # preprocess\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing generic value 'decimal'\n", " decimal_dictionary = {'key': 'decimal'}\n", " configure_datatypes(decimal_dictionary)\n", "\n", " # evaluate\n", " # validate function called exactly once\n", " self.mock_add_decimal_datatype.assert_called_once()\n", "\n", " # success: validate dictionary passed contains value 'decimal(&lt;xx&gt;,&lt;yy&gt;', function add_decimal_datatype called\n", " def test_configure_datatypes_decimal_2(self): \n", " # preprocess\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing specific value 'decimal(12,5)'\n", " decimal_dictionary = {'key': 'decimal(12,5)'}\n", " configure_datatypes(decimal_dictionary)\n", "\n", " # evaluate\n", " # validate function called exactly once\n", " self.mock_add_decimal_datatype.assert_called_with('decimal(12,5)', any)\n", "\n", " # success: validate dictionary passed contains generic value 'decimal' specific 'decimal(&lt;xx&gt;,&lt;yy&gt;), \n", " # function add_decimal_datatype called reference\n", " def test_configure_datatypes_multiple_decimals(self):\n", " # preprocess\n", " # define list expected function-calls add_decimal_datatype\n", " expected_calls = [call('decimal', any), call('decimal(12,5)', any)]\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing generic value 'decimal' specific value 'decimal(12,5)'\n", " decimal_dictionary = {'key': 'decimal', 'other_key': 'decimal(12,5)'}\n", " configure_datatypes(decimal_dictionary)\n", "\n", " # evaluate\n", " # validate function called instance 'decimal' \n", " self.mock_add_decimal_datatype.assert_has_calls(expected_calls, any_order=true)\n", "\n", "\n", " # success: validate dictionary passed contains generic value 'varchar(max)', function add_varchar_datatype called\n", " def test_configure_datatypes_varchar(self):\n", " # preprocess\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing generic value referencing 'varchar'\n", " varchar_dictionary = {'key': 'varchar(max)'}\n", " configure_datatypes(varchar_dictionary)\n", "\n", " # evaluate\n", " # validate function add_varchar_datatype() called exactly once\n", " self.mock_add_varchar_datatype.assert_called_once()\n", "\n", " # success: validate dictionary passed contains specific value 'varchar(4)', function add_varchar_datatype called\n", " def test_configure_datatypes_varchar_2(self):\n", " # preprocess\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing specific value referencing 'varchar(4)'\n", " varchar_dictionary = {'key': 'varchar(4)'}\n", " configure_datatypes(varchar_dictionary)\n", "\n", " # evaluate\n", " # validate function called exactly once\n", " self.mock_add_varchar_datatype.assert_called_with('varchar(4)', any)\n", "\n", " # success: validate dictionary passed contains generic value 'varchar(max)' specific value 'varchar(4)', \n", " # function add_varchar_datatype called reference\n", " def test_configure_datatypes_multiple_varchars(self):\n", " # preprocess\n", " # define list expected function-calls add_varchar_datatype\n", " expected_calls = [call('varchar(max)', any), call('varchar(4)', any)]\n", "\n", " # execute\n", " # run function non-empty dictionary argument containing generic value 'varchar(max)' specific value 'varchar(4)'\n", " varchar_dictionary = {'key': 'varchar(max)', 'other_key': 'varchar(4)'}\n", " configure_datatypes(varchar_dictionary)\n", "\n", " # evaluate\n", " # validate function called instance 'varchar'\n", " self.mock_add_varchar_datatype.assert_has_calls(expected_calls, any_order=true)\n", "\n", "\n", " # failure: test invalid (or non-configured) datatype passed, error thrown\n", " def test_configure_datatypes_invalid_datatypes(self):\n", " # preprocess\n", " # expect following error thrown:\n", " expected_error = f\"the value 'invalid_datatype' configured datatype current set-up.\"\n", "\n", " # execute\n", " # run function invalid datatype\n", " decimal_dictionary = {'key': 'string', 'other_key': 'invalid_datatype'}\n", " self.assertraises(valueerror) error:\n", " configure_datatypes(decimal_dictionary)\n", " actual_error = str(error.exception)\n", "\n", " # evaluate\n", " # validate thrown error equal expected thrown error\n", " self.assertequal(expected_error, actual_error, f'the returned error add_decimal_datatype \"{actual_error}\", expected error \"{expected_error}\".')\n", "" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: adddecimaldatatype_v3\n", "# class test configure_datatypes() method defined deploydeltatable_v3 notebook\n", "class test_adddecimaldatatype_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # test add_decimal_datatype function deploydeltatable_v2 notebook\n", " # add_decimal_datatype method adds specific sql.type.decimaltype references limited characters comma\n", "\n", " def setup(self):\n", " # set-up needed. keeping function potential future use\n", " return\n", "\n", " def teardown(self):\n", " # tear-down needed. keeping function potential future use\n", " return\n", "\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_decimal_datatype() function\n", "\n", " # success: add decimal-datatype reference types-dictionary\n", " def test_add_decimal_datatype(self):\n", " # preprocess\n", " # expect following dictionary returned:\n", " expected_dictionary = {\"decimal(12,5)\": sql.types.decimaltype(12,5)}\n", "\n", "\n", " # execute\n", " # run function following arguments\n", " types = dict()\n", " value = 'decimal(12,5)'\n", " actual_dictionary = add_decimal_datatype(value, types)\n", "\n", " # evaluate\n", " # assert equality dictionaries\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\n", "\n", " # failure: deny additional reference invalid decimal configuration types-dictionary \n", " def test_invalid_value(self):\n", " # preprocess\n", " # expect following dictionary returned:\n", " expected_error = f\"the decimal value decimal(12) valid datatype value cannot configured correctly. aborting delta table deployment...\"\n", " \n", " # execute\n", " # run function following arguments\n", " types = dict()\n", " value = 'decimal(12)'\n", " self.assertraises(valueerror) error:\n", " add_decimal_datatype(value, types)\n", " actual_error = str(error.exception)\n", "\n", " # evaluate\n", " # validate expected error actually returned\n", " self.assertequal(expected_error, actual_error, f'the returned error add_decimal_datatype \"{actual_error}\", expected error \"{expected_error}\".')\n", "" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: addvarchardatatype_v3\n", "# class test configure_datatypes() method defined deploydeltatable_v3 notebook\n", "class test_addvarchardatatype_v3(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # test add_varchar_datatype function deploydeltatable_v2 notebook\n", " # add_varchar_datatype method adds specific sql.type.varchartype references limited characters comma\n", "\n", " def setup(self):\n", " # set-up needed. keeping function potential future use\n", " return\n", "\n", " def teardown(self):\n", " # tear-down needed. keeping function potential future use\n", " return\n", "\n", " \n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls add_varchar_datatype() function\n", "\n", " # success: add varchar-datatype reference types-dictionary\n", " def test_add_varchar_datatype(self):\n", " # preprocess\n", " # expect following dictionary returned:\n", " expected_dictionary = {\"varchar(12)\": sql.types.stringtype()}\n", "\n", "\n", " # execute\n", " # run function following arguments\n", " types = dict()\n", " value = 'varchar(12)'\n", " actual_dictionary = add_varchar_datatype(value, types)\n", "\n", " # evaluate\n", " # assert equality dictionaries\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\n", "\n", " # failure: deny additional reference invalid varchar configuration types-dictionary \n", " def test_invalid_value(self):\n", " # preprocess\n", " # expect following dictionary returned:\n", " expected_error = f\"the varchar value varchar valid datatype value cannot configured correctly. aborting delta table deployment...\"\n", " \n", " # execute\n", " # run function following arguments\n", " types = dict()\n", " value = 'varchar'\n", " self.assertraises(valueerror) error:\n", " add_varchar_datatype(value, types)\n", " actual_error = str(error.exception)\n", "\n", " # evaluate\n", " # validate expected error actually returned\n", " self.assertequal(expected_error, actual_error, f'the returned error add_varchar_datatype \"{actual_error}\", expected error \"{expected_error}\".')\n", "" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# class test_validatetable(unittest.testcase):\n", "# # test validate_table method deltatable class (see notebook: deploydeltatable_v2)\n", "# # validate_table method validates features delta table\n", "# def setup(self):\n", "# # create deltatable-object using parameters\n", "# self.table_name = 'unittest_tablename'\n", "# self.container_name = 'unittest_database'\n", "# self.storage_account = 'unittest_storageaccount'\n", "# self.table_description = 'unittest created table'\n", "# self.column_info = \"[{ 'column_name': 'unittest', 'data_type': 'string' }]\"\n", "\n", "# self.delta_table_object = deltatableclass(self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info)\n", "\n", " \n", "# # patch functions called validate_table-method deltatable-class\n", "# # patched functions self-made functions functions coming external libaries.\n", "# # self-made functions tested separately, \"expected\" work called functions\n", "\n", "# ## method coming library pyspark.sql\n", "# self.mock_sparksession_sql_patcher = patch.object(sparksession, 'sql')\n", "# self.mock_sparksession_sql = self.mock_sparksession_sql_patcher.start()\n", "\n", "# ## method coming library delta.table\n", "# self.mock_deltatable_patcher = patch.object(dt, 'deltatable')\n", "# self.mock_deltatable = self.mock_deltatable_patcher.start()\n", "\n", "# # validate_table method queries details 'to deployed' delta table.\n", "# # details checked are: format, name, location\n", "# # method, spark.sql returns dataframe values\n", "# # here, schema relevant parameters created. \n", "# # schema used testing-methods (below) create spark dataframes relevant table details tested\n", "# self.schema = dt.structtype([\n", "# dt.structfield(name='format', datatype=sql.types.stringtype(), nullable=false),\n", "# dt.structfield(name='name', datatype=sql.types.stringtype(), nullable=false),\n", "# dt.structfield(name='location', datatype=sql.types.stringtype(), nullable=false)\n", "# ])\n", "\n", "# def teardown(self):\n", "# # stop patcher functions\n", "# self.mock_sparksession_sql_patcher.stop()\n", "# self.mock_deltatable_patcher.stop()\n", "\n", "# def test_validate_table(self):\n", "# # create dataframe values passed validate_table method\n", "# valid_data = [\n", "# ('delta', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# valid_df = spark.createdataframe(valid_data, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = valid_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = true \n", "# self.mock_deltatable.isdeltatable.return_value = true\n", "\n", "# # expected returned value: true -&gt; delta table validated passed checks\n", "# expected_return = true\n", "\n", "# # run function: validate_table delta-table object 'self.delta_table_object'\n", "# actual_return = self.delta_table_object.validate_table()\n", "\n", "# # assert actual return value equal expectated return value\n", "# self.assertequal(expected_return, actual_return, f'the return value validate_delta method \"{actual_return}\", expected return \"{expected_return}\".')\n", "\n", "# def test_validate_table_format(self):\n", "# # create dataframe format-value invalid (csv instead delta)\n", "# invalid_format = [\n", "# ('csv', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# invalid_format_df = spark.createdataframe(invalid_format, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = invalid_format_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = true \n", "# self.mock_deltatable.isdeltatable.return_value = true\n", "\n", "# # expected returned value: expect error thrown\n", "# expected_error = \"the format table delta table\"\n", " \n", "# # run function: validate_table delta-table object 'self.delta_table_object'\n", "# self.assertraises(typeerror) error:\n", "# self.delta_table_object.validate_table()\n", "# actual_error = str(error.exception)\n", " \n", "# # assert actual return value equal expectated return value\n", "# self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "# def test_validate_full_table_name_1(self):\n", "# # create dataframe database-value invalid name-parameter (invalid_database instead unittest_database)\n", "# invalid_database = [\n", "# ('delta', f'invalid_database.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# invalid_database_df = spark.createdataframe(invalid_database, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = invalid_database_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = true \n", "# self.mock_deltatable.isdeltatable.return_value = true\n", "\n", "# # expected returned value: expect error thrown\n", "# expected_error = f\"the name table {self.table_name} namespace {self.container_name} match name table invalid_database.{self.table_name}\"\n", "# self.assertraises(typeerror) error:\n", "# self.delta_table_object.validate_table()\n", " \n", "# actual_error = str(error.exception)\n", "# self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "# def test_validate_full_table_name_2(self):\n", "# # create dataframe tablename-value invalid name-parameter (invalid_tablename instead unittest_table)\n", "# invalid_tablename = [\n", "# ('delta', f'{self.container_name}.invalid_tablename', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# invalid_tablename_df = spark.createdataframe(invalid_tablename, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = invalid_tablename_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = true \n", "# self.mock_deltatable.isdeltatable.return_value = true\n", "\n", "# # expected returned value: expect error thrown\n", "# expected_error = f\"the name table {self.table_name} namespace {self.container_name} match name table {self.container_name}.invalid_tablename\"\n", " \n", "# # run function: validate_table delta-table object 'self.delta_table_object'\n", "# self.assertraises(typeerror) error:\n", "# self.delta_table_object.validate_table()\n", "# actual_error = str(error.exception)\n", "\n", "# # assert actual return value equal expectated return value\n", "# self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "\n", "# def test_validate_filelocation_1(self):\n", "# # create dataframe storage_account-value invalid name-parameter (invalid_storageaccount instead unittest_storageaccount)\n", "# invalid_storageaccount = [\n", "# ('delta', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# invalid_storageaccount_df = spark.createdataframe(invalid_storageaccount, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = invalid_storageaccount_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = true \n", "# self.mock_deltatable.isdeltatable.return_value = true\n", "\n", "# # expected returned value: expect error thrown\n", "# expected_error = f\"the table stored given storage account {self.storage_account}. storage location abfss://{self.container_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}\"\n", "\n", "# # run function: validate_table delta-table object 'self.delta_table_object'\n", "# self.assertraises(valueerror) error:\n", "# self.delta_table_object.validate_table()\n", "# actual_error = str(error.exception)\n", "\n", "# # assert actual return value equal expectated return value\n", "# self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "# def test_validate_filelocation_2(self):\n", "# # create dataframe database-value invalid name-parameter (invalid_database instead unittest_database)\n", "# invalid_database = [\n", "# ('delta', f'{self.container_name}.{self.table_name}', f'abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# invalid_database_df = spark.createdataframe(invalid_database, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = invalid_database_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = true \n", "# self.mock_deltatable.isdeltatable.return_value = true\n", "\n", "# # expected returned value: expect error thrown\n", "# expected_error = f\"the table stored expected container {self.container_name}. storage location abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}\"\n", " \n", "# # run function: validate_table delta-table object 'self.delta_table_object'\n", "# self.assertraises(valueerror) error:\n", "# self.delta_table_object.validate_table()\n", "# actual_error = str(error.exception)\n", "\n", "# # assert actual return value equal expectated return value\n", "# self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", "\n", "# def test_validate_isdeltatable(self):\n", "# # create dataframe table actual deltatable (isdeltatable = false)\n", "# valid_data = [\n", "# ('delta', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n", "# ]\n", "# valid_df = spark.createdataframe(valid_data, self.schema)\n", "\n", "# # configure return_value spar.sql query equal created dataframe\n", "# self.mock_sparksession_sql.return_value = valid_df\n", "# # set return_value delta.table.deltatable.isdeltatable method = false \n", "# self.mock_deltatable.isdeltatable.return_value = false\n", "\n", "# # expected returned value: expect error thrown\n", "# expected_error = f\"the files stored path {self.delta_table_object.silver_path} table {self.table_name} delta table\"\n", "\n", "# # run function: validate_table delta-table object 'self.delta_table_object'\n", "# self.assertraises(typeerror) error:\n", "# self.delta_table_object.validate_table()\n", "# actual_error = str(error.exception)\n", "\n", "# # assert actual return value equal expectated return value\n", "# self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\n", "" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\Test_Deploy_v2.json">{ "name": "test_deploy_v2", "properties": { "folder": { "name": "scrapnotebooks" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "false", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "1", "spark.autotune.trackingid": "d5574afc-4903-47b0-a5e2-96ec07197f2b" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.3", "nodecount": 3, "cores": 4, "memory": 28 }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test deploy\r\n", "**purpose**: notebook use unittest library pyspark.testing.utils library python pyspark execute unit tests spark notebooks folder *deploy*.\r\n", "\r\n", "unit tests: test method expect do, untit test written. unit test isolates method overall environment executes method. aim able define expected outcome method see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute method successfully\r\n", "\r\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\r\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\r\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary packages" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "import unittest\r\n", "from unittest.mock import patch, magicmock, mock, any, call\r\n", "from notebookutils import mssparkutils\r\n", "from io import stringio\r\n", "from pyspark.sql import sparksession\r\n", "import delta.tables dt\r\n", "import pyspark.sql sql\r\n", "" ], "execution_count": 16 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## parameters\r\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "# parameters need given create delta table\r\n", "environment_code = 'dev'" ], "execution_count": 17 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\r\n", "storage_account = f'{env_code}dapstdala1' # storage account delta table expected stored\r\n", "database_name = 'silver'" ], "execution_count": 18 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" methods need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/cleanworkspace" ], "execution_count": 19 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run deploy/deploydeltatable_v2" ], "execution_count": 20 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test classes" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_initialisationdeltatable(unittest.testcase):\r\n", " # test __init__ function deltatable class (see notebook: deploydeltatable_v2)\r\n", " # tests attempt create deltatable-object\r\n", " def setup(self):\r\n", " # set parameters used initialise deltatable-object\r\n", " self.table_name = 'unittest_table'\r\n", " self.database_name = 'unittest_database'\r\n", " self.storage_account = 'unittest'\r\n", " self.table_description = 'unittest created table'\r\n", " self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string' }, { 'column_name': 'column_2','data_type': 'integer' }, { 'column_name': 'column_3', 'data_type': 'decimal(12,5)' }]\"\r\n", "\r\n", "\r\n", " # patch functions called init-function deltatable-class\r\n", " # patched functions self-made functions functions coming external libaries.\r\n", " # self-made functions tested separately, \"expected\" work called functions\r\n", "\r\n", " ## function deploydeltatable notebook \r\n", " self.mock_process_column_info_string_patcher = patch('__main__.process_column_info_string')\r\n", " self.mock_process_column_info_string = self.mock_process_column_info_string_patcher.start()\r\n", " \r\n", " ## method deploydeltatable notebook \r\n", " self.mock_configure_datatypes_patcher = patch('__main__.configure_datatypes')\r\n", " self.mock_configure_datatypes = self.mock_configure_datatypes_patcher.start()\r\n", "\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher-functions \r\n", " self.mock_process_column_info_string_patcher.stop()\r\n", " self.mock_configure_datatypes_patcher.stop()\r\n", "\r\n", "\r\n", " def test_delta_table_init(self):\r\n", " # based parameters defined above, following object-parameters expected created __init__ function deltatable class\r\n", " expected_full_name = 'unittest_database.unittest_table'\r\n", " expected_silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.database_name, self.storage_account, self.table_name)\r\n", " expected_column_dict = { \"column_1\": \"string\", \"column_2\": \"integer\", \"column_3\": \"decimal(12,5)\" }\r\n", "\r\n", " # since output process_column_info_string function called used __init__ function, output (= return_value) mocked\r\n", " # assuming process_column_info_string function works, following output would expected returned:\r\n", " self.mock_process_column_info_string.return_value = { \"column_name\": \"column_1\", \"data_type\": \"string\" }, { \"column_name\": \"column_2\",\"data_type\": \"integer\" }, { \"column_name\": \"column_3\", \"data_type\": \"decimal(12,5)\" }\r\n", "\r\n", " # initialize deltatable-object, thereby calling to-be tested __init__ function\r\n", " delta_table = deltatableclass( self.table_name, self.database_name, self.storage_account, self.table_description, self.column_info )\r\n", "\r\n", " # initialization, following object-parameters created\r\n", " actual_full_name = delta_table.full_name\r\n", " actual_silver_path = delta_table.silver_path\r\n", " actual_column_dict = delta_table.column_dict\r\n", "\r\n", " # validate expected object-parameters equal actual object-parameters\r\n", " self.assertequal(expected_full_name, actual_full_name, 'something went wrong initialisation full_name')\r\n", " self.assertequal(expected_silver_path, actual_silver_path, 'something went wrong initialisation silver_path')\r\n", " self.assertequal(expected_column_dict, actual_column_dict, 'something went wrong initialisation column_dict')\r\n", "" ], "execution_count": 21 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_deploydeltatable(unittest.testcase):\r\n", " # test deploy_delta_table method deltatable class (see notebook: deploydeltatable_v2)\r\n", " # deploy_delta_table method invokes functions, depending certain parameters\r\n", " # tests validate correct functions invoked using different parameters\r\n", " def setup(self):\r\n", "\r\n", " # create deltatable-object using parameters\r\n", " self.table_name = 'unittest_table'\r\n", " self.database_name = 'unittest_database'\r\n", " self.storage_account = 'unittest'\r\n", " self.table_description = 'unittest created table'\r\n", " self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string' }, { 'column_name': 'column_2','data_type': 'integer' }, { 'column_name': 'column_3', 'data_type': 'decimal(12,5)' }]\"\r\n", "\r\n", " self.delta_table_object = deltatableclass(self.table_name, self.database_name, self.storage_account, self.table_description, self.column_info)\r\n", " \r\n", " # patch functions called deploy_delta_table-method deltatable-class\r\n", " # patched functions self-made functions functions coming external libaries.\r\n", " # self-made functions tested separately, \"expected\" work called functions\r\n", "\r\n", " ## method deltatable class\r\n", " self.mock_validate_table_patcher = patch.object(deltatableclass, 'validate_table')\r\n", " self.mock_validate_table = self.mock_validate_table_patcher.start()\r\n", " \r\n", " ## method deltatable class \r\n", " self.mock_create_table_patcher = patch.object(deltatableclass, 'create_table')\r\n", " self.mock_create_table = self.mock_create_table_patcher.start()\r\n", "\r\n", " ## function cleanworkspace notebook\r\n", " self.mock_clean_table_patcher = patch('__main__.clean_table')\r\n", " self.mock_clean_table = self.mock_clean_table_patcher.start()\r\n", "\r\n", " ## method coming library pyspark.sql\r\n", " self.mock_sparksession_sql_patcher = patch.object(sparksession, 'sql')\r\n", " self.mock_sparksession_sql = self.mock_sparksession_sql_patcher.start()\r\n", "\r\n", " ## method coming library notebookutils.mssparkutils\r\n", " self.mock_mssparkutils_fs_patcher = patch.object(mssparkutils, 'fs')\r\n", " self.mock_mssparkutils_fs = self.mock_mssparkutils_fs_patcher.start()\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " self.mock_create_table_patcher.stop()\r\n", " self.mock_validate_table_patcher.stop()\r\n", " self.mock_clean_table_patcher.stop()\r\n", " self.mock_sparksession_sql_patcher.stop()\r\n", " self.mock_mssparkutils_fs_patcher.stop()\r\n", "\r\n", "\r\n", " def test_deploy_delta_table_call_validate(self):\r\n", " # since output spark.sql.collect() function called used deploy_delta_table method, output (= return_value) mocked\r\n", " # test 1:\r\n", " # spark.sql.collect(): returns list tables present database. one tables one needs 'deployed'\r\n", " # mssparkutils.fs.exists: returns 'true', indicating table also exists storage account\r\n", " # expected result: since table exists database storage account, validate_tabe needs called\r\n", "\r\n", " self.mock_sparksession_sql.return_value.collect.return_value = [[mock(), self.table_name], [mock(), 'non_existing_table']]\r\n", " self.mock_mssparkutils_fs.exists.return_value = true\r\n", "\r\n", " # execute function\r\n", " self.delta_table_object.deploy_delta_table()\r\n", "\r\n", " # assert whether validate_table called exactly once\r\n", " self.mock_validate_table.assert_called_once()\r\n", "\r\n", " def test_deploy_delta_table_call_clean_and_create(self):\r\n", " # since output spark.sql.collect() function called used deploy_delta_table method, output (= return_value) mocked\r\n", " # test 2:\r\n", " # spark.sql.collect(): returns list tables present database. one tables one needs 'deployed'\r\n", " # mssparkutils.fs.exists: returns 'false', indicating table exists storage account\r\n", " # expected result: since table exists database, storage account, table \"cleaned\" database created scratch using clean_table create_table\r\n", " \r\n", " self.mock_sparksession_sql.return_value.collect.return_value = [[mock(), self.table_name], [mock(), 'non_existing_table']]\r\n", " self.mock_mssparkutils_fs.exists.return_value = false\r\n", "\r\n", " # execute function: deploy_delta_table\r\n", " self.delta_table_object.deploy_delta_table()\r\n", "\r\n", " # assert whether clean_table create_table called exactly once\r\n", " self.mock_clean_table.assert_called_once()\r\n", " self.mock_create_table.assert_called_once()\r\n", "\r\n", "\r\n", " def test_deploy_delta_table_call_create(self):\r\n", " # since output spark.sql.collect() function called used deploy_delta_table method, output (= return_value) mocked\r\n", " # test 3:\r\n", " # spark.sql.collect(): returns list tables present database. none tables one needs 'deployed'\r\n", " # expected result: since table exist database, created scratch using create_table\r\n", " self.mock_sparksession_sql.return_value.collect.return_value = [[mock(), 'non_existing_table'], [mock(), 'other_non_existing_table']]\r\n", "\r\n", " # execute function\r\n", " self.delta_table_object.deploy_delta_table()\r\n", "\r\n", " # assert whether create_table called exactly once\r\n", " self.mock_create_table.assert_called_once()" ], "execution_count": 22 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_validatetable(unittest.testcase):\r\n", " # test validate_table method deltatable class (see notebook: deploydeltatable_v2)\r\n", " # validate_table method validates features delta table\r\n", " def setup(self):\r\n", " # create deltatable-object using parameters\r\n", " self.table_name = 'unittest_tablename'\r\n", " self.database_name = 'unittest_database'\r\n", " self.storage_account = 'unittest_storageaccount'\r\n", " self.table_description = 'unittest created table'\r\n", " self.column_info = \"[{ 'column_name': 'unittest', 'data_type': 'string' }]\"\r\n", "\r\n", " self.delta_table_object = deltatableclass(self.table_name, self.database_name, self.storage_account, self.table_description, self.column_info)\r\n", "\r\n", " \r\n", " # patch functions called validate_table-method deltatable-class\r\n", " # patched functions self-made functions functions coming external libaries.\r\n", " # self-made functions tested separately, \"expected\" work called functions\r\n", "\r\n", " ## method coming library pyspark.sql\r\n", " self.mock_sparksession_sql_patcher = patch.object(sparksession, 'sql')\r\n", " self.mock_sparksession_sql = self.mock_sparksession_sql_patcher.start()\r\n", "\r\n", " ## method coming library delta.table\r\n", " self.mock_deltatable_patcher = patch.object(dt, 'deltatable')\r\n", " self.mock_deltatable = self.mock_deltatable_patcher.start()\r\n", "\r\n", " # validate_table method queries details 'to deployed' delta table.\r\n", " # details checked are: format, name, location\r\n", " # method, spark.sql returns dataframe values\r\n", " # here, schema relevant parameters created. \r\n", " # schema used testing-methods (below) create spark dataframes relevant table details tested\r\n", " self.schema = dt.structtype([\r\n", " dt.structfield(name='format', datatype=sql.types.stringtype(), nullable=false),\r\n", " dt.structfield(name='name', datatype=sql.types.stringtype(), nullable=false),\r\n", " dt.structfield(name='location', datatype=sql.types.stringtype(), nullable=false)\r\n", " ])\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " self.mock_sparksession_sql_patcher.stop()\r\n", " self.mock_deltatable_patcher.stop()\r\n", "\r\n", " def test_validate_table(self):\r\n", " # create dataframe values passed validate_table method\r\n", " valid_data = [\r\n", " ('delta', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " valid_df = spark.createdataframe(valid_data, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = valid_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = true \r\n", " self.mock_deltatable.isdeltatable.return_value = true\r\n", "\r\n", " # expected returned value: true -&gt; delta table validated passed checks\r\n", " expected_return = true\r\n", "\r\n", " # run function: validate_table delta-table object 'self.delta_table_object'\r\n", " actual_return = self.delta_table_object.validate_table()\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_return, actual_return, f'the return value validate_delta method \"{actual_return}\", expected return \"{expected_return}\".')\r\n", "\r\n", " def test_validate_table_format(self):\r\n", " # create dataframe format-value invalid (csv instead delta)\r\n", " invalid_format = [\r\n", " ('csv', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " invalid_format_df = spark.createdataframe(invalid_format, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = invalid_format_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = true \r\n", " self.mock_deltatable.isdeltatable.return_value = true\r\n", "\r\n", " # expected returned value: expect error thrown\r\n", " expected_error = \"the format table delta table\"\r\n", " \r\n", " # run function: validate_table delta-table object 'self.delta_table_object'\r\n", " self.assertraises(typeerror) error:\r\n", " self.delta_table_object.validate_table()\r\n", " actual_error = str(error.exception)\r\n", " \r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "\r\n", " def test_validate_full_table_name_1(self):\r\n", " # create dataframe database-value invalid name-parameter (invalid_database instead unittest_database)\r\n", " invalid_database = [\r\n", " ('delta', f'invalid_database.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " invalid_database_df = spark.createdataframe(invalid_database, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = invalid_database_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = true \r\n", " self.mock_deltatable.isdeltatable.return_value = true\r\n", "\r\n", " # expected returned value: expect error thrown\r\n", " expected_error = f\"the name table {self.table_name} namespace {self.database_name} match name table invalid_database.{self.table_name}\"\r\n", " self.assertraises(typeerror) error:\r\n", " self.delta_table_object.validate_table()\r\n", " \r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "\r\n", " def test_validate_full_table_name_2(self):\r\n", " # create dataframe tablename-value invalid name-parameter (invalid_tablename instead unittest_table)\r\n", " invalid_tablename = [\r\n", " ('delta', f'{self.database_name}.invalid_tablename', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " invalid_tablename_df = spark.createdataframe(invalid_tablename, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = invalid_tablename_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = true \r\n", " self.mock_deltatable.isdeltatable.return_value = true\r\n", "\r\n", " # expected returned value: expect error thrown\r\n", " expected_error = f\"the name table {self.table_name} namespace {self.database_name} match name table {self.database_name}.invalid_tablename\"\r\n", " \r\n", " # run function: validate_table delta-table object 'self.delta_table_object'\r\n", " self.assertraises(typeerror) error:\r\n", " self.delta_table_object.validate_table()\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "\r\n", "\r\n", " def test_validate_filelocation_1(self):\r\n", " # create dataframe storage_account-value invalid name-parameter (invalid_storageaccount instead unittest_storageaccount)\r\n", " invalid_storageaccount = [\r\n", " ('delta', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " invalid_storageaccount_df = spark.createdataframe(invalid_storageaccount, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = invalid_storageaccount_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = true \r\n", " self.mock_deltatable.isdeltatable.return_value = true\r\n", "\r\n", " # expected returned value: expect error thrown\r\n", " expected_error = f\"the table stored given storage account {self.storage_account}. storage location abfss://{self.database_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}\"\r\n", "\r\n", " # run function: validate_table delta-table object 'self.delta_table_object'\r\n", " self.assertraises(valueerror) error:\r\n", " self.delta_table_object.validate_table()\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "\r\n", " def test_validate_filelocation_2(self):\r\n", " # create dataframe database-value invalid name-parameter (invalid_database instead unittest_database)\r\n", " invalid_database = [\r\n", " ('delta', f'{self.database_name}.{self.table_name}', f'abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " invalid_database_df = spark.createdataframe(invalid_database, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = invalid_database_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = true \r\n", " self.mock_deltatable.isdeltatable.return_value = true\r\n", "\r\n", " # expected returned value: expect error thrown\r\n", " expected_error = f\"the table stored expected container {self.database_name}. storage location abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}\"\r\n", " \r\n", " # run function: validate_table delta-table object 'self.delta_table_object'\r\n", " self.assertraises(valueerror) error:\r\n", " self.delta_table_object.validate_table()\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "\r\n", "\r\n", " def test_validate_isdeltatable(self):\r\n", " # create dataframe table actual deltatable (isdeltatable = false)\r\n", " valid_data = [\r\n", " ('delta', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n", " ]\r\n", " valid_df = spark.createdataframe(valid_data, self.schema)\r\n", "\r\n", " # configure return_value spar.sql query equal created dataframe\r\n", " self.mock_sparksession_sql.return_value = valid_df\r\n", " # set return_value delta.table.deltatable.isdeltatable method = false \r\n", " self.mock_deltatable.isdeltatable.return_value = false\r\n", "\r\n", " # expected returned value: expect error thrown\r\n", " expected_error = f\"the files stored path {self.delta_table_object.silver_path} table {self.table_name} delta table\"\r\n", "\r\n", " # run function: validate_table delta-table object 'self.delta_table_object'\r\n", " self.assertraises(typeerror) error:\r\n", " self.delta_table_object.validate_table()\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error validate_delta \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "" ], "execution_count": 23 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_processcolumninfostring(unittest.testcase):\r\n", " # test process_column_info_string function deploydeltatable_v2 notebook\r\n", " # process_column_info_string method converts string json-object\r\n", " def setup(self):\r\n", " # set-up needed. keeping function potential future use\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # tear-down needed. keeping function potential future use\r\n", " return\r\n", "\r\n", " def test_process_column_info_string(self):\r\n", " # define parameters:\r\n", " # push following string object function\r\n", " self.string_object = '[{\"column_name\":\"column_1\", \"data_type\":\"string\"}, {\"column_name\":\"column_2\", \"data_type\":\"integer\"}, {\"column_name\":\"column_3\", \"data_type\":\"decimal(12,5)\"}]'\r\n", "\r\n", " # expected returned object following: \r\n", " expected_object = [{'column_name':'column_1', 'data_type':'string'}, {'column_name':'column_2', 'data_type':'integer'}, {'column_name':'column_3', 'data_type':'decimal(12,5)'}]\r\n", " \r\n", " # invoke function process_column_info_string self.string_object parameter\r\n", " actual_object = process_column_info_string(self.string_object)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_object, actual_object, f\"the column info string processed correctly. expected {expected_object}, got {actual_object}\")\r\n", "\r\n", " def test_process_column_info_string_2(self):\r\n", " # define parameters:\r\n", " # push following string object function\r\n", " self.string_object = '{\"key\":\"value\"}'\r\n", "\r\n", " # expected returned object following: \r\n", " expected_object = {\"key\": \"value\"}\r\n", "\r\n", " # invoke function process_column_info_string self.string_object parameter\r\n", " actual_object = process_column_info_string(self.string_object)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_object, actual_object, f\"the column info string processed correctly. expected {expected_object}, got {actual_object}\")\r\n", "\r\n", " def test_process_column_info_string_invalid_json(self):\r\n", " # define parameters:\r\n", " # push following string object function\r\n", " self.string_object = '[invalid_json_object]'\r\n", "\r\n", " # expect following dictionary returned:\r\n", " expected_error = f\"the string_object passed cannot converted json object. passed string: {self.string_object}\"\r\n", "\r\n", " # run function\r\n", " self.assertraises(typeerror) error:\r\n", " process_column_info_string(self.string_object)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error process_column_info_string \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "" ], "execution_count": 24 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_configuredatatypes(unittest.testcase):\r\n", " # test configure_data_types function deploydeltatable_v2 notebook\r\n", " # configure_data_types method creates dictionary datatypes respective sql.types-method\r\n", " # \r\n", " def setup(self):\r\n", "\r\n", " # patch functions called validate_table-method deltatable-class\r\n", " # patched functions self-made functions functions coming external libaries.\r\n", " # self-made functions tested separately, \"expected\" work called functions\r\n", "\r\n", " ## function deploydeltatable_v2 notebook\r\n", " self.mock_add_decimal_datatype_patcher = patch('__main__.add_decimal_datatype')\r\n", " self.mock_add_decimal_datatype = self.mock_add_decimal_datatype_patcher.start()\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " self.mock_add_decimal_datatype_patcher.stop()\r\n", "\r\n", " def test_configure_datatypes(self):\r\n", " # pass empty dictionary configure_datatypes function\r\n", " empty_dictionary = dict()\r\n", "\r\n", " # expectation: dictionary returned default data types\r\n", " expected_dictionary = {\"array\": sql.types.arraytype(sql.types.stringtype()), \"binary\": sql.types.binarytype(), \r\n", " \"boolean\": sql.types.booleantype(), \"date\": sql.types.datetype(), \"string\": sql.types.stringtype(), \r\n", " \"timestamp\": sql.types.timestamptype(), \"decimal\": sql.types.decimaltype(38,4), \"float\": sql.types.floattype(), \r\n", " \"byte\": sql.types.bytetype(), \"integer\": sql.types.integertype(), \"long_integer\": sql.types.longtype()\r\n", " }\r\n", "\r\n", " # run function get actual returned dictionary\r\n", " actual_dictionary = configure_datatypes(empty_dictionary)\r\n", "\r\n", " # assert expectations match actuals\r\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\r\n", "\r\n", " def test_configure_datatypes_decimal(self):\r\n", " # dictionary contains value 'decimal', add_decimal_datatype called\r\n", " decimal_dictionary = {'key': 'decimal'}\r\n", "\r\n", " # run function\r\n", " configure_datatypes(decimal_dictionary)\r\n", "\r\n", " # validate function called exactly once\r\n", " self.mock_add_decimal_datatype.assert_called_once()\r\n", "\r\n", " def test_configure_datatypes_decimal_2(self):\r\n", " # dictionary contains value 'decimal', add_decimal_datatype called\r\n", " decimal_dictionary = {'key': 'decimal(12,5)'}\r\n", "\r\n", " # run function\r\n", " configure_datatypes(decimal_dictionary)\r\n", "\r\n", " # validate function called exactly once\r\n", " self.mock_add_decimal_datatype.assert_called_with('decimal(12,5)', any)\r\n", "\r\n", " def test_configure_datatypes_non_decimal(self):\r\n", " # dictionary contain value 'decimal', add_decimal_datatype called\r\n", " decimal_dictionary = {'key': 'string'}\r\n", "\r\n", " # run function\r\n", " configure_datatypes(decimal_dictionary)\r\n", "\r\n", " # validate function called\r\n", " self.mock_add_decimal_datatype.assert_not_called()\r\n", "\r\n", " def test_configure_datatypes_multiple_decimals(self):\r\n", " # dictionary contains multiple values 'decimal', add_decimal_datatype called multiple times\r\n", " decimal_dictionary = {'key': 'decimal', 'other_key': 'decimal(12,5)'}\r\n", "\r\n", " # run function\r\n", " configure_datatypes(decimal_dictionary)\r\n", "\r\n", " # validate function called instance 'decimal'\r\n", " # expected calls:\r\n", " expected_calls = [call('decimal', any), call('decimal(12,5)', any)]\r\n", " self.mock_add_decimal_datatype.assert_has_calls(expected_calls, any_order=true)\r\n", "\r\n", " def test_configure_datatypes_invalid_datatypes(self):\r\n", " # dictionary contains invalid_datatype, error thrown\r\n", " decimal_dictionary = {'key': 'string', 'other_key': 'invalid_datatype'}\r\n", "\r\n", " # expect following dictionary returned:\r\n", " expected_error = f\"the value 'invalid_datatype' configured datatype current set-up.\"\r\n", "\r\n", " # run function\r\n", " self.assertraises(valueerror) error:\r\n", " configure_datatypes(decimal_dictionary)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error add_decimal_datatype \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "" ], "execution_count": 25 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_adddecimaldatatype(unittest.testcase):\r\n", " # test process_column_info_string function deploydeltatable_v2 notebook\r\n", " # process_column_info_string method converts string json-object\r\n", " def setup(self):\r\n", " # set-up needed. keeping function potential future use\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # tear-down needed. keeping function potential future use\r\n", " return\r\n", "\r\n", " def test_add_decimal_datatype(self):\r\n", " # define parameters passed add_decimal_datatype function\r\n", " types = dict()\r\n", " value = 'decimal(12,5)'\r\n", "\r\n", " # expect following dictionary returned:\r\n", " expected_dictionary = {\"decimal(12,5)\": sql.types.decimaltype(12,5)}\r\n", "\r\n", " # get actual dictionary returned running function\r\n", " actual_dictionary = add_decimal_datatype(value, types)\r\n", "\r\n", " # assert equality dictionaries\r\n", " self.assertequal(expected_dictionary, actual_dictionary, f\"the returned datatypes dictionary match expectations. actual: {actual_dictionary} ; \\n expected=: {expected_dictionary}\")\r\n", "\r\n", " def test_invalid_value(self):\r\n", " # define parameters passed add_decimal_datatype function\r\n", " types = dict()\r\n", " value = 'decimal(12)'\r\n", "\r\n", " # expect following dictionary returned:\r\n", " expected_error = f\"the decimal value {value} valid datatype value cannot configured correctly. aborting delta table deployment...\"\r\n", "\r\n", " # run function\r\n", " self.assertraises(valueerror) error:\r\n", " add_decimal_datatype(value, types)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # assert actual return value equal expectated return value\r\n", " self.assertequal(expected_error, actual_error, f'the returned error add_decimal_datatype \"{actual_error}\", expected error \"{expected_error}\".')\r\n", "" ], "execution_count": 26 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# # first, create file write every line it\r\n", "# log_file = 'output.txt'\r\n", "# open(log_file, \"w\") f:\r\n", "# runner = unittest.texttestrunner(f, verbosity=2)\r\n", "# findings = unittest.main(argv=[''], testrunner=runner, verbosity=2, exit=false, warnings='ignore')\r\n", "\r\n", "# # then, read what's list write designated container\r\n", "# open(log_file, \"r\") f:\r\n", "# test = f.read()\r\n", "# print(test)" ], "execution_count": 27 } ] } }</file><file name="src\synapse\studio\notebook\Test_ErrorHandling.json">{ "name": "test_errorhandling", "properties": { "folder": { "name": "modules/test_modules" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore2", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "677a0b75-d53d-4491-82a6-f7233a0966a7" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore2", "name": "devdapsspcore2", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore2", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test classes\r\n", "**purpose**: notebook using unittest library pyspark.testing.utils library python pyspark execute unit tests spark notebooks folder *modules*.\r\n", "\r\n", "unit tests: test method expect do, untit test written. unit test isolates method overall environment executes method. aim able define expected outcome method see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute method successfully\r\n", "\r\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\r\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\r\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary libraries" ] }, { "cell_type": "code", "source": [ "import unittest\r\n", "from unittest.mock import patch, magicmock, mock, call, any\r\n", "\r\n", "import pyspark.testing.utils # pyspark version 3.5 contains useful functions (assertdataframeequal, assertschemaequal...) exist current 3.3.1 pyspark version\r\n", "\r\n", "from pyspark.sql.types import structtype,structfield, stringtype, integertype\r\n", "from delta.tables import deltatable\r\n", "from pyspark.sql.functions import col\r\n", "import pyspark\r\n", "\r\n", "import pandas pd\r\n", "from pyspark.sql import sparksession\r\n", "\r\n", "from io import stringio\r\n", "import os\r\n", "from py4j.java_gateway import javaobject\r\n", "\r\n", "import time\r\n", "import datetime\r\n", "from notebookutils import mssparkutils" ], "execution_count": 47 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## parameters\r\n", "the parameters contain values used throughout notebook, changed externally calling notebook. testing notebook internally, defaults cell used" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\r\n", "environment_code = 'dev'" ], "execution_count": 48 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\r\n", "\r\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\r\n", "# according ci/cd, notebook never even deployed there, case ;)\r\n", "if env_code ['dev', 'int']:\r\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 49 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# define global parameters use entire notebook\r\n", "container_name = 'unittest'\r\n", "storage_account = f'{env_code}dapstdala1'" ], "execution_count": 50 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" methods need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": 51 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test cases" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### get_exception_information()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# case test get_exception_information defined errorhandling notebook\r\n", "class test_errorhandling_getexceptioninformation(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown testcass class\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls function get_exception_information()\r\n", " # success: throw py4javaerror (java) expect underlying error returned\r\n", " def test_getexceptioninformation_errorhandling_success_javaerror(self):\r\n", " # preprocess\r\n", " # catch java-native error: filenotfounderror\r\n", " try:\r\n", " mssparkutils.fs.ls(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/nonexisting_foldername')\r\n", " except exception thrown_error:\r\n", " exception = thrown_error\r\n", "\r\n", " expected_exception_class = 'filenotfoundexception'\r\n", " expected_exception_message = 'operation failed: \"the specified path exist.\"'\r\n", " \r\n", " # execute\r\n", " # run function test\r\n", " actual_exception_class, actual_exception_message = get_exception_information(exception)\r\n", "\r\n", " # evaluate\r\n", " # validate expectations match actual results\r\n", " self.assertequal(expected_exception_class, actual_exception_class, f'[errorhandling:getexceptioninformation] 1. expectations match actuals: \\nexpected: {expected_exception_class} \\nactual: {actual_exception_class}')\r\n", " self.assertregex(actual_exception_message, expected_exception_message, f'[errorhandling:getexceptioninformation] 1. expectations match actuals: \\nexpected: {expected_exception_message} \\nactual: {actual_exception_message}')\r\n", "\r\n", " # success: throw valueerror (python) expect underlying error returned\r\n", " def test_getexceptioninformation_errorhandling_success_pythonerror(self):\r\n", " # preprocess\r\n", " # catch java-native error: filenotfounderror\r\n", " exception = valueerror(\"this custom error message\")\r\n", "\r\n", " expected_exception_class = 'valueerror'\r\n", " expected_exception_message = 'this custom error message'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_exception_class, actual_exception_message = get_exception_information(exception)\r\n", "\r\n", " # evaluate\r\n", " # validate expectations match actual results\r\n", " self.assertequal(expected_exception_class, actual_exception_class, f'[errorhandling:getexceptioninformation] 2. expectations match actuals: \\nexpected: {expected_exception_class} \\nactual: {actual_exception_class}')\r\n", " self.assertequal(expected_exception_message, actual_exception_message, f'[errorhandling:getexceptioninformation] 2. expectations match actuals: \\nexpected: {expected_exception_message} \\nactual: {actual_exception_message}')\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_errorhandling_getexceptioninformation)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error errorhandling getexceptioninformation tests, something went wrong!\")" ], "execution_count": 52 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### is_custom_error()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# case test is_custom_error defined errorhandling notebook\r\n", "class test_errorhandling_iscustomerror(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown testcass class\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls function is_custom_error()\r\n", "\r\n", " # success: initialize errorhandling class-object valid arguments \r\n", " def test_iscustomerror_errorhandling_success_customerror(self):\r\n", " # preprocess\r\n", " error_class = 'middlewareerror'\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_result = is_custom_error(error_class)\r\n", "\r\n", " # evaluate\r\n", " self.asserttrue(actual_result, f'[errorhandling:iscustomerror] expected {error_class} custom error, is_custom_error returned false')\r\n", "\r\n", " # success: initialize errorhandling class-object valid arguments \r\n", " def test_iscustomerror_errorhandling_success_noncustomerror(self):\r\n", " # preprocess\r\n", " error_class = 'valueerror'\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_result = is_custom_error(error_class)\r\n", "\r\n", " # evaluate\r\n", " self.assertfalse(actual_result, f'[errorhandling:iscustomerror] expected {error_class} custom error, is_custom_error returned true')\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_errorhandling_iscustomerror)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error errorhandling iscustomerror tests, something went wrong!\")" ], "execution_count": 53 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### handle_exception()\r\n", "**dev-info**: test case implicitly tests custom error class. therefore, explicit test case written classes" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# case test handle_exception defined errorhandling notebook\r\n", "class test_errorhandling_handleexception(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " # define env_code parameter use throughout class\r\n", " self.env_code = env_code\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # teardown testcass class\r\n", " return\r\n", " \r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls function handle_exception()\r\n", "\r\n", " # success: initialize errorhandling class-object valid arguments \r\n", " def test_handleexception_errorhandling_success_customerror_configurationerror(self):\r\n", " # preprocess\r\n", " error_message = \"custom message configuration error\"\r\n", " notebook_name = \"test_errorhandling\"\r\n", " class_name = \"test_errorhandling_handleexception\"\r\n", " error_class = \"configurationerror\"\r\n", "\r\n", " error_object = configurationerror(custom_message=error_message, notebook_name=notebook_name, class_name=class_name)\r\n", "\r\n", " expected_result = {\r\n", " \"custom_message\": error_message,\r\n", " \"custom_error_class\": error_class,\r\n", " \"error_location_in_notebooks\": f\"[{notebook_name}:{class_name}:]\",\r\n", " \"responsible_team\": \"dap engineers (add email)\"\r\n", " }\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = handle_exceptions(exception_class=error_class, exception_message=error_message, exception_object=error_object)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_result, actual_result)\r\n", "\r\n", " # success: initialize errorhandling class-object valid arguments \r\n", " def test_handleexception_errorhandling_success_customerror_middlewareerror(self):\r\n", " # preprocess\r\n", " error_message = \"custom message middleware error\"\r\n", " notebook_name = \"test_errorhandling\"\r\n", " class_name = \"test_errorhandling_handleexception\"\r\n", " error_class = \"middlewareerror\"\r\n", "\r\n", " error_object = middlewareerror(custom_message=error_message, notebook_name=notebook_name, class_name=class_name)\r\n", "\r\n", " expected_result = {\r\n", " \"custom_message\": error_message,\r\n", " \"custom_error_class\": error_class,\r\n", " \"error_location_in_notebooks\": f\"[{notebook_name}:{class_name}:]\",\r\n", " \"responsible_team\": \"middleware team (add email)\"\r\n", " }\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = handle_exceptions(exception_class=error_class, exception_message=error_message, exception_object=error_object)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_result, actual_result)\r\n", "\r\n", " # success: initialize errorhandling class-object valid arguments \r\n", " def test_handleexception_errorhandling_success_noncustomerror(self):\r\n", " # preprocess\r\n", " error_message = \"this value error message\"\r\n", " error_class = \"valueerror\"\r\n", "\r\n", " error_object = valueerror(error_message)\r\n", "\r\n", " expected_result = {\r\n", " \"custom_message\": \"exception: unknown exception. rerun task %{task_name}% (task_id: %{task_id}% , plan_id: %{plan_id}%) debug-mode see error coming from. debugging, make sure add use case errorhandling notebook.\",\r\n", " \"custom_error_class\": \"exception\",\r\n", " \"error_location_in_notebooks\": \"[metanotebook::]\",\r\n", " \"responsible_team\": \"dap core engineers\",\r\n", " \"python_error_class\": error_class,\r\n", " \"python_error_message\": error_message\r\n", " }\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = handle_exceptions(exception_class=error_class, exception_message=error_message, exception_object=error_object)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_result, actual_result)\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_errorhandling_handleexception)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error errorhandling handleexception tests, something went wrong!\")" ], "execution_count": 60 } ] } }</file><file name="src\synapse\studio\notebook\Test_GenericFunctions.json">{ "name": "test_genericfunctions", "properties": { "folder": { "name": "functions/test_functions" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "targetsparkconfiguration": { "referencename": "core_configuration", "type": "sparkconfigurationreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "c99f2635-0b8c-4347-b2eb-15baa94c6dc4" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30, "targetsparkconfiguration": "core_configuration" }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test genericfunctions\n", "**purpose**: notebook using unittest library pyspark.testing.utils library python pyspark execute unit tests functions spark notebook *genericfunctions* folder *functions*.\n", "\n", "unit tests: test function expect do, untit test written. unit test isolates function overall environment executes function. aim able define expected outcome function see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute function successfully\n", "\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary libraries" ] }, { "cell_type": "code", "source": [ "import unittest\n", "from unittest.mock import patch, magicmock, any, call\n", "\n", "import pyspark.testing.utils # pyspark version 3.5 contains useful functions (assertdataframeequal, assertschemaequal...) exist current 3.3.1 pyspark version\n", "import os\n", "import datetime\n", "import ast" ], "execution_count": 1 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## initialize environment_code parameter\r\n", "in almost notebooks, reference parameter found. ci/cd pipelines overwrite specific environment-references defined notebooks. since notebooks deployed several environments, use environment_code parameter chosen. \r\n", "the environment_code used spark session environment_code-argument defined. argument linked core_configuration, found manage -&gt; apache spark configurations\r\n", "the use parameters arguments that, deployment, 1 place references need overwritten." ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\n", "environment_code = 'dev'" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\n", "\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 3 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# define global parameters use entire notebook\r\n", "container_name = 'unittest'\r\n", "storage_account = f'{env_code}dapstdala1'" ], "execution_count": 4 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" functions need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/genericfunctions" ], "execution_count": 6 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# define test classes" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## validate_argument()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_validate_argument(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: string argument part allowed_list\r\n", " def test_validateargument_genericfunctions_success_stringvalue(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_string\"\r\n", " argument_value = \"success\"\r\n", " allowed_list = [\"success\", \"foo\", \"bar\"]\r\n", "\r\n", " \r\n", " # execute &amp; evalute\r\n", " # run function test\r\n", " # validate error thrown valid string value\r\n", " try:\r\n", " validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n", " except valueerror:\r\n", " self.fail(f\"[genericfunctions:validateargument] 1. validate_argument() thrown value-error\")\r\n", " except exception:\r\n", " self.fail(f\"[genericfunctions:validateargument] 1. validate_argument() thrown exception\")\r\n", " \r\n", " # success: integer argument part allowed_list\r\n", " def test_validateargument_genericfunctions_success_integervalue(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_integer\"\r\n", " argument_value = 123\r\n", " allowed_list = [456, 123, 789]\r\n", "\r\n", " \r\n", " # execute &amp; evalute\r\n", " # run function test\r\n", " # validate error thrown valid integer value\r\n", " try:\r\n", " validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n", " except valueerror:\r\n", " self.fail(f\"[genericfunctions:validateargument] 2. validate_argument() thrown value-error\")\r\n", " except exception:\r\n", " self.fail(f\"[genericfunctions:validateargument] 2. validate_argument() thrown exception\")\r\n", " \r\n", " # success: list argument part allowed_list\r\n", " def test_validateargument_genericfunctions_success_listvalue(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_integer\"\r\n", " argument_value = [\"list\", \"of\", \"strings\"]\r\n", " allowed_list = [[\"foo\", \"bar\"], [\"list\", \"of\", \"strings\"]]\r\n", "\r\n", " \r\n", " # execute &amp; evalute\r\n", " # run function test\r\n", " # validate error thrown valid list value\r\n", " try:\r\n", " validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n", " except valueerror:\r\n", " self.fail(f\"[genericfunctions:validateargument] 3. validate_argument() thrown value-error\")\r\n", " except exception:\r\n", " self.fail(f\"[genericfunctions:validateargument] 3. validate_argument() thrown exception\")\r\n", "\r\n", "\r\n", " # failure: argument part allowed_list\r\n", " def test_validateargument_genericfunctions_failure_notinallowedlist(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_failtest\"\r\n", " argument_value = \"failure\"\r\n", " allowed_list = [\"success\", \"foo\", \"bar\"]\r\n", " # define expected return values\r\n", " expected_error = f\"the {argument_name}-argument '{str(argument_value)}' listed allowed {argument_name} list: {allowed_list}\"\r\n", " \r\n", " # execute\r\n", " # run function test\r\n", " self.assertraises(valueerror) error:\r\n", " validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n", "\r\n", " # evaluate\r\n", " # expect error returned\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f\"[genericfunctions:validateargument] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_validate_argument)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error validate_argument() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## validate_argumentlist()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_validateargumentlist(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: string argument part allowed_list\r\n", " def test_validateargumentlist_genericfunctions_success_singleitem(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_string\"\r\n", " argument_list = [\"success\"]\r\n", " allowed_list = [\"success\", \"foo\", \"bar\"]\r\n", "\r\n", " \r\n", " # execute &amp; evalute\r\n", " # run function test\r\n", " # validate error thrown valid list one item\r\n", " try:\r\n", " validate_argument_list(argument_name=argument_name, argument_list=argument_list, allowed_list=allowed_list)\r\n", " except valueerror:\r\n", " self.fail(f\"[genericfunctions:validateargument] 1. validate_argument_list() thrown value-error\")\r\n", " except exception:\r\n", " self.fail(f\"[genericfunctions:validateargument] 1. validate_argument_list() thrown exception\")\r\n", "\r\n", " # success: integer argument part allowed_list\r\n", " def test_validateargumentlist_genericfunctions_success_multipleitems(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_integer\"\r\n", " argument_list = [\"success\", \"foo\", \"success\", \"bar\"]\r\n", " allowed_list = [\"success\", \"foo\", \"bar\"]\r\n", "\r\n", " \r\n", " # execute &amp; evalute\r\n", " # run function test\r\n", " # validate error thrown valid list multiple items\r\n", " try:\r\n", " validate_argument_list(argument_name=argument_name, argument_list=argument_list, allowed_list=allowed_list)\r\n", " except valueerror:\r\n", " self.fail(f\"[genericfunctions:validateargument] 2. validate_argument_list() thrown value-error\")\r\n", " except exception:\r\n", " self.fail(f\"[genericfunctions:validateargument] 2. validate_argument_list() thrown exception\")\r\n", " \r\n", "\r\n", " # failure: argument part allowed_list\r\n", " def test_validateargumentlist_genericfunctions_failure_notinallowedlist(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " argument_name = \"unittest_failtest\"\r\n", " argument_list = [\"success\", \"failure\", \"error\"]\r\n", " allowed_list = [\"success\", \"foo\", \"bar\"]\r\n", "\r\n", " # define expected return values\r\n", " not_allowed_list = [\"failure\", \"error\"]\r\n", " expected_error = f\"[genericfunctions] {argument_name}-argument take following list give parameters: '{', '.join(not_allowed_list)}'; allowed values: {', '.join(allowed_list)}\"\r\n", "\r\n", " \r\n", " # execute\r\n", " # run function test\r\n", " self.assertraises(valueerror) error:\r\n", " validate_argument_list(argument_name=argument_name, argument_list=argument_list, allowed_list=allowed_list)\r\n", "\r\n", " # evaluate\r\n", " # validate thrown error matches expectations\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f\"[genericfunctions:validateargument] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_validateargumentlist)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error validate_argumentlist() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## validate_linkedservice()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_validatelinkedservice(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: string argument part allowed_list\r\n", " def test_validatelinkedservice_genericfunctions_success_exists(self):\r\n", " # preprocess\r\n", " linked_service = \"ls_dap_sql_meta\"\r\n", "\r\n", " # execute &amp; evalute\r\n", " # run function test\r\n", " # validate error thrown valid (existing) linked service\r\n", " try:\r\n", " validate_linked_service(linked_service=linked_service)\r\n", " except exception:\r\n", " self.fail(f\"[genericfunctions:validateargument] 1. validate_argument_list() thrown exception\")\r\n", "\r\n", "\r\n", " # tests:\r\n", " # success: call mssparkutils.credentials.getpropertiesall()\r\n", " def test_validatelinkedservice_genericfunctions_success_functioncalls(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " linked_service = \"ls_dap_sql_meta\"\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " patch.object(mssparkutils.credentials, \"getpropertiesall\") mock_get_properties_all:\r\n", " validate_linked_service(linked_service=linked_service)\r\n", "\r\n", " # evaluate\r\n", " # validate mocked functions called expected arguments\r\n", " mock_get_properties_all.assert_called_once_with(linked_service)\r\n", "\r\n", "\r\n", " # failure: linked service exist\r\n", " def test_validatelinkedservice_genericfunctions_success_doesnotexist(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " linked_service = \"does_not_exist\"\r\n", " # define expected return values\r\n", " expected_error = f\"linked service {linked_service} exist published\"\r\n", " \r\n", " # execute\r\n", " # run function test\r\n", " self.assertraises(valueerror) error:\r\n", " validate_linked_service(linked_service=linked_service)\r\n", "\r\n", "\r\n", " # evaluate\r\n", " # validate thrown error matches expectations\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f\"[genericfunctions:validateargument] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_validatelinkedservice)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error validate_linked_service() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# filter_list()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_filterlist(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " # create list temporary file objects\r\n", " self.files = []\r\n", " path= f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/filter_list/'\r\n", " path_contents = mssparkutils.fs.ls(path)\r\n", " item path_contents:\r\n", " self.files.append(item)\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: return two files using regex filter\r\n", " def test_filterlist_genericfunctions_success_with_regex(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " pattern = '.*(mmdai).*'\r\n", " # define expected return values\r\n", " # dev-note: classes take entire object, need extract name compare expected_filtered_list\r\n", " expected_filtered_list=[\"mmdai\",\"mmdaily5365\"]\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_filtered_list=filter_list(self.files,pattern)\r\n", "\r\n", " # evalute\r\n", " # validate returned filtered list matches expectations\r\n", " actual_filtered_list_name=[]\r\n", " item actual_filtered_list:\r\n", " actual_filtered_list_name.append(item.name)\r\n", " self.assertequal(expected_filtered_list,actual_filtered_list_name, f\"[genericfunctions:filterlist] 1.the lists equal. \\nexpected {expected_filtered_list} \\nactual{actual_filtered_list_name}\")\r\n", " \r\n", " # success: return one file using non-regex\r\n", " def test_filterlist_genericfunctions_success_without_regex(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " pattern = '^mmdai$'\r\n", " # define expected return values\r\n", " # dev-note: classes take entire object, need extract name compare expected_filtered_list\r\n", " expected_filtered_list=[\"mmdai\"]\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_filtered_list=filter_list(self.files,pattern)\r\n", "\r\n", " # evalute\r\n", " # validate returned filtered list matches expectations\r\n", " actual_filtered_list_name=[]\r\n", " item actual_filtered_list:\r\n", " actual_filtered_list_name.append(item.name)\r\n", " self.assertequal(expected_filtered_list,actual_filtered_list_name, f\"[genericfunctions:filterlist] 2. lists equal. \\nexpected {expected_filtered_list} \\nactual{actual_filtered_list_name}\")\r\n", " \r\n", " # success: return files using pattern='*'\r\n", " def test_filterlist_genericfunctions_success_all(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " pattern = '.*'\r\n", " # define expected return values\r\n", " # dev-note: classes take entire object, need extract name compare expected_filtered_list\r\n", " expected_filtered_list=[\"mmdai\",\"mmdaily5365\",\"reqgrg3\"]\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_filtered_list=filter_list(self.files,pattern)\r\n", "\r\n", " # evalute\r\n", " # validate returned filtered list matches expectations\r\n", " actual_filtered_list_name=[]\r\n", " item actual_filtered_list:\r\n", " actual_filtered_list_name.append(item.name)\r\n", " self.assertequal(expected_filtered_list,actual_filtered_list_name, f\"[genericfunctions:filterlist] 3. lists equal. \\nexpected {expected_filtered_list} \\nactual{actual_filtered_list_name}\")\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_filterlist)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error filter_list() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## contains_regex_special_chars()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_containsregexspecialchars(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: return true value contains regex character\r\n", " def test_containsregexspecialchars_genericfunctions_success_true(self):\r\n", " # prepare\r\n", " # define function arguments\r\n", " value = \"this string .* regex characters\"\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result:bool = contains_regex_special_chars(value=value)\r\n", "\r\n", " # evaluate\r\n", " # validate returned result true\r\n", " self.asserttrue(actual_result, f\"[genericfunctions:containsregexspecialchars] expected return true string regex, got {actual_result}\")\r\n", "\r\n", " # success: return false value contain regex character\r\n", " def test_containsregexspecialchars_genericfunctions_success_false(self):\r\n", " # prepare\r\n", " # define function arguments\r\n", " value = \"this string without regex characters\"\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result:bool = contains_regex_special_chars(value=value)\r\n", "\r\n", " # evaluate\r\n", " # validate returned result false\r\n", " self.assertfalse(actual_result, f\"[genericfunctions:containsregexspecialchars] expected return false string regex, got {actual_result}\")\r\n", "\r\n", " # success: return false value contains slash\r\n", " def test_containsregexspecialchars_genericfunctions_success_slash(self):\r\n", " # prepare\r\n", " # define function arguments\r\n", " value = \"pass/a/path/like/string\"\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result:bool = contains_regex_special_chars(value=value)\r\n", "\r\n", " # evaluate\r\n", " # validate returned result false\r\n", " self.assertfalse(actual_result, f\"[genericfunctions:containsregexspecialchars] expected return false string regex, got {actual_result}\")\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_containsregexspecialchars)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error list_directory_content() tests, something went wrong!\")" ], "execution_count": 8 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## split_string_at_regex()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_splitstringatregex(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: basic split simple regex\r\n", " def test_splitstringatregex_genericfunctions_basic_success(self):\r\n", " # preprocess\r\n", " value = \"this/is/a/$w+/regex/path\"\r\n", " split_character = \"/\"\r\n", " expected_path = \"this/is/a\"\r\n", " \r\n", " # execute\r\n", " actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n", " \r\n", " # evaluate\r\n", " self.assertequal(actual_path, expected_path)\r\n", "\r\n", "\r\n", " # success: split space regex\r\n", " def test_splitstringatregex_genericfunctions_space_split_success(self):\r\n", " # preprocess\r\n", " value = \"this *w+ regex .* sentence\"\r\n", " split_character = \" \"\r\n", " expected_path = \"this is\"\r\n", " \r\n", " # execute\r\n", " actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n", " \r\n", " # evaluate\r\n", " self.assertequal(actual_path, expected_path)\r\n", "\r\n", "\r\n", " # success: regex present, return full string\r\n", " def test_splitstringatregex_genericfunctions_no_regex_success(self):\r\n", " # preprocess\r\n", " value = \"this/is/a/normal/path\"\r\n", " split_character = \"/\"\r\n", " expected_path = \"this/is/a/normal/path\"\r\n", " \r\n", " # execute\r\n", " actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n", " \r\n", " # evaluate\r\n", " self.assertequal(actual_path, expected_path)\r\n", "\r\n", "\r\n", " # success: multiple split characters\r\n", " def test_splitstringatregex_genericfunctions_multiple_split_characters_success(self):\r\n", " # preprocess\r\n", " value = \"part1;part2;part3/$w+/part4\"\r\n", " split_character = \";\"\r\n", " expected_path = \"part1;part2\"\r\n", " \r\n", " # execute\r\n", " actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n", " \r\n", " # evaluate\r\n", " self.assertequal(actual_path, expected_path)\r\n", "\r\n", "\r\n", " # edge case: empty string input\r\n", " def test_splitstringatregex_genericfunctions_empty_string(self):\r\n", " # preprocess\r\n", " value = \"\"\r\n", " split_character = \"/\"\r\n", " \r\n", " # execute &amp; evaluate\r\n", " self.assertraises(configurationerror):\r\n", " split_string_at_regex(value=value, split_character=split_character)\r\n", "\r\n", "\r\n", " # edge case: string regex characters\r\n", " def test_splitstringatregex_genericfunctions_only_regex_characters(self):\r\n", " # preprocess\r\n", " value = \".*$w+\"\r\n", " split_character = \"/\"\r\n", " expected_path = \".*$w+\"\r\n", " \r\n", " # execute\r\n", " actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n", " \r\n", " # evaluate\r\n", " self.assertequal(actual_path, expected_path)\r\n", "\r\n", "\r\n", " # edge case: split character present\r\n", " def test_splitstringatregex_genericfunctions_no_split_character(self):\r\n", " # preprocess\r\n", " value = \"thisisaregex$w+\"\r\n", " split_character = \",\"\r\n", " expected_path = \"thisisaregex$w+\"\r\n", " \r\n", " # execute\r\n", " actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n", " \r\n", " # evaluate\r\n", " self.assertequal(actual_path, expected_path)\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_splitstringatregex)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error list_directory_content() tests, something went wrong!\")" ], "execution_count": 12 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## locate_static_path()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# implicitly test locate_static_path function calling list_directory_contents\r\n", "class test_genericfunctions_locatestaticpath(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: list set files path regex-expression part regex \r\n", " def test_locatestaticpath_genericfunctions_regexpath(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/.*/filtered'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n", " \r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " self.assertequal(len(actual_files), 2)\r\n", " file_paths = [file.path file actual_files]\r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 1. could find file 'subfolder/filtered/included.txt' returned file_list: {file_paths}\")\r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 1. could find file 'other_subfolder/filtered/included.txt' returned file_list: {file_paths}\") \r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 1. unexpectedly found file 'subfolder/ignored/skipped.txt' returned file_list: {file_paths}\")\r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 1. unexpectedly found file 'other_subfolder/ignored/skipped.txt' returned file_list: {file_paths}\")\r\n", "\r\n", "\r\n", " # success: list set files path ends regex expression uses subfolder\r\n", " def test_locatestaticpath_genericfunctions_regexpath_1(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/.*'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n", " \r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " self.assertequal(len(actual_files), 2)\r\n", " file_paths = [file.path file actual_files]\r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 2. could find file 'subfolder/filtered/included.txt' returned file_list: {file_paths}\")\r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 2. unexpectedly found file 'other_subfolder/filtered/included.txt' returned file_list: {file_paths}\") \r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 2. could find 'subfolder/ignored/skipped.txt' returned file_list: {file_paths}\") \r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 2. unexpectedly found file 'other_subfolder/ignored/skipped.txt' returned file_list: {file_paths}\")\r\n", "\r\n", "\r\n", "\r\n", " # success: list set files path ends regex expression lists everything parent folder\r\n", " def test_locatestaticpath_genericfunctions_regexpath_2(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/.*'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n", " \r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " self.assertequal(len(actual_files), 4)\r\n", " file_paths = [file.path file actual_files]\r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 3. could find file 'subfolder/filtered/included.txt' returned file_list: {file_paths}\")\r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 3. could find file 'other_subfolder/filtered/included.txt' returned file_list: {file_paths}\") \r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 3. could find file 'subfolder/ignored/skipped.txt' returned file_list: {file_paths}\") \r\n", " self.assertin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 3. could find file 'other_subfolder/ignored/skipped.txt' returned file_list: {file_paths}\")\r\n", "\r\n", "\r\n", " # success: list set files path regex expression filters non-existing path\r\n", " def test_locatestaticpath_genericfunctions_regexpath_3(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/.*/nonexisting_subfolder'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n", " \r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " self.assertequal(len(actual_files), 0)\r\n", " file_paths = [file.path file actual_files]\r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 4. unexpectedly found file 'subfolder/filtered/included.txt' returned file_list: {file_paths}\")\r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 4. unexpectedly found file 'other_subfolder/filtered/included.txt' returned file_list: {file_paths}\") \r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 4. unexpectedly found file 'subfolder/ignored/skipped.txt' returned file_list: {file_paths}\") \r\n", " self.assertnotin(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths ,f\"[genericfunctions:locatestaticpath] 4. unexpectedly found file 'other_subfolder/ignored/skipped.txt' returned file_list: {file_paths}\")\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_locatestaticpath)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error list_directory_content() tests, something went wrong!\")" ], "execution_count": 19 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## list_directory_content()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_listdirectorycontent(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: return full directory check expected files directory\r\n", " def test_listdirectorycontent_genericfunctions_success(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/list_directory_content'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_files, actual_folders = list_directory_content(path, list(), list())\r\n", "\r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " self.assertequal(len(actual_files), 3)\r\n", " self.assertequal(len(actual_folders), 2)\r\n", " file_paths = [file.path file actual_files]\r\n", " folder_paths = [folder.path folder actual_folders]\r\n", " self.assertin(f'{path}/root.txt', file_paths)\r\n", " self.assertin(f'{path}/parent/parent.csv', file_paths)\r\n", " self.assertin(f'{path}/parent/child/child.json', file_paths)\r\n", " self.assertin(f'{path}/parent', folder_paths)\r\n", " self.assertin(f'{path}/parent/child', folder_paths)\r\n", " \r\n", " def test_listdirectorycontent_folder_does_not_exist_success(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/list_directory_content/folderdoesnotexist'\r\n", " # execute\r\n", " # run method test\r\n", " files,folders=list_directory_content(path,list(),list())\r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " self.asserttrue(len(files) ==0, f'[genericfunctions:list_directory_content] expected files returned found {files}.')\r\n", " self.asserttrue(len(folders) ==0,f'[genericfunctions:list_directory_content] expected folders returned found {folders}.')\r\n", "\r\n", " def test_listdirectorycontent_folder_failed(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path_failed=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunction/list_directory_content/folderdoesnotexist'\r\n", " list_files=[]\r\n", " list_folders=[\"test\"]\r\n", " expected_message=\"there issue parent folder [test]\"\r\n", " # execute\r\n", " # run method test\r\n", " self.assertraises(exception) error:\r\n", " list_directory_content(path_failed,list_files,list_folders)\r\n", " # evaluate\r\n", " # validate returned list contains expected values\r\n", " actual_error = str(error.exception)\r\n", " actual_message = ast.literal_eval(actual_error)['custom_message']\r\n", " self.assertequal(expected_message, actual_message, f\"[genericfunctions:list_directory_content] expected error match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_listdirectorycontent)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error list_directory_content() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## remove_extension()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_removeextension(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: successfully remove extension\r\n", " def test_removeextension_genericfunctions_success_exists(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " string_with_extension = 'string.txt'\r\n", " extension_value = 'txt'\r\n", " # define expected return values\r\n", " expected_result = 'string'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = remove_extension(string_with_extension = string_with_extension, extension_value=extension_value)\r\n", "\r\n", " # evaluate\r\n", " # validate result matches expectations\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:removeextension] expected result match actual: \\nexpected: {expected_result} \\nactual: {actual_result}\")\r\n", "\r\n", "\r\n", " # failure: throw error since '.' string value\r\n", " def test_removeextension_genericfunctions_failure(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " string_with_extension = 'string'\r\n", " extension_value = 'txt'\r\n", " # define expected return values\r\n", " expected_message = f\"trying remove extension {extension_value} {string_with_extension}, extension end string\"\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", "\r\n", " self.assertraises(configurationerror) error:\r\n", " remove_extension(string_with_extension = string_with_extension, extension_value=extension_value)\r\n", " \r\n", " # evaluate\r\n", " # # validate returned error expected error\r\n", " actual_error = str(error.exception)\r\n", " actual_error = ast.literal_eval(actual_error)\r\n", " actual_message = actual_error[\"custom_message\"]\r\n", " self.assertequal(expected_message, actual_message, f\"[genericfunctions:removeextension] expected error message match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", "\r\n", " # success: successfully remove extension\r\n", " def test_removeextension_genericfunctions_success_noextension(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " string_with_extension = 'string'\r\n", " extension_value = none\r\n", " # define expected return values\r\n", " expected_result = 'string'\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = remove_extension(string_with_extension = string_with_extension, extension_value=extension_value)\r\n", "\r\n", " # evaluate\r\n", " # validate result matches expectations\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:removeextension] expected result match actual: \\nexpected: {expected_result} \\nactual: {actual_result}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_removeextension)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error remove_extension() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## process_string_to_json()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_processstringtojson(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: return single json object\r\n", " def test_processstringtojson_genericfunctions_success_simpleobject(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " string_object = \"{'key': 'value', 'foo': 'bar'}\"\r\n", " # define expected return values\r\n", " expected_result = {'key': 'value', 'foo': 'bar'}\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " actual_result = process_string_to_json(string_object=string_object)\r\n", "\r\n", " # evaluate\r\n", " # validate result matches expectations\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:processstringtojson] expected result match actual: \\nexpected: {expected_result} \\nactual: {actual_result}\")\r\n", "\r\n", " # success: return list json objects\r\n", " def test_processstringtojson_genericfunctions_success_complexobject(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " string_object = \"[{'key': 'value', 'foo': 'bar'}, {'key2': 'value2', 'bar': 'foo'}]\"\r\n", " # define expected return values\r\n", " expected_result = [{'key': 'value', 'foo': 'bar'}, {'key2': 'value2', 'bar': 'foo'}]\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " actual_result = process_string_to_json(string_object=string_object)\r\n", "\r\n", " # evaluate\r\n", " # validate result matches expectations\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:processstringtojson] expected result match actual: \\nexpected: {expected_result} \\nactual: {actual_result}\") \r\n", "\r\n", " # failure: invalid json object\r\n", " def test_processstringtojson_genericfunctions_failure(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " string_object = \"not json\"\r\n", " # define expected return values\r\n", " expected_error = f\"the string_object passed cannot converted json object. passed string: {string_object}\"\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " self.assertraises(typeerror) error:\r\n", " process_string_to_json(string_object=string_object)\r\n", "\r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(expected_error, actual_error, f\"[genericfunctions:processstringtojson] expected error message match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\") \r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_processstringtojson)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error process_string_to_json() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## remove_drop_folder()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_removedropfolder(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " #files delete empty folders\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: return list non-empty folder\r\n", " def test_removedropfolder_success(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " self.path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/remove_parent_folder/'\r\n", " path_contents = mssparkutils.fs.ls(self.path)\r\n", " expected_folder_names=['20240802_112048','20240802_112049','20240802_112052','test2','20240802_112053','test3']\r\n", " \r\n", " mssparkutils.fs.rm(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/remove_parent_folder/20240802_112047/to_delete')\r\n", " mssparkutils.fs.rm(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/remove_parent_folder/20240802_112051/test1/to_delete')\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " folder path_contents:\r\n", " remove_dropfolder(str(folder.path))\r\n", " \r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " actual_folder_names=[]\r\n", " actual_files,actual_folders=list_directory_content(self.path, list(), list())\r\n", " folder actual_folders :\r\n", " actual_folder_names.append(folder.name)\r\n", " self.assertequal(actual_folder_names,expected_folder_names, f\"[genericfunctions:filterlist] 3. lists equal. \\nexpected {expected_folder_names} \\nactual{actual_folder_names}\")\r\n", " \r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_removedropfolder)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error process_string_to_json() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## locate_dropfolder()" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_locate_dropfolder(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # success: returns path final folder timestamp\r\n", " def test_locate_dropfolder_success(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path_success=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_dropfolder/test_drop_folder_success/20240802_112047/subfolder/subsubfolder'\r\n", " expected_path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_dropfolder/test_drop_folder_success/20240802_112047'\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " actual_path=locate_dropfolder(path_success)\r\n", " \r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " self.assertequal(actual_path,expected_path, f\"[genericfunctions:locate_dropfolder] 3. lists equal. \\nexpected {expected_path} \\nactual{actual_path}\")\r\n", " \r\n", " # failure: timestamp folder path\r\n", " def test_locate_dropfolder_failed(self): \r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path_failed=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/locate_dropfolder/test_drop_folder_failed/subfolder/subsubfolder'\r\n", " expected_error='you reached parent folder without finding drop folder'\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " self.assertraises(exception) error:\r\n", " locate_dropfolder(path_failed)\r\n", " \r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " actual_error = str(error.exception)\r\n", " self.assertequal(actual_error,expected_error, f\"[genericfunctions:locate_dropfolder] 3. lists equal. \\nexpected {expected_error} \\nactual{actual_error}\")\r\n", "\r\n", " def test_locate_dropfolder_timestamp_file_success(self):\r\n", " # preprocess\r\n", " # define parameters arguments used test-functions\r\n", " path_success=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/list_directory_content/test/integration_tests/data/dv360/prd/media-spending/20241025_112048/dv360_nalo_20240916_011416_1262814714_4763705405.csv'\r\n", " expected_path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/genericfunctions/list_directory_content/test/integration_tests/data/dv360/prd/media-spending/20241025_112048'\r\n", " \r\n", " # execute\r\n", " # run method test\r\n", " actual_path=locate_dropfolder(path_success)\r\n", " # evaluate\r\n", " # validate error matches expectations\r\n", " self.assertequal(actual_path,expected_path, f\"[genericfunctions:locate_dropfolder] 3. lists equal. \\nexpected {expected_path} \\nactual{actual_path}\")\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_locate_dropfolder)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error process_string_to_json() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: test_convertliststolistofdicts\r\n", "# class test convert_lists_to_list_of_dicts() function defined ingestionfunction notebook\r\n", "class test_convertliststolistofdicts(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls convert_lists_to_list_of_dicts function\r\n", "\r\n", " # success: \r\n", " def test_convertliststolistofdicts_succes(self):\r\n", " # preprocess\r\n", " list_of_lists = [\r\n", " [\"value11\", \"value12\"], \r\n", " [\"value21\", \"value22\"]\r\n", " ]\r\n", " list_of_keys = [\"key1\", \"key2\"]\r\n", "\r\n", " expected_result = [\r\n", " {\r\n", " \"key1\": \"value11\", \r\n", " \"key2\": \"value21\"\r\n", " },\r\n", " {\r\n", " \"key1\": \"value12\", \r\n", " \"key2\": \"value22\"\r\n", " }\r\n", " ]\r\n", " \r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " actual_result:list = convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n", " # evaluate\r\n", " self.assertequal(expected_result, actual_result, f\"[ingestionfunctions:convertliststolistsofdicts] expected result match actuals: \\nexpected: {expected_result} \\nactual: {actual_result}\")\r\n", "\r\n", " def test_convertliststolistofdicts_failure_raiseerror_too_many_keys(self):\r\n", " # preprocess\r\n", " list_of_lists = [\r\n", " [\"value11\", \"value12\"], \r\n", " [\"value21\", \"value22\"]\r\n", " ]\r\n", " list_of_keys = [\"key1\", \"key2\", \"key3\"]\r\n", "\r\n", " expected_error = f\"number keys match number lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n", " \r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " self.assertraises(valueerror) error:\r\n", " convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_error, actual_error, f\"[ingestionfunctions:convertliststolistsofdicts] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", " def test_convertliststolistofdicts_failure_raiseerror_too_many_lists(self):\r\n", " # preprocess\r\n", " list_of_lists = [\r\n", " [\"value11\", \"value12\"], \r\n", " [\"value21\", \"value22\"],\r\n", " [\"value31\", \"value32\"]\r\n", " ]\r\n", " list_of_keys = [\"key1\", \"key2\"]\r\n", "\r\n", " expected_error = f\"number keys match number lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n", " \r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " self.assertraises(valueerror) error:\r\n", " convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_error, actual_error, f\"[ingestionfunctions:convertliststolistsofdicts] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_convertliststolistofdicts)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_returnfilteredkeys(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", "\r\n", " # success: return list keys contain value \"test\"\r\n", " def test_genericfunctions_returnfilteredkeys_success_reverse(self):\r\n", " # preprocess\r\n", " # define function arguments\r\n", " obj = {\r\n", " \"column1\": none,\r\n", " \"column2\": {\"test\": false},\r\n", " \"column3\": {\"test\": true},\r\n", " \"column4\": {\"anothertest\": false},\r\n", " \"column5\": {\"test\": false, \"anothertest\": false},\r\n", " \"column6\": {\"test\": true, \"anothertest\": false},\r\n", " }\r\n", " value = \"test\"\r\n", "\r\n", " # define expected values\r\n", " expected_result = [\"column1\", \"column2\", \"column4\", \"column5\"]\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = return_filtered_keys(obj=obj, prop=value, reverse=true)\r\n", " \r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:returnfilteredkeys] 1. expectations match actuals: \\nexpected: {expected_result} \\nactuals:{actual_result}\")\r\n", "\r\n", " # success: return list keys contain value \"test\"\r\n", " def test_genericfunctions_returnfilteredkeys_success_noreverse(self):\r\n", " # preprocess\r\n", " # define function arguments\r\n", " obj = {\r\n", " \"column1\": none,\r\n", " \"column2\": {\"test\": false},\r\n", " \"column3\": {\"test\": true},\r\n", " \"column4\": {\"anothertest\": false},\r\n", " \"column5\": {\"test\": false, \"anothertest\": true},\r\n", " \"column6\": {\"test\": true, \"anothertest\": false},\r\n", " }\r\n", " value = \"test\"\r\n", "\r\n", " # define expected values\r\n", " expected_result = [\"column3\", \"column6\"]\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = return_filtered_keys(obj=obj, prop=value, reverse=false)\r\n", " \r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:returnfilteredkeys] 2. expectations match actuals: \\nexpected: {expected_result} \\nactuals:{actual_result}\")\r\n", "\r\n", " # success: return list keys contain value \"test\"\r\n", " def test_genericfunctions_returnfilteredkeys_success_zeroone(self):\r\n", " # preprocess\r\n", " # define function arguments\r\n", " obj = {\r\n", " \"column1\": none,\r\n", " \"column2\": {\"test\": 0},\r\n", " \"column3\": {\"test\": 1},\r\n", " \"column4\": {\"anothertest\": 0},\r\n", " \"column5\": {\"test\": 0, \"anothertest\": 1},\r\n", " \"column6\": {\"test\": 1, \"anothertest\": 0},\r\n", " }\r\n", " value = \"test\"\r\n", "\r\n", " # define expected values\r\n", " expected_result = [\"column3\", \"column6\"]\r\n", "\r\n", " # execute\r\n", " # run method test\r\n", " actual_result = return_filtered_keys(obj=obj, prop=value, reverse=false)\r\n", " \r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:returnfilteredkeys] 2. expectations match actuals: \\nexpected: {expected_result} \\nactuals:{actual_result}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_returnfilteredkeys)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error process_string_to_json() tests, something went wrong!\")" ], "execution_count": null }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_genericfunctions_createjsonobject(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " \r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " def test_genericfunctions_createjsonobject_sucess( self):\r\n", " # preprocess\r\n", " # define function arguments\r\n", " string='this test'\r\n", " value=3\r\n", " date='2024-12-19'\r\n", " # define expected values\r\n", " expected_result=\"\"\"{\"string\": \"this test\", \"value\": 3, \"date\": \"2024-12-19\"}\"\"\"\r\n", " # execute\r\n", " # run method test\r\n", " actual_result=create_json_object(string=string,value=value,date=date)\r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " self.assertequal(expected_result, actual_result, f\"[genericfunctions:createjsonobject] expectations match actuals: \\nexpected: {expected_result} \\nactuals:{actual_result}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_genericfunctions_createjsonobject)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error process_string_to_json() tests, something went wrong!\")" ], "execution_count": null } ] } }</file><file name="src\synapse\studio\notebook\Test_IngestionFunctions.json">{ "name": "test_ingestionfunctions", "properties": { "folder": { "name": "functions/test_functions" }, "nbformat": 4, "nbformat_minor": 2, "bigdatapool": { "referencename": "devdapsspcore", "type": "bigdatapoolreference" }, "sessionproperties": { "drivermemory": "28g", "drivercores": 4, "executormemory": "28g", "executorcores": 4, "numexecutors": 1, "runasworkspacesystemidentity": true, "conf": { "spark.dynamicallocation.enabled": "true", "spark.dynamicallocation.minexecutors": "1", "spark.dynamicallocation.maxexecutors": "2", "spark.autotune.trackingid": "b61b5575-3fbb-4b1f-9847-29724ce8e9ea" } }, "metadata": { "saveoutput": true, "enabledebugmode": true, "kernelspec": { "name": "synapse_pyspark", "display_name": "synapse pyspark" }, "language_info": { "name": "python" }, "a365computeoptions": { "id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourcegroups/dev-dap-rg-core/providers/microsoft.synapse/workspaces/dev-dap-syn-core/bigdatapools/devdapsspcore", "name": "devdapsspcore", "type": "spark", "endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyapi/versions/2019-11-01-preview/sparkpools/devdapsspcore", "auth": { "type": "aad", "authresource": "https://dev.azuresynapse.net" }, "sparkversion": "3.4", "nodecount": 3, "cores": 4, "memory": 28, "automaticscalejobs": true }, "sessionkeepalivetimeout": 30 }, "cells": [ { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "# test functions\n", "**purpose**: notebook using unittest library pyspark.testing.utils library python pyspark execute unit tests spark notebooks folder *functions*.\n", "\n", "unit tests: test function expect do, untit test written. unit test isolates function overall environment executes function. aim able define expected outcome function see aligns actual result. this, might necessary prepare files, patch functions/methods, mock variables needed execute function successfully\n", "\n", "- prepare files: functions try move open files, generate list files within certain directory. test properly, set files uploaded test-container delta lake. done automatically powershell script.\n", "- patch functions: functions dependent functions methods. functions/methods execute certain logic potentially return set results. however, part unit test aim execute function isolation. this, patching used. here, possible say 'when function x called, return result set' 'when function x called, skip execution entirely'\n", "- mock variables: defining class-instance function requires set parameters, possible mock variables. simply put, means define explicit value parameters/variables, also leave empty. system interpret mocked parameter still execute successfully. can, example, used parameter directly used function passed another function. class-parameter necessary needing defined execute specific method class. " ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## import necessary libraries" ] }, { "cell_type": "code", "source": [ "import unittest\n", "from unittest.mock import patch, magicmock, any, call\n", "\n", "import pyspark.testing.utils # pyspark version 3.5 contains useful functions (assertdataframeequal, assertschemaequal...) exist current 3.3.1 pyspark version\n", "\n", "from pyspark.sql.types import structtype,structfield, stringtype, integertype, timestamptype\n", "from delta.tables import deltatable\n", "\n", "from types import simplenamespace # used mimic json-object returned object\n", "\n", "import pandas pd\n", "from pyspark.sql import sparksession\n", "from pyspark.sql.functions import udf, lit, col\n", "import datetime" ], "execution_count": 1 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "tags": [ "parameters" ] }, "source": [ "## environment base phase notebook currently (dev/acc/prod)\n", "environment_code = 'dev'" ], "execution_count": 2 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "env_code = spark.conf.get('spark.environment_code', environment_code)\n", "\n", "# consistency check: testnotebook may ran development integration stages, never test, acceptance production\n", "# according ci/cd, notebook never even deployed there, case ;)\n", "if env_code ['dev', 'int']:\n", " raise valueerror(f\"testnotebook allowed run outside dev int environment. run invoked {env_code}. canceling...\")" ], "execution_count": 3 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## \"import\" functions need tested" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run functions/ingestionfunctions" ], "execution_count": 4 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/classes" ], "execution_count": 5 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "%run modules/errorhandling" ], "execution_count": 6 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## define test classes" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test case convert_lists_to_list_of_dicts -&gt; move genericfunctions possible work test case\r\n", "# lists = [\r\n", "# [\"string\", \"integer\", \"timestamp\"],\r\n", "# [\"varchar(max)\", \"varchar(4)\", \"date\"]\r\n", "# ]\r\n", "\r\n", "# keys = [\"column_name\", \"data_type\"]\r\n", "\r\n", "# result = convert_lists_to_list_of_dicts(lists=lists, keys=keys)\r\n", "# print(result)\r\n", "# # expected: [{column_name: string, data_type:varchar(max)}, {column_name: integer, data_type:varchar(4)}, {column_name: timestamp, data_type:date}]" ], "execution_count": 7 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: test_convertliststolistofdicts\r\n", "# class test convert_lists_to_list_of_dicts() function defined ingestionfunction notebook\r\n", "class test_convertliststolistofdicts(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls convert_lists_to_list_of_dicts function\r\n", "\r\n", " # success: \r\n", " def test_convertliststolistofdicts_succes(self):\r\n", " # preprocess\r\n", " list_of_lists = [\r\n", " [\"value11\", \"value12\"], \r\n", " [\"value21\", \"value22\"]\r\n", " ]\r\n", " list_of_keys = [\"key1\", \"key2\"]\r\n", "\r\n", " expected_result = [\r\n", " {\r\n", " \"key1\": \"value11\", \r\n", " \"key2\": \"value21\"\r\n", " },\r\n", " {\r\n", " \"key1\": \"value12\", \r\n", " \"key2\": \"value22\"\r\n", " }\r\n", " ]\r\n", " \r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " actual_result:list = convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n", " # evaluate\r\n", " self.assertequal(expected_result, actual_result, f\"[ingestionfunctions:convertliststolistsofdicts] expected result match actuals: \\nexpected: {expected_result} \\nactual: {actual_result}\")\r\n", "\r\n", " def test_convertliststolistofdicts_failure_raiseerror_too_many_keys(self):\r\n", " # preprocess\r\n", " list_of_lists = [\r\n", " [\"value11\", \"value12\"], \r\n", " [\"value21\", \"value22\"]\r\n", " ]\r\n", " list_of_keys = [\"key1\", \"key2\", \"key3\"]\r\n", "\r\n", " expected_error = f\"number keys match number lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n", " \r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " self.assertraises(valueerror) error:\r\n", " convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_error, actual_error, f\"[ingestionfunctions:convertliststolistsofdicts] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", " def test_convertliststolistofdicts_failure_raiseerror_too_many_lists(self):\r\n", " # preprocess\r\n", " list_of_lists = [\r\n", " [\"value11\", \"value12\"], \r\n", " [\"value21\", \"value22\"],\r\n", " [\"value31\", \"value32\"]\r\n", " ]\r\n", " list_of_keys = [\"key1\", \"key2\"]\r\n", "\r\n", " expected_error = f\"number keys match number lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n", " \r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " self.assertraises(valueerror) error:\r\n", " convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_error, actual_error, f\"[ingestionfunctions:convertliststolistsofdicts] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_convertliststolistofdicts)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 8 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: test_replacelistitemsbykeyvalues\r\n", "# class test replace_list_items_by_key_values() function defined ingestionfunction notebook\r\n", "class test_replacelistitemsbykeyvalues(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " def setup(self):\r\n", " # define parameters arguments used test-functions\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher functions\r\n", " return\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls replace_list_items_by_key_values function\r\n", "\r\n", " # success: \r\n", " def test_replacelistitemsbykeyvalues_success(self):\r\n", " # preprocess\r\n", " # define arguments pass function create_partitioning_list()\r\n", " list_items = [\"foo\", \"bar\"]\r\n", " key_value_pairs = {\"test\": \"test\", \"foo\":\"foo\", \"bar\": \"bar\"}\r\n", " expected_result = [\"foo\", \"bar\"]\r\n", "\r\n", " # execute\r\n", " # run create_partitioning_list() define actual returned values\r\n", " actual_result = replace_listitems_by_keyvalues(list_items, key_value_pairs)\r\n", " \r\n", " # evaluate\r\n", " # validate actuals match expectations\r\n", " self.assertequal(expected_result, actual_result, f\"[ingestionfunctions:replacelistitemsbykeyvalues] expected result match actual: \\nexpected: {expected_result} \\nactual: {actual_result}\")\r\n", "\r\n", " # success: \r\n", " def test_replacelistitemsbykeyvalues_failure_listitem_notin_keyvaluepairs(self):\r\n", " # preprocess\r\n", " # define arguments pass function create_partitioning_list()\r\n", " list_items = [\"foo\", \"string\", \"bar\"]\r\n", " key_value_pairs = {\"test\": \"test\", \"foo\":\"foo\", \"bar\": \"bar\"}\r\n", " expected_error = \"key string cannot replaced value. process stopped safety reasons...\"\r\n", "\r\n", " # execute\r\n", " # run convert_lists_to_list_of_dicts() define actual returned values\r\n", " self.assertraises(valueerror) error:\r\n", " replace_listitems_by_keyvalues(list_items, key_value_pairs)\r\n", " actual_error = str(error.exception)\r\n", "\r\n", " # evaluate\r\n", " self.assertequal(expected_error, actual_error, f\"[ingestionfunctions:replacelistitemsbykeyvalues] expected error match actual: \\nexpected: {expected_error} \\nactual: {actual_error}\")\r\n", "\r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_replacelistitemsbykeyvalues)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 9 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: test_createpartitioninglist\n", "# class test create_partioning_list() function defined ingestionfunction notebook\n", "class test_createpartitioninglist(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", " # define parameters arguments used test-functions\n", " self.dataframe = spark.createdataframe(data=[\n", " ('2024-07-17', 'test'),\n", " ('2023-04-03', 'function'),\n", " ('2024-02-28', 'call')\n", " ],\n", " schema = ['date', 'string']\n", " )\n", "\n", " # mock function cast_partitioning_timestamp() notebook ingestionfunction\n", " self.mock_cast_partitioning_timestamp_patcher = patch('__main__.cast_partitioning_timestamp')\n", " self.mock_cast_partitioning_timestamp = self.mock_cast_partitioning_timestamp_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " self.mock_cast_partitioning_timestamp_patcher.stop()\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls create_partioning_list function\n", "\n", " # success: \n", " def test_createpartitioninglist_succes_notimestamp(self):\n", " # preprocess\n", " # define arguments pass function create_partitioning_list()\n", " partitioning_objects = [\n", " {\"name\": \"string\", \"sequence\": 1}\n", " ]\n", "\n", " # define expected returned values\n", " ### expected partition_list returned function\n", " expected_partition_list = ['string']\n", " ### expected dataframe returned function\n", " expected_dataframe = self.dataframe\n", "\n", "\n", " # execute\n", " # run create_partitioning_list() define actual returned values\n", " actual_df, actual_partition_list = create_partioning_list(self.dataframe, partitioning_objects)\n", " \n", " # evaluate\n", " # validate actuals match expectations\n", " self.assertequal(expected_partition_list, actual_partition_list, f\"returned partitioning list match expectations: {actual_partition_list} versus {expected_partition_list}\")\n", " self.assertequal(expected_dataframe, actual_df, f\"returned dataframe match expectations: {actual_df.show()} versus {expected_dataframe.show}\")\n", " # validate cast_partitioning_timestamp called\n", " self.mock_cast_partitioning_timestamp.assert_not_called()\n", "\n", " # success\n", " def test_createpartitioninglist_succes_withtimestamp(self):\n", " # preprocess\n", " # define arguments pass function create_partitioning_list()\n", " partitioning_objects = [\n", " {\"name\": \"date\", \"sequence\": 1, \"datepart\": \"year\"},\n", " {\"name\": \"date\", \"sequence\": 1, \"datepart\": \"day\"}\n", " ]\n", " \n", " # define expected returned values\n", " ### expected partition_list returned function\n", " expected_partition_list = ['p_year', 'p_day']\n", " ### expected dataframe column names returned function\n", " expected_dataframe_columns = ['date', 'string', 'p_year', 'p_day']\n", " ### expected function calls done mocked cast_partitioning_timestamp function\n", " expected_function_calls = [call(any, 'date'), call(any, 'date')]\n", "\n", "\n", " # execute\n", " # run create_partitioning_list() expect error thrown\n", " actual_df, actual_partition_list = create_partioning_list(self.dataframe, partitioning_objects)\n", " actual_dataframe_columns = actual_df.columns\n", "\n", "\n", " # evaluate\n", " # validate actual returned values match expectations\n", " self.assertequal(expected_partition_list, actual_partition_list, f\"returned partitioning list match expectations: {actual_partition_list} versus {expected_partition_list}\")\n", " self.assertequal(expected_dataframe_columns, actual_dataframe_columns, f\"returned dataframe columns match expectations: {actual_dataframe_columns} versus {expected_dataframe_columns}\")\n", " self.mock_cast_partitioning_timestamp.assert_has_calls(expected_function_calls, any_order=true)\n", "\n", "\n", " # failure: invalid value datepart value throw error\n", " def test_createpartitioninglist_fail_withtimestamp(self):\n", " # preprocess\n", " # define arguments pass function create_partitioning_list()\n", " invalid_datepart = 'invalid'\n", " partitioning_objects = [\n", " {\"name\": \"date\", \"sequence\": 1, \"datepart\": \"year\"},\n", " {\"name\": \"date\", \"sequence\": 1, \"datepart\": invalid_datepart}\n", " ]\n", "\n", " # expect following error thrown\n", " expected_error = f'invalid key value datepart: {invalid_datepart}. values year, month, day allowed.'\n", "\n", "\n", " # execute\n", " # run create_partitioning_list() expect error thrown\n", " self.assertraises(valueerror) error:\n", " create_partioning_list(self.dataframe, partitioning_objects)\n", " actual_error = str(error.exception)\n", "\n", " # evaluate\n", " # validate thrown error matches expectations\n", " self.assertequal(expected_error, actual_error, f\"returned error create_partioning_list() match expectations: {actual_error} versus {expected_error}\")\n", " \n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_createpartitioninglist)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 10 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: createpartitioningtimestamp\n", "# class test cast_partitioning_timestamp() function defined ingestionfunctions notebook\n", "class test_createpartitioningtimestamp(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", " # define parameters arguments used test-functions\n", " self.dataframe = spark.createdataframe(data=[\n", " ('2024-07-17', 'test'),\n", " ('2023-04-03', 'function'),\n", " ('2024-02-28', 'call')\n", " ],\n", " schema = ['date', 'string']\n", " )\n", "\n", " return\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls cast_partitioning_timestamp function\n", "\n", " # success: date-column actually timestamp errors expected\n", " def test_createpartitioningtimestamp_succes(self):\n", " # preprocess\n", " # define arguments pass function cast_partitioning_timestamp()\n", " partitioning_key = 'date'\n", " # define expected return values\n", " expected_date = datetime.datetime(2024, 7, 17, 0, 0)\n", " expected_mismatch = 0\n", "\n", "\n", " # execute\n", " # run cast_partitioning_timestamp() function\n", " returned_df = cast_partitioning_timestamp(self.dataframe, partitioning_key)\n", " \n", "\n", " # evaluate\n", " # validate retunrned\n", " actual_date = returned_df.collect()[0][0]\n", " \n", " self.assertequal(expected_date, actual_date, f\"dates equal: {expected_date} versus {actual_date}\")\n", "\n", " # validate casted_date date values result\n", " # dev-note: preventive check catches potential logic-changes casted_values method checks-class\n", " # reverse test (check casted null original not) could cause logic issues future\n", " actual_mismatch = returned_df.filter(\n", " col('casted_date').isnull() &amp;\n", " col('date').isnotnull()\n", " ).count()\n", "\n", " # validate mismatch throw error otherwise\n", " self.assertequal(expected_mismatch, actual_mismatch, \n", " \"the null values casted columns match null values original columns. conversion issue occured checks function casted_values\")\n", "\n", " # failure: date-column timestamp conversion error expected\n", " def test_createpartitioningtimestamp_fail_on_conversion(self):\n", " # preprocess\n", " # define arguments pass udf-function cast_partitioning_timestamp()\n", " partitioning_key = 'string'\n", " # define expected error returned\n", " expected_error = f'casting error: null values column {partitioning_key} casting: could convert {self.dataframe.count()} columns'\n", "\n", "\n", " # execute\n", " # run cast_partitioning_timestamp() function expect error thrown\n", " self.assertraises(assertionerror) error:\n", " cast_partitioning_timestamp(self.dataframe, partitioning_key)\n", " actual_error = str(error.exception)\n", "\n", "\n", " # evaluate\n", " # validate thrown error matches expectations\n", " self.assertequal(expected_error, actual_error, f\"returned error match expectations: {actual_error} versus {expected_error}\")\n", " \n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_createpartitioningtimestamp)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 11 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: skipfirstlines\n", "# class test skip_first_lines() function defined ingestionfunctions notebook\n", "class test_function_skipfirstlines(unittest.testcase):\n", "\n", " def setup(self):\n", " self.skip_lines = 4\n", " self.separator = ';'\n", "\n", " return\n", "\n", " def teardown(self):\n", " return\n", "\n", " def test_skipfirstlines_headerless_return(self):\n", " landing_path_headerless = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/ingestionfunctions/skip_first_lines_headerless.csv'\n", " header = false\n", "\n", " expected_count = 3\n", " expected_headers = ['_c0', '_c1', '_c2']\n", "\n", " df = skip_first_lines(landing_path_headerless, self.separator, header, self.skip_lines)\n", " \n", " actual_count = df.count()\n", " actual_headers = df.columns\n", "\n", " self.assertequal(expected_headers, actual_headers, f\"headerless: returned header-line {actual_headers} match expectations {expected_headers}\")\n", " self.assertequal(expected_count, actual_count, f\"headerless: number rows df {actual_count} match expectations {expected_count}\")\n", "\n", " def test_skipfirstlines_header_return(self):\n", " landing_path_headerless = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/ingestionfunctions/skip_first_lines_headers.csv'\n", " header = true\n", "\n", " expected_count = 3\n", " expected_headers = ['column_1', 'column_2', 'column_3']\n", "\n", " df = skip_first_lines(landing_path_headerless, self.separator, header, self.skip_lines)\n", " \n", " actual_count = df.count()\n", " actual_headers = df.columns\n", "\n", " self.assertequal(expected_headers, actual_headers, f\"headers: returned header-line {actual_headers} match expectations {expected_headers}\")\n", " self.assertequal(expected_count, actual_count, f\"headers: number rows df {actual_count} match expectations {expected_count}\")\n", "\n", "\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_skipfirstlines)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 12 }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: createdeltatable_v3\n", "# class test try_cast_dataframe() function checks-class defined classes notebook\n", "class test_enforcevarcharconstraint(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", " # define parameters arguments used test-functions\n", " return\n", "\n", " def teardown(self):\n", " # stop patcher functions\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # tests:\n", " # define set functions test successful unsuccesful calls try_cast_dataframe function\n", "\n", " # failure: __char_varchar_type_string key contain varchar-value, thrown valueerror\n", " def test_enforcevarcharconstraint_empty_varchar(self):\n", " # preprocess\n", " # define arguments pass udf-function enforce_varchar_constraint()\n", " metadata = {'__char_varchar_type_string': ''}\n", " expected_error = f\"metadata contain correct varchar-definition: {metadata}. expected something like {{'__char_varchar_type_string': 'varchar(xx)'}}\"\n", "\n", " # execute\n", " # run enforce_varchar_constraint() udf expect error thrown\n", " self.assertraises(valueerror) error:\n", " enforce_varchar_constraint(metadata)\n", "\n", " # evaluate\n", " # validate thrown error matches expectations\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"expected error '{expected_error}' thrown. got '{actual_error}' instead\")\n", "\n", " def test_enforcevarcharconstraint_invalid_metadata(self):\n", " # preprocess\n", " # define arguments pass udf-function enforce_varchar_constraint()\n", " metadata = {'column_description': 'this set'}\n", " expected_error = f\"metadata contain correct varchar-definition: {metadata}. expected something like {{'__char_varchar_type_string': 'varchar(xx)'}}\"\n", "\n", " # execute\n", " # run enforce_varchar_constraint() udf expect error thrown\n", " self.assertraises(valueerror) error:\n", " enforce_varchar_constraint(metadata)\n", "\n", " # evaluate\n", " # validate thrown error matches expectations\n", " actual_error = str(error.exception)\n", " self.assertequal(expected_error, actual_error, f\"expected error '{expected_error}' thrown. got '{actual_error}' instead\")\n", "\n", "\n", " def test_max_length_invalid(self):\n", " # preprocess\n", " # define arguments pass udf-function enforce_varchar_constraint()\n", " metadata = {'__char_varchar_type_string': 'varchar(10)'}\n", " df = spark.createdataframe([(\"this long string exceeds max length\",)], [\"col\"])\n", " \n", " # execute\n", " # run enforce_varchar_constraint() udf metadata\n", " varchar_udf = enforce_varchar_constraint(metadata)\n", " # enforce udf dataframe\n", " result_df = df.withcolumn(\"col\", varchar_udf(df[\"col\"]))\n", "\n", " # evaluate\n", " actual_result = result_df.collect()[0][0]\n", " # validate long string set none\n", " self.assertisnone(actual_result)\n", "\n", "\n", " def test_max_length_valid(self):\n", " # preprocess\n", " # define arguments pass udf-function enforce_varchar_constraint()\n", " metadata = {'__char_varchar_type_string': 'varchar(10)'}\n", " expected_result = 'short'\n", " df = spark.createdataframe([('short',)], [\"col\"])\n", "\n", " # execute\n", " # run enforce_varchar_constraint() udf metadata\n", " varchar_udf = enforce_varchar_constraint(metadata)\n", " result_df = df.withcolumn(\"col\", varchar_udf(df[\"col\"]))\n", "\n", " # evaluate\n", " actual_result = result_df.collect()[0][0]\n", " # validate long string set none\n", " self.assertequal(actual_result, 'short')\n", "\n", " def test_udf_type(self):\n", " # preprocess\n", " # define arguments pass udf-function enforce_varchar_constraint()\n", " metadata = {'__char_varchar_type_string': 'varchar(10)'}\n", " \n", " # execute\n", " # run enforce_varchar_constraint() udf metadata\n", " udf_func = enforce_varchar_constraint(metadata)\n", "\n", " # evaluate\n", " # validate dealing object-type variable\n", " self.assertisinstance(udf_func, object)\n", " # validate return-type udf\n", " self.assertequal(udf_func.returntype, stringtype())\n", " \n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_enforcevarcharconstraint)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 13 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: list_directory_content()\n", "function purpose: given directory-path, return recursive list folders files contained path.\n", "" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_function_list_directory_content(unittest.testcase):\n", "\n", " def test1_list_directory_content(self):\n", " # test_1: analyze lowest level directory\n", " # directory unittest/env/timestamp/ expected contain 1 file (\"test1_timestamp.txt\") folders\n", "\n", " # define variables\n", " path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"unittest/env/timestamp/\")\n", "\n", " # execute function\n", " test1_files, test1_folders = list_directory_content(path, list(), list())\n", "\n", " # unit tests\n", " # test file name expect actually returned\n", " expected_file = [\"test1_timestamp.txt\"]\n", " files = [file.name file test1_files]\n", " self.assertequal(files, expected_file)\n", "\n", " # test folder name expect actually returned\n", " expected_folder = []\n", " folder = [folder.name folder test1_folders]\n", " self.assertequal (folder, expected_folder)\n", "\n", " def test2_list_directory_content(self):\n", " # test_2: analyze hihgest level directory\n", " # directory unittest/ expected contain 3 files (\"test1_unittest.txt\", \"test1_env.txt\", \"test1_timestamp.txt\") 2 folders (env timestamp)\n", "\n", " # define variables\n", " path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"unittest\")\n", "\n", " # execute function\n", " test2_files, test2_folders = list_directory_content(path, list(), list())\n", "\n", " # unit tests:\n", " # test number files expect actually returned\n", " expected_files = [\"test1_unittest.txt\", \"test1_env.txt\", \"test1_timestamp.txt\"]\n", " files = [file.name file test2_files]\n", " self.assertcountequal(files, expected_files)\n", "\n", " # test number folders expect actually returned\n", " expected_folders = [\"env\", \"timestamp\"]\n", " folders = [folder.name folder test2_folders]\n", " self.assertcountequal (folders, expected_folders)\n", "\n", " # do: compare lists -&gt; possible library (assertitemsequal expired python v3)" ], "execution_count": 14 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: filter_directory_content\n", "function purpose: get items based certain path, filter certain regex match / file pattern" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_filter_directory_content(unittest.testcase):\n", " def setup(self):\n", " self.mock_list_directory_content_patcher = patch('__main__.list_directory_content')\n", " self.mock_list_directory_content = self.mock_list_directory_content_patcher.start()\n", "\n", " def teardown(self):\n", " self.mock_list_directory_content_patcher.stop()\n", "\n", " def test_filter_directory_content_pass(self):\n", " # test: see 'mock_item' filtered list right items remain (i.e. 'list' inside them)\n", " self.mock_list_directory_content.return_value = [\n", " simplenamespace(name=\"list_item1\"),\n", " simplenamespace(name=\"list_item2\"),\n", " simplenamespace(name=\"mock_item\")], []\n", "\n", " # loop sake unittest, since need .name attribute\n", " # can't really set json-object easily return value inside unit test\n", " filtered_list = [folder.name folder filter_directory_content(\"mock_path/child_path\", \"list\")]\n", "\n", " self.assertequal([\"list_item1\", \"list_item2\"], filtered_list)\n", "\n", " def test_filter_directory_content_pass2(self):\n", " # test: see everything gets returned '*' wildcard operator used\n", " self.mock_list_directory_content.return_value = [\n", " simplenamespace(name=\"list_item1\"),\n", " simplenamespace(name=\"list_item2\"),\n", " simplenamespace(name=\"mock_item\")], []\n", "\n", " # loop sake unittest, since need .name attribute\n", " # can't really set json-object easily return value inside unit test\n", " filtered_list = [folder.name folder filter_directory_content(\"mock_path/child_path\", \"*\")]\n", "\n", " self.assertequal([\"list_item1\", \"list_item2\", \"mock_item\"], filtered_list)\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_filter_directory_content)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 15 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: removerowsdf\n", "function purpose: remove n-rows pyspark dataframe" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_removerowsdf(unittest.testcase):\n", " def setup(self):\n", " self.df_path = f\"abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/ingestionfunctions/remove_rows_dataframe.csv\"\n", "\n", " def test_removerowsdf_pass(self):\n", " # test: see able remove rows dataframe\n", " df = spark.read.load(self.df_path, header=\"true\", sep=\",\", format=\"csv\")\n", "\n", " # save necessary rows check later\n", " row_three = list(df.collect()[2])\n", " row_four = list(df.collect()[3])\n", "\n", " df_new = removerowsdf(df, [0, 1])\n", " row_one_new = list(df_new.collect()[0])\n", " row_two_new = list(df_new.collect()[1])\n", "\n", " # row three original dataframe first row new dataframe\n", " self.assertequal(row_three, row_one_new)\n", "\n", " # row four original dataframe second row new dataframe\n", " self.assertequal(row_four, row_two_new)\n", "\n", " # new dataframe, 8 rows right now\n", " self.assertequal(8, df_new.rdd.count())\n", "\n", " def test_removerowsdf_outofbounds_error(self):\n", " # test: see error thrown index &gt; last index dataframe\n", " df = spark.read.load(self.df_path, header=\"true\", sep=\",\", format=\"csv\")\n", " \n", " self.assertraisesregex(exception, \"^(removerowsdf).*(9).*(10).*(outofbounds)$\"):\n", " df_new = removerowsdf(df, [0, 10])\n", "\n", " def test_removerowsdf_noint_error(self):\n", " # test: see error thrown item list int\n", " df = spark.read.load(self.df_path, header=\"true\", sep=\",\", format=\"csv\")\n", " \n", " self.assertraisesregex(exception, \"^(removerowsdf).*('test' inside rows_to_remove).*(no).*(int).*([oo]nly).*(int).*(allowed)$\"):\n", " df_new = removerowsdf(df, [0, \"test\"])\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_removerowsdf)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 16 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: pk_match()\n", "function purpose: given primary column names delta table raw data, define statement matches names.\n", "\n", " example: delta_table.primary_column_name = raw_data.primary_column_name delta_table.primary_column_name_2 = raw_data.primary_column_name_2" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# test class: ingestionfunctions\n", "# class test pk_match function defined ingestionfunctions notebook\n", "class test_function_pk_match(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", " def setup(self):\n", " return\n", "\n", " def teardown(self):\n", " # stop patcher-functions \n", " return\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", " def test1_pk_match(self):\n", " # test_1: test result one primary column\n", " # given delta table primary column 'id' raw data primary column 'id', return statement 'testaliasold.id = testaliasnew.id'\n", "\n", " # variables\n", " old_alias = 'testaliasold'\n", " new_alias = 'testaliasnew'\n", " pk_columns = dict({ 'id': 'id' })\n", " \n", " # execute function\n", " actual_statement = pk_match(old_alias, new_alias, pk_columns)\n", "\n", " # define expected return value\n", " expected_statement = 'testaliasold.id = testaliasnew.`id`'\n", "\n", " # compare expected actual\n", " self.assertequal(expected_statement, actual_statement)\n", "\n", " def test2_pk_match(self):\n", " # test_2: test result two primary columns\n", " # given delta table primary column 'id' raw data primary column 'id', return statement 'testaliasold.id = testaliasnew.id testaliasold.date = testaliasnew.date'\n", " \n", " # variables\n", " old_alias = 'testaliasold'\n", " new_alias = 'testaliasnew'\n", " pk_columns = dict({ 'id': 'id', 'date': 'date' })\n", " \n", " # execute function\n", " actual_statement = pk_match(old_alias, new_alias, pk_columns)\n", "\n", " # define expected return value\n", " expected_statement = 'testaliasold.id = testaliasnew.`id` testaliasold.date = testaliasnew.`date`'\n", "\n", " # compare expected actual\n", " self.assertequal(expected_statement, actual_statement)\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_pk_match)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 17 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "## test: fill_extraction_date\r\n", "function purpose: determine extraction date source file based configuration option target_options" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "collapsed": false }, "source": [ "# test class: ingestionfunctions\r\n", "# class test fill_extraction_date function defined ingestionfunctions notebook\r\n", "class test_function_fillextractiondate(unittest.testcase):\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # set-up: \r\n", " # define setup teardown functions class\r\n", " # used patch objects used several function-calls\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", " def setup(self):\r\n", " # initialize reusable dataframe\r\n", " self.dataframe = spark.createdataframe(\r\n", " data=[\r\n", " ('20241025_150230', '20241026', '20241032_150230', '20241026', '25102024_150230', '26102024', none, none, none),\r\n", " ('20241025_150230', '20241026', '20241032_150230', 'invalid', '25102024_150230', '26102024', none, none, none),\r\n", " ('20241025_150230', '20241026', '20241032_150230', '20241026', '25102024_150230', '26102024', none, none, none)\r\n", " ],\r\n", " schema='valid_timestamp string, valid_date string, invalid_timestamp string, invalid_date string, european_timestamp string, european_date string, t_extract_date timestamp, t_update_date timestamp, t_insert_date timestamp '\r\n", " )\r\n", " return\r\n", "\r\n", " def teardown(self):\r\n", " # stop patcher-functions \r\n", " return\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", "\r\n", "\r\n", " # ------------------------------------------------------------------------------------------------------------------\r\n", " # tests:\r\n", " # define set functions test successful unsuccesful calls fill_extraction_date() function\r\n", "\r\n", " # success: return dataframe t_extract_date current time\r\n", " def test_fillextractiondate_ingestionfunctions_success_notargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {}\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # get list values column t_extract_date\r\n", " extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatmap(lambda x: x).collect()\r\n", " # validate values t_extract_date none values datatype 'datetime'\r\n", " self.assertisnotnone(extract_date)\r\n", " self.asserttrue(all(item none item extract_date), f\"[ingestionfunctions:fillextractiondate] 1. expected extract date empty: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(isinstance(item, datetime.datetime) item extract_date), f\"[ingestionfunctions:fillextractiondate] 1. expected extract date contain datetime objects: \\nactual: {extract_date}\")\r\n", "\r\n", "\r\n", " # success: return dataframe t_extract_date specific datetime\r\n", " def test_fillextractiondate_ingestionfunctions_success_timestamptargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"valid_timestamp\",\r\n", " \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n", " \"extract_date_format\": \"yyyymmdd_hhmmss\"\r\n", " }\r\n", " }\r\n", "\r\n", " # define expected datetime value values t_extract_date\r\n", " expected_datetime = datetime.datetime(2024, 10, 25, 15, 2, 30)\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # get list values column t_extract_date\r\n", " extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatmap(lambda x: x).collect()\r\n", " # validate values t_extract_date none values datatype 'datetime' value '2024:10:25 15:02:30'\r\n", " self.assertisnotnone(extract_date)\r\n", " self.asserttrue(all(item none item extract_date), f\"[ingestionfunctions:fillextractiondate] 2. expected extract date empty: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(isinstance(item, datetime.datetime) item extract_date), f\"[ingestionfunctions:fillextractiondate] 2. expected extract date contain datetime datatpes: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(actual_datetime == expected_datetime actual_datetime extract_date), f\"[ingestionfunctions:fillextractiondate] 2. expected extract date contain timestamp 2024:10:25 15:02:30: \\nactual: {extract_date}\")\r\n", "\r\n", "\r\n", " # success: return dataframe t_extract_date specific date extracted datetime\r\n", " def test_fillextractiondate_ingestionfunctions_success_timestamptargetoptions_capturinggroup(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"valid_timestamp\",\r\n", " \"regex_expression\": \"(\\\\d{8})_\\\\d{6}\",\r\n", " \"extract_date_format\": \"yyyymmdd\"\r\n", " }\r\n", " }\r\n", " expected_datetime = datetime.datetime(2024, 10, 25, 0, 0)\r\n", "\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # get list values column t_extract_date\r\n", " extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatmap(lambda x: x).collect()\r\n", " # validate values t_extract_date none values datatype 'datetime' value '2024:10:25 00:00'\r\n", " \r\n", " self.assertisnotnone(extract_date)\r\n", " self.asserttrue(all(item none item extract_date), f\"[ingestionfunctions:fillextractiondate] 3. expected extract date empty: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(isinstance(item, datetime.datetime) item extract_date), f\"[ingestionfunctions:fillextractiondate] 3. expected extract date contain datetime datatpes: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(actual_datetime == expected_datetime actual_datetime extract_date), f\"[ingestionfunctions:fillextractiondate] 3. expected extract date contain timestamp 2024:10:25 00:00:00: \\nactual: {extract_date}\")\r\n", " \r\n", " \r\n", "\r\n", " # success: return dataframe t_extract_date specific date\r\n", " def test_fillextractiondate_ingestionfunctions_success_datetargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"valid_date\",\r\n", " \"regex_expression\": \"(\\\\d{8})\",\r\n", " \"extract_date_format\": \"yyyymmdd\"\r\n", " }\r\n", " }\r\n", "\r\n", " expected_datetime = datetime.datetime(2024, 10, 26, 0, 0)\r\n", "\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate \r\n", " # get list values column t_extract_date\r\n", " extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatmap(lambda x: x).collect()\r\n", " # validate values t_extract_date none values datatype 'datetime' value '2024:10:26 00:00'\r\n", " self.assertisnotnone(extract_date)\r\n", " self.asserttrue(all(item none item extract_date), f\"[ingestionfunctions:fillextractiondate] 4. expected extract date empty: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(isinstance(item, datetime.datetime) item extract_date), f\"[ingestionfunctions:fillextractiondate] 4. expected extract date contain datetime datatpes: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(actual_datetime == expected_datetime actual_datetime extract_date), f\"[ingestionfunctions:fillextractiondate] 4. expected extract date contain timestamp 2024:10:26 00:00:00: \\nactual: {extract_date}\")\r\n", " \r\n", "\r\n", " # success: return dataframe t_extract_date converted european timestamp\r\n", " def test_fillextractiondate_ingestionfunctions_success_europeantimestamptargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"european_timestamp\",\r\n", " \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n", " \"extract_date_format\": \"ddmmyyyy_hhmmss\"\r\n", " }\r\n", " }\r\n", "\r\n", " expected_datetime = datetime.datetime(2024, 10, 25, 15, 2, 30)\r\n", "\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # get list values column t_extract_date\r\n", " extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatmap(lambda x: x).collect()\r\n", " # validate values t_extract_date none values datatype 'datetime' value '2024:10:26 15:02:30'\r\n", " \r\n", " self.assertisnotnone(extract_date) # ensure result none\r\n", " self.asserttrue(all(item none item extract_date), f\"[ingestionfunctions:fillextractiondate] 5. expected extract date empty: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(isinstance(item, datetime.datetime) item extract_date), f\"[ingestionfunctions:fillextractiondate] 5. expected extract date contain datetime datatpes: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(actual_datetime == expected_datetime actual_datetime extract_date), f\"[ingestionfunctions:fillextractiondate] 5. expected extract date contain timestamp 2024:10:26 15:02:30: \\nactual: {extract_date}\")\r\n", " \r\n", " \r\n", " # success: return dataframe t_extract_date european date\r\n", " def test_fillextractiondate_ingestionfunctions_success_europeandatetargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"european_date\",\r\n", " \"regex_expression\": \"(\\\\d{8})\",\r\n", " \"extract_date_format\": \"ddmmyyyy\"\r\n", " }\r\n", " }\r\n", "\r\n", " expected_datetime = datetime.datetime(2024, 10, 26, 0, 0)\r\n", "\r\n", "\r\n", " # execute\r\n", " # run function test\r\n", " actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # get list values column t_extract_date\r\n", " extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatmap(lambda x: x).collect()\r\n", " # validate values t_extract_date none values datatype 'datetime' value '2024:10:26 00:00'\r\n", " \r\n", " self.assertisnotnone(extract_date) # ensure result none\r\n", " self.asserttrue(all(item none item extract_date), f\"[ingestionfunctions:fillextractiondate] 6. expected extract date empty: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(isinstance(item, datetime.datetime) item extract_date), f\"[ingestionfunctions:fillextractiondate] 6. expected extract date contain datetime datatpes: \\nactual: {extract_date}\")\r\n", " self.asserttrue(all(actual_datetime == expected_datetime actual_datetime extract_date), f\"[ingestionfunctions:fillextractiondate] 6. expected extract date contain timestamp 2024:10:26 00:00:00: \\nactual: {extract_date}\")\r\n", " \r\n", "\r\n", " # failure: column contains date-like value cannot converted datetime (eg. 31 february xxxx), throw error\r\n", " def test_fillextractiondate_ingestionfunctions_failure_invalidtimestamptargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"invalid_timestamp\",\r\n", " \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n", " \"extract_date_format\": \"yyyymmdd_hhmmss\"\r\n", " }\r\n", " }\r\n", "\r\n", " expected_message = \"extract date could set using extract_column invalid_timestamp\"\r\n", "\r\n", " # execute\r\n", " # run function test expect configurationerror\r\n", " self.assertraises(configurationerror) error:\r\n", " fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " # convert string-error json object get value \"custom_message\"\r\n", " actual_error = str(error.exception)\r\n", " json_string = actual_error.replace(\"'\", '\"')\r\n", " actual_message = json.loads(json_string)[\"custom_message\"]\r\n", " self.assertequal(actual_message, expected_message, f\"[ingestionfunctions:fillextractiondate] 7. expected error match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", " \r\n", " \r\n", " \r\n", " # failure: column contains value date (eg. string), throw error\r\n", " def test_fillextractiondate_ingestionfunctions_failure_invaliddatetargetoptions(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"invalid_date\",\r\n", " \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n", " \"extract_date_format\": \"yyyymmdd_hhmmss\"\r\n", " }\r\n", " }\r\n", "\r\n", " expected_message = \"extract date could set using extract_column invalid_date\"\r\n", " # execute\r\n", " # run function test expect configurationerror\r\n", " self.assertraises(configurationerror) error:\r\n", " fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " # convert string-error json object get value \"custom_message\"\r\n", " actual_error = str(error.exception)\r\n", " json_string = actual_error.replace(\"'\", '\"')\r\n", " actual_message = json.loads(json_string)[\"custom_message\"]\r\n", " self.assertequal(actual_message, expected_message, f\"[ingestionfunctions:fillextractiondate] 8. expected error match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", " \r\n", " # failure: column exist dataframe, throw error\r\n", " def test_fillextractiondate_ingestionfunctions_failure_invalidcolumnname(self):\r\n", " # preprocess\r\n", " # define arguments used call function\r\n", " target_options = {\r\n", " \"extract_date\": {\r\n", " \"column_name\": \"does_not_exist\"\r\n", " }\r\n", " }\r\n", "\r\n", " expected_message = \"column does_not_exist cannot found given dataframe\"\r\n", " # execute\r\n", " # run function test expect configurationerror\r\n", " self.assertraises(configurationerror) error:\r\n", " fill_extraction_date(self.dataframe, target_options)\r\n", "\r\n", " # evaluate\r\n", " # validate expectations match actuals\r\n", " # convert string-error json object get value \"custom_message\"\r\n", " actual_error = str(error.exception)\r\n", " json_string = actual_error.replace(\"'\", '\"')\r\n", " actual_message = json.loads(json_string)[\"custom_message\"]\r\n", " self.assertequal(actual_message, expected_message, f\"[ingestionfunctions:fillextractiondate] 9. expected error match actual: \\nexpected: {expected_message} \\nactual: {actual_message}\")\r\n", " \r\n", "\r\n", "# test_loader = unittest.testloader()\r\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_fillextractiondate)\r\n", "\r\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\r\n", "\r\n", "# len(test_results.errors) != 0:\r\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 18 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: merge_file\n", "\n", "function purpose: given delta table raw data dataframe, insert raw data delta table\n", "" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } }, "collapsed": false }, "source": [ "# test class: initialisationdeltatable\n", "# class test mergefile function defined ingestionfunctions notebook\n", "class test_function_merge_file(unittest.testcase):\n", " # ------------------------------------------------------------------------------------------------------------------\n", " # set-up: \n", " # define setup teardown functions class\n", " # used patch objects used several function-calls\n", " # ------------------------------------------------------------------------------------------------------------------\n", " def setup(self):\n", " # create class-variables use test functions\n", " self.delta_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), false), structfield('t_extract_date', timestamptype(), false), structfield('t_update_date', timestamptype(), false), structfield('t_insert_date', timestamptype(), false)])\n", "\n", " # define primary columns dictionary column_names dictionary needed initiate functions\n", " self.pk_columns_dict = { 'id': 'id' }\n", " self.column_names_dict = { 'id': 'id', 't_load_date_silver': 't_load_date_silver', 't_extract_date':'t_extract_date', 't_update_date':'t_update_date', 't_insert_date':'t_insert_date'}\n", "\n", " # patch functions called init-function deltatable-class\n", " # patched functions self-made functions functions coming external libaries.\n", " # self-made functions tested separately, \"expected\" work called functions\n", "\n", " ## function ingestionfunctions notebook \n", " self.mock_pk_match_patcher = patch('__main__.pk_match')\n", " self.mock_pk_match = self.mock_pk_match_patcher.start()\n", "\n", " ## function ingestionfunctions notebook \n", " self.mock_columnmatch_patcher = patch('__main__.columnmatch')\n", " self.mock_columnmatch = self.mock_columnmatch_patcher.start()\n", "\n", " ## function ingestionfunctions notebook \n", " self.mock_nomatch_patcher = patch('__main__.nomatch')\n", " self.mock_nomatch = self.mock_nomatch_patcher.start()\n", "\n", " ## function ingestionfunctions notebook \n", " self.mock_create_partioning_list_patcher = patch('__main__.create_partioning_list')\n", " self.mock_create_partioning_list = self.mock_create_partioning_list_patcher.start()\n", "\n", " return\n", "\n", " def teardown(self):\n", " # stop patcher-functions \n", " self.mock_pk_match_patcher.stop()\n", " self.mock_columnmatch_patcher.stop()\n", " self.mock_create_partioning_list_patcher.stop()\n", " return\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", "\n", " # ------------------------------------------------------------------------------------------------------------------\n", "\n", " # success: given empty delta table raw data dataframe one row, insert row delta table\n", " def test_mergefile_success(self):\n", " # preprocess\n", " # remove merge_test folder exists\n", " try:\n", " mssparkutils.fs.rm(f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/merge_test', true)\n", " except:\n", " pass\n", "\n", " # create empty spark dataframe delta_schema save dataframe delta table\n", " deltatable = spark.createdataframe([], self.delta_schema)\n", " deltatable.write.mode('overwrite').format(\"delta\").save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n", " deltatable = deltatable.forpath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n", "\n", " # create spark dataframe raw schema containing one row value '0123456789' id column empty timestamp t_load_date_silver\n", " raw_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), true), structfield('t_extract_date', timestamptype(), true), structfield('t_update_date', timestamptype(), true), structfield('t_insert_date', timestamptype(), true)])\n", " rawdata = spark.createdataframe(data=[('0123456789', none, none, none, none)], schema=raw_schema)\n", "\n", " # set return values mocked functions\n", " self.mock_pk_match.return_value = 'olddata.id = newdata.id'\n", " self.mock_columnmatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date',}\n", " self.mock_nomatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date',}\n", "\n", " # expected results: expect id raw data inserted delta table\n", " expected_id = '0123456789'\n", " not_expected_loaddate = 'null'\n", " \n", " # run\n", " # execute function\n", " mergefile(deltatable, rawdata, self.pk_columns_dict, self.column_names_dict, target_options={})\n", "\n", "\n", " # evaluate\n", " # set variables actual results function-execution\n", " actual_df = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n", " actual_id = actual_df.collect()[0].id\n", " actual_loaddate = actual_df.collect()[0].t_load_date_silver\n", "\n", " # validate expected results match actual results\n", " self.assertequal(expected_id, actual_id)\n", " # validate t_load_date_silver longer 'null' value\n", " self.assertnotequal(not_expected_loaddate, actual_loaddate, 'the actual t_load_date_silver still \"null\" changed')\n", " # validate t_load_date_silver type 'datetime'\n", " self.assertis(type(actual_loaddate), datetime.datetime)\n", "\n", " # failure: rawdata dataframe t_load_date_silver column error thrown\n", " def test_mergefile_fail_on_missing_column(self):\n", " # preprocess\n", " # create empty spark dataframe\n", " deltatable = spark.createdataframe([], self.delta_schema)\n", " # create spark dataframe raw schema containing one row value '0123456789' id column empty timestamp load_date_silver\n", " raw_schema = structtype([structfield('id', stringtype(),true)])\n", " rawdata = spark.createdataframe([('0123456789', )], raw_schema)\n", " \n", " # define expected error\n", " expected_error = \"column 't_load_date_silver' exist raw dataframe. something must gone wrong writing landing raw...\"\n", "\n", " # run\n", " # execute function\n", " self.assertraises(valueerror) error:\n", " mergefile(deltatable, rawdata, self.pk_columns_dict, self.column_names_dict, target_options={})\n", "\n", " # evaluate\n", " # get returned exception string\n", " actual_error = str(error.exception)\n", " # validate returned error equal expectated error\n", " self.assertequal(expected_error, actual_error, f'the returned error process_column_info_string \"{actual_error}\", expected error \"{expected_error}\".')\n", "\n", " def test_mergefile_call_partitioning_function(self):\n", " # preprocess\n", " # create empty spark dataframe\n", " partition_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), true), structfield('t_extract_date', timestamptype(), true), structfield('t_update_date', timestamptype(), true), structfield('t_insert_date', timestamptype(), true)])\n", "\n", " deltatable = spark.createdataframe([], partition_schema)\n", " deltatable.write.mode('overwrite').format(\"delta\").partitionby('id').save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_test\"))\n", " deltatable = deltatable.forpath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_test\"))\n", " # create spark dataframe raw schema containing one row value '0123456789' id column empty timestamp load_date_silver\n", " raw_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), true), structfield('t_extract_date', timestamptype(), true), structfield('t_update_date', timestamptype(), true), structfield('t_insert_date', timestamptype(), true)])\n", " rawdata = spark.createdataframe([('0123456789', none, none, none, none)], raw_schema)\n", " target_options = {'partitioning': [{'name': 'id', 'sequence': 1}]}\n", " source_partitions = [{'name': 'id', 'sequence': 1}]\n", "\n", " self.mock_create_partioning_list.return_value = rawdata, [\"id\"]\n", " self.mock_pk_match.return_value = 'olddata.id = newdata.id'\n", " self.mock_columnmatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date'}\n", " self.mock_nomatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date'}\n", " \n", " patch('__main__.fill_extraction_date') mock_fill_extraction_date:\n", " mock_fill_extraction_date.return_value = rawdata\n", " mergefile(deltatable, rawdata, self.pk_columns_dict, self.column_names_dict, target_options=target_options)\n", "\n", " # evaluate\n", " # dev-note: 'rawdata' object used expected function actual function assertionerror thrown\n", " # may actually same, t_load_date_silver added object therefore, assert-method consider object\n", " self.mock_create_partioning_list.assert_called_once_with(any, source_partitions)\n", " mock_fill_extraction_date.assert_called_once_with(dataframe=any, target_options=target_options)\n", "\n", "\n", " def test_mergefile_test_timestamp_partitioning(self):\n", " # preprocess\n", " # create empty spark dataframe\n", " partition_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), false), structfield('t_extract_date', timestamptype(), true), structfield('t_update_date', timestamptype(), true), structfield('t_insert_date', timestamptype(), true), structfield('p_year', integertype(), false), structfield('p_month', integertype(), false)])\n", "\n", " deltatable = spark.createdataframe([], partition_schema)\n", " deltatable.write.mode('overwrite').format(\"delta\").partitionby('p_year', 'p_month').save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_timestamp_test\"))\n", " deltatable = deltatable.forpath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_timestamp_test\"))\n", "\n", " # create spark dataframe raw schema containing one row value '0123456789' id column empty timestamp load_date_silver\n", " raw_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), true), structfield('t_extract_date', timestamptype(), true), structfield('t_update_date', timestamptype(), true), structfield('t_insert_date', timestamptype(), true)])\n", " rawdata = spark.createdataframe([('0123456789', none, none, none, none)], raw_schema)\n", " target_options = {'partitioning': [{'name': 't_load_date_silver', 'sequence': 1, 'datepart': 'year'}, {'name': 't_load_date_silver', 'sequence': 2, 'datepart': 'month'}]}\n", " \n", "\n", " partitioned_rawdata = rawdata.withcolumn('p_year', lit(2024)).withcolumn('p_month', lit(5))\n", " self.mock_create_partioning_list.return_value = partitioned_rawdata, ['p_year', 'p_month']\n", " self.mock_pk_match.return_value = 'olddata.id = newdata.id'\n", " self.mock_columnmatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date', 'p_year': 'newdata.p_year', 'p_month': 'newdata.p_month'}\n", " self.mock_nomatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date', 'p_year': 'newdata.p_year', 'p_month': 'newdata.p_month'}\n", " expected_columns_dict = {**self.column_names_dict, **{'p_year': 'p_year', 'p_month': 'p_month'}}\n", "\n", " # run\n", " mergefile(deltatable, rawdata, self.pk_columns_dict, self.column_names_dict, target_options=target_options)\n", "\n", " # evaluate\n", " # get returned exception string\n", " self.mock_create_partioning_list.assert_called_once_with(any, target_options['partitioning'])\n", " self.mock_columnmatch.assert_called_once_with(expected_columns_dict, 'newdata')\n", "\n", " # success: add 2 raw dataframes (sequentially) delta table check timestamps t_update_date t_insert_date\n", " def test_mergefile_updateexistingrecord(self):\n", " # drop table exists already\n", " try:\n", " mssparkutils.fs.rm(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"), true)\n", " except:\n", " pass\n", " \n", " # preprocess\n", " # create empty delta table\n", " deltatable = spark.createdataframe([], self.delta_schema)\n", " deltatable.write.mode('overwrite').format(\"delta\").save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"))\n", " deltatable = deltatable.forpath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"))\n", " \n", " # create 2 spark dataframes:\n", " # rawdata_one: contains 2 unique values id\n", " # rawdata_two: contains 1 unique value id one value already rawdata_one\n", " raw_schema = structtype([structfield('id', stringtype(),true), structfield('t_load_date_silver', timestamptype(), true), structfield('t_extract_date', timestamptype(), true), structfield('t_update_date', timestamptype(), true), structfield('t_insert_date', timestamptype(), true)])\n", " rawdata_one = spark.createdataframe(\n", " data=[\n", " ('0123456789', none, none, none, none), \n", " ('9876543210', none, none, none, none)\n", " ], \n", " schema=raw_schema\n", " )\n", " rawdata_two = spark.createdataframe(\n", " data=[\n", " ('0123456789', none, none, none, none), \n", " ('5432109876', none, none, none, none)\n", " ], \n", " schema=raw_schema\n", " )\n", "\n", " # mock outputs functions called mergefile()\n", " self.mock_pk_match.return_value = 'olddata.id = newdata.id'\n", " self.mock_columnmatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date'}\n", " self.mock_nomatch.return_value = {'id': 'newdata.id', 't_load_date_silver': 'newdata.t_load_date_silver', 't_extract_date': 'newdata.t_extract_date', 't_update_date': 'newdata.t_update_date', 't_insert_date': 'newdata.t_insert_date'}\n", " \n", "\n", " # run\n", " # 1. ingest rawdata_one deltatable get values t_update_date t_insert_date inserted id\n", " # note: important cache dataframes change different steps \"lose\" initial status dataframe\n", " mergefile(deltatable, rawdata_one, self.pk_columns_dict, self.column_names_dict, target_options={})\n", " \n", " df_one = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"), format=\"delta\").cache()\n", " df_one_updatedate_id1 = df_one.filter(df_one.id == '0123456789').select(\"t_update_date\").collect()\n", " df_one_insertdate_id1 = df_one.filter(df_one.id == '0123456789').select(\"t_insert_date\").collect()\n", " df_one_updatedate_id2 = df_one.filter(df_one.id == '9876543210').select(\"t_update_date\").collect()\n", " df_one_insertdate_id2 = df_one.filter(df_one.id == '9876543210').select(\"t_insert_date\").collect()\n", "\n", " \n", " # 2. ingest rawdata_two deltatabel get values t_update_date t_insert_date existing ids table\n", " deltatable_two = deltatable.forpath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"))\n", " mergefile(deltatable_two, rawdata_two, self.pk_columns_dict, self.column_names_dict, target_options={})\n", " \n", " df_two = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"), format=\"delta\").cache()\n", " df_two_updatedate_id1 = df_two.filter(df_two.id == '0123456789').select(\"t_update_date\").collect()\n", " df_two_insertdate_id1 = df_two.filter(df_two.id == '0123456789').select(\"t_insert_date\").collect()\n", " df_two_updatedate_id2 = df_two.filter(df_two.id == '9876543210').select(\"t_update_date\").collect()\n", " df_two_insertdate_id2 = df_two.filter(df_two.id == '9876543210').select(\"t_insert_date\").collect()\n", " df_two_updatedate_id3 = df_two.filter(df_two.id == '5432109876').select(\"t_update_date\").collect()\n", " df_two_insertdate_id3 = df_two.filter(df_two.id == '5432109876').select(\"t_insert_date\").collect()\n", "\n", " # evaluate\n", " # 1. id1 (0123456789): \n", " # initially, df_one, t_update_date t_insert_date expected same\n", " # ingesting rawdata_two, expect t_update_date different df_two compared previous version df_one\n", " # however, t_insert_date expected changed time\n", " self.assertequal(df_one_updatedate_id1, df_one_insertdate_id1, f'[ingestionfunctions:mergefile] 1. expectations match actuals: \\nexpected {df_one_updatedate_id1}\\n actuals: {df_one_insertdate_id1}')\n", " self.assertnotequal(df_two_updatedate_id1, df_two_insertdate_id1, f'[ingestionfunctions:mergefile] 2. expectations match actuals: \\nexpected {df_two_updatedate_id1}\\n actuals: {df_two_insertdate_id1}')\n", " self.assertequal(df_two_insertdate_id1, df_one_insertdate_id1, f'[ingestionfunctions:mergefile] 3. expectations match actuals: \\nexpected {df_two_insertdate_id1}\\n actuals: {df_one_insertdate_id1}')\n", "\n", " # 2. id2 (9876543210): \n", " # initially, df_one, t_update_date t_insert_date expected same\n", " # ingesting rawdata_two, changes expected happened either column changed time\n", " self.assertequal(df_one_updatedate_id2, df_one_insertdate_id2, f'[ingestionfunctions:mergefile] 4. expectations match actuals: \\nexpected {df_one_updatedate_id2}\\n actuals: {df_one_insertdate_id2}')\n", " self.assertequal(df_two_updatedate_id2, df_two_insertdate_id2, f'[ingestionfunctions:mergefile] 5. expectations match actuals: \\nexpected {df_two_updatedate_id2}\\n actuals: {df_two_insertdate_id2}')\n", " self.assertequal(df_one_insertdate_id2, df_two_insertdate_id2, f'[ingestionfunctions:mergefile] 6. expectations match actuals: \\nexpected {df_one_insertdate_id2}\\n actuals: {df_two_insertdate_id2}')\n", " self.assertequal(df_one_updatedate_id2, df_two_updatedate_id2, f'[ingestionfunctions:mergefile] 7. expectations match actuals: \\nexpected {df_one_updatedate_id2}\\n actuals: {df_two_updatedate_id2}')\n", " \n", "\n", " # 2. id2 (5432109876): \n", " # initially, rawdata_one contain row nothing expected\n", " # ingesting rawdata_two, expect t_update_date t_insert_date same\n", " # safety check: values id1 (0123456789) timestamps staticly created\n", " self.assertequal(df_two_updatedate_id3, df_two_insertdate_id3, f'[ingestionfunctions:mergefile] 8. expectations match actuals: \\nexpected {df_two_updatedate_id3}\\n actuals: {df_two_insertdate_id3}')\n", " self.assertequal(df_two_updatedate_id1, df_two_insertdate_id3, f'[ingestionfunctions:mergefile] 9. expectations match actuals: \\nexpected {df_two_updatedate_id1}\\n actuals: {df_two_insertdate_id3}')\n", " self.assertequal(df_two_updatedate_id1, df_two_insertdate_id3, f'[ingestionfunctions:mergefile] 10. expectations match actuals: \\nexpected {df_two_updatedate_id1}\\n actuals: {df_two_insertdate_id3}')\n", "\n", " # remove dataframes memory (remove cache)\n", " df_two.unpersist()\n", " df_one.unpersist()\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_function_merge_file)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")\n", "" ], "execution_count": 24 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: remove_ext\n", "\n", "function purpose: remove file / path extension given string" ] }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: get_parent_folder\n", "\n", "function purpose: retrieve parent folder file" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_getparentfolder(unittest.testcase):\n", " def test_get_parent_folder_pass(self):\n", " # test: get parent folder child_file depth = 1\n", " parent_folder = get_parent_folder(\"mock_parent_folder/mock_child_file.csv\")\n", "\n", " self.assertequal(parent_folder, \"mock_parent_folder\")\n", "\n", " def test_get_parent_folder_pass2(self):\n", " # test: get parent folder child_file depth = 2\n", " parent_folder = get_parent_folder(\"mock_grandparent_folder/mock_parent_folder/mock_child_file.csv\")\n", "\n", " self.assertequal(parent_folder, \"mock_parent_folder\")\n", "\n", " def test_get_parent_folder_pass3(self):\n", " # test: get parent folder child_file depth = 5\n", " parent_folder = get_parent_folder(\"mock_ggggrandparent_folder/mock_gggrandparent_folder/mock_ggrandparent_folder/mock_grandparent_folder/mock_parent_folder/mock_child_file.csv\")\n", "\n", " self.assertequal(parent_folder, \"mock_parent_folder\")\n", " \n", " def test_get_parent_folder_error(self):\n", " # test: error expected look like path / parent folder\n", " \n", " self.assertraisesregex(exception, \"^(get_parent_folder).*(mock_child_file\\.csv).*(doesn't qualify path).*(doesn't parent folder)$\"):\n", " parent_folder = get_parent_folder(\"mock_child_file.csv\")\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_getparentfolder)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 20 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test: jsons_to_dict\n", "\n", "function purpose: turn list flat-json objects, stored strings, dictionary" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "class test_jsonstodict(unittest.testcase):\n", " def test_jsons_to_dict_pass(self):\n", " # test see everything works properly\n", " input_dict = ['{\"mock_key1\": 1}', '{\"mock_key2\": 2}']\n", "\n", " output_dict = jsons_to_dict(input_dict)\n", " self.assertequal({'mock_key1': 1, 'mock_key2': 2}, output_dict)\n", "\n", " def test_jsons_to_dict_pass2(self):\n", " # test see everything works properly multiple json objects one string\n", " input_dict = ['{\"mock_key1\": 1}, {\"mock_key2\": 2}']\n", "\n", " output_dict = jsons_to_dict(input_dict)\n", " self.assertequal({'mock_key1': 1, 'mock_key2': 2}, output_dict)\n", "\n", " def test_jsons_to_dict_parse_error(self):\n", " # test see parsing multiple json objects one string fails there's typo\n", " input_dict = ['{\"mock_k 1}, {\"mock_key2\": 2}']\n", "\n", " self.assertraises(exception) parse_error:\n", " output_dict = jsons_to_dict(input_dict)\n", "\n", " self.assertequal(str(parse_error.exception), 'unable parse string {\"mock_k1} json')\n", "\n", " def test_jsons_to_dict_parse_error2(self):\n", " # test see error thrown get json typo inside\n", " input_dict = ['{\"mock_k 1}', '{\"mock_key2\": 2}']\n", "\n", " self.assertraises(exception) parse_error:\n", " output_dict = jsons_to_dict(input_dict)\n", " \n", " self.assertequal(str(parse_error.exception), 'unable parse string {\"mock_k 1} json')\n", "\n", "# test_loader = unittest.testloader()\n", "# test_suite = test_loader.loadtestsfromtestcase(test_jsonstodict)\n", "\n", "# test_results = unittest.texttestrunner(verbosity=2).run(test_suite)\n", "\n", "# len(test_results.errors) != 0:\n", "# raise exception(\"error header tests, something went wrong!\")" ], "execution_count": 21 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### execute test classes" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# unittest.main(argv=[''], verbosity=2,exit=false, warnings='ignore')" ], "execution_count": 22 }, { "cell_type": "markdown", "metadata": { "nteract": { "transient": { "deleting": false } } }, "source": [ "### test class explored: check possible compare/assert different pandas dataframes instead compare individual values" ] }, { "cell_type": "code", "metadata": { "jupyter": { "source_hidden": false, "outputs_hidden": false }, "nteract": { "transient": { "deleting": false } } }, "source": [ "# try: improved unit testing cases\n", "\n", "# pyspark.conf import sparkconf\n", "# pyspark.testing.pandasutils import pandasonsparktestcase\n", "# help(pyspark.testing.pandasutils)\n", "\n", "# class pysparktestcase(unittest.testcase):\n", "# @classmethod\n", "# def setupclass(cls):\n", "# config = {\n", "# \"name\": \"runtime_datasource\",\n", "# \"class_name\": \"datasource\",\n", "# \"module_name\": \"great_expectations.datasource\",\n", "# \"execution_engine\": {\n", "# \"module_name\": \"great_expectations.execution_engine\",\n", "# \"class_name\": \"sparkdfexecutionengine\",\n", "# \"spark_config\":{\n", "# \"spark.app.name\": spark.sparkcontext.getconf().get(\"spark.app.name\")\n", "# }\n", "# },\n", "# \"data_connectors\": {\n", "# \"default_runtime_data_connector_name\": {\n", "# \"class_name\": \"runtimedataconnector\",\n", "# \"batch_identifiers\": [\"default_identifier_name\"],\n", "# },\n", "# },\n", "# }\n", "# cls.spark = sparksession.builder.appname(\"sample pyspark etl\").getorcreate()\n", "# # sparksession.builder.master(\"local\").appname(\"word count\").config(\"spark.some.config.option\", \"some-value\").getorcreate()\n", "\n", "# @classmethod\n", "# def teardownclass(cls):\n", "# cls.spark.stop()\n", "\n", "\n", "# class test_function_merge_file(unittest.testcase):\n", "\n", "# # do: fix function -&gt; insert row raw delta. set expected_id = *inserted value* actual_id extracted delta table itself\n", "# def test1_merge_file(self):\n", "# # mssparkutils.fs.rm(f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/merge_test', true)\n", "\n", "# # # variables\n", "# # delta_schema = structtype([structfield('id', stringtype(),true)])\n", "# # raw_schema = structtype([structfield('id', stringtype(),true)])\n", "\n", "# # deltatable = spark.createdataframe([], delta_schema)\n", "# # deltatable.write.mode('overwrite').format(\"delta\").save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n", "\n", "# # deltatable = deltatable.forpath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n", "# # rawdata= spark.createdataframe([('0123456789', )], raw_schema)\n", "# # pk_columns_dict = { 'id': 'id' }\n", "# # column_names_dict = { 'id': 'id' }\n", "\n", "# # # display(rawdata)\n", "# # # display(deltatable)\n", "\n", "# # # mock functions\n", "# # (patch(\"__main__.pk_match\") mock_pk_match,\n", "# # patch(\"__main__.columnmatch\") mock_column_match,\n", "# # patch(\"__main__.nomatch\") mock_no_match):\n", "# # # set return values called functions\n", "# # mock_pk_match.return_value = 'olddata.id = newdata.id'\n", "# # mock_column_match.return_value = {'id': 'newdata.id'}\n", "# # mock_no_match.return_value = {'id': 'newdata.id'}\n", " \n", "# # # execute function\n", "# # mergefile(deltatable, rawdata, pk_columns_dict, column_names_dict)\n", "\n", "# # expected results\n", "# expected_id = '0123456789'\n", "\n", "# # actual results\n", "# actual_df = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n", "# actual_id = actual_df.collect()[0].id\n", "\n", "# # \n", "# # # compare actual expected\n", "# self.assertequal(expected_id, actual_id)\n", "\n", "# # def teardown(self):\n", "# # mock_pk_match.close()\n", "# # mock_column_match.close()\n", "# # mock_no_match.close()\n", "\n", "# unittest.main(argv=[''], verbosity=2,exit=false)" ], "execution_count": 23 } ] } }</file><file name="src\synapse\studio\pipeline\obsolete_pl_start_ingestion.json">{ "name": "obsolete_pl_start_ingestion", "properties": { "activities": [ { "name": "get metadata1", "type": "getmetadata", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "dataset": { "referencename": "ds_binary_file", "type": "datasetreference", "parameters": { "container_name": "ziptest", "folder_directory": "data_source/*/*", "file_name": "pdm_extracts_20220613.gz" } }, "fieldlist": [ "itemname", "itemtype" ], "storesettings": { "type": "azureblobfsreadsettings", "enablepartitiondiscovery": false }, "formatsettings": { "type": "binaryreadsettings" } } } ], "annotations": [], "lastpublishtime": "2023-09-29t12:09:56z" }, "type": "microsoft.synapse/workspaces/pipelines" }</file><file name="src\synapse\studio\pipeline\pl_dummy_worker.json">{ "name": "pl_dummy_worker", "properties": { "activities": [ { "name": "start_task", "description": "executes stored procedure start_task meta db", "type": "sqlserverstoredprocedure", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_start_task]", "storedprocedureparameters": { "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "plan_id": { "value": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "type": "int32" }, "task_id": { "value": { "value": "@pipeline().parameters.task.task_id", "type": "expression" }, "type": "int32" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "dummy_task", "description": "insert wait mimic small task executed", "type": "wait", "dependson": [ { "activity": "start_task", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "waittimeinseconds": 1 } }, { "name": "end_task", "description": "executed stored procedure meta.usp_end_task", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "dummy_task", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_end_task]", "storedprocedureparameters": { "comment": { "value": "success", "type": "string" }, "plan_id": { "value": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "true", "type": "boolean" }, "task_id": { "value": { "value": "@pipeline().parameters.task.task_id", "type": "expression" }, "type": "int32" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } } ], "parameters": { "plan_id": { "type": "int" }, "task": { "type": "object" } }, "folder": { "name": "workers" }, "annotations": [], "lastpublishtime": "2023-09-19t09:05:43z" }, "type": "microsoft.synapse/workspaces/pipelines" }</file><file name="src\synapse\studio\pipeline\pl_meta_pipeline.json">{ "name": "pl_meta_pipeline", "properties": { "activities": [ { "name": "for task task group", "type": "foreach", "dependson": [], "userproperties": [], "typeproperties": { "items": { "value": "@json(pipeline().parameters.tasks)", "type": "expression" }, "activities": [ { "name": "run pipeline", "type": "switch", "dependson": [], "userproperties": [], "typeproperties": { "on": { "value": "@item().worker_name", "type": "expression" }, "cases": [ { "value": "pl_unzip_worker", "activities": [ { "name": "execute pl_start_unzip", "description": "execute pl_start_unzip", "type": "executepipeline", "dependson": [], "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_unzip_worker", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "taskid": { "value": "@item().task_id", "type": "expression" }, "planid": { "value": "@pipeline().parameters.plan_id", "type": "expression" } } } } ] }, { "value": "pl_dummy_worker", "activities": [ { "name": "execute pl_dummy_worker", "description": "execute pl_dummy_worker", "type": "executepipeline", "dependson": [], "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_dummy_worker", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "plan_id": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "task": { "value": "@item()", "type": "expression" } } } } ] } ] } } ] } } ], "parameters": { "tasks": { "type": "string" }, "plan_id": { "type": "int", "defaultvalue": 1 } }, "variables": { "task_array": { "type": "array" } }, "annotations": [] } }</file><file name="src\synapse\studio\pipeline\pl_start_plan.json">{ "name": "pl_start_plan", "properties": { "activities": [ { "name": "run ingest tasks", "description": "invoke pipeline pl_start_task_group ingest tasks", "type": "executepipeline", "dependson": [ { "activity": "run preprocess tasks", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_start_task_group", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "task_group": "ingest", "plan_id": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "run_id": { "value": "@pipeline().parameters.run_id", "type": "expression" } } } }, { "name": "run dummy tasks", "description": "invoke pipeline pl_start_task_group dummy tasks", "type": "executepipeline", "dependson": [], "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_start_task_group", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "task_group": "dummy", "plan_id": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "run_id": { "value": "@pipeline().parameters.run_id", "type": "expression" } } } }, { "name": "run preprocess tasks", "description": "invoke pipeline pl_start_task_group preprocessing tasks", "type": "executepipeline", "dependson": [ { "activity": "run dummy tasks", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_start_task_group", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "task_group": "preprocess", "plan_id": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "run_id": { "value": "@pipeline().parameters.run_id", "type": "expression" } } } } ], "parameters": { "plan_id": { "type": "int" }, "run_id": { "type": "int" } }, "folder": { "name": "meta" }, "annotations": [], "lastpublishtime": "2023-09-18t08:56:15z" }, "type": "microsoft.synapse/workspaces/pipelines" }</file><file name="src\synapse\studio\pipeline\pl_start_run.json">{ "name": "pl_start_run", "properties": { "activities": [ { "name": "new run", "description": "check log_plans &amp; start new run", "type": "lookup", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "azuresqlsource", "sqlreaderstoredprocedurename": "[meta].[usp_new_run]", "storedprocedureparameters": { "debug": { "type": "boolean", "value": "false" }, "new_plan": { "type": "boolean", "value": { "value": "@pipeline().parameters.new_plan", "type": "expression" } }, "no_select": { "type": "boolean", "value": "false" }, "pipeline_id": { "type": "string", "value": { "value": "@pipeline().runid", "type": "expression" } }, "plan_name": { "type": "string", "value": { "value": "@pipeline().parameters.plan_name", "type": "expression" } } }, "partitionoption": "none" }, "dataset": { "referencename": "ds_dap_sql_meta", "type": "datasetreference", "parameters": { "schema": "meta", "target": "log_plans" } }, "firstrowonly": true } }, { "name": "set run_id", "description": "sets run_id based output \"lookup - new run\"", "type": "setvariable", "dependson": [ { "activity": "new run", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "run_id", "value": { "value": "@activity('new run').output.firstrow.run_id", "type": "expression" } } }, { "name": "start run", "description": "execute stored procedure 'meta.usp_start_run'", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "set run_id", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_start_run]", "storedprocedureparameters": { "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "get plans", "description": "run stored procedure 'get plans'", "type": "lookup", "dependson": [ { "activity": "start run", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "azuresqlsource", "sqlreaderstoredprocedurename": "[meta].[usp_get_plans]", "storedprocedureparameters": { "plan_name": { "type": "string", "value": { "value": "@pipeline().parameters.plan_name", "type": "expression" } }, "run_id": { "type": "int32", "value": { "value": "@variables('run_id')", "type": "expression" } } }, "partitionoption": "none" }, "dataset": { "referencename": "ds_dap_sql_meta", "type": "datasetreference", "parameters": { "schema": "meta", "target": "log_plans" } }, "firstrowonly": false } }, { "name": "for plan", "type": "foreach", "dependson": [ { "activity": "get plans", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "items": { "value": "@activity('get plans').output.value", "type": "expression" }, "issequential": true, "activities": [ { "name": "start plan", "description": "execute stored procedure start_plan meta db", "type": "sqlserverstoredprocedure", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_start_plan]", "storedprocedureparameters": { "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "plan_id": { "value": { "value": "@item().plan_id", "type": "expression" }, "type": "int32" }, "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "end plan", "description": "execute stored procedure end_plan meta db, set comment success", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "execute plan", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_end_plan]", "storedprocedureparameters": { "comment": { "value": "success", "type": "string" }, "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "plan_id": { "value": { "value": "@item().plan_id", "type": "expression" }, "type": "int32" }, "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "true", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "execute plan", "description": "invoke pipeline pl_start_plan", "type": "executepipeline", "dependson": [ { "activity": "start plan", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureinput": false }, "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_start_plan", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "plan_id": { "value": "@item().plan_id", "type": "expression" }, "run_id": { "value": "@variables('run_id')", "type": "expression" } } } }, { "name": "end plan failure", "description": "execute stored procedure end_plan meta db, set comment failed", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "execute plan", "dependencyconditions": [ "failed" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_end_plan]", "storedprocedureparameters": { "comment": { "value": "failed", "type": "string" }, "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "plan_id": { "value": { "value": "@item().plan_id", "type": "expression" }, "type": "int32" }, "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "false", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } } ] } }, { "name": "end run success", "description": "execute stored procedure end_run meta db, setting success", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "for plan", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_end_run]", "storedprocedureparameters": { "comment": { "value": "success", "type": "string" }, "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "true", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "end run failure foreach", "description": "execute stored procedure end_run meta db, setting failed", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "for plan", "dependencyconditions": [ "failed" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_end_run]", "storedprocedureparameters": { "comment": { "value": "failed", "type": "string" }, "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "false", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "end run failure lookup", "description": "execute stored procedure end_run meta db, setting run failed", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "get plans", "dependencyconditions": [ "failed" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "[meta].[usp_end_run]", "storedprocedureparameters": { "comment": { "value": "failed", "type": "string" }, "pipeline_id": { "value": { "value": "@pipeline().runid", "type": "expression" }, "type": "string" }, "run_id": { "value": { "value": "@variables('run_id')", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "false", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } } ], "parameters": { "new_plan": { "type": "bool", "defaultvalue": true }, "plan_name": { "type": "string", "defaultvalue": "qualifio_plan" } }, "variables": { "run_id": { "type": "integer" } }, "folder": { "name": "meta" }, "annotations": [], "lastpublishtime": "2023-10-23t07:37:01z" }, "type": "microsoft.synapse/workspaces/pipelines" }</file><file name="src\synapse\studio\pipeline\pl_start_task_group.json">{ "name": "pl_start_task_group", "properties": { "activities": [ { "name": "get tasks notebooks", "description": "run stored procedure get_plan_metadata meta db", "type": "lookup", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "azuresqlsource", "sqlreaderstoredprocedurename": "[meta].[usp_get_plan_metadata]", "storedprocedureparameters": { "num_tasks": { "type": "int32", "value": null }, "plan_id": { "type": "int32", "value": { "value": "@pipeline().parameters.plan_id", "type": "expression" } }, "results_as_select": { "type": "boolean", "value": "true" }, "run_id": { "type": "int32", "value": { "value": "@pipeline().parameters.run_id", "type": "expression" } }, "task_group": { "type": "string", "value": { "value": "@pipeline().parameters.task_group", "type": "expression" } }, "task_type": { "value": "spark_notebook" } }, "partitionoption": "none" }, "dataset": { "referencename": "ds_dap_sql_meta", "type": "datasetreference", "parameters": { "schema": "meta", "target": "get_plan_metadata" } }, "firstrowonly": true } }, { "name": "if notebook tasks run", "description": "only activates statement true, anything false", "type": "ifcondition", "dependson": [ { "activity": "get tasks notebooks", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "expression": { "value": "@not(equals(activity('get tasks notebooks').output.firstrow.num_tasks, 0))", "type": "expression" }, "iftrueactivities": [ { "name": "run ingestion notebook", "description": "run notebook 'metanotebook'", "type": "synapsenotebook", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "notebook": { "referencename": "metanotebook", "type": "notebookreference" }, "parameters": { "task_list": { "value": { "value": "@string(activity('get tasks notebooks').output.firstrow.tasks)", "type": "expression" }, "type": "string" }, "plan_id": { "value": { "value": "@pipeline().parameters.plan_id", "type": "expression" }, "type": "string" }, "env_code": { "value": { "value": "@variables('env_code')", "type": "expression" }, "type": "string" }, "debug": { "value": "false", "type": "bool" } }, "snapshot": true, "sparkpool": { "referencename": { "value": "@variables('spark_pool')", "type": "expression" }, "type": "bigdatapoolreference" }, "executorsize": "small", "conf": { "spark.dynamicallocation.enabled": false }, "driversize": "small" } } ] } }, { "name": "get tasks pipelines", "description": "run stored procedure get_plan_metadata meta db", "type": "lookup", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "azuresqlsource", "sqlreaderstoredprocedurename": "[meta].[usp_get_plan_metadata]", "storedprocedureparameters": { "num_tasks": { "type": "int32", "value": null }, "plan_id": { "type": "int32", "value": { "value": "@pipeline().parameters.plan_id", "type": "expression" } }, "results_as_select": { "type": "boolean", "value": "true" }, "run_id": { "type": "int32", "value": { "value": "@pipeline().parameters.run_id", "type": "expression" } }, "task_group": { "type": "string", "value": { "value": "@pipeline().parameters.task_group", "type": "expression" } }, "task_type": { "value": "synapse_pipeline" } }, "partitionoption": "none" }, "dataset": { "referencename": "ds_dap_sql_meta", "type": "datasetreference", "parameters": { "schema": "meta", "target": "get_plan_metadata" } }, "firstrowonly": true } }, { "name": "if pipeline tasks run", "description": "only activates statement true, anything false", "type": "ifcondition", "dependson": [ { "activity": "get tasks pipelines", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "expression": { "value": "@not(equals(activity('get tasks pipelines').output.firstrow.num_tasks, 0))", "type": "expression" }, "iftrueactivities": [ { "name": "execute meta pipeline", "description": "execute pl_meta_pipeline", "type": "executepipeline", "dependson": [], "policy": { "secureinput": false }, "userproperties": [], "typeproperties": { "pipeline": { "referencename": "pl_meta_pipeline", "type": "pipelinereference" }, "waitoncompletion": true, "parameters": { "tasks": { "value": "@activity('get tasks pipelines').output.firstrow.tasks", "type": "expression" }, "plan_id": { "value": "@pipeline().parameters.plan_id", "type": "expression" } } } } ] } } ], "parameters": { "task_group": { "type": "string", "defaultvalue": "dummy" }, "plan_id": { "type": "int", "defaultvalue": 1 }, "run_id": { "type": "int", "defaultvalue": 1 } }, "variables": { "env_code": { "type": "string", "defaultvalue": "dev" }, "spark_pool": { "type": "string", "defaultvalue": "devdapsspcore" } }, "folder": { "name": "meta" }, "annotations": [], "lastpublishtime": "2023-10-30t15:55:50z" }, "type": "microsoft.synapse/workspaces/pipelines" }</file><file name="src\synapse\studio\pipeline\pl_unzip_worker.json">{ "name": "pl_unzip_worker", "properties": { "activities": [ { "name": "set worker variables", "type": "ifcondition", "dependson": [], "userproperties": [], "typeproperties": { "expression": { "value": "@equals(1, 1)", "type": "expression" }, "iftrueactivities": [ { "name": "set task progress", "type": "sqlserverstoredprocedure", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "meta.usp_start_task", "storedprocedureparameters": { "task_id": { "value": { "value": "@pipeline().parameters.taskid", "type": "expression" }, "type": "int32" }, "plan_id": { "value": { "value": "@pipeline().parameters.planid", "type": "expression" }, "type": "int32" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "get task metadata", "type": "lookup", "dependson": [ { "activity": "set task progress", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "azuresqlsource", "sqlreaderstoredprocedurename": "meta.usp_get_task_metadata", "storedprocedureparameters": { "task_id": { "type": "int32", "value": { "value": "@pipeline().parameters.taskid", "type": "expression" } } }, "querytimeout": "02:00:00", "partitionoption": "none" }, "dataset": { "referencename": "ds_dap_sql_meta", "type": "datasetreference", "parameters": { "schema": "meta", "target": "get_task_metadata" } } } }, { "name": "set sink folder variable", "type": "setvariable", "dependson": [ { "activity": "get task metadata", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "sinkfoldername", "value": { "value": "@json(activity('get task metadata').output.firstrow.task_config)[0].table_name", "type": "expression" } } }, { "name": "set source folder variable", "type": "setvariable", "dependson": [ { "activity": "get task metadata", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "sourcefoldername", "value": { "value": "@json(activity('get task metadata').output.firstrow.task_config)[0].source_folder", "type": "expression" } } }, { "name": "set source container variable", "type": "setvariable", "dependson": [ { "activity": "get task metadata", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "containername", "value": { "value": "@json(activity('get task metadata').output.firstrow.task_config)[0].container_name", "type": "expression" } } }, { "name": "set source file extension", "type": "setvariable", "dependson": [ { "activity": "get task metadata", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "sourceextension", "value": { "value": "@json(activity('get task metadata').output.firstrow.source_config)[0].file_extension", "type": "expression" } } }, { "name": "set source file pattern", "type": "setvariable", "dependson": [ { "activity": "get task metadata", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "sourcefilepattern", "value": { "value": "@json(activity('get task metadata').output.firstrow.source_config)[0].file_pattern", "type": "expression" } } } ] } }, { "name": "get content list", "type": "ifcondition", "dependson": [ { "activity": "set worker variables", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "expression": { "value": "@equals(1,1)", "type": "expression" }, "iftrueactivities": [ { "name": "get zipped content container", "type": "getmetadata", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "dataset": { "referencename": "ds_binary_file", "type": "datasetreference", "parameters": { "container_name": { "value": "@variables('containername')", "type": "expression" }, "folder_directory": { "value": "@variables('sourcefoldername')", "type": "expression" }, "file_name": { "value": "@''", "type": "expression" } } }, "fieldlist": [ "childitems" ], "storesettings": { "type": "azureblobfsreadsettings", "recursive": true, "enablepartitiondiscovery": false }, "formatsettings": { "type": "binaryreadsettings" } } }, { "name": "set childitemslist", "type": "setvariable", "dependson": [ { "activity": "get zipped content container", "dependencyconditions": [ "succeeded" ] } ], "policy": { "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "variablename": "childitemslist", "value": { "value": "@activity('get zipped content container').output.childitems", "type": "expression" } } }, { "name": "filter source file pattern", "type": "filter", "dependson": [ { "activity": "filter extension", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "items": { "value": "@activity('filter extension').output.value", "type": "expression" }, "condition": { "value": "@contains(item().name, variables('sourcefilepattern'))", "type": "expression" } } }, { "name": "filter extension", "type": "filter", "dependson": [ { "activity": "set childitemslist", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "items": { "value": "@variables('childitemslist')", "type": "expression" }, "condition": { "value": "@endswith(item().name, variables('sourceextension'))", "type": "expression" } } } ] } }, { "name": "for item childitemslist", "type": "foreach", "dependson": [ { "activity": "get content list", "dependencyconditions": [ "succeeded" ] } ], "userproperties": [], "typeproperties": { "items": { "value": "@activity('filter source file pattern').output.value", "type": "expression" }, "activities": [ { "name": "unzip_folder", "type": "copy", "dependson": [ { "activity": "log new file", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "binarysource", "storesettings": { "type": "azureblobfsreadsettings", "recursive": false, "wildcardfolderpath": { "value": "@variables('sourcefoldername')", "type": "expression" }, "wildcardfilename": { "value": "@item().name", "type": "expression" }, "deletefilesaftercompletion": false }, "formatsettings": { "type": "binaryreadsettings", "compressionproperties": { "type": "zipdeflatereadsettings", "preservezipfilenameasfolder": true } } }, "sink": { "type": "binarysink", "storesettings": { "type": "azureblobfswritesettings", "copybehavior": "preservehierarchy" } }, "enablestaging": false }, "inputs": [ { "referencename": "ds_binary_zip_folder", "type": "datasetreference", "parameters": { "container_name": { "value": "@variables('containername')", "type": "expression" }, "folder_directory": { "value": "@variables('sourcefoldername')", "type": "expression" }, "file_name": { "value": "@item().name", "type": "expression" } } } ], "outputs": [ { "referencename": "ds_binary_file", "type": "datasetreference", "parameters": { "container_name": { "value": "@variables('containername')", "type": "expression" }, "folder_directory": { "value": "@variables('sinkfoldername')", "type": "expression" }, "file_name": { "value": "@''", "type": "expression" } } } ] }, { "name": "log new file", "type": "sqlserverstoredprocedure", "dependson": [], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "meta.usp_new_file", "storedprocedureparameters": { "task_id": { "value": { "value": "@pipeline().parameters.taskid", "type": "expression" }, "type": "int32" }, "plan_id": { "value": { "value": "@pipeline().parameters.planid", "type": "expression" }, "type": "int32" }, "filename": { "value": { "value": "@item().name", "type": "expression" }, "type": "string" }, "extended_filename": { "value": { "value": "@item().name", "type": "expression" }, "type": "string" }, "info_message": { "value": { "value": "@concat('abfss://',\n variables('containername'),\n '@',\n variables('env_code'),\n 'dapstdala1.dfs.core.windows.net/',\n variables('sourcefoldername'),\n item().name)", "type": "expression" }, "type": "string" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "log success movement archive", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "copy zipfolder archive", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "meta.usp_update_file_activity", "storedprocedureparameters": { "extended_filename": { "value": { "value": "@item().name", "type": "expression" }, "type": "string" }, "activity": { "value": "archive", "type": "string" }, "success": { "value": "true", "type": "boolean" }, "info_message": { "value": { "value": "@concat('abfss://',\n 'archive',\n '@',\n variables('env_code'),\n 'dapstdala1.dfs.core.windows.net/',\n variables('sinkfoldername'),\n item().name)", "type": "expression" }, "type": "string" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "log failed movement archive_copy1", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "unzip_folder", "dependencyconditions": [ "failed" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "meta.usp_update_file_activity", "storedprocedureparameters": { "extended_filename": { "value": { "value": "@item().name", "type": "expression" }, "type": "string" }, "activity": { "value": "archive", "type": "string" }, "success": { "value": "false", "type": "boolean" }, "info_message": { "value": { "value": "@concat('abfss://',\n variables('containername'),\n '',\n variables('env_code'),\n 'dapstdala1.dfs.core.windows.net/',\n variables('sinkfoldername'),\n item().name)", "type": "expression" }, "type": "string" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "copy zipfolder archive", "type": "copy", "dependson": [ { "activity": "unzip_folder", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "source": { "type": "binarysource", "storesettings": { "type": "azureblobfsreadsettings", "recursive": true, "wildcardfolderpath": { "value": "@variables('sourcefoldername')", "type": "expression" }, "deletefilesaftercompletion": true }, "formatsettings": { "type": "binaryreadsettings" } }, "sink": { "type": "binarysink", "storesettings": { "type": "azureblobfswritesettings", "copybehavior": "preservehierarchy" } }, "enablestaging": false }, "inputs": [ { "referencename": "ds_binary_file", "type": "datasetreference", "parameters": { "container_name": { "value": "@variables('containername')", "type": "expression" }, "folder_directory": { "value": "@''", "type": "expression" }, "file_name": { "value": "@item().name", "type": "expression" } } } ], "outputs": [ { "referencename": "ds_binary_file", "type": "datasetreference", "parameters": { "container_name": "archive", "folder_directory": { "value": "@variables('sourcefoldername')", "type": "expression" }, "file_name": { "value": "@''", "type": "expression" } } } ] } ] } }, { "name": "set task success", "description": "set status batch 'meta.batch_tracking' \"success\"", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "for item childitemslist", "dependencyconditions": [ "succeeded" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "meta.usp_end_task", "storedprocedureparameters": { "task_id": { "value": { "value": "@pipeline().parameters.taskid", "type": "expression" }, "type": "int32" }, "plan_id": { "value": { "value": "@pipeline().parameters.planid", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "true", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } }, { "name": "set task failed", "description": "set status batch 'meta.batch_tracking' \"failed\"", "type": "sqlserverstoredprocedure", "dependson": [ { "activity": "for item childitemslist", "dependencyconditions": [ "failed" ] } ], "policy": { "timeout": "0.12:00:00", "retry": 0, "retryintervalinseconds": 30, "secureoutput": false, "secureinput": false }, "userproperties": [], "typeproperties": { "storedprocedurename": "meta.usp_end_task", "storedprocedureparameters": { "task_id": { "value": { "value": "@pipeline().parameters.taskid", "type": "expression" }, "type": "int32" }, "plan_id": { "value": { "value": "@pipeline().parameters.planid", "type": "expression" }, "type": "int32" }, "success_flag": { "value": "false", "type": "boolean" } } }, "linkedservicename": { "referencename": "ls_dap_sql_meta", "type": "linkedservicereference" } } ], "parameters": { "taskid": { "type": "string", "defaultvalue": "4" }, "planid": { "type": "string", "defaultvalue": "2" } }, "variables": { "containername": { "type": "string" }, "sourcefoldername": { "type": "string" }, "sinkfoldername": { "type": "string" }, "sourceextension": { "type": "string" }, "sourcefilepattern": { "type": "string" }, "childitemslist": { "type": "array" }, "env_code": { "type": "string", "defaultvalue": "dev" } }, "folder": { "name": "workers" }, "annotations": [] } }</file><file name="src\synapse\studio\sparkConfiguration\core_configuration.json">{ "name": "core_configuration", "properties": { "description": "configuration file environment variables used across notebooks used ingestion process synapse core workspace", "configs": { "spark.environment_code": "dev", "spark.sql.legacy.timeparserpolicy": "legacy" }, "created": "2024-04-11t13:55:30.6290000+02:00", "createdby": "pla917@nationale-loterij.be", "annotations": [], "configmergerule": { "artifact.currentoperation.spark.environment_code": "replace" } } }</file><file name="src\synapse\studio\sparkConfiguration\core_configuration.md">**understanding spark.sql.legacy.timeparserpolicy** spark.sql.legacy.timeparserpolicy setting spark determines parse timestamps sql queries. setting introduced spark 2.3.0 provide flexibility control timestamp parsing. **options spark.sql.legacy.timeparserpolicy** three options setting: **1. legacy** **behavior**: default behavior spark 2.2 earlier versions. uses java.sql.timestamp class parse timestamps, lead incorrect results certain timestamp formats. **advantages**: - none significant. **drawbacks**: - lead incorrect results certain timestamp formats. - support parsing timestamps timezone information. **2. corrected** **behavior**: option uses java.time.instant class parse timestamps, provides accurate correct results. **advantages**: - provides accurate correct results timestamp parsing. - supports parsing timestamps timezone information. **drawbacks**: - break existing queries rely legacy behavior. - may require changes existing code queries. **2. exception** **behavior**: option throws exception encounters timestamp cannot parsed using java.time.instant class. **advantages**: - provides accurate correct results timestamp parsing. - supports parsing timestamps timezone information. - helps identify fix issues timestamp parsing queries. **drawbacks**: - break existing queries rely legacy behavior. - may require changes existing code queries. **decision** whilst desciption would seem favor exception corrected, legacy option chosen. jave.time.instant class converts 4-digit integers timestamps adding default values. is, value '2024' passed timestamp column, spark format '2024-01-01 00:00:00.0000' instead throwing error. whilst still feature/bug, option legacy remains appropriate.</file><file name="src\synapse\_test\metadata\datasets\filworker_pivot.json">{ "datasets": [ { "name": "filworker_pivot", "description": "test see file .fil gets properly turned csv", "kind": "csv", "task_type": "spark_notebook", "worker": "filworker", "ingestion": { "target_table": "preprocess_test/filworker_test/preprocess", "source_folder": "preprocess_test/filworker_test", "container": "landing", "match_pattern": "firstrow_to_headers.fil", "extension": "fil", "encoding": "utf-8", "column_delimiter": "\t", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": false }, "columns": [ { "sequence": 1, "name": "fk_connection", "dimension": "pk", "data_type": "integer" }, { "sequence": 2, "name": "fk_game", "dimension": "pk", "data_type": "integer" }, { "sequence": 3, "name": "cat_class", "dimension": "pk", "data_type": "integer" }, { "sequence": 4, "name": "nb_payment", "data_type": "integer" }, { "sequence": 5, "name": "amt_payment", "data_type": "decimal" }, { "sequence": 6, "name": "x_dt_extract", "data_type": "integer" }, { "sequence": 7, "name": "x_dt_transaction", "data_type": "integer" }, { "sequence": 8, "name": "drawnumber", "data_type": "integer" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_checks_headerless.json">{ "datasets": [ { "name": "ingestionworker_checks_headerless", "description": "test see everything works headerless dataset", "kind": "csv", "task_type": "spark_notebook", "worker": "ingestionworker", "ingestion": { "target_table": "unittest_spark_checks_headerless", "source_folder": "ingestion_test", "container": "landing", "match_pattern": "headerless_header_check.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ",", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": false }, "checks": [ { "name": "header" } ], "columns": [ { "sequence": 1, "name": "character_name", "dimension": "pk" }, { "sequence": 2, "name": "level", "data_type": "integer" }, { "sequence": 3, "name": "race" }, { "sequence": 4, "name": "class" }, { "sequence": 5, "name": "guild" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_defaultreplace.json">{ "datasets": [ { "name": "ingestionworker_csv_defaultreplace", "description": "execute csv integration test file", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "csv_defaultreplace", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "csv_defaultreplace.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "checks": [ { "name": "header" }, { "name": "data_type" }, { "name": "default_replace" } ], "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "timestamp" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "date" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_recurse_search.json">{ "datasets": [ { "name": "ingestionworker_csv_recurse_search", "description": "execute csv integration test file test structure checked recursively", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "csv_recurse_search", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "recurse.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "columns": [ { "sequence": 1, "name": "column_a" , "sink_name": "column_a", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_b" , "sink_name": "column_b", "data_type": "integer" }, { "sequence": 3, "name": "column_c", "sink_name": "column_c", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_specialstring.json">{ "datasets": [ { "name": "ingestionworker_csv_specialstring", "description": "execute csv integration test file", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "csv_specialstring", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "csv_specialstring.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ",", "row_delimiter": "", "escape_character": "\"", "quote_character": "\"", "header_line": true }, "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "integer"}, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "string" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "string", "dimension": "pk" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_task.json">{ "datasets": [ { "name": "ingestionworker_csv_task", "description": "execute csv integration test file", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "csv_integration", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": ".*(csv_test).*", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "checks": [ { "name": "landing_rows", "config_params": "{\"landing_rows_expected\": 3}" }, { "name": "header" }, { "name": "data_type" } ], "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "integer" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_task_replace_null_pk.json">{ "datasets": [ { "name": "ingestionworker_csv_task_replace_null_pk", "description": "execute csv integration test file", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "csv_integration_replace_value", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": ".*(csv_replace_value).*", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "checks": [ { "name": "replace_primary_key" } ], "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "string", "dimension": "pk" , "column_info":{"replace_value":"error"}}, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "integer" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "string", "dimension": "pk" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_task_vacuum.json">{ "datasets": [ { "name": "ingestionworker_csv_task_vacuum", "description": "execute csv integration test file", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "csv_integration_vacuum", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": ".*(csv_test_vacuum).*", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true, "target_options": { "retention_time":170 } } , "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "integer" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_datatypes.json">{ "datasets": [ { "name": "ingestionworker_datatypes", "description": "deploy delta table data types", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "datatype_table", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "datatype_test.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "checks": [ { "name": "data_type" } ], "columns": [ { "sequence": 1, "name": "stringcolumn" , "sink_name": "string_column", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "integercolumn" , "sink_name": "integer_column", "data_type": "integer" }, { "sequence": 3, "name": "decimalcolumn", "sink_name": "decimal_column", "data_type": "decimal(12,5)" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_formatting.json">{ "datasets": [ { "name": "ingestionworker_formatting_task", "description": "execute formatting integration test file", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "formatting_integration", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "formatting_test.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "checks": [ { "name": "header" }, { "name": "data_type" } ], "columns": [ { "sequence": 1, "name": "primary_key", "sink_name": "primary_key", "data_type": "integer", "dimension": "pk" }, { "sequence": 2, "name": "decimal_comma", "sink_name": "decimal_comma", "data_type": "decimal(4,2)", "dimension": "scd2" , "column_info":{ "decimal_separator": "," } }, { "sequence": 3, "name": "decimal_dot", "sink_name": "decimal_dot", "data_type": "decimal(4,2)", "dimension": "scd2" }, { "sequence": 4, "name": "timestamp_us", "sink_name": "timestamp_us", "data_type": "timestamp", "dimension": "scd2" , "column_info":{ "format": "yyyy-mm-dd hh:mm:ss" } }, { "sequence": 5, "name": "timestamp_eu", "sink_name": "timestamp_eu", "data_type": "timestamp", "dimension": "scd2" , "column_info":{ "format": "dd-mm-yyyy hh:mm:ss" } }, { "sequence": 6, "name": "date_us", "sink_name": "date_us", "data_type": "date", "dimension": "scd2" , "column_info":{ "format": "yyyy-mm-dd" } }, { "sequence": 7, "name": "date_eu", "sink_name": "date_eu", "data_type": "date", "dimension": "scd2" , "column_info":{ "format": "dd-mm-yyyy" } }, { "sequence": 8, "name": "date_nosep", "sink_name": "date_nosep", "data_type": "date", "dimension": "scd2" , "column_info":{ "format": "ddmmyyyy" } }, { "sequence": 9, "name": "date_us_slash", "sink_name": "date_us_slash", "data_type": "date", "dimension": "scd2" , "column_info":{ "format": "yyyy/mm/dd" } }, { "sequence": 10, "name": "date_eu_slash", "sink_name": "date_eu_slash", "data_type": "date", "dimension": "scd2" , "column_info":{ "format": "dd/mm/yyyy" } }, { "sequence": 11, "name": "unix_format", "sink_name": "unix_format", "data_type": "timestamp", "dimension": "scd2" , "column_info":{ "format": "epoch" } } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_json_task.json">{ "datasets": [ { "name": "ingestionworker_json_task", "description": "execute json integration test file", "kind": "json", "task_type": "spark_notebook", "worker": "ingestionworker", "ingestion": { "target_table": "json_integration", "source_folder": "ingestion_test/", "container": "landing", "match_pattern": ".*(json_test).*", "extension": "json", "encoding": "utf-8", "column_delimiter": ",", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "columns": [ { "sequence": 1, "name": "id", "dimension":"pk", "data_type": "integer" }, { "sequence": 2, "name": "data", "sink_name": "employees", "date_type": "string" }, { "sequence": 3, "name": "company", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_optional_columns.json">{ "datasets": [ { "name": "ingestionworker_optional_columns", "description": "execute optional_columns test", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "options_optional_columns", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "csv_optional_columns.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "string" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "string", "column_info": { "optional": true } } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_options_partitioning.json">{ "datasets": [ { "name": "ingestionworker_options_partitioning", "description": "execute csv partitioning validate workings partitioning-functionality integration-mode", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "options_partitioning", "target_options": { "partitioning": [ { "name": "column_c", "sequence": 1, "datepart": "year" }, { "name": "column_c", "sequence": 2, "datepart": "month" } ] }, "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "csv_partitioning.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "columns": [ { "sequence": 1, "name": "column a" , "sink_name": "column_a", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column b" , "sink_name": "column_b", "data_type": "integer" }, { "sequence": 3, "name": "column c", "sink_name": "column_c", "data_type": "date" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_options_partitioning_on_filename.json">{ "datasets": [ { "name": "ingestionworker_options_partitioning_on_filename", "description": "execute csv partitioning validate workings partitioning-functionality integration-mode", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "options_filename_partitioning", "target_options": { "partitioning": [ { "name": "t_extract_date", "sequence": 1, "datepart": "year" }, { "name": "t_extract_date", "sequence": 2, "datepart": "month" }, { "name": "t_extract_date", "sequence": 3, "datepart": "day" } ], "extract_date": { "column_name": "t_file_name", "regex_expression": "_(\\d{8})", "extract_date_format": "yyyymmdd" } }, "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "(partition_on_filename_\\d{8}).csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "columns": [ { "sequence": 1, "name": "column_a" , "sink_name": "column_a", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_b" , "sink_name": "column_b", "data_type": "integer" }, { "sequence": 3, "name": "column_c", "sink_name": "column_c", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_parquet_task.json">{ "datasets": [ { "name": "ingestionworker_parquet_task", "description": "execute parquet integration test file", "kind": "parquet", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "parquet_integration", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "parquet_test.parquet", "extension": "parquet", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true }, "checks": [ { "name": "landing_rows", "config_params": "{\"landing_rows_expected\": 3}" }, { "name": "header" }, { "name": "data_type" } ], "columns": [ { "sequence": 1, "name": "column_1" , "sink_name": "column_1", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_2" , "sink_name": "column_2", "data_type": "integer" }, { "sequence": 3, "name": "column_3", "sink_name": "column_3", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\ingestionworker_skiplines.json">{ "datasets": [ { "name": "ingestionworker_skiplines", "description": "execute csv skiplines validate skip_first_lines parameter functionality integration-mode", "kind": "csv", "task_type": "spark_notebook", "worker" : "ingestionworker", "ingestion": { "target_table": "skiplines", "source_folder": "ingestion_test", "container" : "landing", "match_pattern": "csv_skiplines.csv", "extension": "csv", "encoding": "utf-8", "column_delimiter": ";", "row_delimiter": "", "escape_character": "", "quote_character": "", "header_line": true, "skip_first_lines": 4 }, "checks": [ { "name": "header" } ], "columns": [ { "sequence": 1, "name": "column_a" , "sink_name": "column_a", "data_type": "string", "dimension": "pk" }, { "sequence": 2, "name": "column_b" , "sink_name": "column_b", "data_type": "integer" }, { "sequence": 3, "name": "column_c", "sink_name": "column_c", "data_type": "string" } ] } ] }</file><file name="src\synapse\_test\metadata\datasets\preprocess_test.json">{ "preprocess": [ { "name": "preprocess_test", "description": "test flow preprocess synapse workspace", "task_type": "synapse_pipeline", "worker_pipeline": "pl_preprocess_template", "source_pattern": "preprocessing" } ] }</file><file name="src\synapse\_test\metadata\datasets\unittest_spark_dummy_task.json">{ "datasets": [ { "name": "unittest_spark_dummy_task", "description": "", "kind": "csv", "task_type": "spark_notebook", "worker" : "dummyworker", "ingestion": { "target_table": "mandatory_field", "source_folder": "mandatory_field", "match_pattern": "mandatory_field", "extension": "csv" } } ] }</file><file name="src\synapse\_test\metadata\datasets\unittest_synapse_dummy_task.json">{ "datasets": [ { "name": "unittest_synapse_dummy_task", "description": "test dummy invocation", "kind": "csv", "task_type": "synapse_pipeline", "worker": "pl_dummy_worker", "ingestion": { "target_table": "mandatory_field", "source_folder": "mandatory_field", "match_pattern": "mandatory_field", "extension": "csv" } } ] }</file><file name="src\synapse\_test\metadata\datasets\unittest_synapse_unzip_gzfolder.json">{ "datasets": [ { "name": "unittest_synapse_unzip_gzfolder", "description": "execute unzip zipped unittest folder .gz extension", "kind": "zip", "task_type": "synapse_pipeline", "worker": "pl_unzip_worker", "ingestion": { "target_table": "preprocess_test/unzipped_test_gz", "source_folder": "preprocess_test/zipped_content_test", "container": "landing", "match_pattern": "gzipped_landing", "extension": "gz" } } ] }</file><file name="src\synapse\_test\metadata\datasets\unittest_synapse_unzip_zipfolder.json">{ "datasets": [ { "name": "unittest_synapse_unzip_zipfolder", "description": "execute unzip zipped unittest folder .zip extension", "kind": "zip", "task_type": "synapse_pipeline", "worker": "pl_unzip_worker", "ingestion": { "target_table": "preprocess_test/unzipped_test_zip", "source_folder": "preprocess_test/zipped_content_test", "container": "landing", "match_pattern": "zipped_landing", "extension": "zip" } } ] }</file><file name="src\synapse\_test\metadata\plantasks\preprocessing.json">{ "plans": [ { "name": "preprocessing", "description": "test preprocessing dev int", "environments" : "dev|int", "enabled": true, "task_groups": [ { "name": "preprocess", "tasks": [ {"name": "preprocess_test", "sequence": 1, "enabled": true} ] } ] } ] }</file><file name="src\synapse\_test\metadata\plantasks\unittest_filworker.json">{ "plans": [ { "name": "unittest_filworker", "description": "validate filworker", "environments" : "dev|int", "enabled": true, "task_groups": [ { "name": "preprocess", "tasks": [ {"name": "filworker_pivot", "sequence": 1, "enabled": true} ] } ] } ] }</file><file name="src\synapse\_test\metadata\plantasks\unittest_ingestionworker.json">{ "plans": [ { "name": "unittest_ingestionworker", "description": "validate ingestionworker", "environments" : "dev|int", "enabled": true, "task_groups": [ { "name": "ingest", "tasks": [ { "name": "ingestionworker_csv_task", "sequence": 1, "enabled": true }, { "name": "ingestionworker_json_task", "sequence": 2, "enabled": true }, { "name": "ingestionworker_csv_recurse_search", "sequence": 3, "enabled": true }, { "name": "ingestionworker_checks_headerless", "sequence": 4, "enabled": true }, { "name": "ingestionworker_parquet_task", "sequence": 5, "enabled": true }, { "name": "ingestionworker_datatypes", "sequence": 6, "enabled": true }, { "name": "ingestionworker_options_partitioning", "sequence": 7, "enabled": true }, { "name": "ingestionworker_skiplines", "sequence": 8, "enabled": true }, { "name": "ingestionworker_csv_defaultreplace", "sequence": 9, "enabled": true }, { "name": "ingestionworker_formatting_task", "sequence": 10, "enabled": true }, {"name": "ingestionworker_csv_specialstring", "sequence": 11, "enabled": true}, {"name": "ingestionworker_options_partitioning_on_filename", "sequence": 12, "enabled": true}, {"name": "ingestionworker_optional_columns", "sequence": 13, "enabled": true}, {"name": "ingestionworker_csv_task_replace_null_pk", "sequence": 14, "enabled": true}, {"name": "ingestionworker_csv_task_vacuum", "sequence": 15, "enabled": true} ] } ] } ] }</file><file name="src\synapse\_test\metadata\plantasks\unittest_spark_dummy.json">{ "plans": [ { "name": "unittest_spark_dummy", "description": "check spark meta notebook structure works", "environments" : "dev|int", "enabled": true, "task_groups": [ { "name": "dummy", "tasks": [ { "name": "unittest_spark_dummy_task", "sequence": 1, "enabled": true } ] } ] } ] }</file><file name="src\synapse\_test\metadata\plantasks\unittest_synapse_dummy.json">{ "plans": [ { "name": "unittest_synapse_dummy", "description": "check synapse meta pipeline structure works", "environments" : "dev|int", "enabled": true, "task_groups": [ { "name": "dummy", "tasks": [ { "name": "unittest_synapse_dummy_task", "sequence": 1, "enabled": true } ] } ] } ] }</file><file name="src\synapse\_test\metadata\plantasks\unittest_synapse_unzip.json">{ "plans": [ { "name": "unittest_synapse_unzip", "description": "check synapse pipeline pl_unzip_worker works", "environments": "dev|int", "enabled": true, "task_groups": [ { "name": "preprocess", "tasks": [ { "name": "unittest_synapse_unzip_zipfolder", "sequence": 1, "enabled": true }, { "name": "unittest_synapse_unzip_gzfolder", "sequence": 2, "enabled": true } ] } ] } ] }</file><file name="src\synapse\_test\test_files\fulltest_files\ingestion_test\20240802_112048\json_test.json">{"id": 123, "data": [{"name": "xxx", "function": "external"}, {"name": "yyy", "function": "internal"}], "company": "zzz"}</file><file name="src\synapse\_test\test_files\fulltest_files\ingestion_test\20240802_112048\json_test_multiline.json">{ "id": 126, "data": [ { "name": "xxx", "function": "external" }, { "name": "yyy", "function": "internal" } ], "company": "ccc" }</file><file name="src\synapse\_test\test_files\fulltest_files\ingestion_test\20240802_112048\json_test_multiobject.json">[ { "id": 125, "data": [ { "name": "xxx", "function": "external" }, { "name": "yyy", "function": "internal" } ], "company": "bbb" }, { "id": 124, "data": [ { "name": "xxx", "function": "external" }, { "name": "yyy", "function": "internal" } ], "company": "aaa" } ]</file><file name="src\synapse\_test\test_files\unittest_files\test1_root.txt">test file</file><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\archive\placeholder.txt">delete file, make sure folder actually created pws script.</file><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\landing\move_to_raw\MOCK_MULTILINE_move.json">{ "id": 126, "data": [ { "name": "xxx", "function": "external" }, { "name": "yyy", "function": "internal" } ], "company": "ccc" }</file><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\raw\placeholder.txt">delete file, make sure folder actually created pws script.</file><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\ANewFile.txt.txt" /><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\Test.txt.txt" /><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\Treasure.txt.txt" /><file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\Worked.txt.txt" /><file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\list_directory_content\root.txt" /><file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\list_directory_content\parent\child\child.json" /><file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\other_subfolder\filtered\included.txt" /><file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\other_subfolder\ignored\skipped.txt" /><file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\subfolder\filtered\included.txt" /><file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\subfolder\ignored\skipped.txt" /><file name="src\synapse\_test\test_files\unittest_files\SparkDataFrameClass\load_dataframe\load_json_test\json_test_multiobject.json">[ { "id": 125, "data": [ { "name": "xxx", "function": "external" }, { "name": "yyy", "function": "internal" } ], "company": "bbb" }, { "id": 124, "data": [ { "name": "xxx", "function": "external" }, { "name": "yyy", "function": "internal" } ], "company": "aaa" }, { "id": 127, "data": [ { "name": "aaa", "function": "external" }, { "name": "bbb", "function": "internal" } ], "company": "ccc" } ]</file><file name="src\synapse\_test\test_files\unittest_files\unittest\test1_unittest.txt">test file</file><file name="src\synapse\_test\test_files\unittest_files\unittest\env\test1_env.txt">test file</file><file name="src\synapse\_test\test_files\unittest_files\unittest\env\timestamp\test1_timestamp.txt">test file</file><file name="src\synapse\_test\test_scripts\sql\filworker_preprocess_logcheck.sql">-- check files present file logging tables (for preprocess task filworker) -- dev note: include failing tests declare @error_message nvarchar(max); declare @expected_count int; declare @expected_status nvarchar(max); declare @actual_count int; declare @actual_status nvarchar(max); set @expected_count = 1; set @expected_status = 'succeeded' set @actual_count = (select count(*) meta.log_files [filename] like 'firstrow_to_headers%') set @actual_status = (select archive_status meta.log_files [filename] like 'firstrow_to_headers%') @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number files log files table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') preprocess task filworker.'); throw 50000, @error_message, 1; end @expected_status &lt;&gt; @actual_status begin set @error_message = concat('the expected ingestion status log files table (', isnull(@expected_status,'null'), ') equal actual status (', isnull(@actual_status,'null'), ') preprocess task filworker.'); throw 500000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\sql\ingestionworker_csv_logcheck.sql">-- check files present file logging tables (for csv only) -- dev note: include failing tests declare @error_message nvarchar(max); declare @expected_count int; declare @expected_status nvarchar(max); declare @actual_count int; declare @actual_status nvarchar(max); declare @expected_updated_row int; declare @expected_inserted_row int; declare @expected_source_row int; declare @actual_updated_row int; declare @actual_inserted_row int; declare @actual_source_row int; set @expected_count = 1; set @expected_status = 'succeeded' set @expected_updated_row = 2; set @expected_inserted_row = 1; set @expected_source_row =3; set @actual_count = (select count(*) meta.log_files [filename] like 'csv_test') set @actual_status = (select archive_status meta.log_files [filename] like 'csv_test') set @actual_updated_row = (select json_value(silver_info,'$.row_updated') meta.log_files [filename] = 'csv_test_t2') set @actual_inserted_row = (select json_value(silver_info,'$.row_inserted') meta.log_files [filename] = 'csv_test_t2') set @actual_source_row = (select json_value(silver_info,'$.source_row') meta.log_files [filename] = 'csv_test_t2') @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number files log files table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_task.'); throw 50000, @error_message, 1; end @expected_status &lt;&gt; @actual_status begin set @error_message = concat('the expected ingestion status log files table (', isnull(@expected_status,'null'), ') equal actual status (', isnull(@actual_status,'null'), ') csv_task.'); throw 500000, @error_message, 1; end @expected_updated_row &lt;&gt; @actual_updated_row begin set @error_message = concat('the expected number updated rows log files table (', isnull(@expected_updated_row,'null'), ') equal actual count (', isnull(@actual_updated_row,'null'), ') csv_test_t2.'); throw 50000, @error_message, 1; end @expected_inserted_row &lt;&gt; @actual_inserted_row begin set @error_message = concat('the expected number inserted rows log files table (', isnull(@expected_inserted_row,'null'), ') equal actual count (', isnull(@actual_inserted_row,'null'), ') csv_test_t2.'); throw 50000, @error_message, 1; end @expected_source_row &lt;&gt; @actual_source_row begin set @error_message = concat('the expected number rows log files table (', isnull(@expected_source_row,'null'), ') equal actual count (', isnull(@actual_source_row,'null'), ') csv_test_t2.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\sql\ingestionworker_csv_recurse_logcheck.sql">-- check files present file logging tables (for csv_struct only) -- dev note: include failing tests declare @error_message nvarchar(max); declare @expected_count int; declare @expected_status nvarchar(max); declare @actual_count int; declare @actual_status nvarchar(max); set @expected_count = 1; set @expected_status = 'succeeded' set @actual_count = (select count(*) meta.log_files [filename] like 'recurse') set @actual_status = (select archive_status meta.log_files [filename] like 'recurse') @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number files log files table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') recurse.'); throw 50000, @error_message, 1; end @expected_status &lt;&gt; @actual_status begin set @error_message = concat('the expected ingestion status log files table (', isnull(@expected_status,'null'), ') equal actual status (', isnull(@actual_status,'null'), ') recurse.'); throw 500000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\sql\ingestionworker_json_logcheck.sql">-- check files present logging tables (for json only) -- dev note: include failing tests declare @error_message nvarchar(max); declare @expected_count int; declare @expected_status nvarchar(max); declare @actual_count int; declare @actual_status nvarchar(max); set @expected_count = 1; set @expected_status = 'succeeded' set @actual_count = (select count(*) meta.log_files [filename] like 'json_test') set @actual_status = (select archive_status meta.log_files [filename] like 'json_test') @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number files log files table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') json_task.'); throw 50000, @error_message, 1; end @expected_status &lt;&gt; @actual_status begin set @error_message = concat('the expected ingestion status log files table (', isnull(@expected_status,'null'), ') equal actual status (', isnull(@actual_status,'null'), ') json_task.'); throw 500000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\sql\ingestionworker_parquet_logcheck.sql">-- check files present file logging tables (for parquet only) -- dev note: include failing tests declare @error_message nvarchar(max); declare @expected_count int; declare @expected_status nvarchar(max); declare @actual_count int; declare @actual_status nvarchar(max); set @expected_count = 1; set @expected_status = 'succeeded' set @actual_count = (select count(*) meta.log_files [filename] like 'parquet_test') set @actual_status = (select archive_status meta.log_files [filename] like 'parquet_test') @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number files log files table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') parquet_test.'); throw 50000, @error_message, 1; end @expected_status &lt;&gt; @actual_status begin set @error_message = concat('the expected ingestion status log files table (', isnull(@expected_status,'null'), ') equal actual status (', isnull(@actual_status,'null'), ') parquet_test.'); throw 500000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\sql\spark_dummy_notebook.sql">-- query return error specific logs can't found tables -- check execution dummyworker inside metanotebook declare @error_message nvarchar(max); declare @succeeded_count int; set @succeeded_count = ( select count(*) succeeded_count meta.log_tasks task_name = 'unittest_spark_dummy_task' current_status = 'succeeded' ); @succeeded_count = 0 begin set @error_message = 'did find test logs. dummyworker executed inside metanotebook'; throw 50404, @error_message, 404; end</file><file name="src\synapse\_test\test_scripts\sql\synapse_dummy_pl.sql">-- query return error specific logs can't found tables -- check execution pl_dummy_worker declare @error_message nvarchar(max); declare @succeeded_count int; set @succeeded_count = ( select count(*) succeeded_count meta.log_tasks task_name = 'unittest_synapse_dummy_task' current_status = 'succeeded' ); @succeeded_count = 0 begin set @error_message = 'did find test logs. pl_dummy_worker invoked'; throw 50404, @error_message, 404; end</file><file name="src\synapse\_test\test_scripts\sql\synapse_unzip_archive.sql">-- query return error specific file logs can't found tables -- check successfull execution file movement archive inside unittest_unzip declare @expected_count int; declare @actual_count int; declare @error_message nvarchar(max); set @expected_count = 2 set @actual_count = ( select count(*) meta.log_files archive_status = 'succeeded' ([filename] like 'gzipped_landing%' [filename] like 'zipped_landing%') ); @expected_count &lt;&gt; @actual_count begin set @error_message = concat('found (', @actual_count, ') succeeded movements archive. expected (', @expected_count, ') file movements plan unittest_synapse_unzip'); throw 50404, @error_message, 404; end</file><file name="src\synapse\_test\test_scripts\sql\synapse_unzip_tasks.sql">-- query return error specific tasks linked unzip plan can't found tables -- check successfull execution tasks inside plan unittest_unzip declare @expected_count int; declare @actual_count int; declare @error_message nvarchar(max); set @expected_count = 2 set @actual_count = ( select count(*) meta.log_tasks task_name like 'unittest_synapse_unzip%' current_status = 'succeeded' ); @expected_count &lt;&gt; @actual_count begin set @error_message = concat('found (', @actual_count, ') succeeded tasks. expected (', @expected_count, ') tasks plan unittest_synapse_unzip'); throw 50404, @error_message, 404; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 7; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_integration', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_task.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_defaultreplace.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_defaultreplace', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_defaultreplace.'); throw 50000, @error_message, 1; end set @expected_count = 1; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_defaultreplace', format = 'delta' ) result column_2 null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values column_2 delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_defaultreplace.'); throw 50000, @error_message, 1; end set @expected_count = 1; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_defaultreplace', format = 'delta' ) result column_3 null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values column_3 delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_defaultreplace.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_replace_null_pk.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; declare @expected_value nvarchar(80); declare @actual_value nvarchar(80); set @expected_count = 3; set @expected_value = 'error' set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_integration_replace_value', format = 'delta' ) result ) set @actual_value = ( select column_1 openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_integration_replace_value', format = 'delta' ) result column_3='test_replace_value' ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_test_replace_value.'); throw 50000, @error_message, 1; end @expected_value &lt;&gt; @actual_value begin set @error_message = concat('the expected value delta table (', isnull(@expected_value,'null'), ') actual (', isnull(@actual_value,'null'), ') csv_test_replace_value.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_skiplines.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/skiplines', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_task.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_specialstring.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 4; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_specialstring.'); throw 50000, @error_message, 1; end ------------------------------------------------------------------------------------ set @expected_count = 4; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring', format = 'delta' ) result column_3 null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_specialstring column3.'); throw 50000, @error_message, 1; end ------------------------------------------------------------------------------------ set @expected_count = 2; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring', format = 'delta' ) result column_2 = ', belliard, 25' ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_specialstring column2 (belliard).'); throw 50000, @error_message, 1; end ------------------------------------------------------------------------------------ set @expected_count = 2; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring', format = 'delta' ) result column_2 = '""escaped""' ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_specialstring column2 (escaped).'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_struct_lines.sql">-- check: amount lines tables (csv recurse) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/csv_recurse_search', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') csv_task.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_formatting.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_task.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result decimal_comma null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values decimal_comma delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result decimal_dot null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values decimal_comma delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result timestamp_us null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values timestamp_us delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result timestamp_eu null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values timestamp_eu delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result date_us null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values date_us delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result date_eu null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values date_eu delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result date_nosep null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values date_nosep delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result date_us_slash null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values date_us_slash delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 0; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result date_eu_slash null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values date_eu_slash delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end ---------------------------------------------------------- set @expected_count = 2; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration', format = 'delta' ) result unix_format = '2024-10-14 08:19:15.0000000' ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number null-values unix_format delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') formatting_integration.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_json.sql">-- check: amount lines tables (json) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 4; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/json_integration', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') json_task.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_optional_columns.sql">-- check: amount lines tables (csv) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/options_optional_columns', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') options_optional_columns.'); throw 50000, @error_message, 1; end ------------------------------------------------------------------------------------------------- set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/options_optional_columns', format = 'delta' ) result column_3 null ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows column_3 delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') options_optional_columns.'); throw 50000, @error_message, 1; end</file><file name="src\synapse\_test\test_scripts\synapse\ingestionworker_parquet.sql">-- check: amount lines tables (parquet) declare @error_message nvarchar(max) declare @expected_count int; declare @actual_count int; set @expected_count = 3; set @actual_count = ( select count(*) openrowset(bulk 'https://$(storage_account).dfs.core.windows.net/silver/parquet_integration', format = 'delta' ) result ) @expected_count &lt;&gt; @actual_count begin set @error_message = concat('the expected number rows delta table (', isnull(@expected_count,'null'), ') equal actual count (', isnull(@actual_count,'null'), ') parquet_test.'); throw 50000, @error_message, 1; end</file><file name="wiki\Architecture\dataconfig_module.md"># dataconfig module last edit: 12/02/2024 - [dataconfig module](#dataconfig-module) - [general information](#general-information) - [purpose](#purpose) - [concepts](#concepts) - [main artefacts](#main-artefacts) - [dataconfig module overview](#dataconfig-module-overview) - [test artefacts](#test-artefacts) - [devops pipeline](#devops-pipeline) ## general information * source code: src/powershell/modules/dataconfig * prerequisites: data_engineering_guide.md * test framework: pester ### purpose purpose module easily deploy metadata describing data sources sql meta database. module allows user configure 2 json files, one describing plans tasks one describing metadata source file. make life user easier, work *insert into* statements sql. module also prevents users configuring metadata allowed. example, time writing (12/02/2024), ingestion framework build deal excel files. user configures file type 'excel', system reject configuration immediately instead user insert database later find allowed. even worse, system ingest data source allowed ingested. also prevents users configuring values mess system. example, time writing (12/02/2024), users expected define worker synapse workspace want use execute task configuring. existing worker *ingestionworker*. user tries define value, configuration files rejected. **note:** module direct link ingestion process, help users efficiently deploy metadata sql meta database. allows developers make sure data database sufficient quality. ### concepts * orchestrator * plan-task configuration * dataset configuration ## main artefacts source code mainly contained **public** folder, contains set powershell files (.ps1). file contains function dataconfig module called upon importing module (see overview below). next, two important files **dataconfig.psm1** **dataconfig.psd1**, describe dataconfig module data respectively. .psm1-file defines files/functions part module looking public (*for non-existent*) private folders. collecting files, script imports files environment running module. .psd1-file contains functions part module, configured using **.build.ps1** script. focusing little functions, **set-ingestionmetadatafromjsonfile** orchestrator process. function expects user pass path configuration file, relevant sql parameters configuration file needs deployed. first, function calls **compare-ngestionmetadatafromjsonfile**, function validates configuration file. specifically, **schemas** folder contains (set of) json file(s) describes expected schema configuration file. schema also contains "quality" checks discussed earlier prevent users inserting low quality metadata. *enum-operator*, example, lists allowed values specific key. configured value list, file rejected. also *additionalproperties-operator* set false, prevents users creating parameter keys. schema-checks passed, file ingestion start. **set-ingestionmetadatafromjsonfile** split 2 sections, depending whether dealing plan-task json file dataset json file. dealing plan-task json file, functions **set-ingestionplanconfiguration** **set-ingestionplantaskconfiguration** executed given metadata. dealing dataset json file, functions **set-ingestiontaskconfiguration**, **set-ingestionsourceconfiguration**, **set-ingestionsourcecolumnconfiguration**, **set-ingestionsourcecheckconfiguration** **set-ingestiondeltatableconfiguration** executed given metadata. configuration-functions constructs query uses one deploy-schema stored procedures. query passed **get-sqlqueryresults** function, interprets query returns logging information number rows configuration inserted, updated, deleted. **note:** build.ps1 script used locally build dataconfig module. moving devops, nuget package artefact created artefact feed downloaded used repositories vms. #### dataconfig module overview | function name | description | parameters | | ----------- | ----------- | ---------- | | set-ingestionmetadatafromjsonfile | serves orchestrator function takes configuration file (plan dataset). depending type configuration file, different functions called metadata file passed correct function. | path json file, sql login parameters, target environment code (dev, int, tst, acc, prd) | | set-ingestionplanconfiguration | insert metadata plan-task configuration file plan_configuration table. | sql login parameters, plan name, plan description | | set-ingestionplantaskconfiguration | insert metadata plan-task configuration file plan_task_configuration table. | sql login parameters, plan name, json tasks need plan | | set-ingestiontaskconfiguration | insert metadata dataset coniguration file task_configuration table. | sql login parameters, taskname, taskdescription, tasktype, workername, filelayout, sourcefolder, containername, tablename | set-ingestionsourceconfiguration | insert metadata dataset coniguration file source_configuration table. | sql login parameters, filelayout, matchpattern, extension, columndelimiter, rowdelimiter, escapecharacter, quotecharacter, sheetname, datarange, encoding | | set-ingestionsourcecolumnconfiguration | insert metadata dataset coniguration file source_column_configuration table. | sql login parameters, list set json objects column, filelayout | | set-ingestionsourcecolumnconfiguration | insert metadata dataset coniguration file source_check_configuration table. | sql login parameters, list set json objects check, filelayout | | set-ingestiondeltatableconfiguration | check target table already exists silver database synapse on-demand server. not, start spark session create delta table | table name, target environment deploy delta tables, description delta table (metadata), list json objects describing columns delta table. | | get-sqlqueryresults | execute deploy-schema stored procedures get metadata configuration tables. function returns table stated many rows inserted, updated, deleted. | sql login parameters, query execute | ## test artefacts dataconfig module also needs properly tested. this, powershell offers *pester* module. module expects set **tests.ps1** scripts public private folders. best practice requires module one scripts function module. functions contain multiple tests function, testing specific functionality. therefore, every file public private folder extension **tests.ps1** considered part test artefact. given module constructed, tests require configuration file. configuration files used testing module found **.test_config** folder. **unit_test.ps1** script script initiate test files. first, context set pester, indicating files included tests results need outputted to. script seen orchestrator script unit tests dataconfig module **pester documentation:** https://pester.dev/docs/quick-start ## devops pipeline devops ci pipeline dataconfig module contains one job: build job. reason this, rather also deploy test job, real deployment needed specific environment. "deployment" creation artefact contains necessary functions, already done build job. pipeline make distinction development integration environment, artefact published different artefact feed devops. build job contains test-section actually building artefact. test-section run unit tests configured test artefact discussed above. tests passed successfully, actual build-section pipeline start. **.build.ps1** script initiated, results published nuget package new directory. package later picked up, converted artefact, published devops artefact feed picked pipeline future. **note:** reason building nuget package current framework uses mulitple git repositories, namely dap_appl_dala_code dap_appl_dala_config. repositories require use dataconfig module. facilitate using devops, artefact feed configured deploy nuget package to. repositories call upon feed needing ingest metadata.</file><file name="wiki\Architecture\sql_meta_database.md"># sql meta database last edit: 13/02/2024 - [sql meta database](#sql-meta-database) ## general information * source code: src/sql * prerequisites: data_engineering_guide.md * test framework: tsqlt ### purpose purpose sql meta database contain configuration data data sources need ingested using ingestion framework, contain logs synapse pipeline runs initiated lifetime ingestion framework. sql meta database central point ingestion framework contains data make process tick. is, synapse workspace calls upon stored procedures stored database, require data configuration tables. ### concepts * metadata: highly used concept metadata-driven ingestion framework. concerns data describing data sources (aka data data), source location source file landed, expected column names, target location sink data source file. basically, metadata describes ingestion framework interpret data coming specific data source, find files landing zone, move files silver layer. ## main artefacts main artefacts stored **metadb** folder. here, schemas, tables, stored procedures found, well **.sqlproj** file. sqlproj-file contains references artefacts need part meta database. **db_build.ps1** script transforms sql-project file dacpac, later deployed specific sql database using **db_deploy.ps1** script. schemas project **deploy** **meta**. deploy-schema used tables stored procedures related deployment data database. meta-schema used tables stored procedures related storage usage data database. ### main artefacts: table overview | table name | table schema | description | primary keys | | ---------- | ------------ | ----------- | ------------ | | log_runs | meta | identity table stores runs synapse pipeline *pl_start_run*. find plan initiated run. also stores status run, allowing quick monitoring succeeded, failed, aborted, cancelled runs.| run_id | | log_plans | meta | identity table stores plans initiated time, together plan id run id run initiated plan. also stores status plan, allowing quick monitoring succeeded, failed, aborted, cancelled plans.| plan_id | | log_tasks | meta | identity table stores tasks initiated time, together task id plan id plan initiated task. also stores status task, allowing quick monitoring succeeded, failed, aborted, cancelled tasks. | task_id | | log_files | meta | identity table stores files processed ingestion framework time. also stores current status ingestion. monitoring capability allows user easily check file process start debugging issue arises. | file_id; **note:** extended_filename expected unique | | log_run_plans | meta | table contains ids runs plans run together.| run_id, plan_id | | log_plan_tasks | meta | table contains ids plans tasks run together.| plan_id, task_id | | plan_configuration | meta | plan_configuration table contains metadata plans part ingestion framework. metadata table concerns name plan, description, *enabled* boolean column indicating whether plan used framework. | plan_name | | task_configuration | meta | task_configuration table contains metadata tasks configured ingestion framework. metadata table concerns name task, description, *enabled* boolean column. depending type task dealing with, different metadata columns need defined. ingestion task_type, system needs know source files stored (source_location) table need ingested (table_name). furthermore, name worker execute task also needs given. *for information workers: synapse_workspace.md* | task_name | | plan_task_configuration | meta | plan_task_configuration table gives overview tasks part plan, task group tasks belong to. again, *enabled* boolean column configured well. | plan_name, task_name, task_group | | source_configuration | meta | source_configuration table contains metadata interpret source file specific data source. task_configuration considers outer workings file (where located?, need know?, worker execute task?...), source_configuration table looks inner workings (what expected name file?, column delimiter, row delimiter, etc.?,...). source_configuration table focuses 'how system need interpret provided file?'. | file_layout | | source_column_configuration | meta | next initial configuration source, expected column names potential replacements also configued. done source_column_configuration. additional information, data types dimension (pk, scd2, scd3...) needs given. | file_layout, column_order | | source_check_configuration | meta | source files require explicit checks executed source files ingested. example, pdm, received zip-folder expected 50 files. case, issue happened provider's side ingestion cannot happen. checks data source configured source_check_configuration. | file_layout, check_name | ### main artefacts: stored procedure overview | stored procedure name | stored procedure schema | description | parameters | | --------------------- | ----------------------- | ----------- | ---------- | | end_plan | meta | given plan id, end plan 'succeeded' 'failed' status | plan_id, success_flag, run_id | | end_run | meta | given run id, end run 'succeeded' 'failed' status | run_id, success_flag | | end_task | meta | given task id, end task 'succeeded' 'failed' status | task_id, plan_id, success_flag | | get_plan_metadata | meta | given plan id task group, return task_type, list tasks, number tasks executed plan. list tasks returned meta.usp_get_task_metadata stored procedure. | run_id, plan_id, task_group | | get_plans | meta | given run id plan name, return list plan ids need started. | run_id, plan_name | | get_task_metadata | meta | given task id, return metadata source file task ingesting table json objects. | task_id | | get_tasks| meta | given plan_id task group, return list json objects. object describes task executed metadata | run_id, plan_id, task_group | | new_file | meta | ingestion framework encounters file landing zone, stored procedure called create new instance log_files table. | task_id, plan_id, filename, extended_filename, folder_path | | new_run | meta | start new pipeline run given plan. based name plan, logging tables initiate new run plan. 'new_plan' parameter *true*, new plan logged and, based plan_task_configuration table, associated tasks also initiated. new instance logging table get 'pending' status | pipeline_id, new_plan, plan_name, run_id, plan_name | | start_plan | meta | given plan id, start plan 'in progress' status | plan_id, run_id | | start_run | meta | given run id, start plan 'in progress' status | run_id | | start_task | meta | given task id, start task 'in progress' status| task_id, plan_id | | update_file_activity | meta | file moved different layer medallion framework, stored procedure updates ingestion_status file ingested. | extended_filename, activity, success, folder_path | | set_plan_configuration | deploy | stored procedure deploy configuration data plan_configuration table | plan_name, plan_description, enabled | | set_plan_task_configuration| deploy | stored procedure deploy configuration data plan_task_configuration table| plan_name, json object containing task information (task group, name, sequence, enabled) | | set_source_check_configuration | deploy | stored procedure deploy configuration data source_check_configuration table | file_layout, json object contianing check information (name, enabled) | | set_source_column_configuration | deploy | stored procedure deploy configuration data source_column_configuration table | file_layout, json object containing column configuration (source name, sink name, data type, dimension, sequence) | | set_source_configuration | deploy | stored procedure deploy configuration data source_configuration table | file_layout, file_pattern, file_extension, column_delimiter, row_delimiter, escape_character, quote_character, header, encoding | | set_task_configuration | deploy | stored procedure deploy configuration data task_configuration table| task_name, task_type, worker_name, task_description, table_name, file_layout, source_folder, container_name, enabled | ## test artefacts test artefacts stored **metadb_test** folder. here, schemas, dependencies, stored procedures found, well **.sqlproj** file. sqlproj-file contains references artefacts need part test framework. **db_build.ps1** script transforms sql-project file dacpac, later deployed specific sql database using **db_deploy.ps1** script. framework used testing **tsqlt** framework sql. framework contained **tsqlt_1.0.5873.27393.sql** script **_test** folder. script contains stored procedures tables used framework. default script, similar would import module python powershell. tests test framework found **t_design_tests** **t_unit_tests** folders. design tests enforce naming conventions table column names. is, make sure lowercase. unit tests validating stored procedures main artefact. t_unit_tests folder contains set folders, one stored procedure meta database. folder similar structure: * one class-script creates schema test. schema added tsqlt.testclass class, later interpreted tsqlt test case. * one setup-script prepares tests executed. recreates existing tables faketables fills test-data. end tsqlt procedure, fake tables data removed automatically framework. * one tests files stored procedure. **run_testclasses.ps1** **tsqlt_with_xml_output.ps1** files used run test cases. former file meant used locally output results, devops pipeline use latter output published pipeline. schema defined test project **test** stored procedures project use schema. stored procedure **usp_reset_database** used completely clean data contained database. procedure allowed production environment, useful development. stored procedure **usp_run_tsqlt_tests** run tests specific type (debug, unit...), called upon run_testclasses.ps1 script. script can, again, used development part production environment. **note:** tsqlt base script dacpac included repository make sure developers access source code worry create new dacpac. **note:** **metadb_test.sqlproj**, tsqlt dacpac included reference. however, building deploying project deploy referenced dacpac. reason including dacpac reference suppress warnings given *dotnet build* build script. story azuev12_master.dacpac. ## devops pipeline devops ci pipeline sql, pipeline-ci-sqlmetadb, initiated 3 jobs: build, deploy test. build-job build dacpac metadb metadb_test sql project. dacpacs converted separate pipeline artefacts, together scripts needed deployment testing. deploy job then, simply, deploy metadb dacpac. test job also deploy metadb_test dacpac, deploying tsqlt dacpac. this, tsqlt_with_xml_output.ps1 script called run unit tests publish results tests.</file><file name="wiki\Architecture\Synapse_workspace.md"># synapse workspace last edit: 12/02/2024 - [synapse workspace](#synapse-workspace) - [general information](#general-information) - [purpose](#purpose) - [concepts](#concepts) - [main artefacts](#main-artefacts) - [pipelines](#pipelines) - [pipeline overview](#pipeline-overview) - [notebooks](#notebooks) - [notebook overview](#notebook-overview) - [test artefacts](#test-artefacts) - [pipelines](#pipelines-1) - [notebooks](#notebooks-1) - [devops pipeline](#devops-pipeline) ## general information * source code: src/synapse/studio/ * prerequisites: data_engineering_guide.md * test framework: unittest module (python) ### purpose synapse workspace serves orchestrator executor ingestion process, embodiment plan-task configuration. workspace contains main code successfully move file landing silver. activity (pipeline, function, method...) driven parameters, refers metadata data source needs ingested. main resources pipelines notebooks, orchestrator. orchestrators meant interpret metadata invoke set activities depending metadata. ### concepts * metadata: highly used concept metadata-driven ingestion framework. concerns data describing data sources (aka data data), source location source file landed, expected column names, target location sink data source file. basically, metadata describes ingestion framework interpret data coming specific data source, find files landing zone, move files silver layer. * orchestrator: concerns artefact use provided metadata invoke set activities, functions, etc. necessary complete certain task. * worker: worker invokes set functions, methods, activities, etc. specific order successfully execute task. *one compare concepts devops pipeline, pipeline invokes set jobs job set tasks. ci pipeline, orchestrator scenario, invokes set jobs. starting jobs, first checks whether development integration environment. depending environment, different variables-template needs called upon. variables-template metadata. based metadata, ci pipeline invokes build, deploy, test jobs. jobs, workers, call upon set scripts tasks goal completing specific activity (e.g. building artefact, deploying source code...).* ## main artefacts main artefacts pipelines notebooks. contain logic, functions, methods, activities ingestion framework. ### pipelines **pl_start_run** pipeline orchestrator pipelines, invoke set underlying pipelines. pl_start_run pipeline requires two parameters: name plan (plan_name) whether new plan rerun previously failed plan (new_plan). pipeline also starting point entire ingestion framework. based plan_name, stored procedure invokes return list plans executed. list contains set plan ids. is, plan name executed past complete successfully, system make sure plan (and failed tasks) executed again. important control ingestion flow make sure files data source ingested correct order. **note:** one run contain multiple plans execute, development team opted use runs run_ids logically group together plans executed pipeline invocation. runs theory part plan-task configuration, assigned individual log table make pipeline debugging maintenance easier. simplify, every time pl_start_run pipeline started, new run started. new run create zero one new plan logging tables, (re)start one plans. pl_start_run pipeline foreach activity loops list ids, id **pl_start_plan** pipeline invoked. pipeline invokes **pl_start_task_group** pipeline multiple times, time different task_group parameter. step makes sure different task groups executed correct order. is, task part group, task group expected independent one another. task depends another task, part another task group executed later point time. **pl_start_task_group** calls stored procedure collect set tasks part invoked task group need started specific plan executed. stored procedure also returns type worker execute tasks hand. is, task groups, logic contained synapse_pipeline, task groups logic contained synapse_notebook. depending worker type, different activitiy started. #### pipeline overview | pipeline name | description | parameters | | ----------- | ----------- | ---------- | | pl_start_run | start run plan. | plan_name, new_plan | | pl_start_plan | execute plan invoking task groups specific order | plan_id, run_id | | pl_start_task_group | collect list tasks execute task group plan invoke orchestrator notebook metanotebook | task_group, plan_id, run_id | ### notebooks worker type (discussed above) synapse_notebook, **metanotebook** invoked. orchestrator notebooks, call upon functions methods defined notebooks. notebook takes set parameters: environment_code (dev, int, tst, acc, prd), plan_id, list tasks execute. orchestrator interpret list tasks execute correct worker (notebook) execute task individually. notebook loop list tasks and, based metadata task, worker called upon. worker contains logic successfully execute task hand. **worker notebooks** **ingestionworker** worker execute methods functions ingest file landing zone delta lake layer. **functions methods** **classes** notebook contains, name suggests, set classes. class set methods (= functions class) called upon class-object. classes contained notebook are: * sqlconnection: sqlconnection class allows user set-up connection object sql server database. object later called upon execute queries stored procedures database, interpret returned results. * plan: invoked plan, plan-object created plan class. time writing (12/02/2024), class real purpose built-in precaution potential future developments. * task: task task list, task-object created. class contains main methods needed complete task, main method collection metadata creating file-objects (for file class) file related task. * file: file requires processing (ingesting, etc.), file object created. file-class contains methods needed move file landig zone delta lake, well logging capabilities sql logging tables. * checks: task potentially requires quality checks executed. methods found checks-class, object created task-class. start_checks method serves orchestrator interprets list checks need executed invokes correct method check. * deltatable: *redundant object, rewritten sprint 2* **ingestionfunctions** notebook contains set functions called execution ingestion tasks. main section "merge functions" section, groups together functions meant merging existing delta table new source file. functions first check whether mathes primary keys delta table source file. match, system executes insert-function (**nomatch**). match, system executes update-function (**columnmatch**). script also contains standalone functions, **list_directory_content** returns list files folders specific azure container. #### notebook overview | notebook name | description | parameters | | ----------- | ----------- | ---------- | | metanotebook | orchestration notebook synapse spark environment. notebook "imports" functions loops tasks certain plan. depending metadata, worker called. | plan id, list tasks, environment code | | ingestionworker | ingestionworker notebook calls functions methods needed ingest file landing zone delta lake | data lake storage account name, list variable names needed execute ingestion function (= metadata) | | classes | notebook contains set classes (sqlconnection, plan, tasks, file, checks), methods. methods called upon class-object | | | ingestionfunctions | set functions used ingestion process. important function **mergefile** merge source file existing delta table. | | | cleanworkspace | set functions used clean workspace. is, remove files data lake containers remove tables synapse database | environment code | ## test artefacts test artefacts pipelines notebooks. contain logic, functions, methods, activities test ingestion framework. ### pipelines **pl_dummy_worker** pipeline used test whether flow pl_start_run pl_start_task_group works whether stored procedures properly integrated. pipeline real purpose invoked other, overarching pipelines. ### notebooks **testnotebook** orchestrates unit testing synapse notebooks. 2 notebooks, **testclassess** **testfunctions** contain set classes modules properly test modules functions **classess** **ingestionfunctions** respectively. 3 test-notebooks "test framework" synapse notebooks set-up. notebooks use **unittest** module pyspark. core module comes developing class function/module needs tested. class contain set modules call to-be-tested function/module assert actual results expected results. whenever developer creates new function module, new test-class added correct notebook. testnotebook contain classes, serves orchestrator unit tests. is, "imports" classes notebooks using unittest.main()-method, executes methods classes. execution, results written file logs-container data lake. here, results analyzed developer devops pipeline invoked testnotebook. **more information:** https://docs.python.org/3/library/unittest.html ## devops pipeline devops ci pipeline synapse workspace **pipeline-ci-synapse.yml** found pipelines folder. whenever change made src/synapse folder main branch, pipeline run make sure breaking changes happened. pipeline also allows developer publish changes publish branch (and synapse live mode). ci pipeline executes 3 jobs, depending branch, different variables passed jobs. 1. build job &amp;rarr; job-build-synapse.yml: * build job generate 4 artefacts, 2 used: synapse_main synapse_test. synapse_main artefact contains arm template describes main artefacts discussed above. done using regex-approach execution devops task creates arm template (synapse workspace deployment@2). synapse_test artefact contains arm template describes tet artefacts discussed above, using regex well. important note make then: follow naming conventions newly developed synapse artifacts important build arm templates. * *fyi 1: build job uses task called "synapse workspace deployment@2", takes json files git integrated synapse workspace converts arm template. step creates artefact containing arm template parameter template overwritten deployment. task, however, gives issue overwrites existing arm templates. since process builds 2 separate arm templates, messes system. another task used copy first arm template separate artefact generating second arm template* * *fyi 2: file (src/synase/studio/template-parameter-definition.json) determines keys json architecture parameterized. important script as, example, name spark pool tailored environment (&lt;environment_code&gt;dapsspcore). synapse workspace arm template deployed different environment, references spark pool need overwritten correct name environment. overriding parameters happen build process, creation parameter template does.* 2. deploy job &amp;rarr; job-deploy-synapse.yml: * deploy job deploys arm template synapse_main artefact synapse workspace specific environment. deploying, parameters arm template overwritten match environment. deployment, task checks whether silver database already exists environment. case, script initiated creates lake database. 3. test job &amp;rarr; job-test-synapse.yml: * test job deploy synapse_test arm template executes testnotebook discussed test individual function method. initial tests, worker also tested individually make sure system well-integrated.</file><file name="wiki\DevOps\unit_tests\AddingTests.md"># adding unit tests yaml pipeline ## purpose document displays process importance adding unit tests devops pipelines. ## importance testing devops pipelines adding unit tests executors unit tests devops pipeline provides extra layer protection make sure nothing broken development. especially implementing changes previously created functions features. creates opportunity activate tests every pull request make, well every deployment environments, ... ## creating new test creating new test devops pipeline looks like quite hassle, actually quite easy. guide take process, without going much detail actually create unit test creating task inside devops pipeline. two parts: first developing locally, creating task inside devops pipeline. ### local creation first step creating test, developing locally repo. that, take look localdeploy [synapse](../../../development/localdeploy/localdeploy_synapse.ps1) script, localdeploy [sql](../../../development/localdeploy/localdeploy_sqldatabase.ps1) script. spaced reserved make sure test everything pc, see steps devops pipeline takes test different aspects infrastructure. inside localdeploy scripts, create logic, execute external scripts check everything works. does, create pull request, get approved continue create test inside devops pipeline. ### devops pipeline adding test devops pipeline bit complex testing something locally, difficult. know look. #### providing resources first all, creating new test, make sure provide necessary materials artifact. artifact functions wrapper around resources needed execute tests inside devops pipeline. this, go right [jobs folder](../../../pipelines/templates/jobs/). look job-build files make resources available artifact. look 'copyfiles@2' tasks inside files, add right paths it. &lt;blockquote style=" display: block; background: #fbe9e9; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1; color: black; text-align: justify; border: 1px solid #cf1212; border-left: 15px solid #cf1212; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt; &amp;#128204; &lt;/span&gt; &lt;b&gt;important:&lt;/b&gt; make sure include repo name root! &lt;/p&gt; &lt;/blockquote&gt; #### adding tasks resources made available inside artifact, access location inside task. &lt;blockquote style=" display: block; background: #d9e8ff; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1; color: black; text-align: justify; border: 1px solid #2780e3; border-left: 15px solid #2780e3; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; &lt;b&gt;note:&lt;/b&gt; note errors deployment certain script can't found, either typo provided inside artifact. &lt;/p&gt; &lt;/blockquote&gt; again, go jobs folder look job-test files. depending part process testing, put file. meaning testing something synapse, going look job-test-synapse, meta database. take look tasks inside files reproduce steps took inside localdeploy scripts. thing that's different here, syntax. besides that, much difference compared inside local deploy scripts. #### testing tests finally, everything configured, start checking tests actually work. going [devops pipelines](https://dev.azure.com/lnlbe/data_platform/_build) inside workspace, take select right pipeline start testing. trial &amp; error see works doesn't.</file><file name="wiki\Flows\Packages\DEP-External-package-in-Azure.md"># dep: external package azure ## purpose [data exfiltration protection (dep)](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection) installed, able install packages notebook via pip %pip install. handle process manually adding packages workspace, follow guide. ## visual ![visualdeppckages](./externalpackagedep.png) ## procedure ### synapse workspace first all, select desired synapse workspace want add package to. going open synapse studio workspace, navigate 'manage' left bar. moment writing, little toolbox bottom navigation bar left. #### manage ##### configuration &amp; libraries clicked "manage" section, inside synapse workspace, see menu different options. section "source control", find "configurations libraries" section. section, click "workspace packages". ###### workspace packages workspace packages, find list installed external packages. cases, empty. meaning add via "upload" top newly openend pop-up. open window upload .whl file. &lt;blockquote style=" display: block; background: #fbe9e9; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1; color: black; text-align: justify; border: 1px solid #cf1212; border-left: 15px solid #cf1212; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt; &amp;#128204; &lt;/span&gt; &lt;b&gt;important:&lt;/b&gt; dep enabled, install .whl files. files get stored section, cause issues later process. look .whl extension installing external packages. &lt;/p&gt; &lt;/blockquote&gt; find right file [pypi](https://pypi.org/). search package need, download right version file. &lt;blockquote style=" display: block; background: #d9e8ff; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1; color: black; text-align: justify; border: 1px solid #2780e3; border-left: 15px solid #2780e3; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; &lt;b&gt;note:&lt;/b&gt; azure synapse workspace frequently update infrastructure, things pip install quite old. means advised download latest version package install. far following external packages known working: &lt;ul&gt; &lt;li&gt;levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&lt;/li&gt; &lt;li&gt;rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&lt;/li&gt; &lt;li&gt;disposable_email_domains-0.0.90-py2.py3-none-any.whl&lt;/li&gt; &lt;/ul&gt; make sure use exact file names. since different files specific version specific package. &lt;/p&gt; &lt;/blockquote&gt; &lt;blockquote style=" display: block; background: #d9e8ff; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1; color: black; text-align: justify; border: 1px solid #2780e3; border-left: 15px solid #2780e3; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; &lt;b&gt;note:&lt;/b&gt; packages dependencies based packages. meaning download package too, install first, installing actual package want use notebook. known dependencies are: &lt;ul&gt; &lt;li&gt;levenshtein dependencies rapidfuzz&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;/blockquote&gt; selected desired package, select "upload" bottom open window. close window again, package added workspace. use packages, link apache spark pool. ##### analytics pools this, go "analytics pools" "manage". here, find spark pools need configure. ###### apache spark pools apache spark pools, find every spark pools configured workspace. now, link external packages pool able use them. hover desired sparkpool, show following three things: ![options](https://imgur.com/1kqjmzm.png) click three dots, open little menu. menu, see "packages". click it, open window. see different types configurations. first all, make sure "allow session level packages" set disabled. needing that. secondly, look "workspace packages". see package here, means still add spark pool. that, click "+ select workspace packages". open list packages added workspace. here, select workspace package press "select" bottom screen. process start upload package try attach pool. &lt;blockquote style=" display: block; background: #d9e8ff; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1; color: black; text-align: justify; border: 1px solid #2780e3; border-left: 15px solid #2780e3; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; &lt;b&gt;note:&lt;/b&gt; process take 20 minutes, patient check activity log synapse workspace see things going. &lt;/p&gt; &lt;/blockquote&gt; errors process, everything working. check packages linked pools, go back "workspace packages", click package added. ![related2](https://imgur.com/fjtdy05.png) see number related section package, know worked attached pool synapse workspace. now, open notebook import packages. sure select right spark pool, good go. &lt;blockquote style=" display: block; background: #e9fbe9; padding: 10px 5px 10px 10px; margin: 10px 2px 10px 2px; position: relative; font-family: georgia, serif; font-size: 13px; line-height: 1.5 ; color: black; text-align: justify; border: 1px solid #04c10a; border-left: 15px solid #04c10a; "&gt; &lt;p&gt; &lt;span style='font-size:15px;'&gt;&amp;#128161;&lt;/span&gt; &lt;b&gt;tip:&lt;/b&gt; sometimes, error trying link package pool. that's quite normal, usually easily solved. common errors (found via activyt log): &lt;ul&gt; &lt;li&gt;&lt;b&gt;xxx.whl supported wheel platform&lt;/b&gt;: means file version accepted azure synapse. find right version pypi. make sure check approved packages list bit higher document. they're there, look yourself.&lt;/li&gt; &lt;li&gt;&lt;b&gt;warning: pip-installed dependencies environment file, list pip one conda dependencies.&lt;/b&gt;: means package trying install, dependency another package. check this, install package locally computer check dependencies there. then, download extra package pypi install install actual needed package.&lt;/li&gt; &lt;/ul&gt; &lt;/p&gt; &lt;/blockquote&gt;</file><file name="wiki\General doc\1.Cloning-and-setting-up.md"># cloning &amp; setting repo ## description readme-file, find everything related cloning repo setting repo pc. ## prerequisites 1. make sure visual studio code (or another code editor) installed pc. 2. make sure git installed pc. 3. basic visual studio code knowledge. 4. basic windows powershell powershell 7 knowledge ## getting started first all, clone project visual studio code (vsc). that, start vsc open terminal vsc itself. check right folderpath (c:\users\nalo_initials\\). choose own, path going important later configuration files. that, go [azure devops repo's](https://dev.azure.com/lnlbe/data_platform/_git/dap_appl_dala_code) select right repo. ![repo select](https://imgur.com/hqwmmuw.png) selected right repo, find everything regarding repo. see file structure, well active branches, ... ![repo overview](https://imgur.com/bw49w0g.png) right upper corner repo, see two buttons. "set build" "clone". need click "clone", opens menu get link need paste vsc. ![clone click](https://imgur.com/vk9vi2z.png) ![select repo link](https://imgur.com/n1ve8y5.png) copied link, open powershell, clone repo right location (c:\users\xxxnnn, e.g. c:\users\ver949) open vsc again. way, start working project. ![paste repo link](https://imgur.com/cmg4d85.png) cloned repo pc. work, need make sure [logged git](https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html). logged in, unable clone anything, right permissions that. make sure ask someone check permissions that's case. cloning project, create branch repo via vsc. need every project work on. that's allowed push directly main, safety reasons ofcourse. cloning succesful, issue creating new branch via vsc. bottom left corner vsc, see branch currently in. 'main', cloned project first time. click create branch / project repo. ![main branch](https://imgur.com/jzvvd6f.png) ![create new branch](https://imgur.com/k0okorb.png) creating branch, always starting topic\_ followed initials (e.g. topic_dv user 'dylan verniers'). every project working on, associated working subfolders. predetermined guideline. ![click branches](https://imgur.com/noyoxok.png) done that, good start first project.</file><file name="wiki\General doc\2.Your-first-project.md"># first project vsc ## description actually start working code, first project need complete setting environment. also nice first practice work git versioning. ## prerequisites 1. looked "cloning setting up" 2. make sure have, least, powershell 7 installed pc ## getting started first all, make sure working main branch. that, look bottom left corner. there's anything else standing besides main, need change it. also commit changes main branch anyway. ![wrong branch](https://imgur.com/noyoxok.png) ![select main](https://imgur.com/crzzldx.png) ![main branch](https://imgur.com/jzvvd6f.png) now, terminal, git pull. make sure every change committed main, pc well. ![git pull](https://imgur.com/9ln6t1b.png) done that, create (first) project branch name topic_dv/env_setup. know already previous guide, explaining anymore. now, switch branch (the one created), write terminal ```git pull origin main```. pull everything that's main branch, branch. know synced latest version main. make changes, could get deleted. time look first project. dap_appl_dala_code/scripts/local-env, find environments every user repo. used make sure right credentials folder paths deploy projects future. ![open env folder](https://imgur.com/moqqtwt.png) copy one files there, use one examples dap_appl_dala_code/scripts/local-env/, start changing credentials user file. things change, things marked red screenshot below. change mailadres nalo also nalo credentials reporoot. set repo somewhere else, make sure update path accordingly. ![open file](https://imgur.com/lsklch3.png) now, run file terminal. make sure save it, otherwise work. ![run file](https://imgur.com/yi3tikv.png) errors step, usually modules found need installed. run script `development/help_scripts/start_up.ps1`. normally, worked, encountering issues still; try executing following commands powershell terminal (in vsc): - ```install-module az.synapse -scope currentuser``` - ```install-module az.accounts -scope currentuser``` - ```install-module sqlserver -scope currentuser``` still issue, could try ```connect-azaccount``` make sure connected azure. this, forget nalo network (via vpn internal wifi). need commit changes git first. everything found source control vsc. make sure pushing main, project / branch. normally, pushing main allowed anyways. ps. normal see commit first, possible see publish button. used publish branch devops, otherwise see there. first need publish branch commit it. ![source control](https://imgur.com/gupeqia.png) now, time generate first pull request.</file><file name="wiki\General doc\3.First-pull-request.md"># first pull request ## description readme-file, find everything related creating pull request. ## prerequisites 1. make sure read 'your first project', actually create pull request. ## getting started currently done lot things, going create request merge main branch. this, go devops platform, repo's [pull requests](https://dev.azure.com/lnlbe/data_platform/_git/dap_appl_dala_code/pullrequests?_a=mine). access can't log in, sure check administrator(s)! top screen, seeing something looks like path (lnlbe/data_platform/repos/pull requests/repo_name). that's normal, make sure selected correct repo connect to. otherwise, able find (git) branch. ![search repo](https://imgur.com/rov7lnk.png) selected repo, usually two options. first case, pull request displayed pop-up top page. select create pull request based latest changes related repo. ![pop pull request](https://imgur.com/sxmqbd0.png) see right branch, create new pull request top. ![new pull request](https://imgur.com/qdjdynr.png) new request, search desired branch would like commit. find it, probably wrong repo. published branch vsc commit anything. ![browse branhces](https://imgur.com/wbhfndg.png) request itself, find several things look at. usually already pre-configured, sometimes need change things too. ![top part request](https://imgur.com/ex2rcq6.png) top request, choose branch want merge into. usually that's main, need something else, change it. files, find overview changes made contrast branch trying merge into. see changes going made. commits hand collection changes made visual studio code. committed synced changes, is. click see changes made commit. parts request: ![fields pull request](https://imgur.com/jc7v4ve.png) - title: need branch name. usually quick summary changed. example 'made wiki' that's it. write 5 words there. - description: say something pr feel like title say plenty. - reviewers: work project, add reviewers get notified request waiting them. approve changes complete pr. - work items link: that's based open tickets sprints. read general readme root repo. make sure create ticket first start work something. - tags: really necessary, could come handy future look up. link right tags request, discuss teamlead / administrator / manager / ... that's it! submit pull request, great! next documentation, we're going look checking pull request approving it.</file><file name="wiki\General doc\4.Approve-and-check-PR.md"># approve &amp; check pr ## description readme-file, find everything related approving (your first/ a) pull request (pr). ## prerequisites 1. make sure checked "first pull request" ## getting started pull requests grouped repo. again, search right repo find pull requests waiting approved checked. click start. ![search repo](https://imgur.com/rov7lnk.png) ![top pr](https://imgur.com/dsmaet6.png) top pr, see several things. tabs discussed later documentation. regarding parts: title see tag, check status. ![tag](https://imgur.com/jbfkllu.png) right side top, approve pull request set auto-complete. auto-complete set make sure that, pull request (pr) approved, merge done automatically approval. ![show autocomplete](https://imgur.com/xv58bcq.png) ![set auto-complete](https://imgur.com/quzyecu.png) leave everything checked shown screenshot, besides maybe required additional check. something check first teamlead someone requested additional check. matter, uncheck make required. ps. set auto-complete, merge manually pr approved. take advice turn on. right side pr, find additional information reviewers required not. well tags linked work items, configure initial pr-form. ![right side pr](https://imgur.com/yg4yb0b.png) now, important stuff pull request, files. parts less important 'conflicts' used conflicting merge issues. resolved developer reviewers pr. ![files pr](https://imgur.com/fcjpocs.png) files, check changes make comments person reviewing pr for. ![files example](https://imgur.com/k4twztj.png) files, find several things: - - file: changes made document. file recently added. - - file: changes made document. things deleted file. - filenames ~~strikethrough~~: means file deleted completely. - filenames without anything behind them: changes made document. something changed document. please check files see changes made. see things would like notify requester about, change something, best leave comment. ![comment](https://imgur.com/ej75pfl.png) want precise part file doubts about, select part colleague know specific part look at. ![specific selection](https://imgur.com/ekcotpy.png) way, know change look something. get back pull request, overview, check comments changes made / things need talked about. ![comment overview](https://imgur.com/epythsf.png) looked comment (and changed something), click resolve pull request ready approved. that's things, general, need know working azure devops. hard, everything covered extensive detail, get picture. now, try project hesitate ask help colleagues!</file><file name="wiki\synapse\unit_tests\functions.md">[[__toc__]] # functions ## description markdown file contains description unit tests executed functions contained synapse spark notebooks. ## unit tests ### list_directory_content() * purpose: recursively list files folders within specific path container * unit tests: &gt; - valid unit tests: &gt;&gt; 1. given full path one specific file (not including file), function return file file-list folder-list empty &gt;&gt; 2. given path container, return list files folders container &gt; - invalid unit tests: ### pk_match() * purpose: create string based primary columns ### column_match() ### no_match() ### merge_file()</file><file name="wiki\synapse\unit_tests\readme.md"># unit tests ## description folder describe unit test cases initiated testing synapse architecture. markdown file focus one specific type test: 1. classes &amp; methods 2. functions 3. workers ## prerequisites able understand unit tests, deeper knowledge required architecture synapse spark notebook environment: (to do: add reference synapse spark architecture markdown file)</file><file name="wiki\user_guides\data_engineering_guide.md"># data engineering guide last edit: 09/02/2024 - [data engineering guide](#data-engineering-guide) - [introduction](#introduction) - [contact details](#contact-details) - [philosophy](#philosophy) - [general introduction: frameworks architectures](#general-introduction-frameworks-and-architectures) - [data lakehouse](#data-lakehouse) - [medallion architecture](#medallion-architecture) - [delta lake framework](#delta-lake-framework) - [plan-task configuration](#plan-task-configuration) - [continuous integration, continuous deployment (ci/cd)](#continuous-integration-continuous-deployment-cicd) - [bringing together](#bringing-it-all-together) - [folder structure](#folder-structure) - [configuration](#configuration) - [development](#development) - [pipelines](#pipelines) - [src/source](#srcsource) - [tests](#tests) - [wiki](#wiki) ## introduction guide meant people taking data engineering role dap team. role data engineer boils to: 1. maintaining existing artefacts sql, powershell, synapse 2. creating new functions pipelines synapse 3. adding new tables stored procedures sql project 4. developing test scenarios business cases **note:** guide meant timeless. meant changes made repo still covered guide. result, use specific names developed functions, tables, etc. kept minimum. ## contact details questions uncertainties, feel free contact: * joachim.baert@keyrus.com * simon.plancke@keyrus.com * dylan.verniers@keyrus.com ## philosophy diving tools specifics repo, let's first shine light philosophy repo orchestration. become clear guide, large focus testing. current development team believes good environment always properly tested deployed production environment. therefore, team *test first, develop later* philosophy. means whenever issue arises, developer expected first write failing test scenario resembles use case. creating test scenario, validating expected error occurs, developer start fixing problem hand. development, test case used check whether new developments solved expected problem. test fails new developments implemented, chances development insufficient solve problem. reason taking *test first* approach developer first needs think problem trying solve. way, impacted already existing solution so. way, test scenario often greater coverage instead solving one specific problem. ## general introduction: frameworks architectures repository described user guide concerns ingestion framework used national lottery. framework implements set well-known frameworks architectures (see below). overall, ingestion framework uses set metadata tables move files different containers data lake. move one container another happen, set validity checks first executed (datatypes, expected column names, expected file format...). process, aim keep files close original possible, calculated derived columns avoided long possible, well dropping columns. aim ingestion process build lakehouse using delta lake framework, move files along data lake containers using medallion architecture. ### data lakehouse data lakehouse open data management architecture combines flexibility, cost-efficiency, scale data lakes data management acid transactions data warehouses. general idea set folders files data lake container combined one large database. metadata layer sits top folders files track files part table database. information: https://www.databricks.com/glossary/data-lakehouse ### medallion architecture medallion architecture data design pattern used logically organize data lakehouse, goal incrementally progressively improving structure quality data flows layer architecture. structure implemented national lottery uses 3 layers, next landing layer source files expected be. two three layers made visible set containers dedicated data lake azure [1], named raw silver, whilst third (gold) layer resembled data marts [2]. three layers, originally bronze, silver, gold, quality checks. idea data source moved landing raw, gets converted parquet file. move raw silver, initial set qualitiy checks done. main checks making sure expected columns names present data columns correct datatype (string, integer...). moving silver gold, source data gets split different data marts necessary different business users. **note:** data source movements landing silver part self-service layer provided technical users national lottery. moving silver gold (for new sources) likely require human intervention therefore automated. information: https://www.databricks.com/glossary/medallion-architecture [1]: see example data lake account devdapstdala1. [2]: time writing (08/02/2024), gold layer developed yet. ### delta lake framework delta lake framework open-source storage framework built around storage layer, providing foundation lakehouse architecture. delta lake, stored silver container, consists set folders. folder represents table lakehouse, consists set parquet json files. querying table on-demand sql endpoint, parquet json files combined form well-known structured sql table. further, set-up allows user "timetravel". less magical, boils fact possible query previous version table. would, example, necessary see contents table last quarter, would theoretically possible. one note make storing files become quite heavy time. policies put place data source "vacuum" table reduce number versions kept [3] [3]: time writing (08/02/2024), feature put place yet. ### plan-task configuration plan-task configuration method used execute set tasks one batch (or plan). idea invoking specific plan, system figure tasks part plan. task also metadata assigned it. example, data engineer wants move set files one container another (not necessary container). plan would called 'move_files_plan'. plan would contain set tasks, one file want move. metadata task would source sink container file needs moved to, potentially metadata file type (csv, json...) expected name file. set files appears every day, instead start etl pipeline every file, data engineer configure one pipeline start plan. based tasks within plan, metadata configured task, files moved desired. ### continuous integration, continuous deployment (ci/cd) process uses ci/cd deployment framework 5 environments: development, integration, test, acceptance, production. environment purpose, settings. 3 building blocks used ingestion process: synapse analytics workspace, self-made powershell modules, azure sql database. building blocks ci/cd pipeline testing framework. ### bringing together repo contains files necessary self-service, metadata-driven ingestion framework delta lake based lakehouse, using ci/cd practices source code promotion. synapse workspace main orchestrator executor framework. notebooks workspace contain functions methods needed successfully ingest data source delta lake. pipelines contain plan-task logic, plan started task plan executed. since plan-task framework metadata-driven, needs strong connection azure sql database. database contains metadata related plan-task configuration metadata task set configuration tables. next that, database also contains set stored procedures query configuration tables return necessary metadata metadata-driven synapse pipeline. ingesting metadata sql tables takes lot time using *insert into* statements, team decided build powershell module called **dataconfig**. module allows user easily insert metadata configuration tables follows: user configures two json files, one plan-tasks one data source metadata (see templates dap_appl_dala_config repo). user wants deploy files configruation tables, invoke powershell function *set-ingestionmetadatafromjsonfile* dataconifg module. module validate schema correct, ingest relevant metadata correct configuration table. ## folder structure understanding structure folders already help long way understanding overall ingestion process. gives first look actual code allows get familiar thinking process team. ### configuration idea configuration folder closely linked deployment different azure services. is, deployment azure service using arm template, parameters might need overwritten. keys values parameters stored configuration folder. example used dealing deployment synapse workspace using triggers. development environment, need triggers activated whilst need activated production environment. another example key vault, depending environment, different passwords need used key example: deployment synapse workspace, linked services depend upon different services depending environment. development environment, example, sql database containing metadata **dev**-dap-sqldb-core-meta whilst environment database **int**-dap-sqldb-core-meta. make sure right links used per environment, configuration file configuration/synapse/parameters.json developed. ### development development folder help locally develop repo artefacts. important **myenv-template.ps1** script. name suggests, template need copy paste folder. rename copy **myenv.ps1** fill parameters depending local environment. parameters defaults, pointing earlier discussed development environment. **note:** important follow naming conventions mentioned here. myenv.ps1 script part .gitignore, prevents file local configurations part main branch pushing changes. use parameters becomes important using scripts **localdeploy** folder. scripts folder meant mimic devops ci pipelines (see folder **pipelines**). ci pipelines, core, execute set scripts specific order. scripts executed localdeploy-pipeline, mimic ci pipeline well possible. conceptually, boils fact one needs first make sure developed scripts work locally adding devops pipeline. localdeploy-pipelines allowed used development environment, use environment parameters defined **myenv.ps1** script. **note:** always possible completely match devops ci pipelines. synapse, example, really straight-forward way build deploy artefacts existing json files repeated ci pipelines. ci pipeline, specific task allows combine synapse architecture files arm template, task executed locally. ### pipelines pipelines folder contains scripts executed ci cd pipelines [4] azure devops. first, orchestration pipeline jobs done pipeline-ci-&lt;artefact&gt; yaml script. scripts backbone pipeline, setting initial configuration parameters, invoking correct pipeline jobs correct variables. scripts jobs found **templates/jobs** folder. scripts created using following orchestration: 1. build job: build job creates two pipeline artefacts contains scripts files necessary successfully deploy test artefact. one pipeline artefact (main artefact) contains scripts necessary deploy source code, another (test artefact) contains scripts necessary test implementation source code. 2. deploy job: deploy job deploys source code contained main artefact correct workspace/service. also artefact later deployed other, higher-level environments (tst, acc, prd). 3. test job: test job deploys test artefact correct workspace/service. often source code relates (unit) testing implementation environment, needed higher-level environments. pipeline-ci-&lt;artefact&gt; scripts, deploy test jobs separated per environment, require different variables depending environment deployment needs happen. variables stored **variables** folder, environment dedicated script. **templates/steps** folder, set reusable tasks found. tasks executed different pipelines act "functions". **scripts** folder, set powershell scripts found. scripts called upon devops pipelines (and localdeploy pipelines), specific purpose ci process. scripts separated per artefact, **general** folder scripts called upon regardless artefact [5]. [4]: time writing (09/02/2024), cd pipelines still need developed. [5]: time writing (09/02/2024), still redundant scripts scripts folder ### src/source src folder, also known source folder, contains source code repo artefacts. place new developments happen regarding architecture ingetion process. time writing (09/02/2024), artifacts are: * powershell: contains self-made modules powershell. module folder structure, main code stored **private** **public** folder. folders contain set functions build module. function also expected tests.ps1 version tests inner workings function. tests need additional files, stored **.test_config** folder. next, module .psm1 .psd1 file describe model data. * sql: contains sql metadata project, test project. artefacts (tables, stored procedures, views...) belong sql metadata project stored **metadb** folder. artefacts needed test metadata project stored **metadb_test** folder. one folders .sqlproj file deployed sql server instance, deploy artefacts contained project. **_build/db_build.ps1** script build .sqlproj files using dotnet. **_build/db_deploy.ps1** deploys file designated sql server instance sql database. debugging purposes, script also allows deploy project script. * synapse: synapse workspace development environment git-integrated, source code workspace found **studio** folder. recommended would use synpase ui development needs done, possible make changes directly jsons well. source code will, ci/cd process molded arm template deployed desired environment. studio folder also contains json file **template-parameter-definition.json** great importance deployment process. indicates values arm template parameterized deployment. example, underlying linked services need changed depending environment. is, working development environment, linked service needed points towards development sql server. deploying arm template integration environment, linked service point integration sql server. **_test** folder, different test files unittesting stored. **note:** discussed another markdown file, depending naming conventions files, part main artefact, test artefact artefact all. possibility organize folder like sql folder, clearly separating main test artefact. therefore, ci pipeline build artefacts based naming conventions files. ### tests folder contains files scripts needed testing. **metadata** folder contains configuration files need deployed sql meta db. files contain metadata needed successfully run unit tests. is, test focused testing interaction different functions, pipelines, stored procedures, etc. mainly aimed towards synapse workspace, metadata oriented. means constant communication sql meta db expects metadata tables. test, example, whether file successfully moved landing container silver container using ingestion worker, metadata needed orchestrate worker. instead write bunch *insert into* statements tests, team chosen use already validated dataconfig module insert metadata already validated sql meta db. **test_scripts** folders contains scripts validate whether tests executed successfully. folder separated sql synapse folder, since services require different credentials access objects. basic sql test validates whether plans, tasks, files successfully logged. synapse tests validate actual contents delta table creates/updated/deleted... ### wiki wiki folder contains documentation regarding repo, including visual flows parts code impact one another.</file></source>