<source type="local_directory" path="C:\Users\simon.plancke\OneDrive - Keyrus\Documents\repositories\dap_appl_dala_code">
<file name="README.md">
# dap_appl_dala_code
## Description
The repo contains the code related to the underlying architecture of the ingestion process. This relates to SQL tables and stored procedures, but also to Synapse pipelines and Spark Notebooks. The two most important things here, are the [notebooks](synapse/notebooks/) and [SQL-code](sql/). If you would like a clearer overview to one of the tickets, user stories and other related items to the new ingestion process, kindly refer to the [dap_appl_dala_config repo](https://dev.azure.com/lnlbe/Data_Platform/_git/dap_appl_dala_config). There you can find more configuration related documents as well as ongoing projects.

## Synapse
Synapse is short for Azure Synapase Workspace. Here you will find the code related to the workspace, such as notebooks and pipelines.

### Notebooks
The notebooks in the repo are structured to stimulate object-oriented programming. 

#### Functions
The functions include methods that are used through all the notebooks, to group several actions together under a specific method. In here, you can find functions to clean containers, as well as functions that aid in the ingestion process. Lastly, there's also a unit test notebook present to check if the functions are still functioning properly.

#### Modules
Modules contains the core notebooks to make sure the ingestion process works as intended. It leverages methods from the 'Functions' folders as well as its own logic. In here, you are also able to find unit test to check if the Classes.ipynb is still working properly.

#### ScrapNotebooks
These notebooks are used to test individual parts of code, but are not used in the ingestion process.

#### Workers
The workers are the executors of the core logic regarding the ingestion process. It leverages the logic from Classes.ipynb, and uses this to make sure the ingestion process runs as intended.

## SQL
SQL is the umbrella that contains all the SQL-logic regarding the new ingestion process. You can find the general structures of the SQL-database LNL_META here, which contains the ingestion configuration &amp; logging tables, as well as the stored procedures.

### SQL-databases
The SQL-databases provide tables which are used to gather the data to start a Synapase Pipeline Run.

#### Configuration tables
| Name                              | Purpose                                                                                                                       |
| ---                               | ---                                                                                                                           |
| meta.plan_configuration           | This contains all the available plans that can be run in a pipeline. You give the plans a description and can (dis/en)able them.   |
| meta.plan_task_configuration      | This contains the combination of plans and tasks. It contains an overview of which tasks will be executed when a plan is initiated, divided into task groups.      |
| meta.source_column_configuration  | This contains all the available headers of the different configured datasets. Here you look at the original name, sink name, order, dimension and datatype. |
| meta.source_configuration         | This contains all the metadata needed to describe a file and successfully ingest it into the system. |
| meta.task_configuration           | This contains information about the expected source and sync location of a data source, no data about the file structure.             |

#### Logging tables
| Name                  | Purpose   |
| ---                   | ---       |
| meta.log_files        | This contains all the files that have been moved or copied at least once, and displays the locations of the copies / movements of the file. |
| meta.log_plan_tasks   | This contains a log of all the tasks that need to be or have been executed for a certain logged plan. |
| meta.log_plans        | This contains how long a run of a plan has lasted, and its status. Last_Pipeline_ID refers to the latest ID of the Synapse Pipeline that tried running the plan. | 
| meta.log_run_plans        | This contains the plans that have been run, and for which pipeline that was. |
| meta.log_runs         | This contains how long a full pipeline run has lasted, and if it was failed or not. This includes the start-up time as well as the run plan(s). |
| meta.log_tasks        | This contains the logs of the tasks that need to be or have been executed in a certain logged plan. You can check their status here and see which are succeeded and which are failed. |


#### Stored procedures
A stored procedure is like a function in SQL. It is mainly used to add, update, and/or retrieve data from existing SQL tables. For example, when a new Synapse pipeline run is started, a stored procedure will be called which will add new rows to the logging tables. When the run is completed, a stored procedure will update the current status of the run. If you want to know more about the different tests, check the [stored procedures](https://dev.azure.com/lnlbe/Data_Platform/_git/dap_appl_dala_code?path=/sql/metadb/stored_procedures) in the repo.

| Name                      | Purpose                                           | Tests         |
| ---                       | ---                                               | ---           |
| meta.usp_end_plan             | End plan in meta.log_plans that was completed (FAILED / SUCCEEDED)  | Fail on bad parameters         |
| meta.usp_end_run              | End run in meta.log_runs that was completed (FAILED / SUCCEEDED)  | Fail on bad parameters 
| meta.usp_end_task             | End task in meta.log_tasks that was completed (FAILED / SUCCEEDED)|Fail on bad parameters |
| meta.usp_get_plan_metadata    | Get the metadata from the configured plan | Fail on bad parameters  |
| meta.usp_get_plans            | Collect all the plans that are: PENDING / FAILED &amp; Have the same plan_name as the activated plan | Fail on bad parameters |
| meta.usp_get_task_metadata    | Get the metadata from the task that you're trying to execute | Fail on bad parameters |
| meta.usp_get_tasks            | Get tasks linked to a plan | Fail on bad parameters |
| meta.usp_new_file             | Log a new file into the meta.log_files table | Fail on bad parameters |
| meta.usp_new_run              | Log a new run into logging tables &amp; When New_Plan = 1: Add new plans and tasks to logging tables | Fail on bad parameters |
| meta.usp_start_plan           | Set the status of a plan to "IN PROGRESS" in meta.log_plans | Fail on bad parameters |
| meta.usp_start_run            | Set the status of a run to "IN PROGRESS" in meta.log_runs | Fail on bad parameters |
| meta.usp_start_task           | Set the status of a task to "IN PROGRESS" in meta.log_tasks | Fail on bad parameters |
| meta.usp_update_file_activity | Update the metadata of a file record (If the file is moving from A to B, this new location should be stored in meta.log_files) | Fail on bad parameters |
</file>
<file name="Technical_debt.md">
Welcome to Simon's brain

TO DO
    * ParameterTemplate: Check how to generate different linked services and managed endpoints depending on Synapse Target WS
        - Synapse Workspace Deployment task source code: https://github.com/Azure/Synapse-workspace-deployment/tree/master
        - Community question on Managed private endpoints: https://github.com/Azure/Synapse-workspace-deployment/issues/73
        - Guide: CI/CD w custom parameters: https://techcommunity.microsoft.com/t5/fasttrack-for-azure/azure-synapse-analytics-ci-cd-with-custom-parameters-made-easy/ba-p/3802517
        - Microsoft CI/CD page for Synapse: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/continuous-integration-delivery#create-custom-parameters-in-the-workspace-template

        =&gt; Target: Enable 'DeleteArtifactsNotInTemplate' when running the task 

    * More extensive error-message in xml files (check_test_files.ps1)
    * User guide of what to do when a new check is introduced into the system
        - Add check to all_checks dict, add parameters to allow_parameters list, etc.
    * Decimal is currently converted to int, Change this so decimal gets stored properly
        - unittest_spark_checks shows this -&gt; price column has decimal values, but it gets stored as an int
    * Add advanced tSQLt-tests for stored procedures
        - Basics tests are failed with bad params, but there needs to be added more
    * Add more extensive unit tests for tSQLt
        -&gt; deploy.usp_set_task_configuration has set_up file, is not used (keep it and save it for future tests)
        -&gt; deploy.deploy.usp_set_source_configuration has set_up file, is not used (keep it and save it for future tests)
        -&gt; deploy.usp_set_source_column_configuration has set_up file, is not used (keep it and save it for future tests)
        -&gt; deploy.set_source_check_config has set_up file, is not used (keep it and save it for future tests)
        -&gt; deploy.set_plan_task_config has set_up file, is not used (keep it and save it for future tests)
        -&gt; deploy.set_plan_config has set_up file, is not used (keep it and save it for future tests)
    * meta.source_configuration.sql, has the [header] column set to nvarchar, where in deploy.usp_set_source_configuration it's bit
        -&gt; Needs to be changed to NVARCHAR in deploy... -&gt; consistency!
        -&gt; If that doesn't work, we can change it to bit inside meta...
        -&gt; Option 1: Changing it to NVARCHAR will break some logic inside the data config module, is it possible anyways?
        -&gt; Option 2: Changing to bit will break some logic in the notebooks, so needs to be updated as well then (Now it's extracted as a string, not as a bit / int / bool, because it's saved as nvarchar. It will be checked as a string in the code 'if header == "True"')
        -&gt; Is option 1 (changing to NVARCHAR) easier to do, or option 2 (changing to bit)?
    * Delta table deployment is currently only allowed for IngestionWorker. This needs to be changed in the future as well.
        -&gt; See Dev-note in src/powershell/modules/DataConfig/public/Set-IngestionMetadataFromJsonFile.ps1 under DeployDeltaTables switch





Improvements:
* Write Python file with test result to file, not to folder with _SUCCESS file
* Rewrite check_test_logs.ps1 -&gt; Possible to make this script more clean
    -&gt; Already do some preprocessing in Python? 
    -&gt; Less steps? Necessary to use XML?
    -&gt; ...
* Rewrite try / except functions according to best practices. No general try / except, but except Exception: (https://python.land/deep-dives/python-try-except)
* Add sleep to invoke_notebook.ps1 inside of logs (while $NotFinished)
* Create a script / function which closes all spark pools when everything is done
* Ingestion.source_folder currently looks recursively through the folder that's been provided (you don't need to put /** after it)
    This should be fixed in the future (see ingestionworker_csv_recurse_search)
* Upload test scripts to a certain folder and check if the folder is empty/contains the expected files after running the pipeline
* Update logic of SQLServerInstance, because it currently gets overwritten inside of localdeploy_synapse.
    The name gets used multiple times, which causes a 'bug' where the variable in the parent script suddenly has the value passed / changed inside of the child script
* Handling of config_params inside of checks: currently this is a string, are we really going to change this to an array with flat json objects / dictionaries?
* localbuild-powershell-module: If unit tests fail, the module still gets deployed locally. This should not be the case.
* Create generic Install-Module step for DevOps CI/CD (DataConfig, Networking, TokenModule)
* Recreate default managed private endpoints for dev Synapse Workspace -&gt; Still refers to sbx
* Replace linked service parameters for environment changes (dev, int, tst, acc, prd) using tokenization instead of overwrite (if possible)
* Make notebook-tokenization for spark configuration more dynamic -&gt; Current code will give all the notebooks the same spark configuration (core_configuration) (tokenize_synapse_notebooks.ps1 and tokenization/detokenize_synapse_arm.ps1)
* Allow 'True' for the 'hasHeaders' parameter (class: CalculatedFieldHeaders, inside Dataframe module)
* Use parameters for the start_index &amp; end_index (class: CalculatedFieldHeaders, when called inside FilWorker). These are now always 1 &amp; 3 respectively.
* Filter on the TASK_GROUP when using the queries in pipelines/scripts/sql/prepare_logging_tables.ps1
    Filter on task_group, as this is needed for when you're unit testing a plan with different task groups 
    (e.g. "PREPROCESS" &amp; "INGEST" combined in one plan -&gt; this script will return all those tasks and probably mix them)
* Check if possible to rewrite synapse tests: do not use OPENROWSET, but just read directly from silver table
* Trabuf issues: nvarchar(8000) for json-column cannot handle the full json when printing -&gt; Not sure if this is an issue when writing to table or only when using 'SELECT' statement


Clean up:
* validate_ingestionworker: Script is not finalised and could be improved based on a quick glance.
* run_notebook: Delete script and replace references by invoke_notebook
* Replace sandbox references in managed private endpoint tokenization script (tokenize_managed_private_endpoints.ps1)
* Overwrite TokenModule reference: Is called from test artefact feed -&gt; Call from main 
* Add documentation to the detokenize script for DevOps: Make sure that DevOps users know what has been done 
* saveDataframe() method inside CalculatedFieldHeaders class, inside Dataframes module -&gt; put inside IngestionFunctions as a seperate function
    - This could be handy in the future for other cases

R&amp;D:
* Usage of "import warnings" inside of PySpark notebooks. Is it necessary?
-- https://stackoverflow.com/questions/3891804/raise-warning-in-python-without-interrupting-program
</file>
<file name=".vscode\extensions.json">
{
    "recommendations": [
        "ms-azure-devops.azure-pipelines",
        "ms-mssql.mssql"
    ]
}
</file>
<file name=".vscode\settings.json">
{
    "files.associations": {
        "**/pipelines/**/*.yaml": "azure-pipelines",
        "**/pipelines/**/*.yml": "azure-pipelines"
    },
    "powershell.pester.debugOutputVerbosity": "Detailed",
    "powershell.pester.outputVerbosity": "Detailed",
    "powershell.pester.useLegacyCodeLens": false
}
</file>
<file name="configuration\synapse\parameters.json">
{
    "default": {       
        "workspaceName": "#{target_environment}#-dap-syn-core",
        "ls_dap_sql_meta_connectionString": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=#{target_environment}#-dap-sql-core.database.windows.net;Initial Catalog=#{target_environment}#-dap-sqldb-core-meta",
        "ls_dap_adls_01_properties_typeProperties_url": "https://#{target_environment}#dapstdala1.dfs.core.windows.net/",
        "ls_dap_kv_core_properties_typeProperties_baseUrl": "https://#{target_environment}#-dap-key-core-syn.vault.azure.net/",
        "(.*)_properties_bigDataPool_referenceName": "#{target_environment}#dapsspcore",
        "pl_(.*)_properties_variables_ENV_CODE_defaultValue": "#{target_environment}#",
        "pl_(.*)_properties_variables_spark_pool_defaultValue": "#{target_environment}#dapsspcore",
        "(.*)_properties_sessionProperties_runAsWorkspaceSystemIdentity": "True",
        "core_configuration_properties_configs_environment_code": "#{target_environment}#",
        "#{target_environment}#-dap-syn-core-WorkspaceDefaultSqlServer_connectionString": "Data Source=tcp:#{target_environment}#-dap-syn-core.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}",
        "#{target_environment}#-dap-syn-core-WorkspaceDefaultStorage_properties_typeProperties_url": "https://#{target_environment}#dapstcoresyn.dfs.core.windows.net/"
    },
    "dev": {
        "workspaceName": "dev-dap-syn-core"
    }
}
</file>
<file name="configuration\synapse\synapse_workspace_tokens.json">
{
    "managed_private_endpoints": [
       { 
            "synapse-ws-dapstdala1": {
                "resource_group": "#{target_environment}#-dap-rg-dala",
                "storage_account": "#{target_environment}#dapstdala1"
            }
        },
        {
            "synapse-ws-dap-sql_core": {
                "server": "#{target_environment}#-dap-sql-core"
            }
        },
        {
            "synapse-ws-dap-key-core-syn": {
                "vault": "#{target_environment}#-dap-key-core-syn",
                "fqdns": "#{target_environment}#-dap-key-core-syn.vault.azure.net"
            }
        }
    ],

    "notebooks": [
        {
            "default": {
                "targetSparkConfiguration": "core_configuration"
            }
        }
    ]
}
</file>
<file name="development\helper_scripts\delete_git_branches.ps1">

# preview branches to be pruned
# git remote prune origin --dry-run
# or real execution
# git remote prune origin --dry-run

# show branches status, shows "gone" if deleted on remote
# git branch -vv

# list only the ones showing "gone"
# git branch -vv | Select-String -pattern "(.*) ....... \[.*: gone\]"

# remove the local branches that are not on the remote any more 
git remote prune origin 
# show list of "gone" branches to be deleted locally
[array] $branchesToDeleteLocally = git branch -vv | Select-String -pattern "(.*) ........ \[.*: gone\]" | Foreach-Object {$_.Matches} | Foreach-Object {$_.Groups[1].Value.Trim()}


if ( $branchesToDeleteLocally.Count -gt 0 ) {

    Write-Host ""
    Write-Host "List of branches to be deleted"
    Write-Host "------------------------------"

    $branchesToDeleteLocally

    # confirm
    $Continue = Read-Host -Prompt "Press Y to continue"
    if ($Continue -ne "Y") {
        Write-Output "Cancelling"
        Exit 0
    }

    # delete them (-D to avoid warning/abort warnings, caused by squash merges)
    $branchesToDeleteLocally | git branch -D $branchesToDeleteLocally
}
else {
    Write-Host "No branches to be deleted."
}

</file>
<file name="development\helper_scripts\install-modules.ps1">
&lt;#

.SYNOPSIS
Install the main modules

.DESCRIPTION
The script installs the following modules
    - SQLServer
    - Az.Accounts
    - Az.KeyVault
    - Az.Resources
    - Az.Storage
    - Az.Synapse
    - AzureAD
    - DataConfig (self-made module contained in the repository)

The script checks whether there is already an AD account connected, and requests to connect if this is not the case

.PARAMETER WorkspaceFolder
    Optionally provide the path to the workspace folder. 
    If not provided, the value in $env:REPOROOT is used.
#&gt;

[CmdletBinding()]

param(
    [String] $WorkspaceFolder = $env:REPOROOT #Run myenv_xx.ps1 script first 
)

Write-Output "Check if all necessary modules are installed"

# Install SqlServer module if needed
if ( -not (Get-module "SqlServer" -ListAvailable) ) {
    Write-Information "Installing module: SqlServer ..."
    Install-Module -Name "SqlServer" -scope CurrentUser -Force
}

# Install Az.Accounts module if needed
if ( -not (Get-module "Az.Accounts" -ListAvailable) ) {
    Write-Information "Installing module: Az.Accounts ..."
    Install-Module -Name "Az.Accounts" -scope CurrentUser -Force
}

# Install Az.Keyvault module if needed
if ( -not (Get-module "Az.Keyvault" -ListAvailable) ) {
    Write-Information "Installing module: Az.Keyvault ..."
    Install-Module -Name "Az.Keyvault" -scope CurrentUser -Force
}

# Install Az.Resources module if needed
if ( -not (Get-module "Az.Resources" -ListAvailable) ) {
    Write-Information "Installing module: Az.Resources ..."
    Install-Module -Name "Az.Resources" -scope CurrentUser -Force
}

# Install Az.Storage module if needed
if ( -not (Get-module "Az.Storage" -ListAvailable) ) {
    Write-Information "Installing module: Az.Storage ..."
    Install-Module -Name "Az.Storage" -scope CurrentUser -Force
}

# Install Az.Synapse module if needed
if ( -not (Get-module "Az.Synapse" -ListAvailable) ) {
    Write-Information "Installing module: Az.Synapse ..."
    Install-Module -Name "Az.Synapse" -scope CurrentUser -Force
}

# Install AzureAD module if needed
if ( -not (Get-module "AzureAD" -ListAvailable) ) {
    Write-Information "Installing module: AzureAD ..."
    Install-Module -Name "AzureAD" -scope CurrentUser -Force
}

# Install Az.Sql module if needed
if ( -not (Get-module "Az.Sql" -ListAvailable) ) {
    Write-Information "Installing module: Az.Sql ..."
    Install-Module -Name "Az.Sql" -scope CurrentUser -Force
}

# Install DataConfig module if needed
if ( -not (Get-module "DataConfig" -ListAvailable) ) {
    Write-Information "Installing NaLo module: DataConfig ..."
    import-module $env:REPOROOT\src\powershell\modules\DataConfig\DataConfig.psm1 -force
}

</file>
<file name="development\helper_scripts\startup.ps1">

# Check if connected to Azure
# If not: Make connection
Write-Output "Check Azure AD connection..."
$context = Get-AzContext
if ($context){
    Write-Output "  already connected to Azure"
} else {
    Connect-AzAccount -Subscription "lnl-006"
} 

</file>
<file name="development\helper_scripts\uninstall-old-az-modules.ps1">
&lt;#
.SYNOPSIS
    Removes all older az.* modules except the latest (installed) version

.DESCRIPTION
    Tip:The command support -Whatif to review the impact first
    
#&gt;
[CmdletBinding(SupportsShouldProcess)]
param()

#local settings
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"
Set-StrictMode -Version "latest"

foreach ($module in (Get-Module -ListAvailable Az*).Name |Get-Unique) {
    $modules = [Array]((Get-Module -ListAvailable $module) ?? @())
    if ($modules.Count -gt 1) {
        $latest_Version = ((Get-Module -ListAvailable $module | Select-Object Version | Sort-Object Version)[-1]).ToString()
        Write-Output "${module}: latest version is ${latest_Version}"
        Get-Module -ListAvailable $module | 
            Where-Object {$_.Version -ne $Latest_Version.Version} | 
            ForEach-Object {            
                write-output "  Uninstalling: $($_.Name) version $($_.Version) ..."
                Uninstall-Module -Name $_.Name -RequiredVersion $_.Version -ErrorVariable "errorVar" -ErrorAction "silentlyContinue" #-verbose
                if ( $errorVar) {
                    if ( $errorVar | Where-Object { $_.Exception.Message -like "*No match was found for the specified search criteria and module names*"} ) {
                        # known problem with modules on onedrive
                        $modulePath = $_.Path | Split-Path
                        Write-Output "  Uninstall failed, probably known issue with OneDrive"
                        Write-Output "    Try to delete module folder: $modulePath ..."
                        Remove-Item -Path $modulePath -Recurse -Force -Confirm:$false
                    } else {
                        Write-Output "  Unexpected error(s) during uninstall: "
                        $errorVar | ForEach-Object { Write-Error "    $($_)"}
                    }
                }
            }
    }
    else {
        Write-Output "${module}: only one version installed $($modules[0].version.ToString())"
    }
}
</file>
<file name="development\local-env\myenv-template.ps1">
&lt;#

Template for local developer settings. 
Copy this file as "myenv.ps1" in the /development/local-env folder and update the below variables to your personal settings.
The myenv.ps1 file is already foreseen in .gitignore so will not be stored in the remote git repository.
#&gt;

Write-Output "$($ENV:USERNAME) environment"
Write-Output "Found powershell version $($PSVersionTable.PSVersion), edition $($PSVersionTable.PSEdition)"


# the user for logging in to azure
$env:USERPRINCIPALNAME = "&lt;firstname&gt;.&lt;lastname&gt;[.ext]@nationale-loterij.be"

# the folder where your git repo is cloned
$env:REPOROOT = "C:\my\path\to\dap_appl_dala_code"

# your sandbox (the environment part in the resources and resourcgroups)
$env:MYDEVENV = "dev"

# full path to sqlpackage.exe (if available in path, can leave as "sqlpackage.exe")
$env:SQLPACKAGEPATH = "sqlpackage.exe"

# location of the dev database
$env:SQL_SERVER = "dev-dap-sql-core.database.windows.net"
$env:SQL_DATABASE = "dev-dap-sqldb-core-meta"


# Parameters for the Synapse Workspace
$env:Synapse_WS = 'dev-dap-syn-core'
$env:Synapse_Server = 'dev-dap-syn-core-ondemand.sql.azuresynapse.net'
$env:Synapse_Database = 'default'

# Check if all modules are installed
. $env:REPOROOT\development\helper_scripts\install-modules.ps1

# other common startup commands
. $env:REPOROOT\development\helper_scripts\startup.ps1

</file>
<file name="development\localdeploy\localbuild_dataconfig_module.ps1">
&lt;#

.SYNOPSIS
    A script to locally test and build the DataConfig powershell module and generate the nuget package.

.DESCRIPTION
    1. runs the unit tests
    2. makes a local copy of the module folder (because the build process modifies the files, changes we don't want to commit)
    3. on the local copy: run the build script to prepare the .psd1 file
    4. packages the module into a nuget package
    
    Note: requires nuget.exe available in the path. 
    * download nuget.exe (latest: 6.8.0) from https://www.nuget.org/downloads
    * make sure it's added to the path

.NOTES
    The script will use the following environment variables if parameters are not provided:
    * $env:REPOROOT

#&gt;
[CmdletBinding()]
param (

    [Parameter(Mandatory = $false, HelpMessage = 'The folder where the repo is cloned, leave empty to use $ENV:REPOROOT') ]
    [String] $WorkspaceFolder = $null ,          # The local folder where the repo is cloned       

    # optional switches to skip steps (useful when running consecutively without real changes)
    [Switch] $SkipUnitTests 
)

# =================
# CONFIG
# =================

# force error action
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"

# check that local environment is configured
if ( -Not $env:REPOROOT) {
    #throw "Environment variable REPOROOT is not set, run local-env script first "
    . "$PSScriptRoot/../local-env/myenv.ps1"
}
if (-not $WorkspaceFolder) {
    $WorkspaceFolder = $env:REPOROOT
}

$modulesFolder = Join-Path $WorkspaceFolder "src" "powershell" "modules"
if ( -not (Test-Path -Path $modulesfolder -PathType "Container")) {
    throw "Could not fine modulesFolder: $modulesFolder"
}
$moduleName = "DataConfig"
Write-Output "using modulesFolder: $modulesFolder"
Write-Output "using moduleName   : $moduleName"

# Run unit tests
if (-Not $SkipUnitTests){

    $testScriptPath = Join-Path $ModulesFolder $moduleName ".build" "unit_tests.ps1"
    if( -not (Test-Path $testScriptPath) ) {
        throw "Could not locate script: $testScriptPath"
    }

    &amp; $testScriptPath

} else {
    Write-Output ""
    Write-Output "(...Skipping unit tests...)"
}

# make a copy of the module
Write-Output ""
Write-Output "Creating local copy of the module files..."
$origFolder = Join-Path $modulesFolder $moduleName
$localRoot = Join-Path $modulesFolder ".local" 
$localFolder = Join-Path $localRoot $moduleName
Write-Output "  using origFolder   : $origFolder"
Write-Output "  using localRoot    : $localRoot"
Write-Output "  using localFolder  : $localFolder"

# check that the ".local" folder exists
if ( -not ( test-path $localRoot )) {
    Write-Output "  creating localRoot: $localRoot"
    New-Item -Path $modulesFolder -Name ".local" -ItemType Directory
}

# remove if exists, and make a clean copy
if ( test-path $localFolder) {
    Write-Output "  removing local copy of the module"
    Remove-Item -path $localFolder -Recurse -Force
}
Copy-Item -Path $origFolder -Destination $localFolder -Recurse 

# build the (local copy of the) module
Write-Output ""
Write-Output "Run build script..."
$buildScriptPath = Join-Path $localFolder ".build" "build.ps1"
&amp; $buildScriptPath `
    -ModuleName "DataConfig" `
    -WorkingDir $localRoot `
    -BuildVersion "0.0.0" `
    -LocalFeedPath (Join-Path $localRoot "localFeed" ) `
    -CleanFolder

</file>
<file name="development\localdeploy\localbuild_networking_module.ps1">
&lt;#

.SYNOPSIS
    A script to locally test and build the Networking powershell module and generate the nuget package.

.DESCRIPTION
    1. runs the unit tests
    2. makes a local copy of the module folder (because the build process modifies the files, changes we don't want to commit)
    3. on the local copy: run the build script to prepare the .psd1 file
    4. packages the module into a nuget package
    
    Note: requires nuget.exe available in the path. 
    * download nuget.exe (latest: 6.8.0) from https://www.nuget.org/downloads
    * make sure it's added to the path

.NOTES
    The script will use the following environment variables if parameters are not provided:
    * $env:REPOROOT

#&gt;
[CmdletBinding()]
param (

    [String] $WorkspaceFolder = $env:REPOROOT,          # The local folder where the repo is cloned       

    # optional switches to skip steps (useful when running consecutively without real changes)
    [Switch] $SkipUnitTests 
)

# =================
# CONFIG
# =================

# force error action
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"

$modulesFolder = Join-Path $WorkspaceFolder "src" "powershell" "modules"
if ( -not (Test-Path -Path $modulesfolder -PathType "Container")) {
    throw "Could not fine modulesFolder: $modulesFolder"
}
$moduleName = "Networking"
Write-Output "using modulesFolder: $modulesFolder"
Write-Output "using moduleName   : $moduleName"

# Run unit tests
if (-Not $SkipUnitTests){

    $testScriptPath = Join-Path $ModulesFolder $moduleName ".build" "unit_tests.ps1"
    if( -not (Test-Path $testScriptPath) ) {
        throw "Could not locate script: $testScriptPath"
    }

    &amp; $testScriptPath

} else {
    Write-Output ""
    Write-Output "(...Skipping unit tests...)"
}

# make a copy of the module
Write-Output ""
Write-Output "Creating local copy of the module files..."
$origFolder = Join-Path $modulesFolder $moduleName
$localRoot = Join-Path $modulesFolder ".local" 
$localFolder = Join-Path $localRoot $moduleName
Write-Output "  using origFolder   : $origFolder"
Write-Output "  using localRoot    : $localRoot"
Write-Output "  using localFolder  : $localFolder"

# check that the ".local" folder exists
if ( -not ( test-path $localRoot )) {
    Write-Output "  creating localRoot: $localRoot"
    New-Item -Path $modulesFolder -Name ".local" -ItemType Directory
}

# remove if exists, and make a clean copy
if ( test-path $localFolder) {
    Write-Output "  removing local copy of the module"
    Remove-Item -path $localFolder -Recurse
}
Copy-Item -Path $origFolder -Destination $localFolder -Recurse 

# build the (local copy of the) module
Write-Output ""
Write-Output "Run build script..."
$buildScriptPath = Join-Path $localFolder ".build" "build.ps1"
&amp; $buildScriptPath `
    -ModuleName "Networking" `
    -WorkingDir $localRoot `
    -BuildVersion "0.0.0" `
    -LocalFeedPath (Join-Path $localRoot "localFeed" ) `
    -CleanFolder

</file>
<file name="development\localdeploy\localbuild_token_module.ps1">
&lt;#

.SYNOPSIS
    A script to locally test and build the TokenModule powershell module and generate the nuget package.

.DESCRIPTION
    1. runs the unit tests
    2. makes a local copy of the module folder (because the build process modifies the files, changes we don't want to commit)
    3. on the local copy: run the build script to prepare the .psd1 file
    4. packages the module into a nuget package
    
    Note: requires nuget.exe available in the path. 
    * download nuget.exe (latest: 6.8.0) from https://www.nuget.org/downloads
    * make sure it's added to the path

.NOTES
    The script will use the following environment variables if parameters are not provided:
    * $env:REPOROOT

#&gt;
[CmdletBinding()]
param (
    [Parameter( Mandatory = $false, HelpMessage="The local folder where the repo is cloned")]
    [string] $WorkspaceFolder = $env:REPOROOT,    

    [Parameter( Mandatory = $false, HelpMessage="optional switches to skip steps (useful when running consecutively without real changes)")]
    [switch] $SkipUnitTests 
)


# force error action
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"

# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables
$inputParameters = @( 
    "WorkspaceFolder"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


$modulesFolder = Join-Path $WorkspaceFolder "src" "powershell" "modules"
if ( -not (Test-Path -Path $modulesfolder -PathType "Container")) {
    throw "Could not fine modulesFolder: $modulesFolder"
}
$moduleName = "TokenModule"
Write-Output "using modulesFolder: $modulesFolder"
Write-Output "using moduleName   : $moduleName"

# Run unit tests
if (-Not $SkipUnitTests){

    $testScriptPath = Join-Path $ModulesFolder $moduleName ".build" "unit_tests.ps1"
    if( -not (Test-Path $testScriptPath) ) {
        throw "Could not locate script: $testScriptPath"
    }

    &amp; $testScriptPath

} else {
    Write-Output ""
    Write-Output "(...Skipping unit tests...)"
}

# make a copy of the module
Write-Output ""
Write-Output "Creating local copy of the module files..."
$origFolder = Join-Path $modulesFolder $moduleName
$localRoot = Join-Path $modulesFolder ".local" 
$localFolder = Join-Path $localRoot $moduleName
Write-Output "  using origFolder   : $origFolder"
Write-Output "  using localRoot    : $localRoot"
Write-Output "  using localFolder  : $localFolder"

# check that the ".local" folder exists
if ( -not ( test-path $localRoot )) {
    Write-Output "  creating localRoot: $localRoot"
    New-Item -Path $modulesFolder -Name ".local" -ItemType Directory
}

# remove if exists, and make a clean copy
if ( test-path $localFolder) {
    Write-Output "  removing local copy of the module"
    Remove-Item -path $localFolder -Recurse
}
Copy-Item -Path $origFolder -Destination $localFolder -Recurse 

# build the (local copy of the) module
Write-Output ""
Write-Output "Run build script..."
$buildScriptPath = Join-Path $localFolder ".build" "build.ps1"
&amp; $buildScriptPath `
    -ModuleName "TokenModule" `
    -WorkingDir $localRoot `
    -BuildVersion "0.0.0" `
    -LocalFeedPath (Join-Path $localRoot "localFeed" ) `
    -CleanFolder


# Write information for script user
Write-Output "*** Ending script: $scriptName ***"
</file>
<file name="development\localdeploy\localdeploy_sqldatabase.ps1">
&lt;#

.SYNOPSIS
    A script to locally validate development work on the metadata database 
    before issueing a PR (pull request).

.DESCRIPTION
    1. Builds and deploys both the database and the tests to the designated
       dev database
    2. Cleans the dev database for a "fresh" start
    2. Runs all tSQLt unit tests.
    3. Deploys the test configuration data
    4. &lt;TODO&gt; Runs integration tests

.NOTES
    The script will use the following environment variables if parameters are not provided:
    * $env:REPOROOT
    * $env:SQLMETA_SERVER
    * $env:SQLMETA_DB


#&gt;
[CmdletBinding()]
param (

    [String] $WorkspaceFolder = $env:REPOROOT,
    [String] $TargetServer = $env:SQL_SERVER,
    [String] $TargetDatabase = $env:SQL_DATABASE,
    [String] $AccessToken = ((Get-AzAccessToken -ResourceUrl https://database.windows.net -AsSecureString -WarningAction "Ignore").Token | ConvertFrom-SecureString -AsPlainText),

    # optional switches to skip steps
    [String] $OutputScriptPath = $null, # provide a folder path to create dacpac deploy scripts 
    [Switch] $SkipBuild,
    [Switch] $SkipDeploy,
    [Switch] $SkipUnitTests

)

# needs the SqlServer module to be installed
#requires -Modules SqlServer

# force error action
$ErrorActionPreference = "Stop"

# path to mssql
$sqlFolder = Join-Path ${WorkspaceFolder} "src" "sql"

# build both projects
Write-Output "------------------------------"
Write-Output "Build main project ..."
Write-Output "------------------------------"
if( $SkipBuild) {
    Write-Output ""
    Write-Output "&lt;skipping build as requested&gt;"
} else {
    . $sqlFolder/_build/db_build.ps1 -ProjectPath "$sqlFolder/metadb/metadb.sqlproj" -ErrorAction "Stop"
}    

Write-Output ""
Write-Output "------------------------------"
Write-Output "Build test project ..."
Write-Output "------------------------------"
if( $SkipBuild) {
    Write-Output ""
    Write-Output "&lt;skipping build as requested&gt;"
} else {
    . $sqlFolder/_build/db_build.ps1 -ProjectPath "$sqlFolder/metadb_test/metadb_test.sqlproj"
    $BuildResult = $LASTEXITCODE
    if ( $BuildResult -ne 0 ) {
        Write-Error "Build not succesful (exit code: $BuildResult), aborting..."
        return
    }
}    


# clean the database
# TODO (there is a test.usp_reset_database but this will not clear config tables, and requires the project to be deployed first ...)

if ( $OutputScriptPath -Or (-Not $SkipDeploy) ) {

    Write-Output ""
    Write-Output "------------------------------"
    Write-Output "Connecting to SQL Server ..."
    Write-Output "------------------------------"

    # test connection to SQL SErver
    $params = @{
        ServerInstance = $TargetServer
        Database = $TargetDatabase
        AccessToken = $AccessToken
    }
    $rows = Invoke-SqlCmd @params -Query "select 'Connected to database: ' + db_name() as message;"  
    $rows.message


    # default parameters for sqlpackage
    $deployParams = @{
        SqlPackageExePath = $env:SQLPACKAGEPATH
        PublishProfilePath = (Join-Path $sqlFolder "/_build/generic.publish.xml")
        TargetServer = $TargetServer
        TargetDatabase = $TargetDatabase
        AccessToken = $AccessToken
    }

    Write-Output ""
    Write-Output "------------------------------"
    Write-Output "Main project ..."
    Write-Output "------------------------------"
    $DacpacName = "metadb"
    $DacpacPath = Join-Path $sqlFolder $DacpacName "bin" "Debug" "$DacpacName.dacpac"
    Write-Output $DacpacPath
    Write-Output $sqlFolder
    if ( $OutputScriptPath ) {
        Write-Output "Creating $DacpacName deploy script in $OutputScriptPath ..."
        $outputFilePath = Join-Path $OutputScriptPath "$DacpacName.sql"
        &amp; $sqlFolder/_build/db_deploy.ps1 @deployParams -DacpacPath $DacpacPath -Action "Script" -OutputScriptPath $outputFilePath
    }
    if ($SkipDeploy) {
        Write-Output ""
        Write-Output "&lt;skipping deploy as requested&gt;"
    } else {
        Write-Output "Deploying..."
        &amp; $sqlFolder/_build/db_deploy.ps1 @deployParams -DacpacPath $DacpacPath -Action "Publish"
    }

    Write-Output ""
    Write-Output "------------------------------"
    Write-Output "Deploy tSQLt project ..."
    Write-Output "------------------------------"
    if ( $SkipDeploy) {
        Write-Output ""
        Write-Output "&lt;skipping deploy as requested&gt;"
    } else {
        $DacpacName = "metadb_test"
        $DacpacPath = Join-Path $sqlFolder $DacpacName "dependencies" "tSQLt_AzureV12_v1.0.5873.27393.dacpac"
        Write-Output $DacpacPath
        Write-Output $sqlFolder
        &amp; $sqlFolder/_build/db_deploy.ps1 @deployParams -DacpacPath $DacpacPath -Action "Publish"   
    }
    
    Write-Output ""
    Write-Output "------------------------------"
    Write-Output "test project ..."
    Write-Output "------------------------------"
    $DacpacName = "metadb_test"
    $DacpacPath = Join-Path $sqlFolder $DacpacName "bin" "Debug" "$DacpacName.dacpac"
    if ( $OutputScriptPath ) {
        Write-Output "Creating $DacpacName deploy script in $OutputScriptPath ..."
        $outputFilePath = Join-Path $OutputScriptPath "$DacpacName.sql"
        &amp; $sqlFolder/_build/db_deploy.ps1 @deployParams -DacpacPath $DacpacPath -Action "Script" -OutputScriptPath $outputFilePath
    }
    if ($SkipDeploy) {
        Write-Output ""
        Write-Output "&lt;skipping deploy as requested&gt;"
    } else {
        Write-Output "Deploying..."
        &amp; $sqlFolder/_build/db_deploy.ps1 @deployParams -DacpacPath $DacpacPath -Action "Publish"
    } 
}


# for now, run the reset proc?
# TODO

if ( -Not $SkipUnitTests) {


    # construct parameters for run_testclasses.ps1
    $testParams = @{
        TargetServer = $TargetServer
        TargetDatabase = $TargetDatabase
        AccessToken = $AccessToken
    }


    # run design rules &amp; unit tests
    Write-Output ""
    Write-Output "-----------------------------------"
    Write-Output "Run unit tests ..."
    Write-Output "-----------------------------------"
    . $sqlFolder/_test/run_testclasses.ps1 @testParams -TestSets "unit" -InformationAction "Continue" -Verbose

    # run design rules 
    Write-Output ""
    Write-Output "-----------------------------------"
    Write-Output "Run design rules tests ..."
    Write-Output "-----------------------------------"
    . $sqlFolder/_test/run_testclasses.ps1 @testParams -TestSets "design" -InformationAction "Continue" -Verbose

}

# run integration tests
# TODO there are none yet :)
# Write-Output ""
# Write-Output "-----------------------------------"
# Write-Output "Run integration ..."
# Write-Output "-----------------------------------"
# . $mssqlFolder/_test/run_testclasses.ps1 -TargetServer $TargetServer -TargetDatabase $TargetDatabase  -TestSets "integration"



</file>
<file name="development\localdeploy\localdeploy_synapse.ps1">
&lt;#

.SYNOPSIS
    Script to run all unit tests and worker tests for the Azure Synapse Workspace

.DESCRIPTION
   Using the existing 'Synapse Live' artefacts, this script will run all the tests for the Synapse workspace.
   This should include the TestNotebook of the Synapse environment, which will unit test all the PySpark code
   This also includes tests for all the workers (pipelines and notebooks) defined in the workspace

.NOTES
    The script will use the following environment variables if parameters are not provided:
    * $env:REPOROOT
    * $env:MYDEVENV


#&gt;
[CmdletBinding()]
param (

    ## General params
    [Parameter( Mandatory = $false, HelpMessage="The local folder where the repo is cloned")]
    [string] $WorkspaceFolder = $env:REPOROOT,
    
    [Parameter( Mandatory = $false, HelpMessage="The local environment from where the script is called")]
    [string] $TargetEnvironment = $env:MYDEVENV,
    
    [Parameter( Mandatory = $false, HelpMessage="Switch to skip running the tests on the Synapse pipelines")]
    [switch] $SkipSynapsePipelines,
    
    [Parameter( Mandatory = $false, HelpMessage="Switch to skip running the test on the Synapse Notebooks")]
    [switch] $SkipSynapseNotebooks
)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables
$inputParameters = @( 
    "WorkspaceFolder"
    "TargetEnvironment"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------

# Local deploy pipeline is only allowed to be run in the development environment
if ( $TargetEnvironment -notin ('dev')) {
    throw "Invalid TargetEnvironment '$TargetEnvironment'. Localdeploy is only allowed to run in development environment."
}

# The script requires the use of the DataConfig Powershell module (= module created by DAP team)
#Requires -Modules DataConfig


# If target environment is valid and the required modules are installed, set the script variables
## SQL variables
$sqlServerName = "$($TargetEnvironment)-dap-sql-core"
$sqlServerInstance = $env:SQL_SERVER
$sqlDatabase = $env:SQL_DATABASE
$accessToken = (Get-AzAccessToken -ResourceUrl https://database.windows.net -Debug:$false).Token

## Synapse variables
$storageAccount = "$($TargetEnvironment)dapstdala1"
$synapseServerInstance = $env:Synapse_SERVER
$synapseDatabase = $env:Synapse_Database


## Spark Session variables
$synapseWSName = $env:Synapse_WS
$sparkPoolName = "$($TargetEnvironment)dapsspcore"
$sparkSessionName = 'LocalDeploy'

$sparkParams = @{
    WorkspaceName = $synapseWSName
    PoolName = $sparkPoolName
    SessionName = $sparkSessionName
}

## General variables
$targetResourceGroup = "$($TargetEnvironment)-dap-rg-core"


# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------
##  General preparation
##      Before running all the tests, make sure a "reset" of the environment has happened
##      Clean the SQL logging tables and ADLS containers
##      Upload the test files to the ADLS containers
##      Deploy the test-metadata configuration to the SQL Meta DB and Silver container

## Make sure SQL Meta DB is online
. $WorkspaceFolder/pipelines/scripts/sql/resume_sqldb.ps1 `
    -ServerName $sqlServerName `
    -SQLServerInstance $sqlServerInstance `
    -SQLDatabase $sqlDatabase `
    -ResourceGroupName $targetResourceGroup


## Clean SQL Logging tables
. $WorkspaceFolder/pipelines/scripts/sql/clean_sql_tables.ps1 `
    -SQLServerInstance $sqlServerInstance `
    -SQLDatabase $sqlDatabase `
    -AccessToken $accessToken `
    -TargetEnvironment $TargetEnvironment


## Clean ADLS containers
. $WorkspaceFolder/pipelines/scripts/datalake/clean_all_containers.ps1 `
    -StorageAccount $storageAccount `
    -TargetEnvironment $TargetEnvironment


# ---------------------------------------------------------------------------------------------------------


. $WorkspaceFolder/pipelines/scripts/datalake/upload_to_container.ps1 `
    -StorageAccount $storageAccount `
    -ContainerName 'unittest' `
    -TestType 'unittest_files'

. $WorkspaceFolder/pipelines/scripts/datalake/upload_to_container.ps1 `
    -StorageAccount $storageAccount `
    -ContainerName 'landing' `
    -TestType 'fulltest_files'


. $WorkspaceFolder/pipelines/scripts/sql/upload_to_tables.ps1 `
    -SQLServerInstance $sqlServerInstance `
    -SQLDatabase $sqlDatabase `
    -AccessToken $accessToken `
    -FilterType 'all' `
    -FolderPath "$WorkspaceFolder/src/synapse/_test/metadata" `
    -DeltaTableConfigurationFile "src/synapse/_test/delta_tables/delta_tables.json"

# ---------------------------------------------------------------------------------------------------------

# If the tests for the Synapse pipelines need to be run
if (-not $SkipSynapsePipelines){

    # ---------------------------------------------------------------
    # Synapse pipeline test: pl_dummy_worker
    #
    #   Description: Invoke pipeline pl_start_run for an 'SYNAPSE_PIPELINE' task type, and 'pl_dummy_worker' WorkerName
    #   Expected result: The pl_dummy_worker pipeline gets invoked and sets the current_status of the task to 'SUCCEEDED'
    #   Purpose: Validate the orchestration of the Synapse pipelines
    #
    #---------------------------------------------------------------
    # Synapse pipeline test: pl_unzip_worker
    #
    #   Description: Invoke pipeline pl_start_task_group for an 'SYNAPSE_PIPELINE' task type, and 'pl_unzip_worker' WorkerName
    #   Expected result: The pl_unzip_worker pipeline gets invoked, unzips the test file and logs the actions in the logging tables
    #   Purpose: Validate the functionalities of the pl_unzip_worker pipeline: Unzip a file/folder and log the new status of the task and file/folder
    #
    #---------------------------------------------------------------
    # Synapse pipeline test: DummyWorker
    #
    #   Description: Invoke pipeline pl_start_task_group for an 'SPARK_NOTEBOOK' task type, and 'DummyWorker' WorkerName
    #   Expected result: The pipeline invokes the MetaNotebook notebook, which invokes the DummyWorker notebook and logs the executed tasks
    #   Purpose: Validate the orchestration from the Synapse pipeline environment to the Synapse notebook environment
    #
    #---------------------------------------------------------------

    ### PREPARE
    ###     Log a new instance of the unittest_synapse_unzip plan
    $unzipWorkerVariables = . $WorkspaceFolder/pipelines/scripts/sql/prepare_logging_tables.ps1 `
        -TargetServer $sqlServerInstance `
        -TargetDatabase $sqlDatabase `
        -PlanName 'unittest_synapse_unzip' `
        -InformationAction 'Continue'

    ###     Log a new instance of the unittest_spark_dummy plan
    $dummWorkerVariables = . $WorkspaceFolder/pipelines/scripts/sql/prepare_logging_tables.ps1 `
        -TargetServer $sqlServerInstance `
        -TargetDatabase $sqlDatabase `
        -PlanName 'unittest_spark_dummy'
    

    ### EXECUTE
    ###     Run a set of pipelines for integration testing
    ###     - pl_start_task_group for the DummyWorker
    ###     - pl_start_task_group for the pl_unzip_worker
    ###     - pl_start_run for the pl_dummy_worker
    . $WorkspaceFolder/pipelines/scripts/synapse/invoke_parallel_synapse_pipelines.ps1 `
        -SynapseWSName $synapseWSName `
        -Pipelines @( @{ pl_start_task_group = @{ Task_Group = 'DUMMY'; Plan_ID = $dummWorkerVariables.planID; Run_ID = $dummWorkerVariables.runID}}, @{ pl_start_task_group = @{ Task_Group = 'PREPROCESS'; Plan_ID = $unzipWorkerVariables.planID; Run_ID = $unzipWorkerVariables.runID}}, @{ pl_start_run = @{Plan_Name = 'unittest_synapse_dummy'; New_Plan = $true}})

    ### EVALUATE
    ###     Run all the SQL Tests scripts for the pl_dummy_worker
    . $WorkspaceFolder/pipelines/scripts/sql/gather_sql_test_scripts.ps1 `
        -SQLServerInstance $sqlServerInstance  `
        -SQLDatabase $sqlDatabase `
        -AccessToken $accessToken `
        -FilterType 'path' `
        -FilterPath '*synapse_dummy_pl*'

    ###     Run all the SQL Tests scripts for the pl_unzip_worker
    . $WorkspaceFolder/pipelines/scripts/sql/gather_sql_test_scripts.ps1 `
        -SQLServerInstance $sqlServerInstance `
        -SQLDatabase $sqlDatabase `
        -AccessToken $accessToken `
        -FilterType 'path' `
        -FilterPath '*synapse_unzip*'

    ###     Run all the SQL Tests scripts for the DummyWorker
    . $WorkspaceFolder/pipelines/scripts/sql/gather_sql_test_scripts.ps1 `
        -SQLServerInstance $sqlServerInstance `
        -SQLDatabase $sqlDatabase `
        -AccessToken $accessToken `
        -FilterType 'path' `
        -FilterPath '*spark_dummy_notebook*'

}   

    
# If the tests for the Synapse notebooks need to be run
if (-not $SkipSynapseNotebooks){

    #---------------------------------------------------------------
    # Synapse notebook tests: TestNotebook
    #
    #   Description: Invoke notebook 'TestNotebook' which will initiate all unittests defined in Test-notebooks
    #   Expected result: A .txt file will be pushed to the ADLS 'logs' container with the results of the unit tests
    #   Purpose: Make sure that each individual function of the Synapse Notebooks works as expected
    #   
    #---------------------------------------------------------------
    # Synapse notebook tests: Filworker
    #
    #   Description: Invoke notebook MetaNotebook with a list of tasks to execute using the Filworker notebook
    #   Expected result: A file with .fil extension gets turned into CSV. The .fil-file moves to archive, and only .csv-file remains
    #   Purpose: Validate the functionalities of the FilWorker
    #   
    #---------------------------------------------------------------
    # Synapse notebook tests: IngestionWorker
    #
    #   Description: Invoke notebook MetaNotebook with a list of tasks to execute using the IngestionWorker notebook
    #   Expected result: A set of files in the landing zone get successfully moved from landing to silver using the IngestionWorker notebook
    #   Purpose: Validate the functionalities of the IngestionWorker: Move files from landing zone to delta lake and log the process
    #
    #---------------------------------------------------------------



    ### PREPARE
    ###     Log a new instance of the unittest_filworker plan
    $filWorkerVariables =. $WorkspaceFolder/pipelines/scripts/sql/prepare_logging_tables.ps1 `
        -TargetServer $sqlServerInstance `
        -TargetDatabase $sqlDatabase `
        -PlanName 'unittest_filworker'


    ###     Log a new instance of the unittest_ingestionworker plan
    $ingestionWorkerVariables =. $WorkspaceFolder/pipelines/scripts/sql/prepare_logging_tables.ps1 `
        -TargetServer $sqlServerInstance `
        -TargetDatabase $sqlDatabase `
        -PlanName 'unittest_ingestionworker' `


    ### EXECUTE
    ###     Run a set of Notebooks for integration testing
    ###     - MetaNotebook for the IngestionWorker
    ###     - MetaNotebook for the FilWorker
    ###     - TestNotebook for the unit tests
    . $WorkspaceFolder/pipelines/scripts/synapse/invoke_parallel_notebooks.ps1 `
        -PoolName $sparkPoolName `
        -SynapseWorkspac $synapseWSName  `
        -TargetEnvironment $TargetEnvironment `
        -Notebooks @(@{ MetaNotebook = @{ folder = 'Workers'; 'parameterMap' =  @{plan_list = "$($ingestionWorkerVariables.taskList)"; Plan_ID = $ingestionWorkerVariables.planID; env_code = $TargetEnvironment }}}, @{ MetaNotebook = @{ folder = 'Workers'; 'parameterMap' =   @{ plan_list = "$($filWorkerVariables.taskList)"; Plan_ID = $filWorkerVariables.planID; env_code = $TargetEnvironment }}}, @{ TestNotebook = @{ folder = 'Workers'}})  `
        -SessionName: $sparkSessionName



    ### EVALUATE
    ###     Check the logs posted in the 'logs' container 
    . $WorkspaceFolder/pipelines/scripts/synapse/check_test_logs.ps1 `
        -StorageAccount $storageAccount `
        -ContainerName 'logs' `
        -Outpath 'src/synapse/_test/output' `
        -IsLocalDeploy `
        -ErrorAction Continue


    ###     Run all the tests for the FilWorker
    . $WorkspaceFolder/pipelines/scripts/synapse/validate_workers/validate_filworker.ps1 `
        -WorkspaceFolder $WorkspaceFolder `
        -TargetEnvironment $TargetEnvironment `
        -SQLServerInstance $sqlServerInstance  `
        -SQLDatabase $sqlDatabase `
        -AccessToken $accessToken


    ###     Run all the tests for the IngestionWorker
    . $WorkspaceFolder/pipelines/scripts/synapse/validate_workers/validate_ingestionworker.ps1 `
        -SQLServerInstance $sqlServerInstance  `
        -SQLDatabase $sqlDatabase `
        -AccessToken $accessToken `
        -StorageAccount $storageAccount `
        -SynapseWorkspace $synapseWSName `
        -SynapseServer $synapseServerInstance `
        -SynapseDatabase $synapseDatabase `
        -WorkspaceFolder: $WorkspaceFolder `
        -Plan_ID $ingestionWorkerVariables.planID

}
</file>
<file name="pipelines\scripts\datalake\clean_all_containers.ps1">
&lt;#

.SYNOPSIS
    Script to clean all containers in an ADLS account

.DESCRIPTION
    In the development and integration environment, it might be necessary to start from a clean slate.
    This script will clean all the containers in an ADLS account.

#&gt;

[CmdletBinding()]
param (
    [Parameter( Mandatory = $true, HelpMessage="Name of the storage account of where to land the files")]  
    [string] $StorageAccount  = 'devdapstdala1', 

    [Parameter( Mandatory = $true, HelpMessage="Environment of where the script is run (dev, int)")]  
    [string] $TargetEnvironment  = $env:MYDEVENV,

    [Parameter( Mandatory = $false, HelpMessage="Exclude containers to clean. Type: [array]" )]
    [array] $ExcludeContainers = @()
)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables
$inputParameters = @( 
    "StorageAccount"
    "TargetEnvironment"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


# Only logging tables in dev and int environments are allowed to be cleaned.
if ( $TargetEnvironment -notin ('dev', 'int')) {
    throw "Invalid TargetEnvironment '$TargetEnvironment'. Only dev and int deployments are allowed."
}

# Set the Storage Context to be able to access the containers in the ADLS account
$context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
# Get the list of containers
[array] $StorageContainers = (Get-AzStorageContainer -Context $context)

if ($StorageContainers){
    $StorageContainerList = $StorageContainers.Name
    # Then check if there are containers to exclude
    # If not: just clean all containers
    if ([bool] $ExcludeContainers){
        Write-Warning "Container(s) '$($ExcludeContainers -join ', ')' is/are skipped in $scriptName"
        $ContainerList = $StorageContainerList | Where-Object { $_ -notin $ExcludeContainers }
    }
    else{
        [array] $ContainerList = $StorageContainerList
    }

    # Loop over each container and remove its contents
    foreach ($container in $ContainerList){
        Write-Information "Clean Container $container..."
        # While the container is not empty, continue retrying to remove all items
        # While container not empty, keep going
        :retry_loop while($true){
            # Get a list of all the blobs in the container
            $blobs = Get-AzStorageBlob -Context $context -Container $container | Sort-Object -Property @{Expression ='Length'; Descending= $true}, @{Expression='Name'; Descending=$true} 
            # Iteratively remove all the blobs
            if ([bool] $blobs){
                foreach($blob in $blobs.Name){
                    Write-Information "     Removing blob: $blob"
                    Remove-AzStorageBlob -Container $container -Context $context -Blob $blob      
                }
            }

            # Check if no more blobs are in the container
            $blobs = Get-AzStorageBlob -Context $context -Container $container | Sort-Object -Property @{Expression ='Length'; Descending= $false}, @{Expression='Name'; Descending=$true} 
            # If no blobs: Break out of the loop
            if(-not [bool]$blobs){
                break retry_loop
            }
        }
    }
}

# Write information for script user
Write-Output "*** Ending script: $scriptName ***"

</file>
<file name="pipelines\scripts\datalake\file_container_existence.ps1">
&lt;#
.SYNOPSIS
    A script which can be used to check the existence or absence of certain files inside a certain container inside a certain storage account

.DESCRIPTION
    You pass on an array of files you expect or don't expect to be present in a certain container inside a certain storage account

.NOTES
    If the $FileList parameter is empty, you can just use this function to look up the files inside a specific container
#&gt;

[CmdletBinding()]
param(
    [Parameter( Mandatory = $false, HelpMessage="The local folder where the repo is cloned")]
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Name of the environment the table needs to be deployed to" )]
    [string] $TargetEnvironment = $env:MYDEVENV,

    [Parameter(Mandatory=$true, HelpMessage="Container name inside of the storage account (e.g. silver)")]
    [string] $ContainerName,

    [Parameter(Mandatory=$false, HelpMessage="The prefix before the actual file name (= the full path excl. the container name)")]
    [string] $BlobPrefix,

    [Parameter(Mandatory=$false, HelpMessage="Array of files to be expected or to be absent in the certain container")]
    [array] $FileList,

    [Parameter(Mandatory=$false, HelpMessage="Flag to check for absence or presence of files. Set to false if you want to check the absence of the files list")]
    [bool] $PresenceCheck = $true
)

# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables
$inputParameters = @( 
    "WorkspaceFolder"
    "TargetEnvironment"
    "ContainerName"
    "FileList"
    "PresenceCheck"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------
# Step 1: Define parameters
# Using the target-environment, define the names of the different Azure services
$StorageAccount = "$($TargetEnvironment)dapstdala1"

if ([bool]$PresenceCheck){
    Write-Information "Checking for existence of files $("'" + ($FileList -join "','") + "'") in container $($ContainerName)..."
}
else{
    Write-Information "Checking for absence of files $("'" + ($FileList -join "','") + "'") in container $($ContainerName)..."
}

$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
    
# Get all possible blobs inside a container according to a specific filter
$Blobs = Get-AzStorageBlob -Context $Context -Container $ContainerName | Sort-Object -Property @{Expression ='Length'; Descending= $true}, @{Expression='Name'; Descending=$true} 

# If there are no contents inside of the Azure Container, the Boolean will be false
if ([bool]$Blobs){
    # Only collect the files (contain a '.' -&gt; file extension)
    # Split-Path -Leaf extracts just the last part of the path
    [array]$FoundFiles = $Blobs | ForEach-Object { if($_.Name -match '\.') {$_.Name}}
    Write-Information "Found files: $("'" + ($FoundFiles -join "','") + "'")"

    # Loop over all the files
    foreach($File in $FileList){
        # If there is a prefix / parent path before the file, then include that in the search
        if([bool]$BlobPrefix){
            # Make sure it is an actual path, and turn the backslashes the other way around
            # Because in the storage account a path is made up like "path/to/something"
            # Vs what Join-path does is "path\to\something"
            $FullPath = (Join-Path $BlobPrefix $File).Replace('\', '/')
        }
        else{
            # This is the case when the file is just in the root of the container
            $FullPath = $File
        }

        # See if File is in the container, but throw error when checking for presence and the file is not found
        if (($FullPath -notin $FoundFiles) -and ([bool]$PresenceCheck)){
            throw "File $($FullPath) was expected in container $($ContainerName) but was not found"
        }
        # See if the File is not in the container, but throw error when checking for absence and the file is found
        elseif (($FullPath -in $FoundFiles) -and (-not [bool]$PresenceCheck)) {
            throw "File $($FullPath) was not expected in container $($ContainerName) but was found"
        }
    }
}
else{
    Write-Warning "The container '$($ContainerName)' was empty for storage account '$($StorageAccount)'"
    
    # Throw error when you are supposed to be looking for certain files, but the container appears to be empty
    if ([bool]$PresenceCheck){
        throw "Container $($ContainerName) was empty. Files $("'" + ($FileList -join "','") + "'") not found"
    }

}

Write-Information ""
Write-Information "***End script: $($scriptName)***"
</file>
<file name="pipelines\scripts\datalake\get_blob_content.ps1">
&lt;#
.SYNOPSIS
    A script which can be used to check the file contents of a stored blob

.DESCRIPTION
    You pass on a container to look in, and then the blob you're trying to access
#&gt;

[CmdletBinding()]
param(
    [Parameter( Mandatory = $false, HelpMessage="The local folder where the repo is cloned")]
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Name of the environment the table needs to be deployed to" )]
    [string] $TargetEnvironment = $env:MYDEVENV,

    [Parameter(Mandatory=$true, HelpMessage="Container name inside of the storage account (e.g. silver)")]
    [string] $ContainerName,

    [Parameter(Mandatory=$false, HelpMessage="The actual Blob you're trying to extract")]
    [string] $Blob
)

# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables
$inputParameters = @( 
    "WorkspaceFolder"
    "TargetEnvironment"
    "ContainerName"
    "Blob"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------
# Step 1: Define parameters
# Using the target-environment, define the names of the different Azure services
$StorageAccount = "$($TargetEnvironment)dapstdala1"

# Step 2: Set the content of the storage account &amp; extract the container
$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
## Note: If the container doesn't exist, it will throw the error "Can not find the container 'XXX'."
$Container = Get-AzStorageContainer -Name $ContainerName -Context $context

# Step 3: Create a virtual client &amp; extract the blob (full path excl. container name)
$Client = $Container.CloudBlobContainer.GetBlockBlobReference($Blob)
$File = $Client.DownloadText()

# $csvData = $File | ConvertFrom-Csv -Delimiter ","  # Specify tab as the delimiter
# $csvData | Format-Table -AutoSize  # Display the data in a table format

# Step 4: Return the file &amp; end the script
Write-Information ""
Write-Information "***End script: $($scriptName)***"

return $File
</file>
<file name="pipelines\scripts\datalake\setup_storage_account_containers.ps1">
&lt;#

.SYNOPSIS
    Checks for the existence of certain containers in a certain storage account.
    If they don't exist, the containers are created

.DESCRIPTION
    When deploying to a complete new environment, some containers are mandatory to make sure all the checks and processes succeed.
    This script makes sure that every necessary containers, based on parameter input, is present or created if not present.

.PARAMETER ContainerNames
    Most PowerShell scripts that are intended to be run from Azure DevOps and need to accept arrays will either 
    expect a single string that they can parse internally, or they will iterate over multiple occurrences of the same parameter.

    So: Put it as a string, not directly as an array

#&gt;

[CmdletBinding()]
param (
    [Parameter( Mandatory = $false)]  [string]      $StorageAccount     = 'devdapstdala1',                        # Name of the storage account of where to land the files
    [Parameter( Mandatory = $false)]  [string]      $ContainerNames     = 'archive, landing, raw, silver'      # Array with containers that are checked in the new environment
)

# Enforce ErrorAction and InformationPreference
$ErrorActionPreference = 'Continue'
$InformationPreference = 'Continue'

# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

$inputParameters = @( 
    "StorageAccount"
    "ContainerNames"
)


$scriptName = $script:MyInvocation.MyCommand.Path
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------

# PREP: STRING WITH CONTAINERS TO ARRAY
## Turn the string of containers into an array of containers
$Containers = $ContainerNames -split ',' | ForEach-Object { $_.Trim() }

# STEP 1: VALIDATION
## Currently no validation is necessary
## Dev-note: In the future: Array of allowed containers to be checked or created?

# STEP 2: CHECK CURRENT CONTAINERS IN STORAGE ACCOUNT
## A: Just get the existing containers already present in the storage account
$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
$ExistingContainers = Get-AzStorageContainer -Context $Context
Write-Information "Containers: "
Write-Information $ExistingContainers

## B: Is the current storage account empty?
### If it is empty, create all the containers inside the array
if (-not [bool]$ExistingContainers){
    Write-Information "Storage account '$($StorageAccount)' is empty. Creating containers:"
    
    foreach ($Container in $Containers) {
        Write-Information "  Create Container: $($Container)..."
        New-AzStorageContainer -Name $Container -Context $Context
    }
}
else{
    ### If it's not empty, check first if the container is already present inside of the storage account
    ### If it's not present, create it, else just output an information message
    Write-Information "Storage account '$($StorageAccount)' has containers. Checking presence of containers:"
    
    foreach ($Container in $Containers){
        if ($Container -notin $ExistingContainers.Name){
            Write-Information "  Create Container: $($Container)..."
            New-AzStorageContainer -Name $Container -Context $Context
        }
        else {
            Write-Information "Container $($Container) already present in storage account '$($StorageAccount)'. No action required."
        }
    }
}

# STEP 3: END OF SCRIPT
# Write information for script user
Write-Output "*** End script: $scriptName ***"
</file>
<file name="pipelines\scripts\datalake\upload_to_container.ps1">
&lt;#

.SYNOPSIS
Run a spark notebook from a synapse workspace

.DESCRIPTION
During development, it might be necessary to run a specific notebook while invoking this notebook from PowerShell.
This is most relevant during the Unit Test process of the development, but this script allows you to run any notebook (without parameters)


.PARAMETER NotebookName
What is the name of the notebook you want to run?

.PARAMETER PoolName
What is the name of the spark pool you want to use to run the notebook?

.PARAMETER SynapseWorkspace
What is the name of the synapse workspace where the pool and notebook are located?

#&gt;

[CmdletBinding()]
param (
    [Parameter( Mandatory = $false)]  [string] $StorageAccount   = 'devdapstdala1',  # Name of the storage account of where to land the files
    [Parameter( Mandatory = $false)] [string] $ContainerName    = 'landing',       # Container name of where to land the files
    [Parameter( Mandatory = $false)] [string] $TestType         = "fulltest_files", # Are we dealing with unit tests or full tests?
    [Parameter( Mandatory = $false)] [string] $WorkspaceFolder  = $env:REPOROOT     # The local folder where the repo is cloned
)

# Enforce ErrorAction and InformationPreference
Set-StrictMode -Version "latest"
$ErrorActionPreference = 'Stop'
$InformationPreference = 'Continue'


# Start region: Print all variables
$inputParameters = @( 
    "StorageAccount"
    "ContainerName"
    "TestType"
    "WorkspaceFolder"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
#endregion

# Step 1: Validation
# Check if folder with test files exists
$DataFolder = Join-Path $WorkspaceFolder "src" "synapse" "_test" "test_files" $TestType
if ( -not ( Test-Path $DataFolder -PathType "Container" )) {
    throw "Data folder for integration tests not found: $DataFolder "
}

# Check if $ContainerName container exists
$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
[array] $ExistingContainers = Get-AzStorageContainer -Context $Context
Write-Information "Containers: "
Write-Information $ExistingContainers

# Create container if it doesn't exist
# Dev-note: Add allowed containers (to be discussed)
if ((-not [bool]$ExistingContainers) -or ($ContainerName -notin $ExistingContainers.Name)){
    Write-Information "  Create Container: $($ContainerName)..."
    New-AzStorageContainer -Name $ContainerName -Context $Context
}

# Upload files in DataFolder to Azure blob container '$ContainerName'
Write-Information "  Upload data to $($ContainerName) container"
$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
$allFiles = Get-ChildItem -Path $DataFolder -Recurse | Where-Object { ! $_.PSIsContainer } 
$allfiles | ForEach-Object{ 
    $filepath = [System.IO.Path]::GetRelativePath($DataFolder, $_.FullName)
    Set-AzStorageBlobContent -Container $ContainerName -Context $Context -File $_.FullName -Blob $filepath -Force
}

# Commented: Clean the containers -&gt; Use other script for this: clean_all_containers
# elseif($ContainerName -eq "landing"){
#     $container_list = "landing", "raw", "silver", "archive"
# }
# else{
#     Write-Error "Unknown container; Stop execution"
# }

# foreach ($container in $container_list){
#     if ( $container -notin $ExistingContainers.Name){
#         Write-Information "  Create Container: $container..."
#         New-AzStorageContainer -Name $container -Context $Context
#     }
#     else {
#         $NotEmpty = $true
#         # While container not empty, keep going
#         while($NotEmpty){
#             # If the container exists in the data lake, clean it
#             if ($container -in $ExistingContainers.Name){
#                 Write-Information "  Clean Container $container..."
#                 # Get a list of all the blobs in the container
#                 $blobs = Get-AzStorageBlob -Context $Context -Container $container | Sort-Object -Property @{Expression ='Length'; Descending= $true}, @{Expression='Name'; Descending=$true} 
#                 # Iteratively remove all the blobs
#                 foreach($blob in $blobs.Name){
#                     Write-Information "      Removing blob: $blob"
#                     Remove-AzStorageBlob -Container $container -Context $Context -Blob $blob
#                 }

#                 # Check if no more blobs are in the container
#                 $blobs = Get-AzStorageBlob -Context $Context -Container $container | Sort-Object -Property @{Expression ='Length'; Descending= $false}, @{Expression='Name'; Descending=$true} 
#                 if($blobs -eq $null){
#                     $NotEmpty = $false
#                 }
#             }
#         }
#     }
# }
</file>
<file name="pipelines\scripts\general\check_environment_code.ps1">
[CmdletBinding()]
param (
    [Parameter()]
    [String]
    $environment_code, 

    [Parameter()]
    [String]
    $target_environment
    

)


Write-Output "Checking target environment ..."
Write-Output "  environment_code  : $environment_code"
Write-Output "  target_environment: $target_environment"

Write-Output "Using new version ..."

# if the environment is sandbox( "sbx"), then the target_environment should match the sandbox naming convention
if ( $environment_code -eq "sbx") {
    if ( $target_environment -notmatch '^sbx[a-z0-9]{2,3}$') {
        Write-Error "Sandbox environments should start with 'sbx' followed by 2 or 3 digits or lowercase letters eg 'sbxab1"
        return
    }
    Write-Output "Looks okay for $environment_code environment!"
    return
} 

# in all other cases, environment_code must be equal to target_environemnt, and it must match one of the accepted ones
if ( @( 'dev', 'int', 'tst', 'tmp', 'acc', 'prd') -contains $environment_code ) {
    if ( $target_environment -ne $environment_code) {
        Write-Error "For non-sandbox environments, target_environment should be equal to environment_code"
        return
    }
    Write-Output "Looks okay for $environment_code environment!"
    return
}

Write-Error "Invalid environment_code provided: $environment_code"
</file>
<file name="pipelines\scripts\general\install_powershell_requirements.ps1">
&lt;#
.SYNOPSIS
    This script installs required modules as specified in the
.DESCRIPTION
    This script relies on getting the list of modules that should be available.
        - If the module is not installed, the script will install it with the required version (optional)
        - If the module exists but with an older version, the required version is installed
        - If the module exists with a newer version, no action is taken
        - If no versions are specified (just the module name) the script will assume any version is ok
        
    Modules are always installed at the CurrentUser scope.

.PARAMETER Requirements
    
    A json array string, where each array member has following properties:
        - Name    : name of the module (mandatory)
        - version : semantic versioning (SemVer) minimum version of the module

    e.g.
    [
        { "Name": "SqlServer" },
        { "Name": "Pester", "Version" : "5.5" }
    ]

#&gt;

[CmdletBinding()]
param (
    [Parameter(Mandatory = $false, HelpMessage = "Json array with required module information")]
    [string]$Requirements = "[]",
    [Parameter(Mandatory = $false, HelpMessage = "Custom directory in which the script will install the modules")]
    [string]$CustomInstallPath = $null
)

#region local script settings
    $ErrorActionPreference = "Stop"
    Set-StrictMode -Version "latest"
#endregion

#region script info
    $inputParameters = $MyInvocation.MyCommand.Parameters.Keys | Where-Object { 
        ($_ -NotIn [System.Management.Automation.PSCmdlet]::CommonParameters ) -and
        ($_ -NotIn [System.Management.Automation.PSCmdlet]::OptionalCommonParameters )
    }
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information ""
    Write-Information "----------------------------------------------------------------------"
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
    Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
    Write-Information "  Parameters:"
    foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
    }
    Write-Information "----------------------------------------------------------------------"
#endregion script info

$requiredModules = [Array] ( $Requirements | ConvertFrom-Json )

:reqLoop foreach ($req in $requiredModules) {

    $reqName = $req.Name
    $reqVersion = ( $req | Get-Member -Name "Version" ) ? $req.Version : $null

    Write-Information ""
    Write-Information "Processing requirement: $reqName $($reqVersion ?? "(any)") ..."

    # validate the version number
    if ( $reqVersion -and $reqVersion -ne "Latest") {
        if ( $reqVersion -match "^\d+\.\d+\.\d+$" -or
             $reqVersion -match "^\d+\.\d+\.\*$" -or
             $reqVersion -match "^\d+\.\*$" ) {
            # just valid
        } elseif ( $reqVersion -match "^\d+$" -or
                   $reqVersion -match "^\d+\.\d+$" ) {
            # add the "*" minor or rev
            $reqVersion = $reqVersion + ".*"
            Write-Information "  Updated SemVer string to '$reqVersion'"
        } else {
            throw "Invalid version number '$reqVersion' for module '$reqName' in requirements parameter"
        }
    }

    # currently installed module versions
    $installedVersions = [Array] (Get-Module $reqName -ListAvailable) | Select-Object @{Name = 'Version'; Expression = {$_.Version.ToString()}}, Path
    Write-Information "  Installed versions:"
    $installedVersions | Foreach-Object { Write-Information "    $($_.Version)   $($_.Path)" }
    $currentMaxVersion = ($installedVersions | Sort-Object -Property "Version" -Descending | Select-Object -First 1).Version

    # continue if module already exists and requirements do not specify any version
    if (-not $reqVersion -and $installedVersions) {
        Write-Information "  Module '$reqName' already installed and no particular version is required."
        continue
    }

    # evaluate version to be installed
    if (($reqVersion -eq "latest") -or (-not $reqVersion)) {
        # not specified or latest -&gt; get latest version
        try {
            $remoteModules = Find-Module -Name $reqName -ErrorAction "Stop" | Sort-Object -Property "Version" -Descending
            Write-Information "  Found available modules:"
            $remoteModules | Foreach-Object { Write-Information "    $($_.Version) in Repository: $($_.Repository)" }
            $remoteModule = $remoteModules | Select-Object -First 1
            $remoteModule

        } catch {            
            $ErrorActionPreference = "Continue"
            Write-Error "Error getting module '$reqName' from PSRepository, message: $($_.Exception.Message)"
            $ErrorActionPreference = "Stop"
            continue reqLoop
        }
        $installVersion = $remoteModule.Version.ToString()
        Write-Information "  Latest version of module '$reqName' PSGallery is '$installVersion'"
        Write-Debug "  Remote versions of module ${reqName}:`n$($remoteModule | Out-String)"
    } elseif ($reqVersion -match "\*") {
        # if wildcards specified, find latest matching version
        try {
            $remoteModule = Find-Module $reqName -AllVersions | `
                Where-Object { $_.Version -like $reqVersion } | `
                Sort-Object -Property "Version" -Descending -Top 1
            } catch {    
                $ErrorActionPreference = "Continue"        
                Write-Error "Error getting module '$reqName' from PSRepository, message: $($_.Exception.Message)"
                $ErrorActionPreference = "Stop"
                continue reqLoop
            }
        $installVersion = $remoteModule.Version.ToString()
        Write-Information "  Latest matching version of module '$reqName' on PSGallery is '$installVersion'"
        Write-Debug "  Remote versions of module ${reqName}:`n$($remoteModule | Out-String)"
    } else {
        # exact version
        $installVersion = $reqVersion
    }

    # continue if module already exists with required or higher version
    #if ( $installedVersions | Where-Object Version -ge $installVersion) {
    if ( $currentMaxVersion -ge $installVersion ) {
        Write-Information "  Module '$reqName' version '$currentMaxVersion' is already at required version '$installVersion' or higher."
        continue reqLoop
    }

    # if we get here, we need to install somthing...
    $moduleConfig = @{
        Name = $reqName
        RequiredVersion = $installVersion
    }

    if ( -not $CustomInstallPath) {
        # if no path was provided we can use install-module at user scope
        $moduleConfig.Add("Scope", "CurrentUser")
        $moduleConfig.Add("Force", $true)
        $moduleConfig.Add("AllowClobber", $true)
        Install-Module @moduleConfig
        Write-Information "  Installed module $($moduleConfig.Name) with version $($moduleConfig.RequiredVersion)"
    } else {
        # if a path was provided we need to save it to the specified folder
        Find-Module @moduleConfig | Save-Module -Path $CustomInstallPath -Force
        Write-Information "  Saved module '$($moduleConfig.Name)' with version '$($moduleConfig.RequiredVersion)' to directory '$CustomInstallPath'"
    }
}

#region end of script
    Write-Information ""
    Write-Information "--- End of script: $($MyInvocation.MyCommand.Name) ---"
    Write-Information ""
#endregion

</file>
<file name="pipelines\scripts\general\show_config_vars.ps1">
[CmdletBinding()]
param (
    [Parameter()]
    [String]
    $Variables = "",

    [Parameter()]
    [bool]
    $ShowFullEnv = $false

)

# the calling devops task is creating a temporary env variable starting with "PARAM_" for each pipeline parameter
Write-Output "Pipeline parameters:"  
$paramList = (Get-ChildItem env: | Where-Object Name -like "PARAM_*")
$maxLength = ($paramList.Key | Measure-Object -Property Length -Maximum).Maximum -6  # substract "PARAM_"
foreach ( $p in $paramList) {
  Write-Output ( "  " + $p.Name.Replace("PARAM_", "").PadRight($maxLength) + " : " + $p.Value )
}

# show variables listes in $Variables parameter
Write-Output ""
Write-Output "Variables:"  
if( $Variables) {
    $varList = $Variables.split(",").trim() 
    $maxLength = ( $varList | Measure-Object -Maximum -Property Length).Maximum
    foreach ( $v in $varList) {
        $envVar = $v.ToUpper().Replace(".","_")
        Write-Output ( "  " + $v.PadRight($maxLength) + " : " + [Environment]::GetEnvironmentVariable($envVar) )
    } 
} else {
    Write-Output "  (none)"
}

# show standard entries
Write-Output ""
Write-Output "System configuration:"
Write-Output "  Build.SourceBranch              : $Env:BUILD_SOURCEBRANCH"
Write-Output "  Build.SourceBranchName          : $Env:BUILD_SOURCEBRANCHNAME"
Write-Output "  System.DefaultWorkingDirectory  : $Env:SYSTEM_DEFAULTWORKINGDIRECTORY"
Write-Output "  Build.ArtifactStagingDirectory  : $Env:BUILD_ARTIFACTSTAGINGDIRECTORY"
Write-Output "  System.ArtifactsDirectory       : $Env:SYSTEM_ARTIFACTSDIRECTORY"
Write-Output "  Pipeline.Workspace              : $Env:PIPELINE_WORKSPACE"

# show standard entries
Write-Output ""
Write-Output "Path:"
($Env:PATH).Split([IO.Path]::PathSeparator) | ForEach-Object { Write-Output "  $_"}

# if requested, show full list of environment variables
if ($ShowFullEnv) {
    Write-Output ""
    Write-Output "-------------------------------------------------"
    Write-Output "Full list of environment variables:"
    Write-Output "-------------------------------------------------"
    (Get-ChildItem env:  ) | Format-Table -HideTableHeaders   
}

</file>
<file name="pipelines\scripts\general\show_folder_contents.ps1">
[CmdletBinding()]
param (
    [Parameter(HelpMessage="Provide the directory, or leave empty for current dir")]
    [String]
    $Path = "",

    [Parameter()]
    [bool]
    $ShowFoldersOnly = $false

)

#region script info
    $inputParameters = $MyInvocation.MyCommand.Parameters.Keys | Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::CommonParameters}
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information "----------------------------------------------------------------------"
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
    Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
    Write-Information "  Parameters:"
    foreach ($param in $inputParameters) {
        Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
    }
    Write-Information "----------------------------------------------------------------------"
#endregion script info

# local script settings
Set-StrictMode -Version "latest"

# if not provided just current working dir
if (-not $Path) {
    $Path = (Get-Item -Path ".").FullName
}
# check that the path exists
elseif (-not (Test-Path $Path) ) {
    Write-Warning "Provided folder `"$Path`" does not exist."
}

Write-Output  "path:    `"$Path`"... "
Write-Output ("showing  : " + ($ShowFoldersOnly) ? "Folders Only" : "Files and folders")
Write-Output "-----------------------------------------------------------------------"

$data = (Get-ChildItem -Path $Path -Recurse -Directory:$ShowFoldersOnly ) | Select-Object -ExpandProperty FullName

$data  | Sort-Object | ForEach-Object  { Write-Output ("  " + $_.Replace($Path + [IO.Path]::DirectorySeparatorChar, ""))}

</file>
<file name="pipelines\scripts\sql\clean_sql_tables.ps1">
&lt;#

.SYNOPSIS
    Clean logging tables from SQL Meta DB

.DESCRIPTION
    To be able to start from scratch, all logging tables in the SQL Meta DB are cleaned. This script executes stored procedure test.usp_reset_database which will truncate the logging tables
    Before truncating, the script checks that we are in the dev or int environment

#&gt;


[CmdletBinding()]
param(
    [Parameter(Mandatory = $true, HelpMessage = "The SQL server where the database can be found")] 
    [string] $SQLServerInstance = $env:SQL_SERVER,

    [Parameter(Mandatory = $true, HelpMessage = "The name of the SQL Database that needs to be cleaned")] 
    [string] $SQLDatabase = $env:SQL_DATABASE,

    [Parameter(Mandatory = $false, HelpMessage="Personal accesstoken linked to Microsoft Account")] 
    [string] $Accesstoken,

    [Parameter(Mandatory = $false, HelpMessage="The environment where the script is run (dev, int)")] 
    [string] $TargetEnvironment = $env:MYDEVENV
)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

$inputParameters = @( 
    "SQLServerInstance"
    "SQLDatabase"
    "TargetEnvironment"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


# Only logging tables in dev and int environments are allowed to be cleaned.
if ( $TargetEnvironment -notin ('dev', 'int')) {
    throw "Invalid TargetEnvironment '$TargetEnvironment'. Only dev and int deployments are allowed."
}

# Get accesstoken if not provided
if (-Not $Accesstoken){
    Write-Information "Token not provided, obtaining..."
    $AccessToken = (Get-AzAccessToken -ResourceUrl https://Database.windows.net -Debug:$false).Token
}


# Group SQL parameters to invoke a SQL command 
$sqlParams = @{
    ServerInstance = $SQLServerInstance
    Database = $SQLDatabase
    AccessToken = $Accesstoken
}

# Invoke SQL Command to clean logging tables
Invoke-Sqlcmd @sqlParams -query "exec test.usp_reset_database @database_name = '$($SQLDatabase)', @sure = 1"


# Write information for script user
Write-Output "*** End script: $scriptName ***"
</file>
<file name="pipelines\scripts\sql\deploy_data_configuration.ps1">
#Requires -Modules DataConfig

&lt;#
.SYNOPSIS
Deploy the data configurationfor plans, tasks, and datasets contained in the JSON files

.DESCRIPTION
Using the ConfigurationFolderPath, deploy all the configuration files necessary:
    - plan_tasks
    - datasets

#&gt;


[CmdletBinding()]
param (

    [Parameter(Mandatory = $false, HelpMessage="SQL Server to access")] 
    [string] $SQLServerInstance = $env:SQL_Server,
    [Parameter(Mandatory = $false, HelpMessage="SQL DB to access")] 
    [string] $SQLDatabase=$env:SQL_DATABASE,
    [Parameter(Mandatory = $false, HelpMessage="Accesstoken to SQL DB for currentuser")] 
    [string] $AccessToken = $null,
    [Parameter(Mandatory = $false, HelpMessage="Folder where the configuration files are located")] 
    [string] $ConfigurationFolderPath = "$($env:REPOROOT)/src/synapse/_test/metadata",
    [Parameter(Mandatory = $false, HelpMessage="Folder where the delta table configuration file is located")] 
    [string] $DeltaTableConfigurationFile, #= "src/synapse/_test/delta_tables/delta_tables.json",
    
    [Parameter(Mandatory = $false, HelpMessage="Boolean: Deploy the plan_tasks configuration files?")]
    [boolean] $DeployPlantasks = $true,
    [Parameter(Mandatory = $false, HelpMessage="Boolean: Deploy the datasets configuration files?")]
    [boolean] $DeployDatasets = $true,
    [Parameter(Mandatory = $false, HelpMessage="Boolean: Deploy the datasets configuration files?")]
    [boolean] $DeployDeltaTables = $true,

    [Parameter(Mandatory = $false, HelpMessage="selection of files to deploy")]
    [ValidateSet('all','path','filelist')]
    [string]$FilterType = 'all',
    [Parameter(Mandatory = $false, HelpMessage="If FilterType = path: '-like' style path filter (REGEX!)")]
    [string]$FilterPath = $null,
    [Parameter(Mandatory = $false, HelpMessage="If FilterType = filelist: path to the file with the list of files")]
    [string]$FileList = $null,

    [Parameter(Mandatory = $false, HelpMessage="Root folder of git repo on local device")]
    [string] $WorkspaceFolder, #= $env:REPOROOT,
    [Parameter(Mandatory = $false, HelpMessage="Environment of where to deploy the configuration files")]
    [string] $TargetEnvironment, #= 'dev',
    [Parameter(Mandatory = $false, HelpMessage="Name of the session that will be used, which will be annexed with a user identifier")]
    [string] $SessionName = "DeployDeltaTables",
    [Parameter(Mandatory = $false, HelpMessage="Boolean: Look recursively?")]
    [boolean] $Recurse = $true,
    [Parameter(Mandatory = $false, HelpMessage="Execute in parallel if &gt; 1")]
    [int32] $Threads = 1

)

#region local script settings
    Set-StrictMode -Version "latest"
    $ErrorActionPreference = "Stop"
    $InformationPreference = 'Continue'
#endregion

#region script info
    $inputParameters = $MyInvocation.MyCommand.Parameters.Keys | 
    Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::CommonParameters} | 
    Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::OptionalCommonParameters}
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information "----------------------------------------------------------------------"
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
    Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
    Write-Information "  Parameters:"
    foreach ($param in $inputParameters) {
        $paramValue = (Get-Variable -Name $param).Value   
        if ( $param -eq "AccessToken" -and $paramValue) {
            $paramValue = $paramValue.Substring(0,20) + ( ($paramValue.Length -gt 20 ) ? "[...] (length: $($paramValue.Length))" : "" )
        }
        Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + $paramValue )
    }
    Write-Information "----------------------------------------------------------------------"
#endregion script info


#region Step 1: Check if the provided parameters are valid
    Write-Information "Validate parameters..."

    # Check 1: Validate that the ConfigurationFolderPath exists in the current repository
    if ( -not ( Test-Path $ConfigurationFolderPath -PathType Container )) {
        Throw "Configuration folder does not exist: $ConfigurationFolderPath"
    }

    # Check 2: If FilterType = path: Make sure FilterPath is not empty
    if ( $FilterType -eq "path") {
        if ( -not $FilterPath ) {
            throw "FilterPath parameter is mandatory when FilterType is 'path'"
        }
    }

    # Check 3: If FilterType = filelist: Make sure FileList is not empty
    if ( $FilterType -eq "filelist") {
        if ( -not $FileList ) {
            throw "FileList parameter is mandatory when FilterType is 'filelist'"
        }
    }

    # Check 4: Check if access token is provided. Create if false
    if ( -not $AccessToken) {
        Write-Information "Token not provided, obtaining..."
        $AccessToken = (Get-AzAccessToken -ResourceUrl https://Database.windows.net -Debug:$false).Token
        # Write-Information "AccessToken: $AccessToken"
    }


    # Check 5: Make sure all relevant Modules are set and that the Accesstoken parameter is not empty
    if ( -not (Get-module "SqlServer" -ListAvailable) ) {
        Write-Information "Installing module: SqlServer ..."
        Install-Module -Name "SqlServer" -scope CurrentUser -Force
    }

    # Check 6: If DeltaTableConfigurationFile does exists, remove it (hard reset)
    if (Test-Path (Join-Path $WorkspaceFolder $DeltaTableConfigurationFile)) {
        # Create the file
        Write-Information "Deletion pre-existing DeltaTableConfigurationFile: $(Join-Path $WorkspaceFolder $DeltaTableConfigurationFile)"
        Remove-Item (Join-Path $WorkspaceFolder $DeltaTableConfigurationFile) -Force
    } 

    # initialize the session variable
    if($env:AGENT_ID){
        $annex="Agent"
    }
    else{
        $annex = $env:USERNAME
    }
    $identifiedSessionName = "$($SessionName)_$($annex)"

#endregion Step 1

#region Step 2: Collect all configuration files from the configuration path $ConfigurationFolderPath and execute a filter if required
    Write-Information "Collect all configuration files..."
    # Get all the json-files in the configurationFolderPath
    $allFiles = Get-ChildItem -Path $ConfigurationFolderPath -Recurse:$Recurse -Filter "*.json" 
    Write-Information "Found $($allFiles.Count) json file(s) before filtering"
    Write-Information "  First 10:"# print sample of files"
    $allFiles | Select-Object -First 10 | ForEach-Object { Write-Information "  $_"}


    # Filter the collected json_files based on FilterType
    switch( $FilterType) {
        'all' {
            $selectedFiles = $allFiles
        }
        'path' {
            $selectedFiles = @($allFiles | Where-Object {$_ -match $FilterPath})
        }
        'filelist' {
            Write-Information "Adjusted contents of ${FileList}:"
            # incoming paths are relative to workspace, and start with "./"
            # so we prepend with workspace path &amp; artifact download path, and resolve it again to get rid of the "/./" in the middle and 
            # allow matching later on
            $filelistcontents = @( Get-Content -Path $FileList | Foreach-Object { Resolve-Path (Join-Path "${env:PIPELINE_WORKSPACE}" "dataconfig" $_)  } ).Path
            $filelistcontents | ForEach-Object { Write-Information "  $_"}
            Write-Information "applying filter..."
            $selectedFiles = $allFiles | Where-Object { $filelistcontents -contains $_ }
        }
    }


    Write-Information "Retained $($selectedFiles.Count) json file(s) after filtering in [$FilterType] mode"
    # Selection of configuration files that passed the filter section
    $selectedFiles | ForEach-Object { Write-Information "  $_"} 
#endregion Step 2

#region Step 3: Deploy the configuration files

    # Set all the parameters needed to successfully deploy the (filtered) configuration files
    $deployParams = @{
        SQLServerInstance = $SQLServerInstance
        SQLDatabase = $SQLDatabase
        AccessToken = $AccessToken 
        DeployPlantasks = $DeployPlantasks
        DeployDatasets = $DeployDatasets
        DeployDeltaTable = $DeployDeltaTables
        DeltaTableConfigurationFile = $DeltaTableConfigurationFile
        WorkspaceFolder = $WorkspaceFolder
        TargetEnvironment = $TargetEnvironment
        InformationAction = $InformationPreference
        ErrorAction = $ErrorActionPreference
        Debug = $DebugPreference
    }    


    Write-Information "Start deploying JSON metadata..."
    # Sequentially deploy all configuration files to the SQL Database and fill configuration file 
    if ( $Threads -eq 1) {
        $i = 1
        foreach ( $file in $selectedFiles) {
            Write-Output ""
            Write-Output "Deploying configuration file ${i}/$($selectedFiles.Count): $file"
            Set-IngestionMetadataFromJsonFile @deployParams -Path $file    
            $i++
        }
    }
    # parallel processing
    # else {
    #     #$ModulePath = (Get-Module -Name "MbbAz").Path
    #     $selectedFiles | ForEach-Object -Parallel { 
    #         #Import-Module $using:ModulePath | Out-Null
    #         Set-IngestionMetadataFromJsonFile @using:deployParams -Path $_
    #     } -ThrottleLimit $Threads    
    # }

#endregion

#region Step 4: Deploy the delta tables

    # Deploy all delta tables
    if ($DeployDeltaTables){
        $deltaTableConfigurationPath = Join-Path $WorkspaceFolder $DeltaTableConfigurationFile
        if ( -not (Test-Path -Path $deltaTableConfigurationPath -PathType "leaf")) {
            throw "Could not find delta table configuration file: $deltaTableConfigurationPath"
        }

        Set-IngestionDeltaTableConfiguration `
        -DeltaTableConfigurationFile "$($WorkspaceFolder)/$($DeltaTableConfigurationFile)" `
        -SessionName $identifiedSessionName `
        -TargetEnvironment $TargetEnvironment `
        -InformationAction 'Continue'

    }
#endregion

Write-Output "*** End script: $($MyInvocation.MyCommand.Name) ***"

</file>
<file name="pipelines\scripts\sql\execute_sqldb_query.ps1">
&lt;#

.SYNOPSIS
    Execute queries on the SQL DB tables

.DESCRIPTION
    Make a connection to the SQL DB. This allows to query the delta lake from powershell.
    The script expects a FileName: This file should contain a query to execute in SQL

#&gt;


[CmdletBinding()]
param(
    [Parameter(Mandatory=$true)]  [string]   $SQLServerInstance,    # SQL server we are trying to connect to
    [Parameter(Mandatory=$true)]  [string]   $SQLDatabase,          # SQL database we are trying to write to
    [Parameter(Mandatory=$true)]  [string]   $AccessToken,          # Personal accesstoken for user

    [Parameter(Mandatory=$true)]  [string]  $Path,                  # File we are trying to execute
    [Parameter(Mandatory=$false)] [string] $TargetEnvironment       # Environment (dev, int) where the sql scripts will be executed 
    
)


$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Start script: $scriptName ***"

# force error action
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"


#region print parameters
# Define list of input parameters
$inputParameters = @( 
    "SQLServerInstance"
    "SQLDatabase"
    "Path"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# endregion


# Set SQL parameters
$SQL_parameters = @{
    ServerInstance = $SQLServerInstance
    Database = $SQLDatabase
    AccessToken = $AccessToken
    IncludeSqlUserErrors = $true
}

$queryContent = Get-Content -Path $Path -Raw

if ($SQLServerInstance -like '*sql.azuresynapse.net'){
    $StorageAccount = "$($TargetEnvironment)dapstdala1"
    Invoke-Sqlcmd @SQL_parameters -Query $queryContent -ConnectionTimeout 60 -OutputSqlErrors $true -Variable @{storage_account = $StorageAccount}
}

else{
    Invoke-Sqlcmd @SQL_parameters -Query $queryContent -ConnectionTimeout 60 -OutputSqlErrors $true
}
</file>
<file name="pipelines\scripts\sql\gather_sql_test_scripts.ps1">
&lt;#
.SYNOPSIS
Gather all the SQL Scripts needed to be executed

#&gt;


[CmdletBinding()]
param (
    # Full server url
    [Parameter(Mandatory = $true)] [String] $SQLServerInstance,         # SQL Server to access
    [Parameter(Mandatory = $true)] [String] $SQLDatabase,               # SQL DB to access
    [Parameter(Mandatory = $true)] [String] $AccessToken = $null,       # Accesstoken to SQL DB for currentuser
    
    [Parameter()]
    [ValidateSet('all','path')]
    [String]$FilterType = 'all',                                        # Selection of files to be gathered (all, path = for regex in FolderPath)
    [String]$FilterPath = $null,                                        # If FilterType = path: "-like" style path filter (REGEX!)

    [String] $WorkspaceFolder = $env:REPOROOT,                          # Root folder of git repo on local device
    [Boolean] $Recurse = $true,                                         # Look recursively?
    [Int32] $Threads = 1                                                # Execute in parallel if &gt; 1

)


$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

#region print parameters
$inputParameters = @( 
    "SQLServerInstance" 
    "SQLDatabase"
    "WorkspaceFolder"
    "FilterType"
    "FilterPath"
    "Recurse"
    "Threads"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)   
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
#endregion

Write-Information "Validate parameters..."
$FullPath = Join-Path $WorkspaceFolder "src" "synapse" "_test" "test_scripts" "sql"

# Check if FolderPath exists
if ( -not ( Test-Path $FullPath -PathType Container )) {
    Throw "SQL-folder does not exist: $FullPath"
}

# If FilterType = path: Make sure FilterPath is not empty
if ( $FilterType -eq "path") {
    if ( -not $FilterPath ) {
        throw "FilterPath parameter is mandatory when FilterType is 'path'"
    }
}

Write-Information "Collect all SQL-files..."
# Get all the SQL-files in the folder path
[array]  $allFiles = Get-ChildItem -Path $FullPath -Recurse:$Recurse -Filter "*.sql" 
Write-Information "Found $($allFiles.Count) SQL-file(s) before filtering"
Write-Information "  First 10:"# print sample of files"
$allFiles | Select-Object -First 10 | ForEach-Object { Write-Information "  $_"}


# Filter the collected json_files based on FilterType
switch( $FilterType) {
    'all' {
        [array] $selectedFiles = $allFiles
    }
    'path' {
        [array] $selectedFiles = $allFiles | Where-Object {$_ -like $FilterPath}
    }
}

Write-Information "Retained $($selectedFiles.Count) SQL file(s) after filtering in [$FilterType] mode"
# Selection of configuration files that passed the filter section
$selectedFiles | ForEach-Object { Write-Information "  $_"} 


# Make sure all relevant Modules are set and that the Accesstoken parameter is not empty
if ( -not (Get-module "SqlServer" -ListAvailable) ) {
    Write-Information "Installing module: SqlServer ..."
    Install-Module -Name "SqlServer" -scope CurrentUser -Force
}

#create token if not specified
if ( -not $AccessToken) {
    Write-Information "Token not provided, obtaining..."
    $AccessToken = (Get-AzAccessToken -ResourceUrl https://Database.windows.net -Debug:$false).Token
} else {
    Write-Information "Using provided token for sql."
}
Write-Debug "AccessToken: $AccessToken" 
Write-Information "Starting executing SQL-scripts..."
# # simple sequential processing
if ( $Threads -eq 1) {
    $i = 1
    foreach ( $file in $selectedFiles) {
        Write-Output ""
        Write-Output "Executing script: ${i}/$($selectedFiles.Count): $file"
        $SQLParams = @{
            SQLServerInstance = $SQLServerInstance
            SQLDatabase = $SQLDatabase
            AccessToken = $AccessToken

            Path = $file
            TargetEnvironment = $TargetEnvironment
        }

        . $WorkspaceFolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @SQLParams
        $i++
    }
}

# parallel processing
# else {
#     #$ModulePath = (Get-Module -Name "MbbAz").Path
#     $selectedFiles | ForEach-Object -Parallel { 
#         #Import-Module $using:ModulePath | Out-Null
#         Set-IngestionMetadataFromJsonFile @using:deployParams -Path $_
#     } -ThrottleLimit $Threads    
# }






</file>
<file name="pipelines\scripts\sql\prepare_logging_tables.ps1">
&lt;#

.SYNOPSIS
    Start a new run for a certain plan and expose the IDs as pipeline variables
    

.DESCRIPTION
    When running a DevOps pipeline to test orchestration and workers, running the ingestion framework from scratch every time does not make sense
    This script will start a new run using the meta.usp_new_run stored procedure
    Next to this, the Plan_ID, Run_ID, and task_list can be exposed as pipeline variables (AdoVariable) which can be called by other tasks
    
#&gt;


[CmdletBinding()]
param (
    [Parameter(Mandatory=$false, HelpMessage="Full url of the SQL server, eg xxxxxxx.database.windows.net")]
    [string] $TargetServer = $env:SQL_SERVER,

    [Parameter(Mandatory=$false, HelpMessage="Full name of the SQL database, eg gm6dwh-xxxxx-sqldb-meta")]
    [string] $TargetDatabase = $env:SQL_DATABASE,

    [Parameter(Mandatory=$true, HelpMessage="Name of the plan that needs to be logged")]
    [string] $PlanName,

    [Parameter(Mandatory=$false, HelpMessage="Personal accesstoken linked to Microsoft Account")]
    [string] $Accesstoken,

    [Parameter(Mandatory=$false, HelpMessage = "Name of Azure DevOps variable expose, will be used by other tasks in the DevOps pipeline")]
    [string[]] $AdoVariable
)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***" | Out-Null

# Start region: Print all variables

$inputParameters = @( 
    "TargetServer"
    "TargetDatabase"
    "PlanName"
    "Accesstoken"
    "AdoVariable"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
Write-Information ""
# ---------------------------------------------------------------------------------------------------------


# Get accesstoken if not provided
if (!$Accesstoken){
    $Accesstoken = (Get-AzAccessToken -ResourceUrl https://database.windows.net).Token
}

# Group the SQL parameters in an overarching variable
$sqlParams = @{
    ServerInstance = $TargetServer
    Database = $TargetDatabase
    AccessToken = $Accesstoken
    IncludeSqlUserErrors = $true   
}



# Invoke stored procedure 'meta.usp_new_run' with the relevant parameters
Write-Information ""
Write-Information "Creating plan: $PlanName..."
$query = "exec meta.usp_new_run @Pipeline_ID = '$($PlanName)', @plan_name = '$($PlanName)', @new_plan = 1, @no_select = 0"

# Return the Run_ID in the select statement
$runPlan = Invoke-SqlCmd @sqlParams -query $query

# Use the Run_ID to get the Plan_ID: Use the highest plan_id for the given run_id
# Dev-note: Filter on task_group, as this is needed for when you're unit testing a plan with different task groups
$query = @"
SELECT max(lp.plan_id) as Plan_ID
FROM 	meta.log_plans as lp 
        inner join meta.log_run_plans as lrp 	on (lrp.plan_id = lp.plan_id)
WHERE 	1=1
        AND lrp.run_id = $($runPlan.Run_ID)
        AND lp.plan_name = '$($PlanName)'
"@

# Get the list of Plans (should only be one)
$planList = Invoke-SqlCmd @sqlParams -query $query

# Using the Plan_ID, get the list of tasks to execute for the plan
# Dev-note: Filter on task_group, as this is needed for when you're unit testing a plan with different task groups
Write-Information ""
Write-Information "Getting tasks in plan: $PlanName ($($planList.Plan_ID))..."
$query = @"
SELECT 
    lpt.*,
    lt.current_status
FROM meta.log_plan_tasks as lpt
INNER JOIN meta.log_tasks as lt on ( lt.task_id = lpt.task_id)
WHERE 1=1
    AND lpt.plan_id = $($planList.Plan_ID)
	AND lpt.Original_Plan_ID = $($planList.Plan_ID)
"@

$taskList = Invoke-SqlCmd @sqlParams -query $query

# All tasks should be pending for the plan
# If not, throw error
$missconfiguredTasks = $taskList | Where-Object {$_.Current_Status -notin ("PENDING")}
if ($missconfiguredTasks) {
    $missconfiguredStatus = ($missconfiguredTasks | Select-Object -ExpandProperty "Current_Status" -Unique) -join "', '"
    Write-Error "Aborting execution as missconfigured tasks have been detected with statuses ['${missconfiguredStatus}']"
}

# If there are no tasks to execute, write warning to the shell
if (-not $taskList) {
    Write-Warning "Could not find any task executions associated with Plan_ID $($runPlan.Plan_ID)"
    Write-Host "##vso[task.logissue type=warning;sourcepath=$PSCommandPath;linenumber=60;]Could not find any task executions associated with Plan_ID $($runPlan.Plan_ID)"
}

# Return the list of tasks as a list of json objects
# Dev-note: Filter on task_group, as this is needed for when you're unit testing a plan with different task groups
$query = @"
DECLARE @tasks nvarchar(max);
set @tasks = (
    SELECT 
        lpt.*,
        lt.current_status
    FROM meta.log_plan_tasks as lpt
        inner join meta.log_tasks as lt on ( lt.task_id = lpt.task_id)
    WHERE 1=1
        and lpt.plan_id = $($planList.Plan_ID)
    	and lpt.Original_Plan_ID = $($planList.Plan_ID)
    ORDER BY task_id
    FOR JSON PATH
);
select @tasks as tasks
"@

$JsontaskList = Invoke-SqlCmd @sqlParams -query $query

# Add files to meta.log_files to prevent 'file does not exist' errors when logging file movements
# Write-Information ""
# Write-Information "------------------------------------"
# Write-Information "Prepare file tracking table..."
# Write-Information "------------------------------------"
# foreach ($task in $taskList) {
#     Write-Information ""
#     $filesToUpload = Get-ChildItem "_test/unittest-files/ingestion_pipeline/$($task.Task_Name)" -Recurse -File
#     Write-Information "Files to register in meta.file_tracking for task [$($task.Task_Name)]:" 
#     $filesToUpload | ForEach-Object { Write-Information "  $_"} 
#     foreach ($file in $filesToUpload.Name){
#         Write-Information "Registering file: $file"
#         $query = "
#         INSERT INTO meta.file_tracking (task_id, plan_id, registration_ts, original_filename, upload_ts, source_folder, extended_filename, landing_container, landing_folder, copied_status, copied_ts)
#         VALUES ($($task.Task_ID), $($task.Plan_ID), GETDATE(), '$file', GETDATE(), 'test/unit_test', '$file', 'landing', 'unit_test', 'true', GETDATE());
#         "
#         Exec-query($query)
#     }
# }


# If there are AdoVariables given, loop over them and expose them
# Dev-Note: Can only expose run_id, plan_id, and task_list
if ([bool]$AdoVariable) {
    Write-Information "Writing Ado variables..."
    foreach ($variable in $AdoVariable){
        Write-Information "Checking variable $variable..."
        if ($variable -like '*run_id*'){
            Write-Information "Set Run_ID variable"
            Write-Host "##vso[task.setvariable variable=${variable};]$($runPlan.Run_ID)"
        }
        elseif($variable -like '*plan_id*'){
            Write-Information "Set Plan_ID variable"
            Write-Host "##vso[task.setvariable variable=${variable};]$($planList.Plan_ID)"
        }
        elseif($variable -like '*task_list*'){
            Write-Information "Set Task List variable"
            Write-Host "##vso[task.setvariable variable=${variable};]$($JsontaskList.Tasks)"
        }
    }  
}

else{
    [hashtable] $variableList = @{
        runID = $($runPlan.Run_ID)
        planID = $($planList.Plan_ID)
        taskList = $($JsontaskList.Tasks)
    }

    write-host $variableList.GetType()
    write-host $variableList
    write-host $variableList.runID

    return $variableList
}

Write-Information ""
Write-Information "***End script: $($scriptName)***"

</file>
<file name="pipelines\scripts\sql\resume_sqldb.ps1">
&lt;#

.SYNOPSIS 
    Start an Az SQL Database if it is currently paused

.DESCRIPTION
    Check the current state of the SQL database on a SQL server.
    If the status is "paused:
        Launch a test query every (sleepSeconds) seconds until the connection is successful, or the (maxAttempts) was reached.
    If maxAttempts is reached, an exception error is raised.

#&gt;

[CmdletBinding()]
param (

    [Parameter(Mandatory = $true, HelpMessage="Name of the SQL Server ")]  
    [string] $ServerName,

    [Parameter(Mandatory = $true, HelpMessage="Name of the SQL Server Host")]  
    [string] $SQLServerInstance = $env:SQL_Server,

    [Parameter(Mandatory = $true, HelpMessage= "Name of the SQL Database to check")]  
    [string] $SQLDatabase = $env:SQL_Database,

    [Parameter(Mandatory = $true, HelpMessage = "Name of the Azure resource group where the SQL Server is located")] 
    [string] $ResourcegroupName = 'dev-dap-rg-core',

    [Parameter(Mandatory = $false, HelpMessage = "Token that gives access to the database")] 
    [string] $AccessToken = $null,

    [parameter(Mandatory = $false, HelpMessage="Number of attempts to start the SQL pool before going to error")] 
    [Int32] $maxAttempts = 4,

    [parameter(Mandatory = $false, HelpMessage="Wait time between start-attempts")] 
    [Int32] $sleepSeconds = 20

)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

$inputParameters = @( 
    "ServerName"
    "SQLServerInstance"
    "SQLDatabase"
    "ResourcegroupName"
    "maxAttempts"
    "sleepSeconds"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)   
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# ---------------------------------------------------------------------------------------------------------


# Get the database instance on the provided ServerName
Write-Information "Get database information..."
$databaseInstance = Get-AzSqlDatabase -ResourceGroupName $resourcegroupName -ServerName $ServerName -DatabaseName $SQLDatabase -WarningAction "ignore"
if ( -not $databaseInstance) {
    Throw "Database '$SQLDatabase' does not exist in resource group $resourceGroupName or you have no permission to access it."
}

# Get the status of the database (paused, started...)
$status = $databaseInstance.Status
Write-Information ("Current database state: " + $status)

if ( $status -eq "Paused") {

    # Install SqlServer module is not already installed
    if ( -not (Get-module SqlServer -ListAvailable) ) {
        Write-Information "Installing module: SqlServer ..."
        Install-Module -Name SqlServer -scope CurrentUser -Force
    }

    Write-Information "Resuming database..."
    
    # Get accesstoken if not provided
    if ( -not ($AccessToken) ) {
            Write-Information "Token not provided, obtaining..."
            $AccessToken = (Get-AzAccessToken -ResourceUrl https://database.windows.net).Token
        }

    # Group SQL parameters to invoke a SQL command 
    $sqlParams = @{
        ServerInstance = $SQLServerInstance
        Database = $SQLDatabase
        AccessToken = $AccessToken
        Query = "select 'hello from ' + db_name() as test"
    }

    $numTries = 0
    :retry_loop while( $true ) {

        # Try to query the database
        # If no error is thrown, break the loop
        try {
            Invoke-Sqlcmd @sqlParams
            Write-Information "  Success!"
            break retry_loop
        }  

        # If an error is thrown
        catch {
            # If timeout-error: continue script 
            if ( $_.Exception.Message -like "*The timeout period elapsed during the post-login phase*") {
                Write-Information ('  Failed: Database not yet available ... ("The timeout period elapsed during the post-login phase")')
            } 
            elseif ( $_.Exception.Message -like "*is not currently available*") {
                Write-Information ('  Failed: Database not yet available ... ("Database is not currently available")')
            } 
            else {
                # If other error: throw error and stop script
                Throw $_.Exception.Message
            }
            
        }

        # If numTries &lt; maxAttempts: Wait and continue loop
        if ( $numTries -lt $maxAttempts ) {
            $numTries ++
            Write-Information "Retrying after $sleepSeconds seconds..."
            Start-Sleep -Seconds $sleepSeconds
        } 
        # Else: Throw error
        else {
            Throw "Connection not successful after $maxAttempts attempts."
        }

    }

}
else {
    Write-Information "No action to take"
}


Write-Output "*** Ending script: $scriptName ***"

</file>
<file name="pipelines\scripts\sql\upload_to_tables.ps1">
&lt;#

.SYNOPSIS
    Upload configuration data to the SQL DB tables

.DESCRIPTION
    Initiate the deploy_data_configuration script for a (set of) configuration files

#&gt;


[CmdletBinding()]
param(
    [Parameter(Mandatory=$true)]  [string] $SQLServerInstance,                  # The SQL server you are trying to connect to
    [Parameter(Mandatory=$true)]  [string] $SQLDatabase,                        # The SQL Database you are trying to connect to
    [Parameter(Mandatory=$true)]  [string] $Accesstoken,                        # An accesstoken for the executor of the script
    [Parameter(Mandatory=$true)]  [string] $FolderPath,                         # The path of where to find the configuration files in the repo
    [Parameter(Mandatory=$true)]  [string] $FilterType,                         # Which configuration files to upload: ['all', 'path', 'filelist']
    [Parameter(Mandatory=$false)] [string] $Path,                               # If FilterType = path: Give the path where the configuration files are stored
    [Parameter(Mandatory=$false)] [array]  $FileList,                           # If FilterType = FileList: Give the list of configuration files to upload
    [Parameter(Mandatory=$false)] [string] $DeltaTableConfigurationFile,        # FilePath of the delta_tables.json file in the repository 
    [Parameter(Mandatory=$false)] [string] $WorkspaceFolder = $env:REPOROOT,    # Directory of where the git repo is stored on the device
    [Parameter(Mandatory=$false)] [string] $TargetEnvironment = $env:MYDEVENV
)

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Start script: $scriptName ***"

# force error action
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"


#region print parameters
# Define list of input parameters
$inputParameters = @( 
    "SQLServerInstance"
    "SQLDatabase"
    "FolderPath"
    "FilterType"
    "Path"
    "FileList"
    "DeltaTableConfigurationFile"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# endregion

$params = @{
    SQLServerInstance = $SQLServerInstance
    SQLDatabase = $SQLDatabase
    AccessToken = $Accesstoken
    TargetEnvironment = $TargetEnvironment
    WorkspaceFolder = $WorkspaceFolder
    ConfigurationFolderPath = $FolderPath
    DeltaTableConfigurationFile = $DeltaTableConfigurationFile
    FilterType = $FilterType
    FilterPath = $Path
    FileList = $FileList
    Recurse = $true
    Threads = 1
}

. $WorkspaceFolder/pipelines/scripts/sql/deploy_data_configuration.ps1 @params -InformationAction 'Continue'

# $synapseParams = @{
#     ServerInstanceInstance = $ServerInstance
#     Database = "LNL_META"
#     AccessToken = $Accesstoken
# }

# ### Gather data to insert in SQL tables
# $files = Get-Childitem -Path $FolderPath
# foreach($file in $files){
#     $queryContent = Get-Content -Path $file -Raw
#     Write-Output "Execute Query $file :"
#     Write-Output "  $queryContent"
#     Invoke-Sqlcmd @synapseParams -Query $queryContent -OutputSqlErrors $true
# }
</file>
<file name="pipelines\scripts\synapse\check_test_logs.ps1">
&lt;#

.SYNOPSIS
Grab the log files from an Azure container and convert it to an XML formatted set of files

.DESCRIPTION
During the testing process, the TestNotebook spark notebook generates a log file for all the tests that have been executed.
This file is published in an Azure storage container 'logs'. This script grabs the file from the container and converts it to an XML format accepted by Azure DevOps.
The resulting XML files are later published to DevOps by another script

.NOTES
This script also allows for a localdeploy option. When this switch is present, no XML files are generated but the content from the output file is printed in the terminal window

#&gt;

[CmdletBinding()]
param (
    [Parameter( Mandatory = $true)]  [string] $StorageAccount = 'devdapstdala1',         # Storage account of where to find the log file
    [Parameter( Mandatory = $true)]  [string] $ContainerName  = 'logs',                 # The container name of where the log file can be found
    [Parameter( Mandatory = $false)] [string] $WorkspaceFolder = $env:REPOROOT,         # The local folder where the repo is cloned
    [Parameter( Mandatory = $true)]  [string] $Outpath = 'src\synapse\_test\output',    # The path of where to output the generated XML files

    [Parameter( Mandatory = $false)] [Switch] $IsLocalDeploy                            # When deploying locally -&gt; Avoid generating XML files and just print results in terminal
)

# Force InformationPreference to Continue
$InformationPreference = 'Continue'

# Start region: Print all variables
$inputParameters = @( 
    "StorageAccount"
    "ContainerName"
    "WorkspaceFolder"
    "Outpath"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
#endregion

# Step 1: Validation
Write-Information "Executing validity checks"
# Check if Outpath exists in the Workspace. If false, create path. If true, remove and create
# Dev-Note: Remove and create should be replaced by a clean function of some sorts: Just get rid of all the files in the folder
$FullOutPath = Join-Path $WorkspaceFolder $Outpath
Write-Information "Full path: $FullOutPath"

If(!(test-path $FullOutPath)) {
    New-Item -ItemType Directory -Force -Path $FullOutPath | Out-Null
}

Else{
    Remove-Item -Path $FullOutPath -Recurse
    New-Item -ItemType Directory -Force -Path $FullOutPath | Out-Null
}

# Check if Storage Account exists
if (!($StorageAccount -in (Get-AzStorageAccount).StorageAccountName)){
    throw "Cannot find $StorageAccount given the current AzContext"
}

$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount

if (!($Containername -in (Get-AzStorageContainer -Context $Context).Name)){
    throw "Cannot find container '$Containername' in Storage Account '$StorageAccount'"
}


# Step 2: Get test log file
# The test logs file will be grabbed from the ADLS and landed in the TestResultPath 
$TestResultPath = Join-Path $FullOutPath 'output.txt'
Write-Information "Grab test logs from container '$ContainerName' and export to '$TestResultPath'"

# Get list of blobs in the Container
$blobs = Get-AzStorageBlob -Container $ContainerName -Context $Context 

# Loop over blobs: There should only be one non-empty file
# Write the non-empty file to the TestResultPath
$blobs | ForEach-Object {
    if ($_.Length -gt 0){ 
        Get-AzStorageBlobContent -Blob $_.Name -Container $ContainerName -Context $Context -Destination $TestResultPath -Force
    }
}


# Step 3: Convert test log file to csv table
Write-Information "Reconfigure test-file to a table"
# Get the content from the test file
$content = Get-Content -Path $TestResultPath 
$table_content= @()
# Loop over each line and write it to the content array table_content
foreach( $line in $content){
    # Hardcoded: The output file is built up in different sections separated by an empty line. The first section can be converted to a table-like structure
    #               while the test of the file contains the specific error messages (if any). Whenever this first empty line is read, the for-loop will break
    # Dev-Note: Analyze if this piece of code can be cleaner
    if ($line -eq ''){
        break
    }
    $table_content += $line
}

# Convert the array to a table with column names 'Name', 'Class', 'Skip', and 'Status'
# Name -&gt; Contains the name of the test that has been run (= the name of the function in Python)
# Class -&gt; Contains the name of the class that contains the function
# Skip -&gt; Contains no relevant information
# Status -&gt; Contains the status of the test: OK, ERROR, FAIL
$csv = $table_content | ConvertFrom-Csv -Delimiter " " -Header Name, Class, Skip, Status


# Step 4: Analyze test results
# If it is a local deploy, then only print in the terminal and don't create XML-files
# Else, create XML-files based on the CSV-table
if ($IsLocalDeploy){
    # Create empty lists to save names of broken functions for user-friendly output
    $List_Errors = New-Object Collections.Generic.List[String]
    $List_Fails = New-Object Collections.Generic.List[String]

    # Only add the objects that actually contain ERROR of FAIL as a status
    $csv | Where-Object { $_.Status -eq 'ERROR'} | ForEach-Object { $List_Errors.Add($_.Name) }
    $csv | Where-Object { $_.Status -eq 'FAIL'} | ForEach-Object { $List_Fails.Add($_.Name) }

    # If there are items in the list, you know there is an error
    if ($List_Errors.Count -ne 0){
        Write-Error "Detection of broken function(s): $List_Errors"
    }

    if($List_Fails.Count -ne 0){
        Write-Error "Detection of wrongly configured test(s): $List_Fails"
    }
}
else {
    # For each row in the table: Create an XML file following a specific template that can be read by the PublishTestResults task of Azure DevOps
    $csv | ForEach-Object {
        # Get rid of the (__main__\.*) structure around the name of the class
        # See test log file: (__main__\.&lt;class_name&gt;) -&gt; Only keep the &lt;class_name&gt;
        $pattern = '\(__main__\.|\)'
        $replacement = ''
        $new_value = $_.Class -replace $pattern, $replacement
        #Write-Information "  $($new_value)"
        $_.Class = $new_value
    }


    Write-Information "Reconfigure table as set of XML files"
    # Group the rows by Class-name
    # Create a testsuite for each individual class, containing the test results of each test in the class
    # Note: There is 1 XML file per Class (containing the results of the tests in the class), not per individual test
    $groups = $csv | Group-Object -Property Class
    Write-Output $groups
    foreach ($group in $groups) {
        $xml = New-Object -TypeName System.Xml.XmlDocument
        $root = $xml.CreateElement('testsuites')
        $xml.AppendChild($root)
        $suite = $xml.CreateElement('testsuite')
        $root.AppendChild($suite)
        # Dev-Note: Add more code to populate the xml document with the group data

        # Set the attributes of the testsuite element
        $suite.SetAttribute('name', $group.Name)        # Name of the class
        $suite.SetAttribute('tests', $($group.Count))   # Number of tests ran by the class

        # Loop over the tests from the group
        # Determine the number of failures and errors
        $failures = 0
        $errors = 0

        # Make an individual key for each function/test
        # If there was an error/failure, create a sub-key that gives the results
        # Dev-Note: sub-key does not give much information as of now -&gt; Check if the error message can also be returned
        foreach ($row in $group.Group) {
            $case = $xml.CreateElement('testcase')
            $suite.AppendChild($case)
            # Set the attributes of the testcase element
            $case.SetAttribute('classname', $row.Class) # Name of the class running the function
            $case.SetAttribute('name', $row.Name)       # Name of the function

            # Dev-Note: If ERROR or FAIL: Also get the error given by Python as a message -&gt; This message is contained in the output file but nothing is done with it
            if ($row.Status -eq 'ERROR'){
                $errors += 1
                $error_element = $xml.CreateElement('error')
                $case.AppendChild($error_element)
                $error_element.SetAttribute('message', 'Test failed')
                $error_element.SetAttribute('type', 'unittest.mock')
            }
            elseif ($row.Status -eq 'FAIL'){
                $failures += 1
                $failure_element = $xml.CreateElement('failure')
                $case.AppendChild($failure_element)
                $failure_element.SetAttribute('message', 'Test failed')
                $failure_element.SetAttribute('type', 'unittest.mock')
            }
        }

        $suite.SetAttribute('failures', $failures)
        $suite.SetAttribute('errors', $errors)

        # Write generated xml template to XML file with the name of the class
        $Outfile = Join-Path $WorkspaceFolder $Outpath "mock_result_$($group.Name).xml"
        $xml.Save($OutFile)
    }
  
}
</file>
<file name="pipelines\scripts\synapse\invoke_parallel_notebooks.ps1">
&lt;#

.SYNOPSIS
    Run multiple spark notebooks from a synapse workspace using mssparkutils.notebook.run()

.DESCRIPTION
    Whenever it would be needed to invoke a (list of) notebook(s) (with or without parameters) from Powershell/DevOps,
    this script can be used.
    First, a check happens to see (for the given spark pool and Session Name) whether an existing spark session is already active. If not, one is started.
    Second, the NotebookList array is looped over, invoking all notebooks with potentially a hashtable of parameters
    Lastly, a check makes sure that all commands have been completed and validates whether their was an error

.PARAMETER Notebooks
    This parameter is the most important parameter as it contains all the information to successfully invoke a notebook.
    The array is expected to contain a set of objects that look like this:
        @{
            &lt;NotebookName&gt; = @{
                folder = &lt;Name of the folder in Synapse where the notebook is stored&gt;
                parameterMap = @{
                    &lt;parameter_name_1&gt; = &lt;parameter_value_1&gt;; 
                    &lt;parameter_name_2&gt; = &lt;parameter_value_2&gt;;
                    etc.
                }
            }
        }
    
    The NotebookName is the reference to the Notebook that needs to be run. For this key, a set of parameters can be defined:
        - folder: This is the name of the folder where the Notebook is located. A check will be executed to validate this
        - parameterMap: This is an hashtable-object containing the list of parameters and their values that need to be passed to the notebook

    EXAMPLE
        [array] $Notebooks = @(
            @{
                MetaNotebook = @{
                    folder = "Workers"
                    parameterMap = @{
                        plan_list = @(); 
                        Plan_ID = -1; 
                        env_code = 'dev'
                    }
                }
            },
            @{
                MetaNotebook = @{
                    folder = "Workers"
                }
            }
        )

.NOTES
    Dev_note: Look for implementation of mssparkutils.notebook.runMultiple()
        https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python
#&gt;

[CmdletBinding()]
param (    
    [Parameter( Mandatory = $false, HelpMessage="Name of the pool used to run the notebook")]   
    [string] $PoolName = 'devdapsspcore',
    
    [Parameter( Mandatory = $false, HelpMessage="Name of the synapse workspace")]   
    [string] $SynapseWorkspace = $env:Synapse_WS,
    
    [Parameter( Mandatory = $false, HelpMessage="Name of the session that will be used, which will be annexed with a user identifier")]   
    [string] $SessionName = 'LocalInvoke',

    [Parameter( Mandatory = $false, HelpMessage="List of parameters taken by the Notebook")]  
    [array[]] $Notebooks = @(),

    [Parameter(Mandatory=$false, HelpMessage="Environment of where to deploy the database")] 
    [string] $TargetEnvironment = $env:MYDEVENV,

    [Parameter(Mandatory=$false, HelpMessage="Environment of where to deploy the database")] 
    [string] $StatusUpdateSeconds = 10
)

# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Information "*** Starting script: $scriptName ***"

# Start region: Print all variables
$inputParameters = @( 
    "PoolName"
    "SynapseWorkspace"
    "TargetEnvironment"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# ---------------------------------------------------------------------------------------------------------



# ---------------------------------------------------------------------------------------------------------
# Validate the environment and start a new spark session
# ---------------------------------------------------------------------------------------------------------
#region locate or create spark session

    # initialize the session variable
    if($env:AGENT_ID){
        $annex="Agent"
    }
    else{
        $annex = $env:USERNAME
    }
    $identifiedSessionName = "$($SessionName)_$($annex)"

    $sessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace $SynapseWorkspace -SynapseSparkPool $PoolName -SessionName $identifiedSessionName -WaitForSession $true -SetUpSession $false
#endregion


#region initialize session
    Write-Information "  Run session setup..."
    # make sure the mssparkutils are loaded
    $code = "from notebookutils import mssparkutils"    
    $statement = $sessionObj  | Invoke-AzSynapseSparkStatement -Language 'PySpark' -Code $code -AsJob

    while ($statement.Output.count -ne 1){
        if ($statement.Output.count -gt 1){
            throw "Too many items. Something went wrong"
        }
        Start-Sleep -Seconds 1 # wait for the async object to load
    }

    $statementId = $statement.Output.Id
    while (  $statement.State -notin ("available", "completed") ) {
        Write-Information "    Statement state is [$($statement.State)], check again in 10 seconds..."
        Start-Sleep -Seconds 10
        $statement = Get-AzSynapseSparkStatement -WorkSpaceName $WorkspaceName -SparkPoolName $PoolName -SessionId $sessionObj.Id | Where-Object { $_.Id -eq $statementId }
    }
    Write-Information "    Statement ready!"
#endregion


#region Execute Spark Statements
    # Print the current active session Id
    # Useful when manually tracking errors
    Write-Information("==========================================")
    Write-Information("Current active id: " + $sessionObj.Id)
    Write-Information("==========================================")
    # ---------------------------------------------------------------------------------------------------------


    # ---------------------------------------------------------------------------------------------------------
    # Code execution: Use mssparkutils.notebook.run to start a Notebook with a set of parameters
    # ---------------------------------------------------------------------------------------------------------

    # $StartDate = ([datetime]($RunInfo).RunStart.ToString('yyyy-MM-dd HH:mm:ss')).Add($ToLocalTimeOffset)

    # If the Synapse Workspace Exists: Run the code
    $synapweWorkspaceExists = Get-AzSynapsePipeline -WorkspaceName $SynapseWorkspace

    If($synapweWorkspaceExists) {
        # For each Notebook, run it with the given parameters
        $($Notebooks) | ForEach-Object {

            # Define a set of variables
            [PSCustomObject] $object = $_
            [string] $notebookName =  $object.Keys
            [string] $notebookFolder = $object.Values.folder

            # If the notebook exists in the given folder:
            if ((Get-AzSynapseNotebook -WorkspaceName $SynapseWorkspace -Name $notebookName ).Properties.Folder.Name -eq $notebookFolder ){
                Write-Information "Executing notebook $notebookName..." 

                # If the parameterMap exists in the object, convert it to a json. Else, pass an empty variable
                if( $object.Values.ContainsKey("parameterMap") ){
                    $jsonParameterObject = $object.Values.parameterMap | ConvertTo-Json
                }
                else{
                    $jsonParameterObject = ""
                }

                # Define the code to execute on the Spark Session
                [string] $code = "mssparkutils.notebook.run ('$($notebookFolder)/$($notebookName)', 3600, $($jsonParameterObject))"

                Write-Information "Executing code on sessionID $($sessionObj.Id):"
                Write-Information $code

                # Execute the code
                $sessionObj | Invoke-AzSynapseSparkStatement -Language 'PySpark' -code $code -AsJob
            }

            else{
                throw "Notebook $($notebookName) does not exist or it is not located in the given foldername $($notebookFolder)"
            }
        }


    }
    else{
        throw "Synapse workspace $($SynapseWorkspace) cannot be found..."
    }

#endregion


#region Wait for code to execute
    # ---------------------------------------------------------------------------------------------------------
    # Code completion: Make sure the statements have executed before going to the next step
    # ---------------------------------------------------------------------------------------------------------
    # The status of the statement should be different from 'waiting' or 'running'
    $StillRunning = $true
    while ($StillRunning){
        $StillRunning = $false
        $Logs = [array] @(Get-AzSynapseSparkStatement -WorkSpaceName $SynapseWorkspace -SparkPoolName $PoolName -SessionId $sessionObj.Id)

        while ($Logs.count -eq 0){
            Start-Sleep -Seconds 1 # wait for the async object to load
        }
        
        $Logs | ForEach-Object {
            if ( ($_.State -eq 'waiting') -or ($_.State -eq 'running') ) {
                $StillRunning = $true
            }
            else{
                # $duration = NEW-TIMESPAN –Start $StartDate –End (GET-DATE)
                # Write-Output "  In Progress... running time: $($duration.ToString('hh\:mm\:ss')) | Started: $($StartDate.ToString('HH:mm:ss')) | Current time: $((GET-DATE).ToString('HH:mm:ss'))"
                Start-Sleep $StatusUpdateSeconds
            }
        }
    }
#endregion

#region Validate code executions
    # ---------------------------------------------------------------------------------------------------------
    # Code validation: Check if any of the statements raised an error
    # ---------------------------------------------------------------------------------------------------------
    # If our logs-object contains a status = error: Throw an error and print the error statement
    if ($Logs.Output.Status.Contains("error")){
        $Logs | ForEach{
            $errors = $Logs | Where-Object { $_.State -eq 'error' }
            $errors | ForEach-Object { 
                #Write-Information "Error for statement: $($_.code) `n"
                Write-Information "ErrorValue: $($_.Output.ErrorValue) `n`n"
            }
        }

        # If there is an error, the current Spark session cannot be used anymore for the current setup as it will always have an error
        # Stop the session completely and throw an error
        Stop-AzSynapseSparkSession -WorkSpaceName $SynapseWorkspace -SparkPoolName $PoolName -LivyId $sessionObj.Id
        throw "$NotebookName has errors..."
    }
#endregion

# Write information for script user
Write-Information "*** Ending script: $scriptName ***"
</file>
<file name="pipelines\scripts\synapse\invoke_parallel_synapse_pipelines.ps1">
&lt;#

.SYNOPSIS
    Invoke a pipeline in Synapse with certain parameters

.DESCRIPTION
    To start a Synapse pipeline, you need to know in which Synapse workspace the pipeline is located. Next to that, you also need to give the name of the pipeline
    you want to start and the parameters needed to start the pipeline.
    Note: The pipelines do build on the metadata in SQL. Make sure the metadata is deployed (correctly) to the SQL DB before running a pipeline
    For a full run of the orchestration, use pipeline 'pl_start_run' with parameters 'Plan_Name' and 'New_Plan'
    
.PARAMETER Pipelines()
    This parameter is the most important parameter as it contains all the information to successfully invoke a Synapse pipeline.
    The array is expected to contain a set of objects that look like this:
        @{
            &lt;PipelineName&gt; = @{
                &lt;parameter_name_1&gt; = &lt;parameter_value_1&gt;; 
                &lt;parameter_name_2&gt; = &lt;parameter_value_2&gt;;
                etc.
            }
        }
    
    The PipelineName is the reference to the pipeline that needs to be run. For this key, a hashtable object can be defined:
        - This object contains the list of parameters and their values that need to be passed to the pipeline


    EXAMPLE
    $Pipelines = @(
        @{
            pl_start_run= @{
                    Plan_Name= 'unittest_synapse_dummy'
                    New_Plan= "True"
                }
        },
        @{
            pl_start_run = @{
                    Plan_Name= 'unittest_spark_dummy'
                    New_Plan= "True"
            }
        }, 
        @{
            pl_start_unzip= @{
                TaskID= 3
                PlanID= 4
            }
        }
    )

.NOTES 
    There is no possibility to run pipelines without a parameter-object defined using this script

#&gt;


[CmdletBinding()]
param (
    
    [Parameter(Mandatory=$false, HelpMessage="Name of the Synapse workspace environment where the pipeline is located")] 
    [string] $SynapseWSName = 'dev-dap-syn-core',

    [Parameter(Mandatory=$false, HelpMessage="Hashtable of the parameters needed to execute the pipeline")] 
    [array[]] $Pipelines = @(),

    [Parameter(Mandatory=$false, HelpMessage="How often do you want to have a status update on the running pipeline")] 
    [int] $StatusUpdateSeconds = 30
)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user
$scriptName = $script:MyInvocation.MyCommand.Path
Write-Information "*** Start script: $scriptName ***"


# Start region: Print all variables
# Define list of input parameters
$inputParameters = @(
    "SynapseWSName" ## Synapse workspace name (e.g. sbx-dap-syn-01)
    "Pipelines"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

# End region
# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------
# Pipeline Execution: Start the pipelines in sequence
# ---------------------------------------------------------------------------------------------------------

# Define a set of variables
[array] $runIDs = @()
[bool] $failedPipelineFlag = $False
[array] $failedPipelines = @()
# offset to convert synapse pipeline UTC to local time
[timespan] $ToLocalTimeOffset  = (Get-TimeZone).BaseUtcOffset

# If the SynapweWorkspace exists, invoke the pipelines
$SynapweWorkspaceExists = Get-AzSynapsePipeline -WorkspaceName $SynapseWSName
If($SynapweWorkspaceExists) {
    # Loop over the pipeline-objects
    $($Pipelines) | ForEach-Object {

        # Get the relevant values from the pipeline-object
        [PSCustomObject] $object = $_
        [string] $pipelineName =  $object.Keys
        [hashtable] $pipelineParameters = $object.Values

        Write-Information ""
        Write-Information "Starting pipeline $pipelineName ..." 

        # Invoke the pipeline
        $RunID = Invoke-AzSynapsePipeline -WorkspaceName $SynapseWSName -PipelineName "$PipelineName" -Parameter $pipelineParameters
        # Get metadata about the pipeline
        $RunInfo = Get-AzSynapsePipelineRun -WorkspaceName $SynapseWSName -PipelineRunId  $RunID.RunID
        # Add the RunID to the list of RunIDs to check for completion
        $newRunID = [PSCustomObject] @{pipelineName=$pipelineName; runID=$($RunInfo.RunId)}
        $runIDs += ($newRunID)
        
        # User information
        Write-Information "  Pipeline triggered!"
        Write-Information "  RunID: $($RunInfo.RunId)"
        Write-Information "  Started: $($RunInfo.RunStart)`n"
        Write-Information "with parameters:"
        Write-Information ($pipelineParameters.Keys | ForEach-Object { "$_ $($pipelineParameters[$_])" }) #-join "|"
    }

    # User information: Print all the pipeline names and their runID
    $($runIDs) | ForEach-Object {
        [string] $pipelineName = $_.pipelineName
        [string] $pipelineRunID = $_.runID

        Write-Information "$($pipelineName): $($pipelineRunID)"
    }

    
    # Loop over the runIDs and validate if they are completed
    # Dev-Note: Only 1 pipeline is being analysed at the same time. 
    #   Since all pipelines are being initiated at the same time, there is no need to check all pipelines at the same time
    #   The total time taken for this for-loop will be the time taking by the longest pipeline
    $($runIDs) | ForEach-Object {
        [string] $pipelineName = $_.pipelineName
        [string] $pipelineRunID = $_.runID
        
        $runInfo =  Get-AzSynapsePipelineRun -WorkspaceName $SynapseWSName -PipelineRunId  $pipelineRunID
        $RunStatus = $runInfo.Status;
        $Retries = 0;
        $MaxRetries = 5;
        # While the pipeline is running, sleep and check status until completed
        While (($RunStatus -eq "InProgress" ) -or ($RunStatus -eq "Queued" ) ) {
            try {
                $runInfo = Get-AzSynapsePipelineRun -WorkspaceName $SynapseWSName -PipelineRunId  $pipelineRunID
                $Retries = 0
            } catch {
                $Retries += 1
                if ( $Retries -le $MaxRetries) {
                    Write-Information "An error occured while getting the runInfo:";
                    Write-Information $_;
                    Write-Information "Retrying in $StatusUpdateSeconds ...";
                    Continue;   
                } else {
                    throw
                }
            }
            $RunStatus = $runInfo.Status
            $StartDate = ([datetime]($runInfo).RunStart.ToString('yyyy-MM-dd HH:mm:ss')).Add($ToLocalTimeOffset)
            #$LastUpdated = ([datetime]($runInfo).LastUpdated.ToString('yyyy-MM-dd HH:mm:ss')).Add($ToLocalTimeOffset)
            $duration = NEW-TIMESPAN –Start $StartDate –End (GET-DATE).Add($ToLocalTimeOffset)
            Write-Information "  In Progress... running time: $($duration.ToString('hh\:mm\:ss')) | Started: $($StartDate.ToString('HH:mm:ss')) | Current time: $((GET-DATE).ToString('HH:mm:ss')).Add($ToLocalTimeOffset)"
            Start-Sleep $StatusUpdateSeconds
        }

        # When finished: Print the status and total duraction of the pipeline
        $duration = ([TimeSpan]::FromMilliseconds($runInfo.DurationInMs)).ToString('hh\:mm\:ss')
        Write-Information "Finished with status $RunStatus, duration: $duration"    
        if ($runInfo.Status -ne "Succeeded"){
            $failedPipelineFlag = $True
            $newFailedPipeline = [PSCustomObject] @{pipelineName=$pipelineName; Message=$($RunInfo.Message)}
            $failedPipelines += $newFailedPipeline
        }

    }

    # If a pipeline failed: Print the return message
    if ($failedPipelineFlag){
        $($failedPipelines) | ForEach-Object {
            Write-Information "There was an error with running pipeline $($_.pipelineName). Returned message was: ""$($_.Message)"" "
        }
        throw "Error running pipelines..."
    }
}
else{
    throw "Synapse workspace $($SynapseWorkspace) cannot be found..."
}


# Write information for script user
Write-Information "*** Ending script: $scriptName ***"
</file>
<file name="pipelines\scripts\synapse\invoke_sparkpools.ps1">
#requires -Modules "Az.Synapse"

&lt;#

.SYNOPSIS
Invoke, run, or resume one or more spark pools from a specific Azure Synapse Workspace

.DESCRIPTION
This script looks for active sparkpools, and/or starts sparkpools in a specific Azure Synapse workspace

If a sessionName is provided, the scripts any existing (re)usable session with that name. 
A new sessions is started if no active session was found.

.PARAMETER WaitForSession
If set to $true (default), the script will wait for the session to start, and run some initial configuration.
If set to $false, don't wait for the session to be available.

.OUTPUTS
A PSSynapseSparkSession object.

#&gt;

[CmdletBinding()]
param (
    [Parameter(Mandatory = $false, HelpMessage = "Name of the synapse workspace")]    
    [string] $WorkspaceName = 'dev-dap-syn-core',
    [Parameter(Mandatory = $false, HelpMessage = "Name of the pool available inside of the workspace")]    
    [string] $PoolName = 'devdapsspcore',
    [Parameter(Mandatory = $false, HelpMessage = "Name of the sessions that will be used, which will be annexed with a user identifier")]   
    [array] $SessionNames = @("LocalTestSimon"),
    [Parameter(HelpMessage = "Controls whethere the script will wait for the session to be ready (default), or not")]
    [bool] $WaitForSession = $true,
    [Parameter(HelpMessage = "Environment of Synapse workspace")]
    [string] $TargetEnvironment
)


# local script settings
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'

#region script info
    $inputParameters = $MyInvocation.MyCommand.Parameters.Keys | 
    Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::CommonParameters } | 
    Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::OptionalCommonParameters }
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information "----------------------------------------------------------------------"
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
    Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
    Write-Information "  Parameters:"
    foreach ($param in $inputParameters) {
        Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
    }
    Write-Information "----------------------------------------------------------------------"
#endregion script info

# Loop over the list of SessionNames and start a session for each one
$SessionNames | ForEach-Object {
    #region locate or create spark session

        # initialize the session variable
        if($env:AGENT_ID){
            $annex="Agent"
        }
        else{
            $annex = $env:USERNAME
        }
        $identifiedSessionName = "$($_)_$($annex)"

        

        $sessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace $WorkspaceName -SynapseSparkPool $PoolName -SessionName $identifiedSessionName -WaitForSession $WaitForSession -SetUpSession $false

    #endregion

    #region initialize session
        # only if WaitForSession is enabled (in that case by now we are sure there is a valid session)
        if ( $WaitForSession) { 
            Write-Information "  Run session setup..."
            # make sure the mssparkutils are loaded
            $code = "from notebookutils import mssparkutils"   
            $statement = $sessionObj  | Invoke-AzSynapseSparkStatement -Language 'PySpark' -Code $code -AsJob

            while ($statement.Output.count -ne 1){
                if ($statement.Output.count -gt 1){
                    throw "Too many items. Something went wrong"
                }
                Start-Sleep -Seconds 1 # wait for the async object to load
            }

            $statementId = $statement.Output.Id
            while (  $statement.State -notin ("available", "completed") ) {
                Write-Information "    Statement state is [$($statement.State)], check again in 10 seconds..."
                Start-Sleep -Seconds 10
                $statement = Get-AzSynapseSparkStatement -WorkSpaceName $WorkspaceName -SparkPoolName $PoolName -SessionId $sessionObj.Id | Where-Object { $_.Id -eq $statementId }

            }
            Write-Information "    Statement ready!"
        }
    #endregion

    # return PSSparkSession to the caller
    $SessionObj
}

# Write information for script user
Write-Information ""
Write-Information "----------------------------------------------------------------------"
Write-Information "End script: $($MyInvocation.MyCommand.Name)"
Write-Information "----------------------------------------------------------------------"

</file>
<file name="pipelines\scripts\synapse\overwrite_synapse_params.ps1">
&lt;#

.SYNOPSIS
Overwrite parameters from TemplateParametersForWorkspace file to deploy Synapse Workspace with the correct parameters

.DESCRIPTION
The DevOps task Synapse Workspace Deployment@2 generates an ARM template for the Synapse workspace, as well as a 
parameters file TemplateParametersForWorkspace.
The parameters in this file need to be overwritten to fit with the target environment of the deployment. 

For example, the linked services pointing to the dev-environment data lake storage should be converted to point to 
the int-environment ADLS.

This script will analyse the TemplateParametersForWorkspace file and compare it to the configuration/synapse/parameters.json file:
    1. If there is a match for the keys, overwrite the parameter with the configured parameter
    2. If the parameter refers to a bigDataPool (= spark pool), overwrite the (normally) empty value with the name of the spark pool used in the target environment
    3. Throw warnings:
        1. If the TemplateParametersForWorkspace contains a non-empty value but there is no configuration for this parameter in the parameters.json file
        2. If the parameters.json file contains a configuration for a parameter, but this parameter is not in the TemplateParametersForWorkspace file
#&gt;


[CmdletBinding()]
param(
    [Parameter( Mandatory = $true)]    [string] $TargetEnvironment      = 'sbx',
    [Parameter( Mandatory = $true)]    [string] $ArmTemplatePath        =  'C:\Users\pla917\dap_appl_dala_code\src\synapse\_test\workspace_templates\TemplateParametersForWorkspace.json', 
    [Parameter( Mandatory = $false)]   [string] $WorkspaceFolder        = $env:REPOROOT
)

# Force information action to 'continue'
$InformationPreference = "continue"


# Start region: Print all parameters
$inputParameters = @( 
    "TargetEnvironment"
    "ArmTemplatePath"
    "WorkspaceFolder"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
#endregion


# Step 1: Validation
# Check that ConfigFile of the default parameters exists
$ConfigFile = Join-Path $WorkspaceFolder "/configuration/synapse/parameters.json"
Write-Information "Checking that file exists: $ConfigFile ..."
$exists = Test-Path $ConfigFile
if ( -not $exists) {
    Throw "ConfigFile not found: $ConfigFile"
}

$configData = Get-Content -Path $configFile -Raw | ConvertFrom-Json


# Validate that there are parameters to replace in the TemplateParametersForWorkspace
if (!(Get-Content -Path $ArmTemplatePath -Raw | ConvertFrom-Json).parameters){
    # Check if there are any parameters in your template that need to be overwritten
    Throw "The parameter file does not contain any parameters"
}

#region Step 2: Build dictionary of replacement values

    $synapseReplacements = @{}

    # first load the "default" configuration
    $defaultConfig = $configData.default
    Write-Information "Loading default configuration..."
    foreach ( $parameter in ($defaultConfig | get-member -MemberType NoteProperty).Name ) {
        Write-Information " $parameter : $($defaultConfig.$parameter)"
        $synapseReplacements.Add($parameter, $defaultConfig.$parameter)
    }

    # now apply any target_environment specific values
    $environmentConfigExists = ($configData.PSobject.Properties.name -match $TargetEnvironment) 
    if ( ! $environmentConfigExists ) {
        Write-Information "No environment-specific parameter section was found."
    } else {
        Write-Information "Apply environment-specific parameters..."
        $specificConfig = $configData.$TargetEnvironment
        foreach ( $parameter in ($specificConfig | get-member -MemberType NoteProperty).Name ) {
            Write-Information " $parameter : $($specificConfig.$parameter)"
            if( $synapseReplacements.ContainsKey($parameter) ) {
                $synapseReplacements[$parameter] = $specificConfig.$parameter
            } else {
                $synapseReplacements.Add($parameter, $specificConfig.$parameter)
            }
        }   
    }

#endregion

#region Step 3: Override parameters for target environment
    Write-Information "Overwrite parameters..."

    # Gather the ARM-template so you can start working with those variables
    $ParameterTemplate = Get-Content -Path $ArmTemplatePath -Raw | ConvertFrom-Json

    # Loop over all the parameters in the ARM parameter template (= TemplateForWorkspace)
    # If there is a match with the default config data, overwrite with target environment values
    # If there is no match:
    #   Warning: ARM contains the parameter with a non-empty value
    #   Warning: Default config contains a parameter that is not in the ARM template
    #   Information: Empty parameter value has been found but no default configuration was given
    foreach ( $parameter in ($ParameterTemplate.parameters | get-member -MemberType NoteProperty).Name) {
        # Tell the user which parameter you're checking and which current value it has inside of the ARM-template
        Write-Information "======================="
        Write-Information "  Checking parameter: $parameter"
        Write-Information "     current ARM-template value: $($ParameterTemplate.parameters.$parameter.value)"

        # Generate an empty array to make sure that you can save the updated parameters somewhere
        $changedParameters = @()

        # Then, inside of the parameters.json, go over all the parameters that need to be overwritten
        foreach ($defaultKeyPair in $synapseReplacements.GetEnumerator()){
            # If there is a regex match between the default and the ARM-template variable, overwrite it
            if ($parameter -match $($defaultKeyPair.Name)){

                Write-Information " Overwriting parameter '$($parameter)' with value '$($defaultKeyPair.Value)'"
                $ParameterTemplate.parameters.$parameter.value = $defaultKeyPair.Value

                # Store the parameter that's changed inside of an array, so that you can check on it later for warnings
                $changedParameters += $parameter

            } 

        }

        # If there are elements in the array, then you know that something was changed
        # If there's nothing there, then nothing was changed and other things need to be checked then
        if($changedParameters.Count -eq 0){
            # Warning: Default config contains a parameter that is not in the ARM template
            if (($synapseReplacements | get-member -MemberType NoteProperty).Name -contains $parameter){
                $value =  $ParameterTemplate.parameters.$parameter.value
                Write-Warning "    There is a value $value configured for parameter $parameter. However, the ARM template does not contain this parameter so none can be overwritten"
            }

            # Warning: ARM contains the parameter with a non-empty value
            elseif ($ParameterTemplate.parameters.$parameter.value){
                Write-Warning "    The ARM template contains a non-empty value '$($ParameterTemplate.parameters.$parameter.value)' for parameter $parameter. This value is however not configured to be overwritten."
            }

            # Information: Empty parameter value has been found but no default configuration was given
            else{
                Write-Information "    No overwrite needed for $parameter"
            }
        }
    }
#endregion


#region Step 4: Save overwritten parameter template 
    Write-Information "Writing updated ARM template to test.json"
    $ParameterTemplate | ConvertTo-Json -depth 32 | Set-Content -Path $ArmTemplatePath -Force
#endregion
</file>
<file name="pipelines\scripts\synapse\tokenization\detokenize_synapse_arm.ps1">
&lt;#

.SYNOPSIS
    Detokenize a set of json files

.DESCRIPTION
    During the build of the Synapse Workspace ARM template, the managed private endpoints were tokenized.
    This script will detokenize the values before the ARM deploy.

.NOTES
    Tokens are string-values with a certain (set of) start and end character(s). These strings need to be replaced depending during deployment time, 
    as the strings do not resemble a meaningful value.
    
.EXAMPLE
    #{storage_account}# will be replaced by the variable storage_account (which will be set in this script)

#&gt;


[CmdletBinding()]
param(

    [Parameter(Mandatory=$false, HelpMessage="Azure Subscription ID")] 
    [string] $SubscriptionID = "07f1b2e8-52dc-4020-967e-3eacb668d07a",

    [Parameter(Mandatory=$false, HelpMessage="Resource group of the Synapse Workspace")] 
    [string] $CoreResourceGroup = "dev-dap-core-rg",

    [Parameter(Mandatory=$true, HelpMessage="Azure Subscription ID")] 
    [psobject] $ARMTemplate = "$($env:REPOROOT)\TemplateForWorkspace (20).json",

    [Parameter(Mandatory=$true, HelpMessage="Resource group of the Synapse Workspace")] 
    [psobject] $ConfigurationPath = "$($env:REPOROOT)\configuration\synapse\synapse_workspace_tokens.json",
    
    [Parameter(Mandatory=$false, HelpMessage="Resource group of the Synapse Workspace")] 
    [psobject] $TargetEnvironment = $env:MYDEVENV
       

)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables

$inputParameters = @( 
    "SubscriptionID"
    "CoreResourceGroup"
    "ARMTemplate"
    "ConfigurationPath"
    "TargetEnvironment"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------


# Check if the path exists in the workspace
if ( -not (Test-Path -Path $ConfigurationPath -PathType "leaf")) {
    throw "Could not fine synapse workspace configuration file: $ConfigurationPath"
}

# Read the contents of the managed_private_endpoints.json configuration file
[psobject] $configurationObjects = Get-Content -Raw -Path $ConfigurationPath 

&lt;#
Joachim: 
by now in the deployment process, tokens in the configuration files should already have been handled
so this logic should be removed. 
#&gt;

# Replace all the tokens in the configurationObjects script (configuration\synapse\synapse_workspace_tokens.json)
[psobject] $detokenizedConfigObjects = Get-DetokenizedString -stringValue $configurationObjects -StartToken "#{" -EndToken "}#"

# Convert the detokenized configuration to extract the members
[psobject] $detokenizedJsonConfig = $detokenizedConfigObjects | ConvertFrom-Json

# Loop over all Managed Private Endpoint (MPE) configuration objects if there are any 
# Note: The code expects there to be a configuration for all MPEs. If there is an MPE that has no replacement values configured, the Get-DetokenizedString function will fail.
if ( Get-Member -InputObject $detokenizedJsonConfig -name "managed_private_endpoints" ){
    $detokenizedJsonConfig.managed_private_endpoints | ForEach-Object {
        # Create a set of variable that will be used to replace the tokens
        # Reason: The Get-DetokenizedString will replace all the tokens in the ARM template file. This file contains tokens #{&lt;token_name&gt;}# 
        #   -&gt; The module will look for a variable with the same name to replace the &lt;token_name&gt; with

        # The tokens were configured to use '_' instead of '-' because the variables (see further) cannot use '-'
        # Create an additional variable where the '-' have been replaced by '_'

        [string] $mpeName = $_.PSObject.Properties.Name
        Write-Information "`nChecking MPE: $($mpeName)"
        [string] $mpeName_ = $mpeName -replace "-", "_"
        # Get the configured tokens for each 
        [object] $mpeConfiguration = $_.PSObject.Properties.Value
        
        # Every MPE has a reference to a subscription and a resource group, regardless of the configuration
        # If there is none configured, use the one passed as a parameter
        if( ! ( Get-Member -InputObject $mpeConfiguration -name "subscription" -MemberType "Properties") ){
            New-Variable -Name "$($mpeName_)_subscription" -Value "$($SubscriptionID)" -Force -Scope script
            Write-Information "     New variable '$($mpeName_)_subscription' with value '$($SubscriptionID)'"

        }
        if( ! (Get-Member -InputObject $mpeConfiguration -name "resource_group" -MemberType "Properties" ) ){
            New-Variable -Name "$($mpeName_)_resource_group" -Value "$($CoreResourceGroup)" -Force -Scope script
            Write-Information "     New variable '$($mpeName_)_resource_group' with value '$($CoreResourceGroup)'"

        }
        
        # Expose all configured  properties to variables
        $mpeConfiguration | Get-Member -MemberType NoteProperty | ForEach-Object{
            $Property = $_.Name
            $PropertyValue = $mpeConfiguration.$Property

            New-Variable -Name "$($mpeName_)_$($Property)" -Value "$($PropertyValue)" -Force -Scope script
            Write-Information "     New variable '$($mpeName_)_$($Property)' with value '$($PropertyValue)'"
        }
    }
}

# Replace all the targetSparkConfiguration tokens by the actual default value configured in the synapse_workspace_token.json file
if (Get-Member -InputObject $detokenizedJsonConfig -name "notebooks" -MemberType "Properties"){

    # Dev-Note: Whenever there would be a need for multiple spark configurations, this code will need to be made more dynamic.
    New-Variable -Name targetSparkConfiguration -Value  $detokenizedJsonConfig.notebooks.default.targetSparkConfiguration -Force -Scope script
    Write-Information "New variable 'targetSparkConfiguration' with value '$($detokenizedJsonConfig.notebooks.default.targetSparkConfiguration)'"

}

# Use the exposes variables to overwrite the tokens in the ARM template
$armContent = Get-Content -Raw -Path $ARMTemplate #| ConvertFrom-Json
$detokenizedARM = Get-DetokenizedString -stringValue $armContent -StartToken "#{" -EndToken "}#"

# Overwrite the ARM template with the detokenized one
$detokenizedARM | Set-Content -Path $ARMTemplate -Force


# ---------------------------------------------------------------------------------------------------------
# Write information for script user
Write-Output "*** Ending script: $scriptName ***"
</file>
<file name="pipelines\scripts\synapse\tokenization\tokenize_managed_private_endpoints.ps1">
&lt;#

.SYNOPSIS
    Tokenize a set of JSON files

.DESCRIPTION
    When building the ARM template of the Synapse workspace, the managed private endpoint values and reference names need to be overwritten. 
    This also includes the subscription id and resource group.
    This script will tokenize the values before the ARM template is build. 
    Another script will then overwrite the tokens with the correct value depending on the environment the ARM template is deployed to.

#&gt;


[CmdletBinding()]
param(
    [Parameter(Mandatory=$false, HelpMessage="Root of the working directory")] 
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Path to the JSON files that need to be tokenized")] 
    [string] $Path = "src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint"

)


# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all parameters

$inputParameters = @( 
    "WorkspaceFolder"
    "Path"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------

# Convention: The environment code will always be 3 characters, but unknown during the build stage. The regex experession complies with these conventions
$environment_code = '[a-z]{3}'
# The Synapse Workspace name will, according to conventions, always look like xxx-dap-syn-core with xxx being the target environment
$synapseWorkspace = "$($environment_code)-dap-syn-core"
# The ADLS instance linked to the Synapse Workspace will, according to conventions, always look like xxxdapstcoresyn with xxx being the target environment
$synapseContainer = "$($environment_code)dapstcoresyn"
# Array: List of default MPEs created during Synapse workspace infrastructure deployment
# Dev-Note: Explicit sbx-references need to be removed, but dev-environment still contains the references
[array] $defaultMPEs = @("synapse-ws-custstgacct--$($synapseWorkspace)-$($synapseContainer)", "synapse-ws-sql--$($synapseWorkspace)", "synapse-ws-sqlOnDemand--$($synapseWorkspace)", "synapse-ws-custstgacct--sbx-dap-syn-01-sbxdapstadls01", "synapse-ws-sql--sbx-dap-syn-01", "synapse-ws-sqlOnDemand--sbx-dap-syn-01")

# The entire path to the folder, including the workspace
$folderPath = Join-Path $WorkspaceFolder $Path

# Check if the path exists in the workspace
if ( -not (Test-Path -Path $folderPath -PathType "Container")) {
    throw "Could not fine modulesFolder: $folderPath"
}

# Get list of MPEs files
$jsonFiles = Get-ChildItem -Path $folderPath -Filter "*.json" 

# Loop over each file
foreach ($file in $jsonFiles){
    # Get the content from the files and convert it to JSON
    $mpeContent = Get-Content -Raw -Path $file
    $mpeJsonContent = $mpeContent | ConvertFrom-Json

    # Get the name of the MPE and its properties
    [string] $mpeName = $mpeJsonContent.name
    [psobject] $properties = $mpeJsonContent.properties

    # If the MPE is not in the $defaultMPE array, tokenize the properties
    if( ! $defaultMPEs.Where({ $mpeName -match $_ })){        
        Write-Information "Managed private endpoint: $($mpeName)"

        # Replace all '-' by '_': Causes issues during detokenization
        $mpeName = $mpeName -replace "-", "_"

        # Create variable privateLinkResource
        [string] $privateLinkResource = $properties.privateLinkResourceId
    
        # Replace subscription ID with #{subscription}#
        $privateLinkResource = $privateLinkResource -replace "/subscriptions/[\w-]+", "/subscriptions/#{$($mpeName)_subscription}#"
        # Replace resource group name with #{resource_group}#
        $privateLinkResource = $privateLinkResource -replace "/resourceGroups/[\w-]+", "/resourceGroups/#{$($mpeName)_resource_group}#"

        # Provider replacements
        # Dev-Note: Might need more in the future.
        $privateLinkResource = $privateLinkResource -replace "/storageAccounts/[\w-]+", "/storageAccounts/#{$($mpeName)_storage_account}#"
        $privateLinkResource = $privateLinkResource -replace "/servers/[\w-]+", "/servers/#{$($mpeName)_server}#"
        $privateLinkResource = $privateLinkResource -replace "/vaults/[\w-]+", "/vaults/#{$($mpeName)_vault}#"
        $privateLinkResource = $privateLinkResource -replace "/workspaces/[\w-]+", "/workspaces/#{$($mpeName)_workspace}#"

        # Overwrite the private link of the MPE with the tokenized value
        $properties.privateLinkResourceId = $privateLinkResource

        # If the MPE contains a property 'fqdns': tokenize
        if( Get-Member -InputObject $properties -name "fqdns"){
            # Overwrite the fqdns of the MPE with the tokenized value
            $properties.fqdns = @("#{$($mpeName)_fqdns}#")
        }
    }

    # Overwrite the current content of the MPE files with the tokenized content
    $mpeJsonContent | ConvertTo-Json | Set-Content $file
}



# ---------------------------------------------------------------------------------------------------------
# Write information for script user
Write-Output "*** Ending script: $scriptName ***"
</file>
<file name="pipelines\scripts\synapse\tokenization\tokenize_synapse_notebooks.ps1">
&lt;#

.SYNOPSIS
    Tokenize a set of JSON files

.DESCRIPTION
    This script will tokenize the notebooks of the Synapse workspace.
    Currently, the following references will be tokenized:
        - Target Spark Configuration
            If there is a sparkConfiguration configured for the notebook, it will be overwritten by the token #{targetSparkConfiguration}#
            If there is no configuration configured for the notebook, a reference will be made with token #{targetSparkConfiguration}#

#&gt;


[CmdletBinding()]
param(
    [Parameter(Mandatory=$false, HelpMessage="Root of the working directory")] 
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Path to the JSON files of the Synapse notebooks that need to be tokenized")] 
    [string] $Path = "src\synapse\studio\notebook"

)

# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables

$inputParameters = @( 
    "WorkspaceFolder"
    "Path"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# ---------------------------------------------------------------------------------------------------------


# The entire path to the folder, including the workspace
$folderPath = Join-Path $WorkspaceFolder $Path

# Check if the path exists in the workspace
if ( -not (Test-Path -Path $folderPath -PathType "Container")) {
    throw "Could not fine modulesFolder: $folderPath"
}

# Get list of files from the folder
$jsonFiles = Get-ChildItem -Path $folderPath -Filter "*.json" 

# Loop over each notebook file
foreach ($file in $jsonFiles){
    # Get the content from the files and convert it to JSON
    $notebookContent = Get-Content -Raw -Path $file
    $notebookJsonContent = $notebookContent | ConvertFrom-Json

    # Get the name of the notebook and its properties
    [string] $notebookName = $notebookJsonContent.name
    [psobject] $properties = $notebookJsonContent.properties

    # Write the name of the notebook that has been found
    Write-Information "Notebook: $($notebookName)"

    # Create token targetSparkConfiguration 
    # Dev-Note: Whenever there would be a need for multiple spark configurations, this code will need to be made more dynamic.

    # If there is already a reference to a spark configuration: Overwrite the reference by token "#{targetSparkConfiguration}#
    if( Get-Member -InputObject $properties.metadata -name "targetSparkConfiguration" -MemberType "Properties" ){
        $properties.metadata.targetSparkConfiguration = "#{targetSparkConfiguration}#"

    }

    # If there is no reference to a spark configuration: Create a reference and give it token value "#{targetSparkConfiguration}#
    else{
        $properties.metadata | Add-Member -Name "targetSparkConfiguration" -value "#{targetSparkConfiguration}#" -MemberType NoteProperty
    }

    # Overwrite the current content of the notebook file with the tokenized content
    $notebookJsonContent | ConvertTo-Json -depth 32 | Set-Content $file
}



# ---------------------------------------------------------------------------------------------------------
# Write information for script user
Write-Output "*** Ending script: $scriptName ***"
</file>
<file name="pipelines\scripts\synapse\tokenization\tokenize_synapse_workspace.ps1">
&lt;#

.SYNOPSIS
    Tokenize a set of JSON files

.DESCRIPTION
    This script will go over different other PowerShell scripts, each of them tokenizing a specific set of Synapse Artefact files.
    The Powershell scripts are stored in directory: pipelines/scripts/synapse/tokenization


#&gt;


[CmdletBinding()]
param(
    [Parameter(Mandatory=$false, HelpMessage="Root of the working directory")] 
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Path to the JSON files that need to be tokenized")] 
    [string] $StudioPath = "src\synapse\studio\"

)

# Force switch parameters
Set-StrictMode -Version "latest"
$ErrorActionPreference = "Stop"
$InformationPreference = "Continue"


# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all parameters

$inputParameters = @( 
    "WorkspaceFolder"
    "StudioPath"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
# ---------------------------------------------------------------------------------------------------------


# Invoke script: tokenize_managed_private_endpoints.ps1
# This script will tokenize the managed private endpoint artefacts of the Synapse workspace
Write-Information "Tokenize Managed Private Endpoints..."
$mpePath = Join-Path $StudioPath "managedVirtualNetwork\default\managedPrivateEndpoint"
. $WorkspaceFolder/pipelines\scripts\synapse\tokenization\tokenize_managed_private_endpoints.ps1 -WorkspaceFolder $WorkspaceFolder -Path $mpePath


# Invoke script: tokenize_synapse_notebooks.ps1
# This script will tokenize the notebook artefacts of the Synapse workspace
Write-Information "Tokenize Synapse Notebooks..."
$notebookPath = Join-Path $StudioPath "notebook"
. $WorkspaceFolder/pipelines\scripts\synapse\tokenization\tokenize_synapse_notebooks.ps1 -WorkspaceFolder $WorkspaceFolder -Path $notebookPath



# ---------------------------------------------------------------------------------------------------------
# Write information for script user
Write-Output "*** Ending script: $scriptName ***"
</file>
<file name="pipelines\scripts\synapse\validate_workers\validate_filworker.ps1">
&lt;#

.SYNOPSIS
    Script executes validation checks to make sure the FilWorker behaved as expected

.DESCRIPTION
    This script will execute the following checks:
        1. Check landing for the absence of the original file &amp; presence of the newly converted file
        2. Check the archive container for the presence of the original file
        3. Check the contents of the converted file .fil -&gt; csv-file to see if everything was converted properly
        4. Check the logs in the MetaDB

#&gt;

[CmdletBinding()]
param(
    [Parameter( Mandatory = $false, HelpMessage="The local folder where the script is based")]
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Name of the environment we're currently in" )]
    [string] $TargetEnvironment = $env:MYDEVENV,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SQLServerInstance = $env:SQL_SERVER,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SQLDatabase = $env:SQL_DATABASE,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $Accesstoken

)

# Force switch parameters
Set-StrictMode -Version "latest"
$InformationPreference = 'Continue'
$ErrorActionPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

#Startregion
# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters 
$inputParameters = @( 
    "WorkspaceFolder"
    "TargetEnvironment"
    "SQLServerInstance"
    "SQLDatabase"
    "Accesstoken"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
Write-Information ""
#Endregion


# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------

### PREP: VALIDATION
if (!$Accesstoken){
    Write-Information "Access token not provided, obtaining..."
    $Accesstoken = (Get-AzAccessToken -ResourceUrl https://database.windows.net).Token
}

### Metadb-params
$sqlParams = @{
    SQLServerInstance = $SQLServerInstance
    SQLDatabase = $SQLDatabase
    AccessToken = $Accesstoken
}

## Storage Account checks
[array] $OriginalFileList = @('firstrow_to_headers.fil')
[array] $ConvertedFileList = @('firstrow_to_headers.csv')

### 1A. The original file should be removed from Landing
. $WorkspaceFolder/pipelines/scripts/datalake/file_container_existence.ps1 `
-WorkspaceFolder $WorkspaceFolder `
-TargetEnvironment $TargetEnvironment `
-ContainerName 'landing' `
-BlobPrefix 'preprocess_test/filworker_test/prd/20240802_112046' `
-FileList $OriginalFileList `
-PresenceCheck $false `
-InformationAction 'Continue'

### 1B. Landing should contain the converted .fil -&gt; .csv-file
. $WorkspaceFolder/pipelines/scripts/datalake/file_container_existence.ps1 `
-WorkspaceFolder $WorkspaceFolder `
-TargetEnvironment $TargetEnvironment `
-ContainerName 'landing' `
-BlobPrefix 'preprocess_test/filworker_test/preprocess/prd/20240802_112046/firstrow_to_headers' `
-FileList $ConvertedFileList `
-InformationAction 'Continue'

### 2. Archive should contain the original .fil-file
. $WorkspaceFolder/pipelines/scripts/datalake/file_container_existence.ps1 `
-WorkspaceFolder $WorkspaceFolder `
-TargetEnvironment $TargetEnvironment `
-ContainerName 'archive' `
-BlobPrefix 'preprocess_test/filworker_test/prd/20240802_112046' `
-FileList $OriginalFileList `
-InformationAction 'Continue'

### 3. Check the contents of the converted file to see if everything was converted properly
$ConvertedFile = . $WorkspaceFolder/pipelines/scripts/datalake/get_blob_content.ps1 `
-WorkspaceFolder $WorkspaceFolder `
-TargetEnvironment $TargetEnvironment `
-ContainerName 'landing' `
-Blob "preprocess_test/filworker_test/preprocess/prd/20240802_112046/firstrow_to_headers/$($ConvertedFileList[0])"

# Dev-Note: At index 0, the output will be 'Write-Output "Start script: $($MyInvocation.MyCommand.Name) with parameters:' (If this is ever removed, then there will be no index)
$DataFrame = $ConvertedFile[1] | ConvertFrom-Csv -Delimiter "`t"

$ExpectedDataFrame = @"
999998	75	0003	1	1642.60	19800101	19800102	82
999998	75	0004	3	2463.90	19800101	19800102	82
999998	75	0005	12	720.00	19800101	19800102	82
999998	75	0006	66	1980.00	19800101	19800102	82

"@ | ConvertFrom-Csv -Delimiter "`t"

# Simple comparison when rows are expected to be in the same order
for ($i = 0; $i -lt $DataFrame.Count; $i++) {
    $actualRow = $DataFrame[$i].PSObject.Properties.Value -join "`t"
    $expectedRow = $ExpectedDataFrame[$i].PSObject.Properties.Value -join "`t"

    if ($actualRow -ne $expectedRow) {
        throw "Mismatch found at row $($i): Expected '$expectedRow', got '$actualRow'. For validation check of FilWorker, step 3"
    }
}

### 4. Check if the file was logged with the suffix _preprocess
$LoggedScripts = Get-ChildItem -Path $WorkspaceFolder/src/synapse/_test/test_scripts/sql | Where-Object -Property Name -like 'filworker*'
foreach ($Script in $LoggedScripts){
    . $WorkspaceFolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @sqlParams -Path $Script.FullName
}

Write-Information ""
Write-Information "***End script: $($scriptName)***"
</file>
<file name="pipelines\scripts\synapse\validate_workers\validate_ingestionworker.ps1">
&lt;#

.SYNOPSIS
    Script executes validation checks to make sure the ingestionworker behaved as expected

.DESCRIPTION
    This script will execute the following checks:
        1. Landing container should not contain any more files
        2. File should be moved to archive
        3. Delta table should be created on silver container
        4. Parquet file should be on raw
        5. File should be logged in logging tables
        6. ingestionworker_csv_task should be executed successfully
        7. Delta table should contain data from csv file

#&gt;

[CmdletBinding()]
param(
    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $StorageAccount = 'devdapstdala1',

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SQLServerInstance = $env:SQL_SERVER,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SQLDatabase = $env:SQL_DATABASE,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $Accesstoken,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SynapseWorkspace = $env:Synapse_WS,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SynapseServer = $env:Synapse_Server,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $SynapseDatabase = $env:Synapse_Database,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $TargetEnvironment = $env:MYDEVENV,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $WorkspaceFolder = $env:REPOROOT,

    [Parameter(Mandatory=$false, HelpMessage="Name of the storage account where the files are expected")]
    [string] $Plan_ID=2

)

# Force switch parameters
Set-StrictMode -Version "latest"
$InformationPreference = 'Continue'
$ErrorActionPreference = 'Continue'


# ---------------------------------------------------------------------------------------------------------
$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

#Startregion
# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters 
$inputParameters = @( 
    "StorageAccount"
    "SQLServerInstance"
    "SQLDatabase"
    "SynapseWorkspace"
    "SynapseServer"
    "SynapseDatabase"
    "TargetEnvironment"
    "WorkspaceFolder"
    "Plan_ID"
)

$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
Write-Information ""
#Endregion


# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------

## Parameters
### Access token
if (!$Accesstoken){
    $Accesstoken = (Get-AzAccessToken -ResourceUrl https://database.windows.net).Token
}

$Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount


### Metadb-params
$sqlParams = @{
    SQLServerInstance = $SQLServerInstance
    SQLDatabase = $SQLDatabase
    AccessToken = $Accesstoken
}

### On-demand params
$synapseParams = @{
    SQLServerInstance = $SynapseServer
    SQLDatabase = $SynapseDatabase
    AccessToken = $Accesstoken
    TargetEnvironment = $TargetEnvironment
}


## Storage Account checks
[array]$Files = @("csv_test.csv", "json_test.json", "headerless_header_check.csv", `
                    "prd/20240802_112046/recurse.csv", "parquet_test.parquet", "csv_skiplines.csv", "csv_partitioning.csv", `
                    "datatype_test.csv")
[string]$BlobPrefix = 'ingestion_test/20240802_112048'

### 1. No files should remain in landing
# Check if the files that are supposed to removed from landing, are actually removed. If not, write error
. $WorkspaceFolder/pipelines/scripts/datalake/file_container_existence.ps1 `
-WorkspaceFolder $WorkspaceFolder `
-TargetEnvironment $TargetEnvironment `
-ContainerName 'landing' `
-BlobPrefix $BlobPrefix `
-FileList $Files `
-PresenceCheck $false `
-InformationAction 'Continue'

Write-Output ""
Write-Output ""
### 2. Files should be moved to archive
# Check if the files that are supposed to be moved to archive, are actually moved. If not, write error
. $WorkspaceFolder/pipelines/scripts/datalake/file_container_existence.ps1 `
-WorkspaceFolder $WorkspaceFolder `
-TargetEnvironment $TargetEnvironment `
-ContainerName 'archive' `
-BlobPrefix $BlobPrefix `
-FileList $Files `
-InformationAction 'Continue'

### 3. Delta tables should exist in silver container
$expectedDeltaTables = @("csv_integration", "json_integration", "datatype_table", "unittest_spark_checks_headerless", `
                            "parquet_integration", "skiplines", "options_partitioning")
$containerName = 'silver'
# Get all possible blobs inside a container according to a specific filter
$Blobs = (Get-AzStorageBlob -Context $Context -Container $containerName).Name # | Sort-Object -Property @{Expression ='Length'; Descending= $true}, @{Expression='Name'; Descending=$true} 
$expectedDeltaTables | ForEach-Object{
    if ($_ -notin $Blobs){
        throw "$($_) was not found in the container $($containerName) after ingestion..."
    }
}

# 4. 

# 5. File should be logged in meta.log_files
# 6. ingestionworker_csv_task should be executed successfully
$test_scripts = Get-ChildItem -Path $WorkspaceFolder/src/synapse/_test/test_scripts/sql | Where-Object -Property Name -like 'ingestionworker*'
foreach ($script in $test_scripts){
    . $WorkspaceFolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @sqlParams -Path $script.FullName
}

# 7. Delta table should contain data from csv file
$test_scripts = Get-ChildItem -Path $WorkspaceFolder/src/synapse/_test/test_scripts/synapse | Where-Object -Property Name -like 'ingestionworker*'
foreach ($script in $test_scripts){
    . $WorkspaceFolder/pipelines/scripts/sql/execute_sqldb_query.ps1 @synapseParams -Path $script.FullName
}

# 8. Special Case: Partitioning: Expect year and month folders to be defined for the delta table

# Get all possible blobs inside a container according to a specific filter
$containerName = 'silver'
$expectedDateParts = @(
    'options_partitioning/p_year=1900', 'options_partitioning/p_year=1900/p_month=01' # task: ingestionworker_options_partitioning
    'options_filename_partitioning/p_year=2024', 'options_filename_partitioning/p_year=2024/p_month=10', 'options_filename_partitioning/p_year=2024/p_month=10/p_day=24') # task: ingestionworker_options_partitioning_on_filename
$partitionedBlobs = Get-AzStorageBlob -Context $Context -Container $ContainerName | Where-Object {$_.Name -like 'options_partitioning*' -or $_.Name -like 'options_filename_partitioning*'}
$expectedDateParts | ForEach-Object{
    if ($_ -notin $partitionedBlobs.Name){
        throw "$($_) was not found in the container $($ContainerName) and folders 'options_partioning'/'options_filename_partitioning' after ingestion..."
    }
}
</file>
<file name="pipelines\templates\steps\install_dataconfig_module.yaml">
parameters:

  # file where nugets can be found eg "$(Build.ArtifactStagingDirectory)/Nuget"
  - name: NugetDirectory
    type: string

  # if not provided, will evaluate to null, and skips tasks relying on Azure connection
  - name: service_connection_name
    type: string
    default: ''

steps:

- ${{ if parameters.service_connection_name }}:
  # check module in AzurePowershell
  - task: AzurePowerShell@5
    displayName: "[debug] check module in AzurePowershell"
    # executes debug if pipeline is executed with option 'Enable system diagnostics'
    condition: |
      and (  
        succeeded(),     
        eq(variables['System.Debug'], true)
      )
    inputs:
      azureSubscription: "${{ parameters.service_connection_name }}"
      workingDirectory: "$(System.DefaultWorkingDirectory)"
      scriptType: InlineScript
      inline: |
        Write-Output "PSModulePath: "
        Write-Output $env:PSModulePath
        Write-Output "-------------------------"
        Write-Output ''
        Write-Output '( get-module -Name "DataConfig" ).Path:'
        ( get-module -Name "DataConfig" ).Path
        Write-Output "-------------------------"
        Write-Output ''
        Write-Output '( get-module -Name "DataConfig" -ListAvailable ).Path:'
        ( get-module -Name "DataConfig" -ListAvailable ).Path
        Write-Output '------------------------------'
        Write-Output ''
        Write-Output 'unload modules'
        if( get-module -Name "DataConfig"  ) {
          Remove-Module -Name "DataConfig"
        }
        Write-Output '------------------------------'
        Write-Output ''
        Write-Output 'delete modules'
        foreach ( $mod in ( get-module -Name "DataConfig" -ListAvailable).Path ) {
          $folder = Split-Path -Path $mod
          Write-Output "delete: $folder"
          Remove-Item -Path $folder -Force -Recurse
        }
        Write-Output '------------------------------'

        # check again
        Write-Output 'Get-Module -Name "DataConfig":'
        Get-Module -Name "DataConfig"
        Write-Output ''
        Write-Output 'Get-Module -Name "DataConfig" -ListAvailable:'
        Get-Module -Name "DataConfig" -ListAvailable
        Write-Output ''
        Write-Output '------------------------------'
        Write-Output 'loaded modules path(s):'
        ( get-module -Name "DataConfig" ).Path

      errorActionPreference: 'stop'
      failOnStandardError: true
      azurePowerShellVersion: 'latestVersion'
      pwsh: true

# install DataConfig module
- task: Powershell@2
  displayName: "Install DataConfig module"
  inputs:
    workingDirectory: "$(Pipeline.Workspace)"
    targetType: Inline
    script: |
      Write-Output 'Get-Module -Name "DataConfig" :'
      Get-Module -Name "DataConfig"
      Write-Output 'Get-Module -Name "DataConfig" -ListAvailable :'
      Get-Module -Name "DataConfig" -ListAvailable     
      Write-Output "----------------------------------"
      $ModuleDest = ($env:PSModulePath -Split [IO.Path]::PathSeparator)[0]
      Write-Output "ModuleDest: $ModuleDest"
      if ( Test-Path -Path "${ModuleDest}/DataConfig") {
        Write-Output 'Deleting "${ModuleDest}/DataConfig"...'
        Remove-Item -Path "${ModuleDest}/DataConfig" -Recurse -Force
        # Uninstall-Package -Name DataConfig -AllVersions #-erroraction "ignore"
      }
      Write-Output "----------------------------------"
      Write-Output 'Install-Package...'
      Install-Package -Name DataConfig -Source "${{ parameters.NugetDirectory }}" -Destination $ModuleDest -ExcludeVersion -InformationAction "Continue" -Force
    errorActionPreference: "stop"                
    failOnStderr: true
    pwsh: true

- ${{ if parameters.service_connection_name }}:
  # check module in AzurePowershell
  - task: AzurePowerShell@5
    displayName: "[debug] check module in AzurePowershell"
    # executes debug if pipeline is executed with option 'Enable system diagnostics'
    condition: |
      and (  
        succeeded(),     
        eq(variables['System.Debug'], true)
      )
    inputs:
      azureSubscription: "${{ parameters.service_connection_name }}"
      workingDirectory: "$(System.DefaultWorkingDirectory)"
      scriptType: InlineScript
      inline: |
        Write-Output "PSModulePath: "
        Write-Output $env:PSModulePath
        Write-Output "-------------------------"
        Write-Output '( get-module -Name "DataConfig" ).Path:'
        ( get-module -Name "DataConfig" ).Path
        Write-Output "-------------------------"
        Write-Output '( get-module -Name "DataConfig" -ListAvailable ).Path:'
        ( get-module -Name "DataConfig" -ListAvailable ).Path
        Write-Output '------------------------------'
        Write-Output ''
        # check again
        Write-Output 'Get-Module -Name "DataConfig":'
        Get-Module -Name "DataConfig"
        Write-Output ''
        Write-Output 'Get-Module -Name "DataConfig" -ListAvailable:'
        Get-Module -Name "DataConfig" -ListAvailable
        Write-Output ''
        Write-Output '------------------------------'
        Write-Output 'loaded modules path(s):'
        ( get-module -Name "DataConfig" ).Path

      errorActionPreference: 'stop'
      failOnStandardError: true
      azurePowerShellVersion: 'latestVersion'
      pwsh: true
</file>
<file name="pipelines\templates\steps\install_required_modules.yaml">
parameters:
  # json string holding the required module names and versions (no version = "any")
  # format '[ { "Name": "&lt;modulename&gt;", "Version" : "latest|x.y.z|x.*|x.y.*" }]'
  - name: requirements
    type: string
  
  # provide custom install path to "download" the module instead performing normal install in CurrentUser scope
  - name: custom_install_path
    type: string
    default: ""

  # working directory (if not the default)
  - name: workingDirectory
    type: string
    default: $(System.DefaultWorkingDirectory)    

steps:

  # Install PowerShell modules in order to be able to execute SQL queries
  - task: PowerShell@2
    displayName: 'Install required modules'
    inputs:
      informationPreference: continue
      failOnStderr: true
      targetType: filePath
      pwsh: true
      workingDirectory: "${{ parameters.workingDirectory }}"
      filePath: "${{ parameters.workingDirectory }}/pipelines/scripts/general/install_powershell_requirements.ps1"
      arguments: &gt;
        -Requirements "${{ parameters.requirements }}"
        -CustomInstallPath "${{ parameters.custom_install_path }}"

</file>
<file name="pipelines\templates\steps\show_config_vars.yaml">
parameters:
  # comma-separated list of variables to show
  - name: variables
    type: string
    default: ""

  # object of parameters of the calling template
  - name: parametersObj
    type: object
    default: 
      - name: "abc"
        value: "def"

  # working directory (if not the default)
  - name: workingDirectory
    type: string
    default: $(System.DefaultWorkingDirectory)

steps:

  # display job configuration values
  - task: PowerShell@2
    displayName: '[Info] Display job configuration values'
    env:
      ${{ each p in parameters.parametersObj }}:
        PARAM_${{ p.key }}: "${{ p.value }}"
    inputs:
      pwsh: true
      targetType: filePath
      filePath: "${{ parameters.workingDirectory }}/pipelines/scripts/general/show_config_vars.ps1"
      errorActionPreference: "Continue"
      failOnStderr: true
      informationPreference: "Continue"   
      arguments: &gt;
        -Variables "${{ parameters.variables }}"
      workingDirectory: "${{ parameters.workingDirectory }}"

</file>
<file name="pipelines\templates\steps\show_folder_contents.yaml">
parameters:
  - name: ShowFoldersOnly
    type: boolean
    default: false

  # folder to show contents for (default is System.DefaultWorkingDirectory)
  - name: folderPath
    type: string
    default: $(System.DefaultWorkingDirectory)    

  # working directory (default is System.DefaultWorkingDirectory)
  - name: workingDirectory
    type: string
    default: $(System.DefaultWorkingDirectory)    

steps:

  # DefaultWorkingDirectory directory listing
  - task: PowerShell@2
    displayName: "[Info] Show working directory contents"  
    inputs:
      pwsh: true
      targetType: filePath
      filePath: "${{ parameters.workingDirectory }}/pipelines/scripts/general/show_folder_contents.ps1"
      errorActionPreference: "Continue"
      failOnStderr: true
      informationPreference: "Continue"   
      arguments: &gt;
        -ShowFoldersOnly $${{ parameters.ShowFoldersOnly }}
        -Path "${{ parameters.folderPath }}"
      workingDirectory: ${{ parameters.workingDirectory }}

</file>
<file name="pipelines\templates\steps\validate_target_environment.yaml">
parameters:
  
  # 
  - name: environment_code
    type: string

  #
  - name: target_environment    
    type: string

  # working directory (if not the default)
  - name: workingDirectory
    type: string
    default: $(System.DefaultWorkingDirectory)

steps:

  # Validate environment code and target environment
  - task: PowerShell@2
    displayName: "[Info] Check Target Environment"
    enabled: true
    inputs:
        targetType: filePath
        filePath: "${{ parameters.workingDirectory }}/pipelines/scripts/general/check_environment_code.ps1"
        arguments: &gt;
          -environment_code "${{ parameters.environment_code }}"
          -target_environment "${{ parameters.target_environment }}"
        pwsh: true 
        errorActionPreference: stop
        informationPreference: "Continue"   
        failOnStderr: true  
        workingDirectory: "${{ parameters.workingDirectory }}"

</file>
<file name="src\powershell\modules\DataConfig\.build\build.ps1">
[CmdletBinding()]
param (
    [Parameter(HelpMessage = "Name of the module")]
    [String] $ModuleName = "DataConfig",

    [Parameter(HelpMessage = "Version number in Major.Minor.Path format")]
    [String] $BuildVersion,

    [Parameter(HelpMessage = "Base directory with the module folder")]
    [String] $Workingdir,   

    [Parameter(HelpMessage = "Directory for the localFeed PSRepository")]
    [String] $LocalFeedPath,   

    [Parameter(HelpMessage = "Set this switch to enable cleaning the module folder before packaging")]
    [Switch] $CleanFolder

)

#local settings
Set-StrictMode -Version "latest"
#Requires -Version 7.2

#region script info
$inputParameters = $MyInvocation.MyCommand.Parameters.Keys | Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::CommonParameters}
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "----------------------------------------------------------------------"
Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
Write-Information "  Parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
Write-Information "PSVersion: $($PSVersiontable.PSVersion.ToString())"
Write-Information "----------------------------------------------------------------------"
#endregion script info

Write-Output ""
Write-Output "Package providers:"
Get-PackageProvider
Write-Output "----------------------------------------------------------------------"


#unload the module if currently loaded
$currentModules = Get-Module -Name $ModuleName -ErrorAction "Ignore"
if( $currentModules) {
    Write-Output "Unloading currently loaded '$ModuleName' modules ..." 
    $currentModules.Version | ForEach-Object { Write-Information "  $_"}
    $currentModules | Remove-Module | Out-Null
} else {
    Write-Output "No currently loaded '$ModuleName' modules (OK)" 
}
#uninstall the package if currently installed
$currentPackages = Get-Package -Name $ModuleName -ErrorAction "Ignore"
if( $currentPackages) {
    Write-Output "Uninstalling currently installed packages ..."
    $currentPackages.Version | ForEach-Object { Write-Information "  $_"}
    $currentPackages | Uninstall-Package | Out-Null
} else {
    Write-Output "No currently installed '$ModuleName' packages (OK)" 
}

#build and check path to the module folder
$modulePath = Join-Path $Workingdir $ModuleName
Write-Information ""
Write-Information "Using modulePath: $modulePath"
if ( -Not (Test-Path $modulePath -PathType "Container") ) {
    Throw "Module '$ModuleName' is not a subfolder of '$WorkingDir'"
} else {
    Write-Information "  modulePath: OK, folder exists"
    $modulePath = (Resolve-Path $modulePath).Path
    Write-Information "  resolved to: $modulePath"
}

#build and check path to the module manifest
$psdFile = $ModuleName + ".psd1"
Write-Information "Using psdFile: $psdFile"
$manifestPath = Join-Path $modulePath $psdFile
Write-Information "Using manifestPath: $manifestPath"
if (-not (test-Path $manifestPath) ) {
    Throw "Manifest does not exist at: $manifestPath"
}

#region Prepare localFeed PSRepository

    Write-Output "Preparing localFeed PSRepository ..."

    #check and resolve localFeedPath
    if (-not (Test-Path $localFeedPath)) {
        New-Item -Path $localFeedPath -ItemType "Directory" | Out-Null
    }
    $LocalFeedPathResolved = ($LocalFeedPath | Resolve-Path -ErrorAction Ignore)
    if ( -not $LocalFeedPathResolved ) {
        Throw "Could not resolve LocalFeedPath: '$LocalFeedPath'"
    } else {
        $LocalFeedPathResolved = $LocalFeedPathResolved.Path
        Write-Information "  Resolved LocalFeedPath to: $LocalFeedPathResolved"
    }

    #check or create "localFeed" PSRepository
    $localFeed = Get-PSRepository -Name localFeed -ErrorAction "Ignore"
    if ( $localFeed ) {
        Write-Output "  Found localFeed PSRepository"
        # $localFeed 
        # $localFeed.ScriptSourceLocation
        # $localFeed.ScriptPublishLocation
        # Write-Output "(members)"
        # $localFeed | Get-Member
    } else {
        Write-Output "  localFeed PSRepository not found"
    }

    if ( $localFeed -and ($localFeed.SourceLocation -ne $LocalFeedPathResolved) ) {
        Write-Warning "  localFeed repository has wrong path ($($localFeed.SourceLocation)), removing it ..."
        #Write-Output "Unregister existing localFeed repository ..."
        Unregister-PSRepository -Name "localFeed"
        $localFeed = $null
    }
    if ( -not $localFeed) {
        Write-Output " Registering 'localFeed' PSRepository with path: $LocalFeedPathResolved ..."
        Register-PSRepository -name localFeed -sourceLocation $LocalFeedPathResolved -publishLocation $LocalFeedPathResolved -InstallationPolicy Trusted
    } else {
        Write-Output "  PSRepository 'localFeed' already present with path: $LocalFeedPathResolved"
    }

# Remove existing package with current version (main to avoid conflict when (re)building version 0.0.0 locally)
$existingPackages = Get-ChildItem -Path $LocalFeedPathResolved -Filter "${ModuleName}.${BuildVersion}.nupkg"
if( -not $existingPackages) {
    Write-Information "  Deleting existing package files : none found [OK]"
} else {
    Write-Information "  Deleting existing package files ..."
    $existingPackages.FullName | ForEach-Object { Write-Information "    $_"}
    $existingPackages  | Remove-Item | Out-Null
}

#endregion

#region create the manifest

    Write-Output ""
    Write-Output "Prepare the module and manifest..."

    # remove the files not required for the nuget package
    if ( $CleanFolder) {                
        Write-Information "  Remove unneeded files for nuget package..."
        Get-ChildItem -Path $modulePath -Include "*.Tests.ps1" -Recurse | Remove-Item -Force
        Get-ChildItem -Path $modulePath -Filter ".build" -Directory | Remove-Item -Recurse -Force
        Get-ChildItem -Path $modulePath -Filter ".test_config" -Directory | Remove-Item -Recurse -Force
        Get-ChildItem -Path $modulePath -Filter "coverage.xml" | Remove-Item -Force
        Get-ChildItem -Path $modulePath -Filter "testResults_unit.xml" | Remove-Item -Force
    } else {
        Write-Information "  (...skipping CleanFolder section...)"
    }

    # Update build version in manifest
    Write-Information "  Updating version number in manifest..."
    $manifestContent = Get-Content -Path $manifestPath -Raw
    $manifestContent = $manifestContent -replace '&lt;ModuleVersion&gt;', $BuildVersion

    # Find all of the public functions and add them as cmdlets
    $publicFuncFolderPath = Join-Path $modulePath public
    $itemParams = @{
        Path = $publicFuncFolderPath
        File = $true
        Recurse = $true
        Filter = '*.ps1'
        Exclude = '*.Tests.ps1'
    }
    if ((Test-Path -Path $publicFuncFolderPath) -and ($publicFunctionNames = Get-ChildItem @itemParams | Select-Object -ExpandProperty BaseName)) {
        $funcStrings = "'$($publicFunctionNames -join "',$([System.Environment]::NewLine)  '")'"
        $funcStrings = $([System.Environment]::NewLine) + "  " + $funcStrings + $([System.Environment]::NewLine)
    } else {
        $funcStrings = $null
    }
    ## Add all public functions to FunctionsToExport attribute
    $manifestContent = $manifestContent -replace "'&lt;FunctionsToExport&gt;'", $funcStrings

    Write-Information "  "
    Write-Information "  functions to export:"
    $funcStrings | ForEach-Object { Write-output "    $_" }

    ## save updated manifest
    $manifestContent | Set-Content -Path "$manifestPath"
    Write-Information "  Updated manifest: $manifestPath"

#endregion

# create nuget package
Write-Output ""
Write-Output "Create and publish nuget package to localFeed repository..."
Publish-Module -path $modulePath -Repository localFeed -NuGetApiKey 'dummy' -Force

# list available packages
Write-Output ""
Write-Output "List available packages in localFeed repository..."
#Find-Package -Source "localFeed" |  Select-Object Version, ProviderName, Source, FullPath
Find-Package -Source "localFeed" |  ForEach-Object {Write-Output "  $($_.Version) -- $($_.ProviderName)"}

# install the package
Write-Output ""
Write-Output "Install the module from localFeed..."
Install-Package -Name $ModuleName -Source "localFeed" -ProviderName "PowerShellGet" -Force |  ForEach-Object {Write-Output "  $($_.Version)"}

# review installed modules
$modules = Get-Module -Name $ModuleName -ListAvailable | Select-Object Name, Version, RepositorySourceLocation, Path
Write-Output ""
Write-Output "Available module versions:"
$modules |  ForEach-Object {Write-Output "  $($_.Version)"}

# check that the "current" version is there
Write-Output ""
Write-Output "Check that current version is available..."
$currentModule = $modules | Where-Object {$_.Version.ToString() -eq $BuildVersion }
if ( -Not $currentModule) {
    Throw "Module with version '$BuildVersion' not found in available modules"
} else {
    Write-Information "  Found available module '$ModuleName' with version '$BuildVersion' (OK)"
}
 
# import the module
Write-Output ""
Write-Output "Import the module ..."
Import-Module -Name $ModuleName
Get-Module -Name $ModuleName |  ForEach-Object {Write-Output "  $($_.Version)"}


</file>
<file name="src\powershell\modules\DataConfig\.build\unit_tests.ps1">
#requires -Modules Az.KeyVault
#requires -Modules Az.Accounts
#requires -Modules Az.Resources
#requires -Modules Az.Synapse
#requires -Modules @{ ModuleName="Pester"; ModuleVersion="5.5.0" }

[CmdletBinding()]
param (

)

#region SECTION: Prerequisites

    Write-Output ""
    Write-Output "Using pester version: $((Get-Module Pester).Version)"

    Write-Output ""
    Write-Output "Module to test:"
    $modulePath = resolve-path ( Join-Path "$PSScriptRoot" "..")
    $moduleName = (Split-Path $modulePath -Leaf)  + '.psm1'
    Write-Output "  modulePath: $modulePath"
    Write-Output "  moduleName: $moduleName"

    # preload the module a first time
    if ( Get-Module -Name $moduleName) {
        Remove-Module $moduleName
    }
    Import-Module -Name (Join-Path  $modulePath $moduleName ) -Force

#endregion

#region SECTION: Pester configuration

    # start with default configuration
    $pesterConfig = [PesterConfiguration]::Default

    # create testResults.xml output
    $pesterConfig.TestResult.Enabled = $true
    $pesterConfig.TestResult.OutputFormat = "NUnitXml"
    $pesterConfig.TestResult.OutputPath = Join-Path $modulePath "testResults_unit.xml"

    # create code coverage output
    $pesterConfig.CodeCoverage.Enabled = $True
    $pesterConfig.CodeCoverage.OutputFormat = "JaCoCo"
    $pesterConfig.CodeCoverage.OutputPath = Join-Path $modulePath "coverage.xml"
    $pesterConfig.CodeCoverage.Path = "$modulePath/[p]*/*.ps1"  #  public, private
    $pesterConfig.CodeCoverage.UseBreakpoints = $False

    # filters (the fullname filter is a powershell "LIKE" statement, not regex)
    # important: the FullName and Tag filters work are combined as "OR", not as "AND"
    $pesterConfig.Filter.FullName = "*.UnitTests.*"
    $pesterConfig.Filter.Tag = 'UnitTest'
    $pesterConfig.Filter.ExcludeTag = "Draft"

    # run parameters
    $pesterConfig.Run.Path = Join-Path $modulePath "public" 
    $pesterConfig.Run.Exit = $False
    $pesterConfig.Run.PassThru = $False

    # show describe/context/it details
    $pesterConfig.Output.Verbosity = "Detailed"
#endregion

#region SECTION: Run tests
    Write-Output "Run tests..."
    Invoke-Pester -Configuration $pesterConfig
#endregion


Write-Output "*** End script: $($MyInvocation.MyCommand.Name)"

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets-extra_key.json">
{

    "datasets": [

        {
            "name": "test_dataset", 
            "description": "Additional description2 key should not exist",
            "kind": "csv",
            "task_type": "SYNAPSE_PIPELINE",
            "description2": "TEST"
    
        }

    ]
    
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets-mandatory_keys.json">
{
    "datasets": [
    {
          "name": "test_dataset_passes_with_all_mandatory_keys",
          "kind": "csv",
          "task_type": "SYNAPSE_PIPELINE",
          "worker": "pl_dummy_worker"
      }
  
  ]
}
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_datatypes.json">
{
    "datasets": [
      {
        "name": "test_datatypes",
        "description": "Dataset testing on all possible data types",
        "kind": "csv",
        "task_type": "SPARK_NOTEBOOK",
        "worker": "DummyWorker",
        "ingestion": {
          "target_table": "test",
          "source_folder": "test",
          "match_pattern": "test",
          "extension": "csv"
        },
        "columns": [
            {
                "sequence": 1,
                "name": "array",
                "data_type": "array"
            },
            {
                "sequence": 2,
                "name": "binary",
                "data_type": "binary"
            },
            {
                "sequence": 3,
                "name": "boolean",
                "data_type": "boolean"
            },
            {
                "sequence": 4,
                "name": "date",
                "data_type": "date"
            },
            {
                "sequence": 5,
                "name": "string",
                "data_type": "string"
            },
            {
                "sequence": 6,
                "name": "timestamp",
                "data_type": "timestamp"
            },
            {
                "sequence": 7,
                "name": "decimal",
                "data_type": "decimal(x,y)"
            },
            {
                "sequence": 8,
                "name": "float",
                "data_type": "float"
            },
            {
                "sequence": 9,
                "name": "byte",
                "data_type": "byte"
            },
            {
                "sequence": 10,
                "name": "integer",
                "data_type": "integer"
            },
            {
                "sequence": 11,
                "name": "long_integer",
                "data_type": "long_integer"
            }
        ]
      }
    ]
  }
  
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_defaults.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "Dataset where all the variables that have a default are left out",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "IngestionWorker",
      "ingestion": {
        "target_table": "tablename",
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": "txt"
      },
      "checks": [
        {
          "name": "header"
        }
      ],
      "columns": [
        {
          "sequence": 1,
          "name": "first_column"
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_filename_precond_all_params.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "This tests that the all paramers are assigned correctly",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "DummyWorker",

      "ingestion": {
        "target_table": "tablename",
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": ".txt",
        "container": "test",
        "preconditions":
          [
            {
              "name":"required_name",
              "type":"file_exists",
              "frequency":"daily",
              "expected_file_mask":".*{yyyyMMdd}.*",
              "enabled":true,
              "description":"my_description"
            }
          ]
      },
      "columns": [
        {
          "sequence": 1,
          "name": "first_column",
          "data_type": "integer",
          "dimension": "PK"
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_filename_precond_required_params.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "This tests that the required paramers and verifies that the default values are assigned correctly",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "DummyWorker",

      "ingestion": {
        "target_table": "tablename",
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": ".txt",
        "container": "test",
        "preconditions":
          [ {
              "name":"required_name",
            "type":"file_exists",
            "frequency":"daily"
            }
          ]
        
      },
      "columns": [
        {
          "sequence": 1,
          "name": "first_column",
          "data_type": "integer",
          "dimension": "PK"
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_formatting.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "This tests if a fully configured dataset passes",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "DummyWorker",

      "ingestion": {
        "target_table": "tablename",
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": "txt",
        "encoding": "UTF-8",
        "column_delimiter": "|",
        "row_delimiter": "",
        "escape_character": "",
        "quote_character": "",
        "header_line": true,
        "container": "test",
        "skip_first_lines": 2
      },
      "columns": [
        {
          "sequence": 1,
          "name": "first_column",
          "data_type": "decimal",
          "dimension": "PK",
          "column_options": {
            "locale": "en-US"
          }
        },
        { "sequence": 2, 
          "name": "second_column", 
          "data_type": "date",
          "dimension": "SCD2",
          "column_options": {
            "format": "yyyy-MM-dd"
          }
        },
        { "sequence": 3, 
          "name": "third_column", 
          "data_type": "timestamp",
          "dimension": "SCD2",
          "column_options": {
            "format": "yyyy-MM-dd'T'HH:mm:ss"
          }
        },
        { "sequence": 4, 
          "name": "fourth_column", 
          "data_type": "decimal",
          "dimension": "SCD2",
          "column_options": {
            "thousand_separator": ",",
            "decimal_separator": "."
          }
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_full.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "This tests if a fully configured dataset passes",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "DummyWorker",

      "ingestion": {
        "target_table": "tablename",
        "target_options": {
          "partitioning": [
            {
              "name": "non_timestamp_field",
              "sequence": 1
            },
            {
              "name": "timestamp_field",
              "datePart": "year",
              "sequence": 2
            },
            {
              "name": "timestamp_field",
              "datePart": "month",
              "sequence": 3
            },
            {
              "name": "timestamp_field",
              "timestamp": "day",
              "sequence": 4
            }
          ],
          "extract_date":{
            "column_name": "t_file_name",
            "regex_expression": "_(\\d{8}_\\d{6})",
            "extract_date_format": "ddMMyyyy"
          }
        },    
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": "txt",
        "encoding": "UTF-8",
        "column_delimiter": "|",
        "row_delimiter": "",
        "escape_character": "",
        "quote_character": "",
        "header_line": true,
        "container": "test",
        "skip_first_lines": 2
      },
      "checks": [
        {
          "name": "header",
          "enabled": false,
          "config_params": "{\"test\":\"dummy\", \"test2\":\"dummy2\"}"
        },
        {
          "name": "data_type"
        }
      ],
      "columns": [
        {
          "sequence": 1,
          "name": "first_column",
          "data_type": "integer",
          "dimension": "PK"
        },
        {
          "sequence": 2,
          "name": "second_column",
          "data_type": "string",
          "dimension": "SCD2"
        },
        { "sequence": 3, 
          "name": "third_column", 
          "data_type": "date",
          "column_info": {
            "optional": true
          }
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_invalid-extension.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "This tests that the ingestion fails when ingestion.extension isn't in the ENUM",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "DummyWorker",

      "ingestion": {
        "target_table": "tablename",
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": ".txt",
        "container": "test"
      },
      "columns": [
        {
          "sequence": 1,
          "name": "first_column",
          "data_type": "integer",
          "dimension": "PK"
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_invalid-tableoptions.json">
{
  "datasets": [
    {
      "name": "test_dataset_csv",
      "description": "This tests if a fully configured dataset passes",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "DummyWorker",

      "ingestion": {
        "target_table": "tablename",
        "target_options": {
          "partitioning": [
            {
              "name": "non_timestamp_field",
              "sequence": 1
            },
            {
              "name": "timestamp_field",
              "datePart": "plus",
              "sequence": 2
            }
          ]
        },    
        "source_folder": "test/",
        "match_pattern": "mypattern",
        "extension": "txt",
        "encoding": "UTF-8",
        "column_delimiter": "|",
        "row_delimiter": "",
        "escape_character": "",
        "quote_character": "",
        "header_line": true,
        "container": "test"
      },
      "checks": [
        {
          "name": "header",
          "enabled": false,
          "config_params": "{\"test\":\"dummy\", \"test2\":\"dummy2\"}"
        },
        {
          "name": "data_type"
        }
      ]
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_invalid.json">
{

    "datasets": [

        {
            "name": "test_dataset", 
            "description": "Tests that the JSON fails because of missing mandatory basic keys"
    
        }

    ]
    
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_tasktype_error.json">
{
  "datasets": [
    {
      "name": "pl_unzip_worker",
      "description": "This is used to test if the JSON-file fails when the task_type is not in ENUM",
      "kind": "csv",
      "task_type": "XXX"
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_tasktype_SPARK_NOTEBOOK.json">
{
    "datasets": [
    {
          "name": "test_json_tasktype_spark_notebook",
          "description": "This tests that SPARK_NOTEBOOK task_type is allowed",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker": "DummyWorker"
      }
  
  ]
}
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_tasktype_SYNAPSE_PIPELINE.json">
{
    "datasets": [
    {
          "name": "test_json_tasktype_synapse_pipeline",
          "description": "This tests that SYNAPSE_PIPELINE task_type is allowed",
          "kind": "csv",
          "task_type": "SYNAPSE_PIPELINE",
          "worker": "pl_dummy_worker"
      }
  
  ]
}
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_worker_pl_dummy_worker.json">
{
    "datasets": [
    {
          "name": "test_json_tasktype_spark_notebook",
          "description": "This tests that pl_dummy_worker worker is allowed",
          "kind": "csv",
          "task_type": "SYNAPSE_PIPELINE",
          "worker": "pl_dummy_worker"
      }
  
  ]
}
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-datasets_worker_pl_unzip_worker.json">
{
    "datasets": [
    {
          "name": "test_json_tasktype_spark_notebook",
          "description": "This tests that pl_unzip_worker worker is allowed",
          "kind": "csv",
          "task_type": "SYNAPSE_PIPELINE",
          "worker": "pl_unzip_worker",
          
          "ingestion": {
            "target_table": "unzipped",
            "source_folder": "zipped",
            "container": "landing",
            "match_pattern": "(.*)",
            "extension": "gz"
          }
      }
  
  ]
}
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-delta_tables.json">
{
  "table_configurations": [
    {
      "table_description": "test description",
      "kind": "csv",
      "table_name": "table_name",
      "container_name": "silver",
      "column_info": "'[{}]'",
      "storage_account": "devdapstdala1"
    }
  ]
}
</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks-mandatory_keys.json">
{
    "plans": [

        {
            "name": "test_plan_passes_with_all_mandatory_keys"
        }
    ]
}


</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks_defaults.json">
{
    "plans": [

        {
            "name": "plan_name_defaults",
            "description": "Plan where all the variables that have a default are left out",
            "task_groups": [
                {
                    "name": "INGEST",
                    "tasks": [
                        {"name": "task_name"}
                    ]
                }
            ]
        }
    ]
}


</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks_full.json">
{
    "plans": [

        {
            "name": "plan_name",
            "description": "This is to test that a fully configured plan passes",
            "environments" : ".*",
            "enabled": true,
            "task_groups": [
                {
                    "name": "PREPROCESS",
                    "tasks":[
                        {"name": "task_name", "sequence": 1, "enabled": true}
                    ]
                },
                {
                    "name": "INGEST",
                    "tasks": [
                        {"name": "task_name", "sequence": 1, "enabled": true}
                    ]
                }
            ]
        }
    ]
}


</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-plantasks_skip_environment.json">
{
    "plans": [
        {
            "name": "skip_plan",
            "description": "This is to test that the plan is NOT deployed when the environment is not matched",
            "environments" : "none"
        }
    ]
}


</file>
<file name="src\powershell\modules\DataConfig\.test_config\testIngestionMetadata-preprocess_valid.json">
{
  "preprocess": [
    {
      "name": "preprocess_test",
      "description": "Run the preprocessing template worker",
      "task_type" : "SYNAPSE_PIPELINE",
      "worker_pipeline": "pl_preprocess_template",
      "source_folder": "", 
      "source_pattern": "adobe_analytics/%/adobe_analytics/%/finished",
      "worker_properties": {
        "foo": "bar"
      }
    }
  ]
}

</file>
<file name="src\powershell\modules\DataConfig\.test_config\testInvalidJson.json">
{
    "invalid" : not valid json
}
</file>
<file name="src\powershell\modules\DataConfig\public\Compare-IngestionMetadataFromJsonFile.ps1">
function Compare-IngestionMetadataFromJsonFile {
    [CmdletBinding()]

    param(
    
        [Parameter (Mandatory=$true, HelpMessage="Path to the file that is being checked")]
        [string] $Path,

        [Parameter (Mandatory=$false, HelpMessage="Name of the schema-file in the schema folder where to compare the to-be-checked file to")]
        [string] $schemaFile
    )

    # Set a default for the schemaFile is none is given
    if (-not $schemaFile){
        $schemaFile = "IngestionMetadata.schema.json"
    }
    $schemaPath = Join-Path $PSScriptRoot "../schemas" $schemaFile
    if ( -Not ( Test-Path $schemaPath) ) {
        Write-Error "Cannot locate IngestionMetadata schema file: $schemaPath"
        return
    }

    #Load assignments data
    $IngestionJson = Get-Content -Raw -Path $Path

    # verify against schema definition
    if ( -Not (Test-Json -Json $IngestionJson -ErrorAction "SilentlyContinue") ) {
        throw "Configuration file '$Path' is not a valid json file"
    }

    $valid = Test-Json -Json $IngestionJson -SchemaFile $schemaPath -ErrorAction "SilentlyContinue" -ErrorVariable "jsonErrors"
    if ( -Not $Valid) {        
        if (Get-Member -inputobject $jsonErrors[0] -name "Message" -Membertype Properties) {
            $message = $jsonErrors[0].Message
        } elseif ((Get-Member -inputobject $jsonErrors[0] -name "ErrorDetails" -Membertype Properties) `
            -and ($jsonErrors[0].ErrorDetails) ){
            $message = $jsonErrors[0].ErrorDetails.Message
        } elseif (Get-Member -inputobject $jsonErrors[0] -name "Exception" -Membertype Properties) {
            $message = $jsonErrors[0].Exception.Message
        } else {
            $message = "Uknown error"
        }
       
        throw ("configuration file '$Path' is not a valid json file for schema: $schemaFile `n" + $message )
    } 

}
</file>
<file name="src\powershell\modules\DataConfig\public\Compare-IngestionMetadataFromJsonFile.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Compare-IngestionMetadataFromJsonFile" {

    Context "UnitTests" {

        BeforeAll {

            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
        }

        It "Dataset should pass when all mandatory parameters are configured" {
                    
            $invalidFile = Join-Path $TestFilePath "testIngestionMetadata-datasets-mandatory_keys.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $invalidFile -ErrorAction "Stop" } | 
            Should -Not -Throw
        }

        It "Plan should pass when all mandatory parameters are configured" {
                    
            $invalidFile = Join-Path $TestFilePath "testIngestionMetadata-plantasks-mandatory_keys.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $invalidFile -ErrorAction "Stop" } | 
            Should -Not -Throw
        }

        It "Preprocessing should pass when all mandatory parameters are configured" {
                    
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-preprocess_valid.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } | 
            Should -Not -Throw
        }

        It "Should fail if there is a non-configured additional key" {
                    
            $invalidFile = Join-Path $TestFilePath "testIngestionMetadata-datasets-extra_key.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $invalidFile -ErrorAction "Stop" } | 
            Should -Throw "*not a valid json file for schema*"
        }

        It "Should fail because of missing mandatory basic keys" {
                    
            $invalidFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_invalid.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $invalidFile -ErrorAction "Stop" } | 
            Should -Throw "*not a valid json file for schema*"
        }

        It "Should fail if extension is not in enum"{
            $invalidFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_invalid-extension.json"   
            
            # support powershell &lt; 7.4 for now
            if ( $PSVersionTable.PSVersion.ToString() -ge "7.4") {
                $expectedErrorMessage = "*should match one of the values specified by the enum*"
            } else {
                $expectedErrorMessage = "*NotInEnumeration*"
            }
            { Compare-IngestionMetadataFromJsonFile -Path $invalidFile -ErrorAction "Stop" } | 
                Should -Throw $expectedErrorMessage
        }

        It "Should fail on incorrect datePart for partitioning" {
            $invalidFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_invalid-tableoptions.json"
            $expectedErrorMessage = "*should match one of the values specified by the enum*datePart*"
            { Compare-IngestionMetadataFromJsonFile -Path $invalidFile -ErrorAction "Stop" } |
            Should -Throw $expectedErrorMessage
        }

        It "Should succeed if valid datasets-schema" {
                    
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_full.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if valid plantask-schema" {
                    
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-plantasks_full.json"   
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" }|
            Should -Not -Throw
        }

        It "Should fail if task_type is not in enum" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_tasktype_error.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Throw "*is*not*a*valid*json*"
        }

        It "Should succeed if task_type is SYNAPSE_PIPELINE" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_tasktype_SYNAPSE_PIPELINE.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if task_type is SPARK_NOTEBOOK"{
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_tasktype_SPARK_NOTEBOOK.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if worker is pl_unzip_worker" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_worker_pl_unzip_worker.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if worker is pl_dummy_worker" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_worker_pl_dummy_worker.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if all datatypes are expected" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_datatypes.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if expected column_options are given" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_formatting.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -ErrorAction "Stop" } |
            Should -Not -Throw
        }

        It "Should succeed if all delta table metadata is expected" {
            $validFile = Join-Path $TestFilePath "testIngestionMetadata-delta_tables.json"
            { Compare-IngestionMetadataFromJsonFile -Path $validFile -schema "DeltaTableData.schema.json" -ErrorAction "Stop" } |
            Should -Not -Throw
        }
    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Get-CompletedSparkStatements.ps1">
function Get-CompletedSparkStatements {
    &lt;#
    .SYNOPSIS
    Loop through all Spark Statements running on a Spark Session and add the completed ones to the CompletedJobs array

    .DESCRIPTION
    As a status report, it is often interesting to know how many initiated jobs have completed
    This function will loop through all jobs running on a Spark Session and append them to the list of completed ones.
    The current functionality is focused on giving more information to the user, but it could be extended in the future with:
        - Stopping the process if a job failed
        - Getting more insights in the jobs running against a spark session
        - ...

    .PARAMETER StartStatementId
    ID of the first statement initiated on the Spark Pool that is relevant for the search.
    Only the statements with an ID higher than the startStatementId will be taken into consideration

    .OUTPUTS
    Return a list of all IDs of Spark Jobs that have completed. This can be used later on for user information

    #&gt;

    [CmdletBinding()]

    param(
        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse workspace where to invoke the session" )]
        [string] $WorkspaceName,

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse spark pool that is running the jobs" )]
        [string] $PoolName,
        
        [Parameter(Mandatory=$true, HelpMessage="PSSynapseSparkSession object")]
        [psobject] $SessionObj,
        
        [Parameter(Mandatory=$false, HelpMessage="Array of jobs that were completed in the past")]
        [array] $CompletedJobs = @(), 

        [Parameter(Mandatory=$false, HelpMessage="ID of the first spark statement that needs to be checked. Ignore all statements with a lower ID as no longer relevant")]
        [string] $StartStatementId = 0
    )

    # Local scripts settings
    Set-StrictMode -Version "latest"

#region Get list of completed spark statements
    # Get the list of spark statements executed on the sessionObject
    $sparkStatements = Get-AzSynapseSparkStatement -WorkSpaceName $WorkspaceName -SparkPoolName $PoolName -SessionId $sessionObj.Id
    # Filter the list to only have the statements with an ID higher than the StartStatementId
    [array] $sparkStatements = ($sparkStatements | Where-Object Id -ge $StartStatementId) ?? @()
    
    # For each (remaining) statement: If it has a status, this means it has completed
    # Add the ID of the statement to the $CompletedJobs array
    $sparkStatements | ForEach-Object {
        if ($_.Output.Status) {
            if (-not ($CompletedJobs -contains $_.Id)){
                $CompletedJobs += $_.Id
            }
        }
    }
#endregion

    # Return the array of completed jobs
    return $CompletedJobs
}
</file>
<file name="src\powershell\modules\DataConfig\public\Get-CompletedSparkStatements.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Get-CompletedSparkStatements" {
    Context "UnitTests" {
        BeforeAll{
            # Mock the Get-AzSynapseSparkStatement cmdlet
            # Define the outputted result: 2 succeeeded and 1 failed spark statement
            Mock -CommandName Get-AzSynapseSparkStatement -MockWith {
                return @(
                    [PSCustomObject]@{ Id = "1"; Output = [PSCustomObject]@{ Status = "Succeeded" } },
                    [PSCustomObject]@{ Id = "2"; Output = [PSCustomObject]@{ Status = "Failed" } },
                    [PSCustomObject]@{ Id = "3"; Output = [PSCustomObject]@{ Status = "Succeeded" } },
                    [PSCustomObject]@{ Id = "4"; Output = [PSCustomObject]@{ Status = $null} }
                )
            }
        }

        It "Should return all jobs that have a non-empty Status property" {
            # Define the parameters used to call the function
            $completedJobs = @(1)
            $workspaceName = "TestWorkspace"
            $poolName = "TestPool"
            $sessionObj = [PSCustomObject]@{ Id =123 }

            # Call the function under test
            $result = Get-CompletedSparkStatements -CompletedJobs $completedJobs -WorkspaceName $workspaceName -PoolName $poolName -SessionObj $sessionObj

            # Expect all completed jobs to be in the array
            $result | Should -Be @(1, 2, 3)
        }

        It "Should return the same array with new completed jobs added" {
            # Define the parameters used to call the function
            $completedJobs = @(4, 5)
            $workspaceName = "TestWorkspace"
            $poolName = "TestPool"
            $sessionObj = [PSCustomObject]@{ Id =123 }

            # Call the function under test
            $result = Get-CompletedSparkStatements -CompletedJobs $completedJobs -WorkspaceName $workspaceName -PoolName $poolName -SessionObj $sessionObj

            # Expect the result to be the same as the input completed jobs
            $result | Should -Be @(4, 5, 1, 2, 3)
        }

        It "Should handle an empty CompletedJobs list" {
            # Define the parameters used to call the function
            $completedJobs = @()
            $workspaceName = "TestWorkspace"
            $poolName = "TestPool"
            $sessionObj = [PSCustomObject]@{ Id =123 }

            # Call the function under test
            $result = Get-CompletedSparkStatements -CompletedJobs $completedJobs -WorkspaceName $workspaceName -PoolName $poolName -SessionObj $sessionObj

            # Expect the result to contain all job IDs with a non-empty Status
            $result | Should -Be @(1, 2, 3)
        }

        It "Should not return jobs with ID lower than the StartStatementId" {
            # Define the parameters used to call the function
            $completedJobs = @(4)
            $workspaceName = "TestWorkspace"
            $poolName = "TestPool"
            $sessionObj = [PSCustomObject]@{ Id =123 }
            $startStatementID = 2

            
            $result = Get-CompletedSparkStatements -CompletedJobs $completedJobs -WorkspaceName $workspaceName -PoolName $poolName -SessionObj $sessionObj -startStatementID $startStatementID

            # Expect the result to contain all job IDs higher than StartStatementId and that were previously in the CompletedJobs array
            $result | Should -Be @(4, 2, 3)   
        }
    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Get-FailedSparkStatements.ps1">
function Get-FailedSparkStatements {
    &lt;#
    .SYNOPSIS
    Return an error message when the Spark Session has failed jobs, and close the session

    .DESCRIPTION
    The execution of Spark Jobs happens without a validation of whether the jobs have completed successfully.
    This function will loop through the States of all jobs that have been executed.

    .OUTPUTS
    $failedStatement: Boolean value indicating whether there was a failed Spark Statement 

    .PARAMETER StartStatementId
    ID of the first statement initiated on the Spark Pool that is relevant for the search.
    Only the statements with an ID higher than the startStatementId will be taken into consideration

    #&gt;

    [CmdletBinding()]

    param(

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse workspace where to invoke the session" )]
        [string] $WorkspaceName,

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse spark pool to invoke a session from" )]
        [string] $PoolName,
        
        [Parameter(Mandatory=$true, HelpMessage="PSSynapseSparkSession object")]
        [psobject] $SessionObj,

        [Parameter(Mandatory=$false, HelpMessage="ID of the first spark statement that needs to be checked. Ignore all statements with a lower ID as no longer relevant")]
        [psobject] $StartStatementId = 0,
        
        [Parameter(Mandatory=$false, HelpMessage="Boolean value indicating whether to raise an error")]
        [psobject] $RaiseError = $false
    )

    # Local scripts settings
    Set-StrictMode -Version "latest"

#region Loop through completed Spark Statements and print an error message when there is a failure
    Write-Verbose "Checking for failed jobs..."
    $logs = Get-AzSynapseSparkStatement -WorkSpaceName $WorkspaceName -SparkPoolName $poolName -SessionId $sessionObj.Id
    # Only list the statements that were initiated after the startStatementId
    [array] $logs = ($logs | Where-Object Id -gt $startStatementId) ?? @()
    [boolean] $failedStatement = $false
    # set $failedStatement = $true when at least one statement has Status = error
    $logs | ForEach-Object {
        if  ($_.Output.Status -eq 'error'){
            Write-Information "ErrorValue: $($_.Output.ErrorValue) `n`n"
            Write-Verbose "Error for statement: $($_.Code)"

            $failedStatement = $true
            $failedStatement | Out-Null #get rid syntax-warning
        }
    }
#endregion

#region Stop the process if there was an error and it RaiseError = $true
    if ($RaiseError -and $failedStatement){
        throw "Stop the process..."
    }
#endregion

    # return the boolean value to use in caller function
    return $failedStatement
}
</file>
<file name="src\powershell\modules\DataConfig\public\Get-FailedSparkStatements.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe 'Get-FailedSparkStatements' {
    
    Context "UnitTests" {
        # Mock the cmdlets used in the function
        BeforeAll {
            mock Get-AzSynapseSparkStatement {}
        }
        # Test case for successful execution with no errors
        It 'Should return $false when there are no failed statements' {
            # Define function parameters
            $WorkspaceName = 'TestWorkspace'
            $poolName = 'TestPool'
            $sessionObj = [pscustomobject]@{ Id = 123 }

            # Mock the output of Get-AzSynapseSparkStatement
            Mock -CommandName Get-AzSynapseSparkStatement -MockWith {
                return @(
                    [pscustomobject]@{ Id = 1; Output = [pscustomobject]@{ Status = 'success' } },
                    [pscustomobject]@{ Id = 2; Output = [pscustomobject]@{ Status = 'success' } }
                )
            }

            # Call the function under test
            $result = Get-FailedSparkStatements -WorkspaceName $WorkspaceName -poolName $poolName -SessionObj $sessionObj

            # Expect $failedStatements = $result = $false
            $result | Should -Be $false
            # Expect the following function calls
            Assert-MockCalled Get-AzSynapseSparkStatement -Exactly 1 -Scope It
        }

        # Test case for execution with one failed statement
        It 'Should return $true when there is at least one failed statement' {
            # Define function parameters
            $WorkspaceName = 'TestWorkspace'
            $poolName = 'TestPool'
            $sessionObj = [pscustomobject]@{ Id = 123 }

            # Mock the output of Get-AzSynapseSparkStatement
            Mock -CommandName Get-AzSynapseSparkStatement -MockWith {
                return @(
                    [pscustomobject]@{ Id = 1; Output = [pscustomobject]@{ Status = 'error'; ErrorValue = 'Some error'}; Code = "unittest"  },
                    [pscustomobject]@{ Id = 2; Output = [pscustomobject]@{ Status = 'success' } }
                )
            }

            # Call the function under test
            $result = Get-FailedSparkStatements -WorkspaceName $WorkspaceName -poolName $poolName -SessionObj $sessionObj

            # Expect $failedStatements = $result = $true
            $result | Should -Be $true
            # Expect the following function calls
            Assert-MockCalled Get-AzSynapseSparkStatement -Exactly 1 -Scope It
        }

        # Test case for no logs returned
        It 'Should return $false when no logs are returned' {
            # Define function parameters
            $WorkspaceName = 'TestWorkspace'
            $poolName = 'TestPool'
            $sessionObj = [pscustomobject]@{ Id = 123 }

            # Mock the output of Get-AzSynapseSparkStatement to return an empty array
            Mock -CommandName Get-AzSynapseSparkStatement -MockWith {
                return @()
            }

            # Call the function under test
            $result = Get-FailedSparkStatements -WorkspaceName $WorkspaceName -poolName $poolName -SessionObj $sessionObj

            # Expect $failedStatements = $result = $false
            $result | Should -Be $false
            # Expect the following function calls
            Assert-MockCalled Get-AzSynapseSparkStatement -Exactly 1 -Scope It
        }

        It 'Should return $false when failed statements have a lower ID than startStatementId' {
            # Define function parameters
            $WorkspaceName = 'TestWorkspace'
            $poolName = 'TestPool'
            $sessionObj = [pscustomobject]@{ Id = 123 }
            $startStatementId = 124

            # Mock the output of Get-AzSynapseSparkStatement to return an empty array
            Mock -CommandName Get-AzSynapseSparkStatement -MockWith {
                [pscustomobject]@{ Id = 122; Output = [pscustomobject]@{ Status = 'error'; ErrorValue = 'Some error'}; Code = "unittest"  },
                [pscustomobject]@{ Id = 124; Output = [pscustomobject]@{ Status = 'success' } }
            }

            # Call the function under test
            $result = Get-FailedSparkStatements -WorkspaceName $WorkspaceName -poolName $poolName -SessionObj $sessionObj -startStatementId $startStatementId

            # Expect $failedStatements = $result = $false
            $result | Should -Be $false
            # Expect the following function calls
            Assert-MockCalled Get-AzSynapseSparkStatement -Exactly 1 -Scope It
        }
    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Get-FilteredFileList.ps1">
&lt;#
.SYNOPSIS
    Filters the files in a directory based on the FilterType parameter

.DESCRIPTION
    Evaluates all files in the SourcePath directory (recursive or not) and filters them
    based on the FilterType set.
    
    Options for FilterType:
    - "all" : loop through all files found in the SourcePath (recursively or not)
    - "filelist" : limit the list of files in the SourcePath to the ones listed in the file specifieid in the FilterValue parameter
    - "path" : limit the files by appling a regex match using the FilterValue parameter
    - "branch" : limit the files based on a git diff against the branch specified in the FilterValue parameter
    - "tag" : use tag instead of branch. will look for the "latest" tag starting with the value in the FilterValue parameter
        by doing a descending sort on the tags.
        For example, specifying "mytag" will select "mytag_1500" and not "mytag_1400" but also never "prefix_mytag"

    Note: When in filelist mode, the paths in the files must either be absolute, or will be evaluated as relative to the SourcePath.

.PARAMETER OutputFilePath
    If specified, the output will be written to the file specified in this parameter. If not, the function
    will write the list of files found to normal output stream.

.PARAMETER OutputRelativePath   
    If set, the output (either to file or output stream) will have the paths relative to the RelativeBasePath.

.PARAMETER RelativeBasePath
    Only used when OutputRelativePath is set. If specified, output paths will be relative to this path.   
    If not specified, output paths will be relative to the current working directory (Get-Location).

    Example: if SourcePath is "c:\src\myfolder" and OutputRelativePath is "c:\src" then the output paths will
    look like './myfolder/file1.txt' instead of 'c:\src\myfolder\file1.txt'

#&gt;

function Get-FilteredFileList {

    [CmdletBinding()]
    param (
        [Parameter(Mandatory=$true, HelpMessage="Filter type: all, path, branch, or tag")]
        [ValidateSet('all','filelist','path','branch','tag')]
        [String]$FilterType,

        # support "*.json" for example
        [Parameter(Mandatory=$false, HelpMessage="Regex filter to apply to files, on top of the FilterType")]
        [String]$BasicFilter = $null,

        [Parameter(Mandatory=$false, HelpMessage="Filter value, eitherthe path, branch name, or tag name")]	
        [string]$FilterValue = $null, 

        [Parameter(Mandatory=$true, HelpMessage="Path to the source directory")]	
        [String]$SourcePath,

        [Parameter(Mandatory=$false, HelpMessage="Recursive find in SourcePath, default is true")]	
        [Boolean]$Recurse = $true,

        [Parameter(Mandatory=$false, HelpMessage="If specified, output file which will have list of the matching files")]	
        [String]$OutputFilePath = $null,

        [Parameter(Mandatory=$false, HelpMessage="If set, output path will be relative to the RelativeBasePath parameter")]	
        [Switch]$OutputRelativePath,

        [Parameter(Mandatory=$false, HelpMessage="If specified, relative paths will be relative to this path. Required when OutputRelativePath is set")]	
        [String]$RelativeBasePath = (Get-Location)

    )


#region local script settings
    Set-StrictMode -Version "latest"
#endregion

#region script info
    $inputParameters = $MyInvocation.MyCommand.Parameters.Keys | 
    Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::CommonParameters} | 
    Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::OptionalCommonParameters}
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information "----------------------------------------------------------------------"
    Write-Information "Starting function: $($MyInvocation.MyCommand.Name)"
    Write-Information "  Parameters:"
    foreach ($param in $inputParameters) {
        $paramValue = (Get-Variable -Name $param).Value   
        Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + $paramValue )
    }
    Write-Information "----------------------------------------------------------------------"
#endregion script info

#region parameter validation

    # check SourcePath
    if ( -not (Test-Path -Path $SourcePath -PathType "Container" ) ) {
        throw "SourcePath does not exist or is not a directory: $SourcePath"
    }

    # check: basic filter
    if ( $BasicFilter ) {
        try {
            [void]([regex]::new($BasicFilter))
        } catch {
            throw "BasicFilter is not a valid regex expression: $BasicFilter"
        }
    }

    # check type: Path
    if ( $FilterType -eq "path") {
        if ( -not $FilterValue ) {
            throw "FilterValue parameter is mandatory when FilterType is 'path'"
        }
        try {
            [void]([regex]::new($BasicFilter))
        } catch {
            throw "BasicFilter is not a valid regex expression: $BasicFilter"
        }

    }

    # check type: filelist
    if ( $FilterType -eq "filelist" ) {
        # file must exist
        if ( -not (Test-Path -Path $FilterValue -PathType Leaf) ) {
            throw "The file provided in FilterValue parameter must exist when FilterType is 'filelist'"
        }
    }    

    # check output file path
    if ( $OutputFilePath ) {
        # create folder if it does not exist
        $OutputFolder = Split-Path -Path $OutputFilePath
        if ( -not (Test-Path -Path $OutputFolder -PathType "Container") ) {
            New-Item -Path $OutputFolder -ItemType "Directory" -Force
        }
    }
#endregion parameter validation

#region get file list and basic filter
    [array] $files = Get-ChildItem -Path $SourcePath -Recurse:$Recurse -File  # -Relative 
    if ( -not $files) {
        if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
        Write-Warning "No files found in SourcePath: $SourcePath"
        return
    }
    [array] $files = $files.FullName 
    Write-Information "Found $($files.Count) files before filtering"
    if ( $files.Count -gt 10) {
         Write-Verbose "(Showing first 10 files only)"
    }
    $files | Select-Object -First 10 | ForEach-Object { Write-Verbose "  $_"}
    
    # apply basic filter (if specified)
    if($BasicFilter) {
        [array] $files =  $files | Where-Object { $_ -match $BasicFilter } 
        if ( -not $files) {
            if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
            Write-Information "No files left after applying basic filter: $BasicFilter"
            return
        }   
        Write-Information "Retained $($files.Count) files after basic filtering"
        if ( $files.Count -gt 10) {
            Write-Verbose "(Showing first 10 files only)"
        }
        $files | Select-Object -First 10 | ForEach-Object { Write-Verbose "  $_"}
    }
#endregion

#region filelist filter
if($FilterType -eq "filelist") {

    # read the filelist file
    $filelistRaw = (Get-Content -Path $FilterValue) ?? @()
    if ( -not $filelistRaw) {
        if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
        Write-Information "The filelist is empty, no files selected"
        return
    } 

    # convert the files to absolute paths (based on the $SourcePath)
    # if the paths were absolute, they will be left as is
    [array] $filelist = ($filelistRaw | Resolve-Path -RelativeBasePath $SourcePath).Path

    # filter the $files
    [array] $files = $files | Where-Object { $filelist -contains $_  }
    if ( -not $files) {
        if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
        Write-Information "No files to filter after applying filelist filter"
        return
    }   
}
#endregion

#region path filter

    if($FilterType -eq "path") {
        [array] $files = $files | Where-Object { $_ -match $FilterValue }
        if ( -not $files) {
            if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
            Write-Information "No files to filter after applying path filter: $FilterValue"
            return
        }   

    }

#endregion

#region branch filter
    if( $FilterType -eq "branch" ) {

        # first get the list of changed files in the branch (exclude deleted files)
        [array] $changedFiles = @(git diff --name-only --diff-filter d "$FilterValue" $SourcePath/* ) | Resolve-Path
        if ( -not $changedFiles) {
            if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
            Write-Information "There are no files different with branch [$FilterValue] in source path [$SourcePath]"
            return
        } else {
            [array] $changedFiles = $changedFiles.Path
            Write-Verbose "Branch filter: detected $($changedFiles.Count) file(s) changed compared to branch [$FilterValue]:"
            if ( $changedFiles.Count -gt 10) {
                Write-Verbose "(Showing first 10 files only)"
            }
            $changedFiles | Select-Object -First 10 | ForEach-Object { Write-Verbose "  $_"}
        }

        # now limit that to the files filtered so far
        [array] $files = $files | Where-Object {$changedFiles -contains $_}            
        if ( -not $files) {
            if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
            Write-Information "No applicable files left after applying branch comparison with branch [$FilterValue]"
            return
        }
    }
#endregion

#region tag filter

    if( $FilterType -eq "tag" ) {

        # first find latest tag
        $LatestTag = git tag | Where-Object { $_ -like "${FilterValue}*"} | Sort-Object -Descending | Select-Object -First 1
        if ( $LatestTag) {
            Write-Information "Found latest tag: $LatestTag"

            # get files changed compared to the tag (exclude deleted files)
            [array] $changedFiles = @(git diff --name-only --diff-filter d "$LatestTag" $SourcePath/* ) | Resolve-Path 
            if ( -not $changedFiles) {
                if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
                Write-Information "There are no files different with tag [$LatestTag] in source path [$SourcePath]"
                return
            } else {
                [array] $changedFiles = $changedFiles.Path
                Write-Verbose "Tag filter: detected $($changedFiles.Count) file(s) changed compared to branch [$LatestTag]:"
                if ( $changedFiles.Count -gt 10) {
                    Write-Verbose "(Showing first 10 files only)"
                }
                    $changedFiles | ForEach-Object { Write-Verbose "  $_"}
            }    
    
            # now limit that to the files filtered so far
            [array] $files = $files | Where-Object {$changedFiles -contains $_}            
            if ( -not $files) {
                if( $OutputFilePath) { New-Item -Path $OutputFilePath -Type File -Force | Out-Null }
                Write-Information "No applicable files left after applying branch comparison with tag [$FilterValue]"
                return
            }

        } else {
            # no tag found , first time? we'll keep all files
            Write-Warning "Could not locate any existing tag starting with '$FilterValue', not applying any filter"
        }

        
    }

#endregion

#region output to file or output stream

    Write-Information "Retained $($files.Count) file(s) after filtering in [$FilterType] mode"

    # make the output relative, if OutputRelativePath is specified
    if( $OutputRelativePath ) {
        $files = $files | Resolve-Path -Relative -RelativeBasePath $RelativeBasePath
    }

    # write to selected output file...
    if ( $OutputFilePath ) {
        $files | Out-File -FilePath $OutputFilePath -Encoding "utf8" -Force

        # still useful to have a sample in information stream
        if ( $files.Count -gt 10) {
            Write-Information "Showing first 10 lines (only):"
        } else {
            $files | Select-Object -First 10 | ForEach-Object { Write-Information "  $_"} 
        }
    }

    # ... or to output stream
    else {
            $files | Write-Output
        }
    
#endregion

Write-Information "*** End function: $($MyInvocation.MyCommand.Name) ***"

}

</file>
<file name="src\powershell\modules\DataConfig\public\Get-FilteredFileList.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    # (none)

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest
    $ErrorActionPreference = "Stop"

}

Describe "Get-FilteredFileList" {

    Context "UnitTests" {

        # Setup of the tests inside the given context
        BeforeAll {

            # get location of testdrive
            # the Get-ChildItem cmdlet returns the absolute path of the testdrive files
            # so we need to store the path of the testdrive to be able to compare the results
            $testFolder =  (Get-Item -Path "TestDrive:").FullName
            $testFolder | Out-Null # suppress warning

            # filename for outputfile$
            $outputFilePath = Join-Path "TestDrive:" "filelist.txt"
            $outputFilePath | Out-Null # suppress warning

            # prepare empty source folder
            $null = New-Item -Path (Join-Path "TestDrive:" "emptyfolder") -ItemType Directory

            # prepare non-empty sourcefolder 
            $null = New-Item -Path 'TestDrive:/src' -ItemType Directory
            $null = New-Item -Path 'TestDrive:/src/aaa.json' -ItemType File
            $null = New-Item -Path 'TestDrive:/src/aab.json' -ItemType File
            $null = New-Item -Path 'TestDrive:/src/bbb.json' -ItemType File
            $null = New-Item -Path 'TestDrive:/src/readme.md' -ItemType File
            $null = New-Item -Path 'TestDrive:/src/sub' -ItemType Directory
            $null = New-Item -Path 'TestDrive:/src/sub/aac.json' -ItemType File
            $null = New-Item -Path 'TestDrive:/src/sub/test.json.obsolete' -ItemType File
            
        }

        Context "Empty or nonexisting source path" {

            It "Should fail when source folder does not exist" {
                # Test that Invoke-Sqlcmd is invoked when provided with right params
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "badfolder"
                    FilterType = 'all'                
                }
                        
                { Get-FilteredFileList @params } | Should -Throw -ExpectedMessage "*SourcePath*"

                Test-Path $outputFilePath | Should -Be $false

            }

            It "Should return no files when source folder is empty" {
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "emptyfolder"
                    FilterType = 'all'
                    WarningAction = 'SilentlyContinue'
                }
                [Array] $response =  Get-FilteredFileList @params
                $response | Should -BeNullOrEmpty           
            }

            It "Should create empty file when source folder is empty" {
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "emptyfolder"
                    FilterType = 'all'
                    OutputFilePath = $outputFilePath
                }
                [Array] $response =  Get-FilteredFileList @params
                $response | Should -BeNullOrEmpty
                
                Test-Path $outputFilePath | Should -Be $true
                Get-Content $outputFilePath | Should -BeNullOrEmpty
            }
        }
            
        Context "Basic filtering" {
                
            It "Should return all files in source folder (recursive by default)" {
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "src"
                    FilterType = 'all'
                }
                [Array] $response =  Get-FilteredFileList @params
                $response.Count | Should -Be 6
            }

            It "Should return nothing, but create filelist with all files" {
                $params = @{
                    SourcePath = 'TestDrive:\src'
                    FilterType = 'all'
                    OutputFilePath = $outputFilePath
                }
                [Array] $response =  Get-FilteredFileList @params
                $response | Should -BeNullOrEmpty
                
                Test-Path $outputFilePath | Should -Be $true
                [array] $response = Get-Content $outputFilePath
                $response.Count | Should -Be 6
            }

            It "Should return all files in source folder (non-recursive)" {
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "src"
                    FilterType = 'all'
                    Recurse = $false
                }
                [Array] $response =  Get-FilteredFileList @params
                $response.Count | Should -Be 4
                $response | Should -Contain (Join-Path "$testFolder" "src" "aaa.json")
                $response | Should -Contain (Join-Path "$testFolder" "src" "aab.json")
                $response | Should -Contain (Join-Path "$testFolder" "src" "bbb.json")
            }        
            
            It "Should return all files in source folder (recursive) with basic filter" {
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "src"
                    FilterType = 'all'
                    Recurse = $true
                    BasicFilter = '\.json$'
                }
                [Array] $response =  Get-FilteredFileList @params
                $response.Count | Should -Be 4
                $response | Should -Contain (Join-Path "$testFolder" "src" "aaa.json")
                $response | Should -Contain (Join-Path "$testFolder" "src" "aab.json")
                $response | Should -Contain (Join-Path "$testFolder" "src" "bbb.json")
                $response | Should -Contain (Join-Path "$testFolder" "src" "sub" "aac.json")
            }

        
        Context "Relative paths" {
            It "Should return relative paths relative to specified path" {

                $relativeBasePath = Join-Path $testFolder "src"
                $params = @{
                    SourcePath = Join-Path "TestDrive:" "src"
                    FilterType = 'all'
                    Recurse = $true
                    BasicFilter = '\.json$'
                    OutputRelativePath = $true
                    RelativeBasePath = $relativeBasePath
                }
                [Array] $response =  Get-FilteredFileList @params
                $response.Count | Should -Be 4

                $response | Should -Contain (Join-Path "." "aaa.json")
                $response | Should -Contain (Join-Path "." "aab.json")
                $response | Should -Contain (Join-Path "." "bbb.json")
                $response | Should -Contain (Join-Path "." "sub" "aac.json")
            } 

        }

        It "Should return relative paths relative to current working dir" {

            # we're mocking Get-Location to return the root of the testdrive
            $testDriveRoot = ($testFolder | Split-Path -Qualifier) + [IO.Path]::DirectorySeparatorChar
            Mock Get-Location { return $testDriveRoot }

            $params = @{
                SourcePath = Join-Path "TestDrive:" "src"
                FilterType = 'all'
                Recurse = $true
                BasicFilter = '\.json$'
                OutputRelativePath = $true
                
            }
            [Array] $response =  Get-FilteredFileList @params
            $response.Count | Should -Be 4

            $expectedBasePath = Join-Path "." ($testFolder.replace($testDriveRoot, ""))

            $response | Should -Contain (Join-Path $expectedBasePath "src" "aaa.json")
            $response | Should -Contain (Join-Path $expectedBasePath "src" "aab.json")
            $response | Should -Contain (Join-Path $expectedBasePath "src" "bbb.json")
            $response | Should -Contain (Join-Path $expectedBasePath "src" "sub" "aac.json")
        } 

        }
        
        It "Should return only files in the source path mentioned in the filefilter path" {

            # create the input filelist
            $filefilterPath = Join-Path "TestDrive:" "input-filelist.txt"
            (Join-Path $testFolder "src" "sub" "aac.json") | Set-Content -Path $filefilterPath

            # run the function
            $params = @{
                SourcePath = 'TestDrive:/src'
                Recurse = $true
                BasicFilter = '\.json$'
                FilterType = 'filelist'
                FilterValue = $filefilterPath
            }
            [Array] $response =  Get-FilteredFileList @params
            $response.Count | Should -Be 1
            $response | Should -Contain (Join-Path "$testFolder" "src" "sub" "aac.json")
        }

        It "Should return only files in the source path mentioned in the filefilter path when it contains relative paths" {

            # create the input filelist
            $filefilterPath = Join-Path "TestDrive:" "input-filelist.txt"
            (Join-Path "." "sub" "aac.json") | Set-Content -Path $filefilterPath

            # run the function
            $params = @{
                SourcePath = Join-Path 'TestDrive:' 'src'
                Recurse = $true
                BasicFilter = '\.json$'
                FilterType = 'filelist'
                FilterValue = $filefilterPath
            }
            [Array] $response =  Get-FilteredFileList @params
            $response.Count | Should -Be 1
            $response | Should -Contain (Join-Path "$testFolder" "src" "sub" "aac.json")
        }

        It "Should return all files in source folder (recursive) with basic and path filter" {
            $params = @{
                SourcePath = 'TestDrive:\src'
                Recurse = $true
                BasicFilter = '\.json$'
                FilterType = 'path'
                FilterValue = 'sub'

            }
            [Array] $response =  Get-FilteredFileList @params
            $response.Count | Should -Be 1
            $response | Should -Contain (Join-Path "$testFolder" "src" "sub" "aac.json")
        }

        It "Should return the files different from main branch" {

            # mock git diff to report some differences
            Mock git { 
                return @(
                        Join-Path $testFolder "src" "aaa.json"
                        Join-Path $testFolder "src" "readme.md"
                        Join-Path $testFolder "src" "sub" "aac.json"
                        Join-Path $testFolder "src" "sub" "test.json.obsolete"
                        )
            }

            $params = @{
                SourcePath = Join-Path "TestDrive:" "src"
                Recurse = $true
                BasicFilter = '\.json$'
                FilterType = 'branch'
                FilterValue = 'origin/main'

            }
            [Array] $response = ( Get-FilteredFileList @params ) ?? @()
            $response.Count | Should -Be 2
            $response | Should -Contain (Join-Path "$testFolder" "src" "aaa.json")
            $response | Should -Contain (Join-Path "$testFolder" "src" "sub" "aac.json")
        }

        It "Should return the files different from tag" {

            # mock git diff to report some differences
            Mock git { 
                return @(
                        Join-Path $testFolder "src" "aaa.json"
                        Join-Path $testFolder "src" "readme.md"
                        Join-Path $testFolder "src" "sub" "aac.json"
                        Join-Path $testFolder "src" "sub" "test.json.obsolete"
                        )
            }

            $params = @{
                SourcePath = Join-Path "TestDrive:" "src"
                Recurse = $true
                BasicFilter = '\.json$'
                FilterType = 'branch'
                FilterValue = 'origin/main'

            }
            [Array] $response = ( Get-FilteredFileList @params ) ?? @()
            $response.Count | Should -Be 2
            $response | Should -Contain (Join-Path "$testFolder" "src" "aaa.json")
            $response | Should -Contain (Join-Path "$testFolder" "src" "sub" "aac.json")
        }


    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Get-IngestionSparkSessionObject.ps1">
function Get-IngestionSparkSessionObject {
    &lt;#
    .SYNOPSIS
        Return an object of class PSSynapseSparkSession

    .DESCRIPTION
        Using the name of a Synapse workspace, spark pool, and session name, look for an existing spark session
        If a session exists, validate that the session is in a usable state. If so, return the object
        If there is no session (in a usable state), create a new session
            WaitForSession: Wait for the session to be started
            SetUpSession: If the session is usable, run basic comments 

    .NOTES

    #&gt;
    [CmdletBinding()]

    param(

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse workspace where to invoke the session" )]
        [string] $SynapseWorkspace,

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse spark pool to invoke a session from" )]
        [string] $SynapseSparkPool,

        [Parameter(Mandatory=$true, HelpMessage="Name of the spark session that will be invoked (or looked for)")]
        [string] $SessionName,

        [Parameter(Mandatory=$false, HelpMessage="Controls whether the script will wait for the session to be ready (default), or not")]
        [bool] $WaitForSession = $true,

        [Parameter(Mandatory=$false, HelpMessage="Controls whether the script will set-up the spark session (default), or not")]
        [bool] $SetUpSession = $true
    )

    # Local scripts settings
    Set-StrictMode -Version "latest"

    #region Step 1: Define parameters
       
        # the states in which sessions can be reused (in order of preference)
        #   idle = not busy but active
        #   busy = doing something at the moment
        #   success = indicates that the previous job completed successfully
        #   not_started and starting = 2 stages of a starting cluster
        $reusableStates = "idle", "success", "starting", "busy", "not_started"    
        $readyStates = "idle", "success", "busy"
    #endregion Step 1

    #region Step 2: Retrieve list of existing and active spark sessions

        # Check if there is an active session based on the status of the existing sessions
        Write-Information("Checking for active spark sessions for session name $($SessionName)...")
        # Get the full list of usable sessions with specified SessionName
        [array] $activeSessions = (Get-AzSynapseSparkSession -WorkspaceName $SynapseWorkspace -SparkPoolName $SynapseSparkPool | Where-Object { $_.Name -eq $SessionName } ) ?? @()
        # filter and sort by state
        if( $activeSessions) {
            $activeSessions = $activeSessions | Where-Object { $reusableStates -contains $_.State  }
            $activeSessions = $activeSessions | Sort-Object { $reusableStates.IndexOf($_.State) }
        }
        # debug output
        # $activeSessions | Select-Object Name, Id, State | ForEach-Object { Write-Information ("  " + $_ ) }
    #endregion

    #region Step 3: Return active spark session or create new one
        # if sessions left after filtering &amp; sorting, select first one
        if ( $activeSessions) {
            Write-Information "Found [$($activeSessions.Count)] usable session(s)"
            $sessionObj = $activeSessions | Select-Object -First 1
            Write-Information "  Selected first session with id [$($SessionObj.Id)], state [$($SessionObj.State)]"

            # wait for cluster to be ready
            $currentState = $sessionObj.State
            if ( $readyStates -contains $currentState ){
                $sessionObj = Get-AzSynapseSparkSession  -WorkspaceName $SynapseWorkspace -SparkPoolName $SynapseSparkPool -Id $sessionObj.Id
                if ($SetUpSession){
                    $code = 'from notebookutils import mssparkutils'
                    Start-IngestionSparkSessionJobs $SynapseWorkspace $SynapseWorkspace -SynapseSparkPool $SynapseSparkPool -SessionObj $sessionObj -Code $code -Job $false | Out-Null # Catch statementobj to not return as well
                }
                return $sessionObj
            }
            
            elseif ($WaitForSession){
                :waitForReadyLoop while( $readyStates -notcontains $currentState) {
                    if ( $reusableStates -notcontains $currentState) {
                        # unexpected (unusuable) state
                        Throw "Cluster in unusable state '$currentState'"
                    }
                    if ( $readyStates -contains $currentState) {
                        Write-Information "Cluster state: $currentState, ready to use!"
                        break waitForReadyLoop
                    }
                    Write-Information "Cluster state: $currentState, not ready, checking again in 30 seconds.."
                    Start-Sleep -Seconds 30;
                    $sessionObj = Get-AzSynapseSparkSession  -WorkspaceName $SynapseWorkspace -SparkPoolName $SynapseSparkPool -Id $sessionObj.Id
                    $currentState = $sessionObj.State
                }
            }
            else{
                return $null
            }
        }    

        # If there are no active sessions: Start a new session
        else {
            # start a new session and wait for it to start
            if( $WaitForSession ) {           
                Write-Information("No active session found, creating new session...")
                $processTimer = [System.Diagnostics.Stopwatch]::StartNew()
                $sessionObj = Start-AzSynapseSparkSession -WorkspaceName $SynapseWorkspace -SparkPoolName $SynapseSparkPool -Name $SessionName -Language 'Python' -ExecutorCount 1 -ExecutorSize Small 
                $processTimer.Stop()
                $ts = $processTimer.Elapsed
                $elapsedString = "{0:0}m {1:0}s" -f $ts.Minutes, $ts.Seconds
                #Write-Information("  Elapsed time: $elapsedString")
                $newSessionId = $sessionObj.Id
                Write-Information("New session created in " + $elapsedString + " with id: " + $NewSessionId)
                #for some reason, the session object returned is incomplete, get it again using the id
                $sessionObj = Get-AzSynapseSparkSession  -WorkspaceName $SynapseWorkspace -SparkPoolName $SynapseSparkPool -Id $newSessionId

                
                Write-Information "Using Session Object: "
                Write-Information  $sessionObj
            } 
            # we don't want to wait, so fire a start-session request in "AsJob" mode
            else {
                Write-Information("No active session found, creating new session (background) ...")
                # in AsJob mode, returns a AzureLongRunningJob object (and not a session object)
                $null = Start-AzSynapseSparkSession -WorkspaceName $SynapseWorkspace -SparkPoolName $SynapseSparkPool -Name $SessionName -Language 'Python' -ExecutorCount 1 -ExecutorSize Small -AsJob                
                return $null          
            }
        }

        # Dev-Info: You should only get here when $WaitForSession is also true. All other condiditions have already returned a value
        if ($SetUpSession){
            $code = 'from notebookutils import mssparkutils'
            Start-IngestionSparkSessionJobs $SynapseWorkspace $SynapseWorkspace -SynapseSparkPool $SynapseSparkPool -SessionObj $sessionObj -Code $code -Job $false | Out-Null # Catch statementobj to not return as well
        }

        return $sessionObj #$null or PSSynapseSparkSession
    #endregion
}

</file>
<file name="src\powershell\modules\DataConfig\public\Get-IngestionSparkSessionObject.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Get-IngestionSparkSessionObject" {

    Context "UnitTests" {

        BeforeAll {
            mock Get-AzContext {
                '{ "Account": "UnitTest" }' | ConvertFrom-Json
            }

            mock Start-IngestionSparkSessionJobs{}
            mock Start-Sleep {}
        }

        Context "Execute functions with idle spark session"{
            BeforeAll{
                $sparkObjectGet = [hashtable]@{
                    State = 'idle'
                    Name = 'Unittest'
                    Id = '123'
                    AppId = 'application_0000000000000_0000'
                    LivyInfo = @{
                        StartingAt = Get-Date
                    }
                    Submitter = "UnitTest"
                }
    
                $sparkObjectStart = [hashtable]@{
                    State = 'idle'
                    Name = 'Unittest'
                    Id = '456'
                    AppId = 'application_0000000000000_0000'
                    LivyInfo = @{
                        StartingAt = Get-Date
                    }
                }
    
                $mockGetSession = New-MockObject -Type Microsoft.Azure.Commands.Synapse.Models.PSSynapseSparkSession -Properties $sparkObjectGet
                $mockStartSession = New-MockObject -Type Microsoft.Azure.Commands.Synapse.Models.PSSynapseSparkSession -Properties $sparkObjectStart
    
                # Mock Az.Synapse functions that are called in the Start-AzSynapseSparkSession function
                mock Start-AzSynapseSparkSession {$mockStartSession}
                mock Get-AzSynapseSparkSession {$mockGetSession}
            }

            It "Should start a new spark session when no active session is found"{
                $localSessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $false -SetUpSession $false

                Should -Not -Invoke Start-AzSynapseSparkSession
                Should -Invoke Get-AzSynapseSparkSession
                $localSessionObj.Id | Should -Be  $sparkObjectGet.Id

            }
            It "Should install mssparkutils module when executing Invoke-AzSynapseSparkStatement call for new session"{
                Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $true -SetUpSession $true
                
                Should -Invoke Start-IngestionSparkSessionJobs -Times 1 -ParameterFilter { $code -eq 'from notebookutils import mssparkutils' }

            }
            It "Should not install mssparkutils module when SetUpSession is disabled"{
                Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $true -SetUpSession $false
                
                Should -Not -Invoke Start-IngestionSparkSessionJobs
            }
        }
        Context "Execute functions with inactive spark session"{
            BeforeAll{
                $sparkObjectGet = [hashtable]@{
                    State = 'unusablestate'
                    Name = 'Unittest'
                    Id = '123'
                    AppId = 'application_0000000000000_0000'
                    LivyInfo = @{
                        StartingAt = Get-Date
                    }
                    Submitter = "UnitTest"
                }
    
                $sparkObjectStart = [hashtable]@{
                    State = 'idle'
                    Name = 'Unittest'
                    Id = '456'
                    AppId = 'application_0000000000000_0000'
                    LivyInfo = @{
                        StartingAt = Get-Date
                    }
                }
    
                $mockGetSession = New-MockObject -Type Microsoft.Azure.Commands.Synapse.Models.PSSynapseSparkSession -Properties $sparkObjectGet
                $mockStartSession = New-MockObject -Type Microsoft.Azure.Commands.Synapse.Models.PSSynapseSparkSession -Properties $sparkObjectStart
    
                # Mock Az.Synapse functions that are called in the Start-AzSynapseSparkSession function
                mock Start-AzSynapseSparkSession {$mockStartSession}
                mock Get-AzSynapseSparkSession {$mockGetSession}
            }

            It "Should start a new spark session when no active session is found"{
                $localSessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $true -SetUpSession $true

                Should -Invoke Start-AzSynapseSparkSession
                Should -Invoke Get-AzSynapseSparkSession -Times 2
                $localSessionObj.Id | Should -Be  $sparkObjectGet.Id

            }

            It "Should not return a session object when WaitForSession is disabled"{
                $localSessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $false -SetUpSession $true

                Should -Invoke Start-AzSynapseSparkSession
                Should -Invoke Get-AzSynapseSparkSession -Times 1
                Should -Not -Invoke Start-IngestionSparkSessionJobs
                $localSessionObj | Should -Be $null

            }
            It "Should install mssparkutils module when executing Invoke-AzSynapseSparkStatement call for new session"{
                Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $true -SetUpSession $true
                
                Should -Invoke Start-IngestionSparkSessionJobs -Times 1 -ParameterFilter { $code -eq 'from notebookutils import mssparkutils' }

            }
            It "Should not install mssparkutils module when SetUpSession is disabled"{
                Get-IngestionSparkSessionObject -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -SessionName 'Unittest' -WaitForSession $true -SetUpSession $false
                
                Should -Not -Invoke Start-IngestionSparkSessionJobs
            }
        }
    }
}

</file>
<file name="src\powershell\modules\DataConfig\public\Get-SQLQueryResults.ps1">
function Get-SQLQueryResults{

    &lt;#
    
    .DESCRIPTION
        Execute an SQL-command, which returns a result of how many inserts, updates, deletes, ... have happened

    #&gt;
    
    # First set parameters
    [CmdletBinding()]
    param(
        [Parameter(Mandatory = $true)]    
        [String] $ServerInstance,                       # Server Instance you're trying to access
    
        [Parameter(Mandatory = $true)]    
        [String] $Database,                             # The database inside the server you're trying to access
    
        [Parameter(Mandatory = $true)]    
        [String] $AccessToken,                          # Your accesstoken

        [Parameter(Mandatory=$true)]
        [String] $Query,                                # The query you want to execute

        [Parameter(Mandatory=$false)]
        [Int64] $numTotalParam = 1,                     # Amount of actions you're expecting 
        
        [Parameter(Mandatory=$false)]
        [Bool] $OutputSqlErrors = $true,                # Do you want to output all the errors?

        [Parameter(Mandatory=$false)]
        [Bool] $IncludeSqlUserErrors = $true,           # Do you want to Include the SQL User Errors?

        [Parameter(Mandatory=$false)]
        [Bool] $AbortOnError = $true,                   # Do you want to abort if error is found?
    
        [Parameter(Mandatory=$false)]
        [Switch] $PrintDetailedResults                  # Do you want detailed output or not?
    )

    # Then you're going to construct your SQL Params as well
    $SQLParameters = @{
        ServerInstance = $ServerInstance 
        Database = $Database 
        AccessToken = $AccessToken
        
        OutputSqlErrors = $OutputSqlErrors
        IncludeSqlUserErrors = $IncludeSqlUserErrors
        AbortOnError = $AbortOnError
    }

    # Execute the query &amp; capture the result in a variable
    $SQLResult = Invoke-Sqlcmd @SQLParameters -Query $Query -ErrorAction Stop

    if ($PrintDetailedResults){
        $SQLResult | Format-Table | Out-String | Write-Information
    }

    $numTotal  = $numTotalParam
    $numInsert = ($SQLResult | Where-Object -Property "action" -eq "INSERT") ? ($SQLResult | Where-Object -Property "action" -eq "INSERT" ).Count : 0
    $numUpdate = ($SQLResult | Where-Object -Property "action" -eq "UPDATE") ? ($SQLResult | Where-Object -Property "action" -eq "UPDATE" ).Count : 0
    $numDelete = ($SQLResult | Where-Object -Property "action" -eq "DELETE") ? ($SQLResult | Where-Object -Property "action" -eq "DELETE" ).Count : 0

    $result = [PSCustomObject]@{
        Total        = $numTotal
        Inserted     = $numInsert
        Updated      = $numUpdate
        Deleted      = $numDelete
    }

    return $result



}


</file>
<file name="src\powershell\modules\DataConfig\public\Get-SQLQueryResults.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Get-SQLQueryResults" {

    Context "UnitTests" {

        # Setup of the tests inside the given context
        BeforeAll {
            # First of all, configure the supposed output of Invoke-Sqlcmd
            $resultset = @(
                [PSCustomObject]@{
                    action = "INSERT"
                    file_layout = "dummy"
                }
                [PSCustomObject]@{
                    action = "UPDATE"
                    file_layout = "dummy"
                }
                [PSCustomObject]@{
                    action = "DELETE"
                    file_layout = 'dummy'
                }
            )

            # Mock the Invoke-Sqlcmd command, else: Will try to make a call to SQL
            Mock Invoke-Sqlcmd { $resultset }
        }

        It "Should invoke Invoke-Sqlcmd with right params" {
            # Test that Invoke-Sqlcmd is invoked when provided with right params
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                Query = 'exec dummy_sp @dummy_var = test'
                numTotalParam = 3
            }
                    
            Get-SQLQueryResults @params
            Should -Invoke -CommandName Invoke-Sqlcmd -Times 1

        }

        It "Should return Total, Inserted, Updated &amp; Deleted object" {
            # Test that, based on the known output of Invoke-Sqlcmd, the right object is returned
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                Query = 'exec dummy_sp @dummy_var = test'
                numTotalParam = 3
            }


            # Capture the result of Get-SQLQueryResults to check it
            $Result = Get-SQLQueryResults @params

            # Assertions
            $Result.Total       | Should -Be $params.numTotalParam
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 1
            $Result.Deleted     | Should -Be 1

        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionDeltaTableConfiguration.ps1">

function Set-IngestionDeltaTableConfiguration {
    &lt;#
    .SYNOPSIS
        Function to deploy empty delta tables from a json configuration file

    .DESCRIPTION
        The function calls upon DeltaTableConfigurationFile 'delta_tables.json' for the metadata on each delta table
        This function will start a spark pool and execute the calls to notebook 'DeployOrchestrator'.
        Each object of the file (representing a delta table configuration) will be passed as a separate Spark Job to the spark session.
        The function will loop over the jobs and complete when all have succeeded successfully

    .NOTES
        Disabled the solution for the below issue as another fix has been implemented
            If the error returns, enable the code again

        When too many calls are made to an active Spark Session, the following error is thrown:
            Status: 429
            {"code":"TooManyRequestsForIdentifier","message":"Request limit has been exceeded for the Identifiers"}
        This error states that there are currently too many requests made to the Spark Session, but this does not mean a failure
        of the deployment. Therefore, this function contains some error handling regarding the above error.
        Idea: 
            Retry to ping to Spark Session after $RetrySeconds at most $MaxRetryCount times.
            If a successful call can be made, the $retryCount is reset to 0
            If no successful call can be made within the set parameters, the error is thrown anyway
    #&gt;
    [CmdletBinding()]

    param(

        [Parameter(Mandatory=$true, HelpMessage="Name of the environment the tables needs to be deployed to" )]
        [string] $TargetEnvironment,

        [Parameter(Mandatory=$true, HelpMessage="Configuration file containing the list of delta table objects to deploy to the delta lake")]
        [string] $DeltaTableConfigurationFile,

        [Parameter(Mandatory=$false, HelpMessage="Name of the spark session that will be invoked (or looked for)")]
        [string] $SessionName='DeployDeltaTables',

        [Parameter(Mandatory=$false, HelpMessage="Number of seconds to wait between API calls to the Spark Session")] 
        [string] $StatusUpdateSeconds = 30,

        # [Parameter(Mandatory=$false, HelpMessage="Maximum number of API call retries when throttling occurs on the session")] 
        # [string] $MaxRetryCount = 5,
        # [Parameter(Mandatory=$false, HelpMessage="Time between API call retries after throtteling")] 
        # [string] $RetrySeconds = 60,

        [Parameter(Mandatory=$false, HelpMessage="Show spark statements")]
        [switch] $ShowSparkStatements
    )

    # Local scripts settings
    Set-StrictMode -Version "latest"

#region Step 1: Define parameters
    # Using the TargetEnvironment, define the names of the different Azure services
    [string] $synapseWorkspace = "$TargetEnvironment-dap-syn-core"           # Name of the synapse workspace to deploy the tables to
    [string] $poolName = "$($TargetEnvironment)dapsspcore"                   # Name of the spark pool that will be used during the deployment

    # Set the Synapse parameters needed to deploy the delta tables  
    [string] $notebookDirectory = "Deploy"                                   # Name of the folder where the deployment notebooks are stored
    [string] $notebookName = "DeployOrchestrator"                            # Name of the notebook that will orchestrate the deployment
    [timespan] $ToLocalTimeOffset  = (Get-TimeZone).BaseUtcOffset

    # Set variables for spark job tracking
    [int]       $maxConcurrentJobs = 5
    [array]     $jobs = @()
    [array]     $completedJobs = @()
#endregion Step 1

#region Step 2: Validate the $DeltaTableConfigurationFile 
    # 1. Check if the file exists
    if (-not (Test-Path $DeltaTableConfigurationFile)){
        throw "Delta table configuration file $($DeltaTableConfigurationFile) does not exist. Cannot deploy anything."
    }
    # 2. Check if the configuration of the file matches that of the schema 
    Compare-IngestionMetadataFromJsonFile -Path $DeltaTableConfigurationFile -schema "DeltaTableData.schema.json"

    $deltaConfigFileContent = Get-Content $DeltaTableConfigurationFile -Raw | ConvertFrom-Json

    # If the configuration file is empty: no work todo
    if(-not $deltaConfigFileContent.table_configurations){
        Write-Information "There are no delta tables to deploy in the configuration file, exiting..."
        return
    }
#endregion Step 2

#region Step 3: Retrieve or start a spark session

    # Check if there is an active session based on the status of the existing sessions
    $sessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace $synapseWorkspace -SynapseSparkPool $poolName -SessionName $SessionName -WaitForSession $true -SetUpSession $true
    [datetime] $startDate = (Get-Date).Add($ToLocalTimeOffset).ToString('HH:mm:ss')

    Write-Information "Run session setup..."
    # Dev-Note: Execute the statement separate from the SessionObj-setup so that the function returns the statement object
    $code = "from notebookutils import mssparkutils"     
    $help = [array] @(Get-AzSynapseSparkStatement -WorkSpaceName $synapseWorkspace -SparkPoolName $poolName -SessionId $sessionObj.Id)
    $debug = [array] @($help | Sort-Object Id -Descending)
    $statement = $debug[0]
    #save the statement so we have a starting point for later
    $startStatementId = $statement.Id
#endregion Step 3


#region Step 4: Deploy the delta tables using configuration file
    # Iteratively launch spark-jobs to the sessionObj
    # Only allow $maxConcurrentJobs to be active at the same time on the same sessionObj
    
    Write-Information "Launch delta table creation spark-jobs..."   
    # Function to wait for available job slots

    # Loop through delta table configurations and invoke spark jobs accordingly
    $tableNumber = 0
    $jobCount = $deltaConfigFileContent.table_configurations.Count
    $deltaConfigFileContent.table_configurations | ForEach-Object{
        $tableNumber += 1
        
        Wait-AvailableJobs -MaxConcurrentJobs $maxConcurrentJobs -Jobs $jobs | Out-Null # Wait for available job slots
        $completedJobs = @(Get-CompletedSparkStatements -StartStatementId $startStatementId -WorkspaceName $synapseWorkspace -PoolName $poolName -SessionObj $sessionObj -CompletedJobs $completedJobs) # Ping to see which jobs have completed already
        
        # print progress information
        $duration = New-Timespan –Start $startDate –End (Get-Date).Add($ToLocalTimeOffset)
        $startTime = $startDate.ToString('HH:mm:ss')
        $currentTime = (Get-Date).Add($ToLocalTimeOffset).ToString('HH:mm:ss')
        Write-Information "  In Progress... running time: $($duration.ToString('hh\:mm\:ss')) | Started: $startTime | Current time: $currentTime | Running jobs: $($jobs.Count)"

        # execute the spark-job on the sessionObj
        $tableData = $_ | ConvertTo-Json -Depth 5 -AsArray
        $code = "mssparkutils.notebook.run ('$notebookDirectory/$notebookName', 3600, { 'table_definition_list' : '''$($tableData)''' })"
        if( $ShowSparkStatements) {
            Write-Information "Invoke code statement:" 
            Write-Information $code
        } else {
            Write-Information "  Deploying $($_.table_name)..."
        }
        $job = Start-IngestionSparkSessionJobs -Code $code -SynapseWorkspace $synapseWorkspace -SynapseSparkPool $poolName -SessionObj $sessionObj -Job $true
        $jobs += $job
        
        # jobCount = Total number of jobs
        # tableNumber = Last added job
        # completedJobs = List of finished jobs
        Write-Verbose "Started jobs: $($tableNumber)/$($jobCount) | Completed jobs:  $($completedJobs.Count)/$($jobCount)"
    }

    # Wait for all jobs to complete
    Write-Verbose "Waiting for jobs to complete..."
    $jobs | Wait-Job
    $completedJobs = @(Get-CompletedSparkStatements -StartStatementId $startStatementId -WorkspaceName $synapseWorkspace -PoolName $poolName -SessionObj $sessionObj -CompletedJobs $completedJobs) # Ping to see which jobs have completed already
    Write-Verbose "Completed jobs:  $($completedJobs.Count)/$($jobCount)"
#endregion Step 4

#region Step 5: Validate that all deployments have been successfull
    # Retrieve results of the spark (job) statements
    Write-Verbose "Checking for failed jobs..."
    $failed = Get-FailedSparkStatements -WorkspaceName $synapseWorkspace -PoolName $poolName -SessionObj $sessionObj -StartStatementId $startStatementId

    if ($failed){
        # Stop the Spark Session to avoid using it again in the future (before timeout)
        Stop-AzSynapseSparkSession -WorkSpaceName $synapseWorkspace -SparkPoolName $poolName -LivyId $sessionObj.Id
        throw "Table deployment has errors"
    }
    Write-Information "All tables have been deployed successfully"
#endregion 5
       
}
# Obsolete code: Deal with throttling issue
# Solution (hopefully): Only allow maxConcurrentJobs at the same time
#         # Reset the retryCount when the ping succeeds
#         $retryCount = 0
#         # If an error gets thrown:
#         catch {
#             # Log the exception for debugging purposes
#             Write-Output $_.Exception
        
#             # Validate that the exception has a Status property
#             if ($_.Exception.PSObject.Properties['Status']) {
#                 # Check if the status indicates a rate limit issue
#                 if ($_.Exception.Status -eq 429) {
#                     $retryCount++
#                     Write-Output "Rate limit exceeded. Retrying in $RetrySeconds seconds..."
#                     Start-Sleep -Seconds $RetrySeconds
#                 } 
#                 else {
#                     throw $_  # Rethrow the exception if it's not a rate limit error
#                 }
#             }
#             else {
#                 throw $_  # Rethrow the exception if there is no Status property
#             }
#         }
#     }

    # evaluate if the deployment was successful
    # $retryCount = 0
    # $finished = $false
    # if ($retryCount -gt $MaxRetryCount){
    #     throw "Number of retries exceeded the maximum allowed number ($MaxRetryCount)"
    # }
    # elseif (-not $finished){
    #     throw "The status of the deploy was not set to finished. Something went wrong during deployment..."
    # }

    # # success!
    # Write-Information "All tables have been deployed successfully"


</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionDeltaTableConfiguration.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    . $PSScriptRoot/Get-CompletedSparkStatements.ps1
    . $PSScriptRoot/Wait-AvailableJobs.ps1
    . $PSScriptRoot/Get-FailedSparkStatements.ps1

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionDeltaTableConfiguration" {

    Context "UnitTests" {

        BeforeAll {

            # mock a spark object
            $sparkObject = [hashtable]@{
                State = 'idle'
                Name = 'DeployDeltaTables'
                Id = '123'
                AppId = 'application_0000000000000_0000'
                LivyInfo = @{
                    StartingAt = Get-Date
                }
            }

            $mockSession = New-MockObject -Type Microsoft.Azure.Commands.Synapse.Models.PSSynapseSparkSession -Properties $sparkObject

            # mock a job object
            $jobObject = @([PSCustomObject]@{
                Id = '123'
                State = "waiting"
            })

            # mock the output of a successful spark statement
            $startStatementOutput = @(
                @{
                    Output = @{
                        Status = 'ok'
                    }
                    Id = 124
                },
                @{
                    Output = @{
                        Status = 'ok'
                    }
                    Id = 125
                }
            )

            $newStatementOutput = @{
                Output = @{
                    Status = 'ok'
                }
                Id = 126
            }

            $script:mockCalled = 0
            $mockTestPath = {
                                $script:mockCalled++                                
                                if($script:mockCalled -eq 1)
                                {
                                    return $startStatementOutput
                                }
                                else
                                {
                                    return $newStatementOutput
                                }
                            }
    
            Mock -CommandName Get-AzSynapseSparkStatement -MockWith $mockTestPath 

            # Mock Az.Synapse functions that are called in the Set-IngestionDeltaTableConfiguration function
            mock Get-IngestionSparkSessionObject {$mockSession}
            # mock Get-AzSynapseSparkStatement {$startStatementOutput}
            mock Invoke-AzSynapseSparkStatement {$startStatementOutput}
            mock Start-IngestionSparkSessionJobs {$jobObject}
            mock Stop-AzSynapseSparkSession {}

            mock Start-Sleep {}
            mock Wait-Job
        }

        Context "Empty configuration file should not call any function"{
            BeforeAll{

                # The configuration object is an empty object
                $configurationContent = [PSCustomObject]@{
                    table_configurations = @(
                    )
                }
                # Write the empty object to a json file on the TestDrive
                # The Set-IngestionDeltaTableConfiguration function will call upon this json to execute its functionalities
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/delta_tables.json'  

                # Mock Az.Synapse function: Function should not be called but has to be mocked for assertion

            }

            It "Should not do anything as there are no configurations to deploy"{
                # RUN: Call the function with the configuration file being the TestDrive-file where the empty configuration is located
                Set-IngestionDeltaTableConfiguration -TargetEnvironment 'dev' -DeltaTableConfigurationFile 'TestDrive:/delta_tables.json' -WarningAction 'SilentlyContinue'
                
                # ASSERT
                #Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -like "*no delta tables*"}
                # Since the configuration file is empty, no spark activities are expected
                Should -Not -Invoke Get-IngestionSparkSessionObject
                Should -Not -Invoke Invoke-AzSynapseSparkStatement
                Should -Not -Invoke Start-IngestionSparkSessionJobs
                Should -Not -Invoke Stop-AzSynapseSparkSession
            }
        }
        Context "Non-empty configuration should start spark session and jobs"{
            BeforeAll{

                # The configuration object is an empty object
                $configurationContent = [PSCustomObject]@{
                    table_configurations = @(
                         @{
                            table_description = 'test description'
                            table_name = 'table_name'
                            column_info = "'[{}]'"
                            storage_account = 'devdapstdala1'
                            container_name = 'silver'
                        }
                    )
                }
                # Write the empty object to a json file on the TestDrive
                # The Set-IngestionDeltaTableConfiguration function will call upon this json to execute its functionalities
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/delta_tables.json'  
            }

            It "Should call functions with correct parameters"{
                # RUN: Call the function with the configuration file being the TestDrive-file where the empty configuration is located
                Set-IngestionDeltaTableConfiguration -TargetEnvironment 'dev' -DeltaTableConfigurationFile 'TestDrive:/delta_tables.json' -WarningAction 'SilentlyContinue'
                
                # ASSERT
                #Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -like "*no delta tables*"}
                # Since the configuration file is empty, no spark activities are expected
                Should -Invoke Get-IngestionSparkSessionObject
                Should -Invoke Start-IngestionSparkSessionJobs -Times 1 -ParameterFilter{
                    $Code -match "mssparkutils.notebook.run"
                    $Code -match "Deploy/DeployOrchestrator"
                    $Code -match $configurationContent.table_configurations
                }
                Should -Invoke Wait-Job
            }
        }

        Context "Multi configurations should start individual jobs"{
            BeforeAll{

                # The configuration object is an empty object
                $configurationContent = [PSCustomObject]@{
                    table_configurations = @(
                         @{
                            table_description = 'test description'
                            table_name = 'table_name'
                            column_info = "'[{}]'"
                            storage_account = 'devdapstdala1'
                            container_name = 'silver'
                        },
                        @{
                            table_description = 'test description'
                            table_name = 'table_name'
                            column_info = "'[{}]'"
                            storage_account = 'devdapstdala1'
                            container_name = 'silver'
                        }
                    )
                }
                # Write the empty object to a json file on the TestDrive
                # The Set-IngestionDeltaTableConfiguration function will call upon this json to execute its functionalities
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/delta_tables.json'  
            }

            It "Should call functions with correct parameters"{
                # RUN: Call the function with the configuration file being the TestDrive-file where the empty configuration is located
                Set-IngestionDeltaTableConfiguration -TargetEnvironment 'dev' -DeltaTableConfigurationFile 'TestDrive:/delta_tables.json' -WarningAction 'SilentlyContinue'
                
                # ASSERT
                #Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -like "*no delta tables*"}
                # Since the configuration file is empty, no spark activities are expected
                Should -Invoke Start-IngestionSparkSessionJobs -Times 1 -ParameterFilter{
                    $Code -match "mssparkutils.notebook.run"
                    $Code -match "Deploy/DeployOrchestrator"
                    $Code -match $configurationContent.table_configurations[0]
                }
                Should -Invoke Start-IngestionSparkSessionJobs -Times 1 -ParameterFilter{
                    $Code -match "mssparkutils.notebook.run"
                    $Code -match "Deploy/DeployOrchestrator"
                    $Code -match $configurationContent.table_configurations[1]
                }
            }
        }

        Context "Job with error should fail"{
            BeforeAll{

                # The configuration object is an empty object
                $configurationContent = [PSCustomObject]@{
                    table_configurations = @(
                         @{
                            table_description = 'test description'
                            table_name = 'table_name'
                            column_info = "'[{}]'"
                            storage_account = 'devdapstdala1'
                            container_name = 'silver'
                        }
                    )
                }
                # Write the empty object to a json file on the TestDrive
                # The Set-IngestionDeltaTableConfiguration function will call upon this json to execute its functionalities
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/delta_tables.json'  
                
    
                $newStatementOutput = @(
                    @{
                        Output = @{
                            Status = 'error'
                            ErrorValue = "Thrown Spark error"
                        }
                        Id = 126
                        Code = "executed code statement"
                    },
                    @{
                        Output = @{
                            Status = 'completed'
                            ErrorValue = "Thrown Spark error"
                        }
                        Id = 127
                        Code = "executed code statement"
                    }
                )
                $script:mockCalled = 0
                $mockTestPath = {
                                    $script:mockCalled++                                
                                    if($script:mockCalled -eq 4)
                                    {
                                        return $newStatementOutput
                                    }
                                    else
                                    {
                                        return $startStatementOutput
                                    }
                                }

                Mock -CommandName Get-AzSynapseSparkStatement -MockWith $mockTestPath 
            }

            It "Should throw an error when statement failed"{
                # RUN: Call the function with the configuration file being the TestDrive-file where the empty configuration is located
                {Set-IngestionDeltaTableConfiguration -TargetEnvironment 'dev' -DeltaTableConfigurationFile 'TestDrive:/delta_tables.json' -WarningAction 'SilentlyContinue'} | Should -Throw -ExpectedMessage "Table deployment has errors"
            }
        }
    }
}


</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionMetadataFromJsonFile.ps1">
function Set-IngestionMetadataFromJsonFile {

[CmdletBinding()]

param(
    [Parameter(Mandatory=$true, HelpMessage="Path to the configuration file to be deployed" )]
    [string] $Path,

    [Parameter(Mandatory=$false, HelpMessage="Path to the configuration file of the delta tables delta_tables.json" )]
    [string] $DeltaTableConfigurationFile = "configuration/delta_tables/delta_tables.json",

    [Parameter(Mandatory=$false, HelpMessage="Boolean: Deploy metadata of the plan-task configuration if exists" )]
    [boolean] $DeployPlantasks = $true,
    [Parameter(Mandatory=$false, HelpMessage="Boolean: Deploy metadata of the dataset if exists" )]
    [boolean] $DeployDatasets = $true,
    [Parameter(Mandatory=$false, HelpMessage="Boolean: Deploy metadata of the preprocessing tasks table if exists" )]
    [boolean] $DeployPreprocessing = $true,
    [Parameter(Mandatory=$false, HelpMessage="Boolean: Deploy metadata of the delta table if exists" )]
    [boolean] $DeployDeltaTables = $true,

    [Parameter(Mandatory=$false, HelpMessage="Name of the SQL Server where to deploy the metadata to" )]
    [string] $SQLServerInstance,
    [Parameter(Mandatory=$false, HelpMessage="Name of the SQL Database where to deploy the metadata to" )]
    [string] $SQLDatabase,
    [Parameter(Mandatory=$false, HelpMessage="Accesstoken to the SQL server and database fo the user" )]
    [string] $AccessToken,

    [Parameter(Mandatory=$true, HelpMessage="Name of the environment the table needs to be deployed to" )]
    [string] $TargetEnvironment,
    [Parameter(Mandatory=$false, HelpMessage="Path to the repository where the function is called from" )]
    [string] $WorkspaceFolder,

    [Parameter(Mandatory=$false, HelpMessage="Switch parameter used when executing SQL commands" )]
    [switch] $PrintDetailedResults
)

#region preparation and validation
        
    Write-Information "Run function: Set-IngestionMetadataFromJsonFile"       

    # file must exist
    if ( -Not ( Test-Path -Path $Path) ) {
        throw [System.IO.FileNotFoundException]::new("Provided path '$Path' does not exist.")
    }
    
    # filename part of $Path, used for logging only
    $jsonFile = Split-Path -Path $Path -Leaf
    
    $sqlParams = @{
        ServerInstance = $SQLServerInstance
        Database = $SQLDatabase
        AccessToken = $AccessToken
    }
    
    # Compare the files from configuration with schema from ../schemas
    Compare-IngestionMetadataFromJsonFile -Path $Path

#endregion

    $IngestionJson = Get-Content -Raw -Path $Path
    $ingestionMetadata = $IngestionJson | ConvertFrom-Json
    
    ##region Helper functions
    
    function Get-PropertyValue{
        param(
                [psobject] $Object
            ,   [string] $Name                            
            ,   [AllowNull()] [psobject] $DefaultValue
            )
            if ($Object.PSObject.Properties.name -contains $Name) {
                $propertyValue = $Object.PSObject.Properties[$Name].Value
                return $propertyValue
            }
            return $DefaultValue
        }
            
    #endregion

    if( $DeployPlantasks ) {
        if( Get-Member -InputObject $ingestionMetadata -name "plans" -MemberType "Properties") {
    
            #loop all plans in the json file (if present)
            Write-Information "[$jsonFile] Found $($ingestionMetadata.plans.Count) plan(s) in file"
            :plantaskLoop ForEach ($plan in $ingestionMetadata.plans ) {
    
                $planName = $plan.name.trim()
                Write-Information "[$jsonFile] Processing plan: $planName ..."

                # check environment is valid for deployment (if not provided then assume "any")
                if ( Get-Member -InputObject $plan -name "environments" -MemberType "Properties" ) {
                    if( $TargetEnvironment -notmatch $plan.environments ) {
                        Write-Information "[$jsonFile].[$planName] Environments filter '[$($plan.environments)]' is not valid for target environment [$TargetEnvironment], skipping"
                        continue :plantaskLoop
                    }
                }
    
                # plan_configuration
                Write-Information "[$jsonFile].[$planName] plan_configuration ..."
                $planDescription = $plan.description.trim()
                if( Get-Member -InputObject $plan -name "enabled" -MemberType "Properties" ) {
                    $planEnabled = $plan.enabled
                } else {
                    $planEnabled = $true
                }
                $planResult = Set-IngestionPlanConfiguration @sqlParams -PlanName $planName -PlanDescription $planDescription -PlanEnabled $planEnabled -Verbose:$VerbosePreference -PrintDetailedResults:$PrintDetailedResults
                Write-Information "[$jsonFile].[$planName] plan_configuration succeeded - Total: $($planResult.Total), Inserted: $($planResult.Inserted), Updated : $($planResult.Updated), Deleted : $($planResult.Deleted)"        
    
                # plan_task_configuration
                Write-Information "[$jsonFile].[$planName] plan_task_configuration ..."
    
                # building required table manually, didn't get it working with nested select-object ...
                $plantasks = [System.Collections.ArrayList]@()      
                foreach ( $taskgroup in $plan.task_groups) {
                    foreach ( $task in $taskgroup.tasks) {
                        # Optional params
                        if( Get-Member -InputObject $task -name "sequence" -MemberType "Properties" ){
                            $task_sequence = $task.sequence
                        }
                        else{
                            $task_sequence = 1
                        }

                        if(Get-Member -InputObject $task -name "enabled" -MemberType "Properties"){
                            $enabled = $task.enabled
                        }
                        else{
                            $enabled = $true
                        }

                        $plantasks.add(  [PSCustomObject]@{
                            task_group = $taskgroup.name
                            task_name = $task.name
                            task_sequence = $task_sequence
                            enabled = $enabled
                        }) | Out-Null
                    }
                }
                
                $plantaskJson = ( $plantasks  | ConvertTo-Json -AsArray ) ?? "[]"
                $plantaskResult = Set-IngestionPlanTaskConfiguration @sqlParams -PlanName $planName -PlanTask_Json $plantaskJson -Verbose:$VerbosePreference -PrintDetailedResults:$PrintDetailedResults
                Write-Information "[$jsonFile].[$planName] plan_task_configuration succeeded - Total: $($plantaskResult.Total), Inserted: $($plantaskResult.Inserted), Updated : $($plantaskResult.Updated), Deleted : $($plantaskResult.Deleted)"
            }
    
    
        } else {
            Write-Information "[$jsonFile] No plans detected in file"
        }
    }
    
    if ($DeployDatasets) {
        #loop all datasets in the json file (if present)
        if( Get-Member -InputObject $ingestionMetadata -name "datasets" -MemberType "Properties" ) {
    
            Write-Information "[$jsonFile] Found $($ingestionMetadata.datasets.Count) dataset(s) in file"
            :datasetLoop ForEach ($dataset in $ingestionMetadata.datasets ) {

                # Mandatory params
                $datasetName = $dataset.name
                Write-Information "[$jsonFile] Processing dataset: $datasetName ..."
                $taskCategory = "INGEST"  
    
                # $kind = $dataset.kind
    
                Write-Information "[$jsonFile].[$datasetName] task_configuration (INGESTION) ..."
                
                # Mandatory params
                $task_type = $dataset.task_type

                # Optional params
                if (Get-Member -InputObject $dataset -name "worker" -MemberType "Properties"){
                    $worker_name = $dataset.worker
                }
                else{
                    $worker_name = 'IngestionWorker'
                }
    
                if (Get-Member -InputObject $dataset -name "ingestion" -MemberType "Properties"){
                    # Mandatory params
                    $target_table = $dataset.ingestion.target_table
                    $source_folder = $dataset.ingestion.source_folder
                    
                    # Optional params
                    if (Get-Member -InputObject $dataset.ingestion -name "container" -MemberType "Properties"){
                        $container_name = $dataset.ingestion.container
                    }
                    else{
                        $container_name = 'landing'
                    }
                    if (Get-Member -InputObject $dataset.ingestion -name "target_options" -MemberType "Properties"){
                        [psobject] $target_options = $dataset.ingestion.target_options # | ConvertTo-Json -Depth 5
                    }
                    else{
                        $target_options = $null
                    }
                }
                else{
                    # If ingestion doesn't exist, still set them
                    $source_folder = $null
                    $target_table = $null
                    $container_name = 'landing'
                    $target_options = $null
                }
    
                # Set meta.task_configuration
                $taskResult = Set-IngestionTaskConfiguration @sqlParams `
                    -TaskName "$datasetName"  `
                    -TaskCategory $taskCategory `
                    -TaskDescription "ingest files that have layout ''$datasetName''" `
                    -TaskType $task_type `
                    -WorkerName $worker_name `
                    -FileLayout $datasetName `
                    -SourceFolder $source_folder `
                    -ContainerName $container_name `
                    -TableName $target_table `
                    -TargetOptions $target_options `
                    -Verbose:$VerbosePreference -PrintDetailedResults:$PrintDetailedResults
                    
                Write-Information "[$jsonFile].[$datasetName] task_configuration (INGESTION) succeeded - Total: $($taskResult.Total), Inserted: $($taskResult.Inserted), Updated : $($taskResult.Updated), Deleted : $($taskResult.Deleted)"        
                
                # If "Ingestion" is present in the JSON file
                if (Get-Member -InputObject $dataset -name "ingestion" -MemberType "Properties"){
                    $ingestion_data = $dataset.ingestion
                  
                    Write-Information "[$jsonFile].[$datasetName] source_configuration ..."
    
                    # check/calculate optional members
                    if( Get-Member -InputObject $ingestion_data -name "column_delimiter" -MemberType "Properties" ) {
                        $column_delimiter =  $ingestion_data.column_delimiter
                    }
                    else {
                        $column_delimiter = ","
                    }
                    
                    if( Get-Member -InputObject $ingestion_data -name "row_delimiter" -MemberType "Properties" ) {
                        $row_delimiter = $ingestion_data.row_delimiter
                    }
                    else {
                        $row_delimiter = '\r\n'
                    }

                    if( Get-Member -InputObject $ingestion_data -name "escape_character" -MemberType "Properties" ) {
                        $escape_character = $ingestion_data.escape_character
                    }
                    else {
                        $escape_character = ''
                    }
                    
                    if( Get-Member -InputObject $ingestion_data -name "quote_character" -MemberType "Properties" ) {
                        $quote_character = $ingestion_data.quote_character
                    }
                    else {
                        $quote_character = ''
                    }
    
                    if( Get-Member -InputObject $ingestion_data -name "header_line" -MemberType "Properties" ) {
                        $header_line =  $ingestion_data.header_line
                    }
                    else {
                        $header_line = $true
                    }
                    

                    if( Get-Member -InputObject $ingestion_data -name "encoding" -MemberType "Properties" ) {
                        $encoding =  $ingestion_data.encoding
                    }
                    else {
                        $encoding = 'UTF-8'
                    }

                    if( Get-Member -InputObject $ingestion_data -name "skip_first_lines" -MemberType "Properties" ) {
                        $skip_first_lines =  $ingestion_data.skip_first_lines
                    }
                    else {
                        $skip_first_lines = $null
                    }
                    
                    $source_conditions = [System.Collections.ArrayList]@()
                    $source_preconditions_raw = Get-PropertyValue -Object $ingestion_data -Name 'preconditions' -DefaultValue $null
                    if ($null -ne $source_preconditions_raw) {
                        foreach ($precond in $source_preconditions_raw) {
                            $source_conditions.add(
                                [PSCustomObject]@{
                                    #mandatory
                                    name        = $precond.name 
                                    type        = $precond.type
                                    frequency   = $precond.frequency
                                    #optional
                                    expected_file_mask  = (Get-PropertyValue -Object $precond -Name 'expected_file_mask' -DefaultValue $null)
                                    enabled             = Get-PropertyValue -Object $precond -name 'enabled'            -DefaultValue $true
                                    description         = (Get-PropertyValue -Object $precond -name 'description'        -DefaultValue $null)
                                } ) | Out-Null
                        }
                    }
                    $source_conditions_json = $source_conditions | ConvertTo-Json -AsArray

                    # Set meta.source_configuration
                    $TaskResult = Set-IngestionSourceConfiguration @sqlParams `
                        -FileLayout $datasetName  `
                        -MatchPattern $ingestion_data.match_pattern `
                        -FileKind $dataset.kind `
                        -Extension $ingestion_data.extension `
                        -ColumnDelimiter $column_delimiter `
                        -RowDelimiter $row_delimiter `
                        -EscapeCharacter $escape_character `
                        -QuoteCharacter $quote_character `
                        -HeaderLine $header_line `
                        -Encoding $encoding `
                        -SkipFirstLines $skip_first_lines `
                        -Preconditions $source_conditions_json `
                        -PrintDetailedResults:$PrintDetailedResults -Verbose:$VerbosePreference

                    Write-Information "[$jsonFile].[$datasetName] source_configuration succeeded - Total: $($TaskResult.Total), Inserted: $($TaskResult.Inserted), Updated : $($TaskResult.Updated), Deleted : $($TaskResult.Deleted)"        
         
                }
                else{
                    Write-Information "No ingestion properties defined"
                }
    
                # If "Columns" is present in the JSON file
                if (Get-Member -InputObject $dataset -name "columns" -MemberType "Properties"){
                    Write-Information "[$jsonFile].[$datasetName] source_column_configuration ..."
    
                    # check/calculate optional members
                    # building required table manually, didn't get it working with nested select-object ...
    
                    $columnData = [System.Collections.ArrayList]@()      
                    # special case: fixedlength files have only 1 column to be defined
                    foreach ( $column in ($dataset.columns) ) {
    
                        # get/calculate optional properties
                        if( Get-Member -InputObject $column -name "sink_name" -MemberType "Properties" ) {
                            $sink_column_name = $column.sink_name.ToLower()
                        }
                        else {
                            $sink_column_name = $column.name.ToLower()
                        }
    
                        if( Get-Member -InputObject $column -name "dimension" -MemberType "Properties" ) {
                            $dimension = $column.dimension
                        }
                        else {
                            $dimension = "SCD2"
                        }
    
                        if( Get-Member -InputObject $column -name "data_type" -MemberType "Properties" ) {
                            $data_type = $column.data_type
                        }
                        else {
                            $data_type = "string"
                        }

                        if( Get-Member -InputObject $column -name "column_info" -MemberType "Properties" ) {
                            [string] $column_info = $column.column_info | ConvertTo-Json -Depth 5 -Compress
                        }
                        else {
                            [string] $column_info = "null"
                        }
    
    
                        # create entry for json
                        $columnData.add(  [PSCustomObject]@{
                            column_sequence = $column.sequence
                            source_column_name = $column.name
                            sink_column_name = $sink_column_name
                            dimension = $dimension
                            data_type = $data_type
                            column_info = $column_info
                        }) | Out-Null
                    }
                    $columnDataJson = $columnData  | ConvertTo-Json
                    # Write-Information $columnDataJson
    
                    # Set meta.source_column_configuration
                    $jobResult = Set-IngestionSourceColumnConfiguration @sqlParams `
                        -FileLayout $datasetName  `
                        -ColumnDataJson $columnDataJson `
                        -PrintDetailedResults:$PrintDetailedResults -Verbose:$VerbosePreference 
                    
                    Write-Information "[$jsonFile].[$datasetName] source_file_column_def succeeded - Total: $($jobResult.Total), Inserted: $($jobResult.Inserted), Updated : $($jobResult.Updated), Deleted : $($jobResult.Deleted)"        
                    
                }
    
                # If "Checks" is present in the JSON file
                if (Get-Member -InputObject $dataset -name "checks" -MemberType "Properties"){
                    Write-Information "[$jsonFile].[$datasetName] source_check_configuration ..."
    
                    # check/calculate optional members
                    # building required table manually, didn't get it working with nested select-object ...
    
                    $ChecksData = [System.Collections.ArrayList]@()      
                    # special case: fixedlength files have only 1 column to be defined
                    foreach ( $column in ($dataset.checks) ) {
    
                        # get/calculate optional properties
                        # Check_name
                        $check_name = $column.name.ToLower()
    
                        # Enabled_flag
                        if( Get-Member -InputObject $column -name "enabled" -MemberType "Properties"){
                            $enabled_flag = $column.enabled
                        }
                        else{
                            $enabled_flag = $true
                        }

                        # Config_params
                        if ( Get-Member -InputObject $column -name "config_params" -MemberType "Properties" ){
                            $config_params = $column.config_params
                        }
                        else{
                            $config_params = $null
                        }
    
    
                        # create entry for json
                        $ChecksData.add(  [PSCustomObject]@{
                            name = $check_name
                            enabled = $enabled_flag
                            config_params = $config_params
                        }) | Out-Null
                    }
                    
                    $ChecksDataArray = $ChecksData | ConvertTo-Json
                    # Write-Information $ChecksDataArray
    
                    # Set meta.source_check_configuration
                    $checkResult = Set-IngestionSourceCheckConfiguration @sqlParams `
                        -FileLayout $datasetName `
                        -ChecksDataArrayJson $ChecksDataArray
    
                    
                    Write-Information "[$jsonFile].[$datasetName] source_check_config succeeded - Total: $($checkResult.Total), Inserted: $($checkResult.Inserted), Updated : $($checkResult.Updated), Deleted : $($checkResult.Deleted)"        
                    
                }
                else{
                    Write-Information "No checks defined for [$datasetName]"
                }
    
            } #foreach dataset
        } else {
            Write-Information "[$jsonFile] Found no dataset(s) in file"
        }
    }
    if ($DeployDeltaTables){
        if( Get-Member -InputObject $ingestionMetadata -name "datasets" -MemberType "Properties" ) {
            Write-Information "[$jsonFile] Found $($ingestionMetadata.datasets.Count) dataset(s) in file"
            :datasetLoop ForEach ($dataset in $ingestionMetadata.datasets ) {

                if ((Get-Member -InputObject $dataset -name "columns" -MemberType "Properties") -and ($dataset.worker -eq "IngestionWorker")){
                    $datasetName = $dataset.name
                    Write-Information "[$jsonFile].[$datasetName] delta_table configuration ..."

                    # check/calculate optional members
                    # building required table manually, didn't get it working with nested select-object ...

                    $columnData = [System.Collections.ArrayList]@()      
                    # special case: fixedlength files have only 1 column to be defined
                    foreach ( $column in ($dataset.columns) ) {
                        # get/calculate optional properties
                        if( Get-Member -InputObject $column -name "sink_name" -MemberType "Properties" ) {
                            $column_name = $column.sink_name.ToLower()
                        }
                        else {
                            $column_name = $column.name.ToLower()
                        }
        
                        if( Get-Member -InputObject $column -name "dimension" -MemberType "Properties" ) {
                            $dimension = $column.dimension
                        }
                        else {
                            $dimension = "SCD2"
                        }
        
                        if( Get-Member -InputObject $column -name "data_type" -MemberType "Properties" ) {
                            $data_type = $column.data_type
                        }
                        else {
                            $data_type = "string"
                        }

                        # create entry for json
                        $columnData.add(  [PSCustomObject]@{
                        column_name = $column_name
                        dimension = $dimension
                        data_type = $data_type
                        }) | Out-Null
                    }

                    if (Get-Member -InputObject $dataset.ingestion -name "target_options" -MemberType "Properties"){
                        [psobject] $target_options = $dataset.ingestion.target_options 
                    }
                    else{
                        $target_options = @{}
                    }

                    # After everything is done, go to create the delta table
                    #$columnDataJson = $columnData  | ConvertTo-Json
                    # Write-Information $columnData

                    # Set delta_tables.json
                    Update-IngestionDeltaTableConfiguration `
                    -TableName $dataset.ingestion.target_table `
                    -ColumnData $columnData `
                    -TargetEnvironment $TargetEnvironment `
                    -ContainerName 'silver' `
                    -DeltaTableConfigurationFile "$($WorkspaceFolder)/$($DeltaTableConfigurationFile)" `
                    -TargetOptions $target_options `
                    -InformationAction "Continue" `
                                            
                    Write-Information "[$jsonFile].[$datasetName] configuration written to delta_tables.json"
                }
                else{
                    Write-Information "Deploying delta table was turned on, but worker $($dataset.worker) for $($jsonFile) was found. No delta table created."
                }
            }
        }
    }

    if ($DeployPreprocessing) {
        #loop all preprocessing tasks in the json file (if present)
        if( Get-Member -InputObject $ingestionMetadata -name "preprocess" -MemberType "Properties" ) {
    
            Write-Information "[$jsonFile] Found $($ingestionMetadata.preprocess.Count) preprocessing task(s) in file"
            :datasetLoop ForEach ($dataset in $ingestionMetadata.preprocess ) {

                # Mandatory params
                $taskName = $dataset.name                
                Write-Information "[$jsonFile].[$taskName] task_configuration (PREPROCESS) ..."
                
                # Mandatory params
                $taskCategory = "PREPROCESS"                
                $task_type = $dataset.task_type
                $worker_pipeline = $dataset.worker_pipeline
                $preprocessPattern = $dataset.source_pattern

                # Optional params

                if (Get-Member -InputObject $dataset -name "description" -MemberType "Properties"){
                    $task_description = $dataset.description
                }
                else {
                    $task_description = $taskName
                }
                if (Get-Member -InputObject $dataset -name "source_container" -MemberType "Properties"){
                    $container_name = $dataset.source_container
                }
                else {
                    $container_name = 'preprocessing'
                }
                if (Get-Member -InputObject $dataset -name "source_folder" -MemberType "Properties"){
                    $source_folder = $dataset.source_folder
                }
                else {
                    $source_folder = 'preprocessing'
                }   
                if (Get-Member -InputObject $dataset -name "worker_properties" -MemberType "Properties"){
                    $preprocessWorkerOptions = $dataset.worker_properties | ConvertTo-Json -Depth 5
                } else {
                    $preprocessWorkerOptions = $null
                }
    
                # Set meta.task_configuration
                $taskResult = Set-IngestionTaskConfiguration @sqlParams `
                    -TaskName "$taskName"  `
                    -TaskCategory $taskCategory `
                    -TaskDescription $task_description `
                    -TaskType $task_type `
                    -WorkerName $worker_pipeline `
                    -FileLayout $null `
                    -SourceFolder $source_folder `
                    -ContainerName $container_name `
                    -TableName $null `
                    -TargetOptions $null `
                    -PreprocessPattern $preprocessPattern `
                    -PreprocessWorkerOptions $preprocessWorkerOptions `
                    -Verbose:$VerbosePreference -PrintDetailedResults:$PrintDetailedResults
                
                Write-Information "[$jsonFile].[$taskName] task_configuration (PREPROCESS) succeeded - Total: $($taskResult.Total), Inserted: $($taskResult.Inserted), Updated : $($taskResult.Updated), Deleted : $($taskResult.Deleted)"        
    
            } #foreach dataset
        } else {
            Write-Information "[$jsonFile] Found no preprocessing task(s) in file"
        }
    }
    
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionMetadataFromJsonFile.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    # . $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    # . $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    # . $PSScriptRoot/Get-MbbAzDetokenizedString.ps1
    # . $PSScriptRoot/Set-IngestionTaskConfiguration.ps1

    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-MbbAzIngestionMetadataFromJsonFile" {

    Context "UnitTests" {

        BeforeAll {

            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            mock Write-Information { Param([string]$messageData) Write-Output $MessageData  }

            Mock Compare-IngestionMetadataFromJsonFile {}
            Mock Set-IngestionPlanConfiguration { $resultset}
            Mock Set-IngestionPlanTaskConfiguration { $resultset}
            Mock Set-IngestionTaskConfiguration { $resultset }
            Mock Set-IngestionSourceConfiguration { $resultset}
            Mock Set-IngestionSourceColumnConfiguration { $resultset }
            Mock Set-IngestionSourceCheckConfiguration { $resultset }
            Mock Update-IngestionDeltaTableConfiguration {}


            $resultset = [PSCustomObject]@{
                Total = 0
                Inserted = 0
                Updated = 0
                Deleted = 0
            }
            $resultset | Out-Null # avoid unused variable warning

            # deploy to a dummy unit test environment
            $commonParams = @{
                TargetEnvironment = "unittest"
                SQLServerInstance = "dummy"
                SQLDatabase = "dummy"
                AccessToken = "dummy"
                ErrorAction = "Stop"
            }
            $commonParams | Out-Null # avoid unused variable warning

            
        }

        Context "Test invalid input file behaviour" {
            BeforeAll {
            }

            It "Should fail if missing file" {
                { Set-IngestionMetadataFromJsonFile @commonParams -Path "unexisting"  } |
                Should -Throw -ExceptionType System.IO.FileNotFoundException
            }
            It "Should fail if invalid json" {            
                # dev-note: actually, checking that invalid files are detected asu such should be unit tested
                # in the Compare function test, so here we only check that errors from the Compare function 
                # are correctly handled
                Mock Compare-IngestionMetadataFromJsonFile {throw}
                $invalidFile = Join-Path $TestFilePath "testInvalidJson.json"
                { Set-IngestionMetadataFromJsonFile @commonParams -Path $invalidFile } | Should -Throw
                Should -Invoke -CommandName Compare-IngestionMetadataFromJsonFile -Times 1
            }
        }

        Context "Test input file orchestration behaviour" {

            BeforeAll {
            }

            It "Should invoke plantask and plan configuration functions" {
                $validFile = Join-Path $TestFilePath "testIngestionMetadata-plantasks_full.json"
                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Compare-IngestionMetadataFromJsonFile -Times 1 -ParameterFilter {$Path -eq $validFile}
                Should -Invoke -CommandName Set-IngestionPlanConfiguration -Times 1
                Should -Invoke -CommandName Set-IngestionPlanTaskConfiguration -Times 1
            } 

            It "Should invoke task, source, and source column configuration functions" {
                $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_full.json"
                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Compare-IngestionMetadataFromJsonFile  -Times 1 -ParameterFilter {$Path -eq $validFile}
                Should -Invoke -CommandName Set-IngestionTaskConfiguration -Times 1
                Should -Invoke -CommandName Set-IngestionSourceConfiguration -Times 1
                Should -Invoke -CommandName Set-IngestionSourceColumnConfiguration -Times 1
                Should -Invoke -CommandName Set-IngestionSourceCheckConfiguration -Times 1
            } 

            It "Should invoke preprocess task configuration functions" {
                $validFile = Join-Path $TestFilePath "testIngestionMetadata-preprocess_valid.json"
                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Compare-IngestionMetadataFromJsonFile -Times 1 -ParameterFilter {$Path -eq $validFile}
                Should -Invoke -CommandName Set-IngestionPlanConfiguration -Times 0
                Should -Invoke -CommandName Set-IngestionPlanTaskConfiguration -Times 0
                Should -Invoke -CommandName Set-IngestionTaskConfiguration -Times 1
            } 

            It "Should not invoke preprocess task configuration functions" {
                $validFile = Join-Path $TestFilePath "testIngestionMetadata-preprocess_valid.json"
                # call with -DeployPreprocessing $false
                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile -DeployPreprocessing $false
                Should -Invoke -CommandName Compare-IngestionMetadataFromJsonFile -Times 1 -ParameterFilter {$Path -eq $validFile}
                Should -Invoke -CommandName Set-IngestionTaskConfiguration -Times 0
            }
            
            It "Should not deploy the plan because the environment does not match"{
                $skippedPlanFile = Join-Path $TestFilePath "testIngestionMetadata-plantasks_skip_environment.json"
                Set-IngestionMetadataFromJsonFile @commonParams -Path $skippedPlanFile
                Should -Invoke -CommandName Compare-IngestionMetadataFromJsonFile -Times 1 -ParameterFilter {$Path -eq $skippedPlanFile}
                Should -Invoke -CommandName Set-IngestionPlanConfiguration -Times 0
                Should -Invoke -CommandName Set-IngestionPlanTaskConfiguration -Times 0
                Should -Invoke -CommandName Write-Information -ParameterFilter {$MessageData -like "*not valid for target environment*"}
            }

        }

        Context "Test default usage when optional variables in dataset are not present"{
            BeforeAll{

                $resultset = [PSCustomObject]@{
                    Total        = 1
                    Inserted     = 1
                    Updated      = 0
                    Deleted      = 0
                }

            }

            It "Should invoke task, source, source column, and source check with defaults"{

                $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_defaults.json"
                $ChecksDataArray = '{"name": "header", "enabled": true}'
                $ColumnDataJson = '{column_sequence": 1, "source_column_name": "first_column", "sink_column_name": "first_column", "dimension": "SCD2", "data_type": "string"}'

                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Set-IngestionSourceConfiguration -Times 1 -ParameterFilter{
                    $ColumnDelimiter    -like ','       -and
                    $RowDelimiter       -like '\r\n'    -and
                    $EscapeCharacter    -like ''        -and
                    $QuoteCharacter     -like ''        -and
                    $HeaderLine         -like $true     -and
                    $Encoding           -like 'UTF-8'   -and
                    $SkipFirstLines     -like 0         -and
                    $Preconditions      -like $null
                }
                Should -Invoke -CommandName Set-IngestionTaskConfiguration -Times 1 -ParameterFilter{
                    $ContainerName -like 'landing'
                }
                Should -Invoke -CommandName Set-IngestionTaskConfiguration -Times 1 -ParameterFilter{
                    $TargetOptions -like $null
                }
                Should -Invoke -CommandName Set-IngestionSourceCheckConfiguration -Times 1 -ParameterFilter{
                    $ChecksDataArray -like "$ChecksDataArray"
                }
                Should -Invoke -CommandName Set-IngestionSourceColumnConfiguration -Times 1 -ParameterFilter{
                    $ColumnDataJson -like "$ColumnDataJson"
                }
            }

            It "Filename Preconditions: Should load all parameters"{
                $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_filename_precond_all_params.json"

                $expected_condition_text = ('[
  {
    "name": "required_name",
    "type": "file_exists",
    "frequency": "daily",
    "expected_file_mask": ".*{yyyyMMdd}.*",
    "enabled": true,
    "description": "my_description"
  }
]')
                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Set-IngestionSourceConfiguration -Times 1 -ParameterFilter{
                    $ColumnDelimiter    -like ','       -and
                    $RowDelimiter       -like '\r\n'    -and
                    $EscapeCharacter    -like ''        -and
                    $QuoteCharacter     -like ''        -and
                    $HeaderLine         -like $true     -and
                    $Encoding           -like 'UTF-8'   -and
                    $SkipFirstLines     -like 0         -and
                    $Preconditions      -eq $expected_condition_text	
                }
            }

            It "Filename Preconditions: Should load with required params load"{
                $validFile = Join-Path $TestFilePath "testIngestionMetadata-datasets_filename_precond_required_params.json"

                $expected_condition_text = ('[
  {
    "name": "required_name",
    "type": "file_exists",
    "frequency": "daily",
    "expected_file_mask": null,
    "enabled": true,
    "description": null
  }
]')

                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Set-IngestionSourceConfiguration -Times 1 -ParameterFilter{
                    $ColumnDelimiter    -like ','       -and
                    $RowDelimiter       -like '\r\n'    -and
                    $EscapeCharacter    -like ''        -and
                    $QuoteCharacter     -like ''        -and
                    $HeaderLine         -like $true     -and
                    $Encoding           -like 'UTF-8'   -and
                    $SkipFirstLines     -like 0         -and
                    $Preconditions -eq $expected_condition_text
                }
            }
        }

        Context "Test default usage when optional variables in plans are not present"{
            BeforeAll{
                $resultset = [PSCustomObject]@{
                    Total        = 1
                    Inserted     = 1
                    Updated      = 0
                    Deleted      = 0
                }
                
            }

            It "Should invoke plan, and plan task config with defaults"{

                $validFile = Join-Path $TestFilePath "testIngestionMetadata-plantasks_defaults.json"
                Set-IngestionMetadataFromJsonFile @commonParams -Path $validFile
                Should -Invoke -CommandName Set-IngestionPlanConfiguration -Times 1 -ParameterFilter{
                    $PlanEnabled -like $true
                }
                Should -Invoke -CommandName Set-IngestionPlanTaskConfiguration -Times 1 -ParameterFilter{
                    $PlanTask_Json -like '*"task_sequence": 1*' -and
                    $PlanTask_Json -like '*"enabled": true*'
                }
            }
        }

    }
}


</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanConfiguration.ps1">
function Set-IngestionPlanConfiguration {

    [CmdletBinding()]
    param (
    [Parameter(Mandatory = $true)]    
    [String] $ServerInstance,

    [Parameter(Mandatory = $true)]    
    [String] $Database,

    [Parameter(Mandatory = $true)]    
    [String] $AccessToken,

    [Parameter(Mandatory = $true)]    
    [String] $PlanName,
    
    [Parameter(Mandatory = $true)]    
    [String] $PlanDescription,    

    [Parameter(Mandatory = $false)]    
    [Boolean] $PlanEnabled = $true,

    [Parameter(Mandatory = $false)]
    [Switch] $PrintDetailedResults
    )

    $sqlParameters = @{
        ServerInstance = $ServerInstance
        Database = $Database
        AccessToken = $AccessToken

        OutputSqlErrors = $true
        IncludeSqlUserErrors = $true
        AbortOnError = $true
    }

    $Plan_Description = ($PlanDescription) ? ("'" + $PlanDescription.Replace("'", "''") + "'") : "null"
    $Enabled = ($PlanEnabled) ? "1" : "0"

    $generatedSql = [System.Text.StringBuilder]::new()
    [void]$generatedSql.AppendLine("exec [deploy].[usp_set_plan_configuration] ")
    [void]$generatedSql.AppendLine("   @plan_name = '$PlanName', ")
    [void]$generatedSql.AppendLine("   @plan_description = $Plan_Description, ")
    [void]$generatedSql.AppendLine("   @enabled = $Enabled ")

    $Query = $generatedSql.ToString()
    # Write-Verbose "Executing query:"
    # Write-Verbose $Query

    $SQLResult = Get-SQLQueryResults @sqlParameters -Query $Query -PrintDetailedResults:$PrintDetailedResults
    
    return $SQLResult
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanConfiguration.Tests.ps1">
# Dev-note: Very basic test because function does not really do a lot -&gt; Potentially pull apart reusable pieces into new function and rewrite part of the module to be more modulair

BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionPlanConfiguration" {

    Context "UnitTests" {

        BeforeAll {


            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
            $resultset = [PSCustomObject]@{
                Total        = 1
                Inserted     = 1
                Updated      = 0
                Deleted      = 0
            }
            
            Mock Get-SQLQueryResults { $resultset }
        }

        It "Should invoke Get-SQLQueryResults function" {

            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                PlanName = 'dummy'
                PlanDescription = "description for the 'dummy' plan"
                PlanEnabled = $false
            }
                    
            Set-IngestionPlanConfiguration @params -ErrorAction "Stop"
            Should -Invoke -CommandName Get-SQLQueryResults -Times 1 -ParameterFilter {
                $Query -like "*deploy*set_plan_configuration*" -and 
                $Query -like "*@plan_description = 'description for the ''dummy'' plan'*" -and
                $Query -like "*@enabled = 0*"
            }
            
            # Dev-Note: Other testing possibility
            # In main function: use $script:query
            # In test function: use $query | Should -BeLike "*@enabled*"

        }

        It "Should return object from Get-SQLQueryResults"{
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                PlanName = 'dummy'
                PlanDescription = "description for the 'dummy' plan"
                PlanEnabled = $false
            }

            $Result = Set-IngestionPlanConfiguration @params -ErrorAction "Stop"
            $Result.Total       | Should -Be 1
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 0
            $Result.Deleted     | Should -Be 0

        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanTaskConfiguration.ps1">
function Set-IngestionPlanTaskConfiguration {

    [CmdletBinding()]
    param (
    [Parameter(Mandatory = $true)]    
    [String] $ServerInstance,

    [Parameter(Mandatory = $true)]    
    [String] $Database,

    [Parameter(Mandatory = $true)]    
    [String] $AccessToken,

    [Parameter(Mandatory = $true)]    
    [String] $PlanName,
    
    [Parameter(Mandatory = $true, HelpMessage = "Json array of plan_task_configurations")]    
    [String] $PlanTask_Json,

    [Parameter(Mandatory = $false)]
    [Switch] $PrintDetailedResults  

    )

    $sqlParameters = @{
        ServerInstance = $ServerInstance 
        Database = $Database 
        AccessToken = $AccessToken
        
        OutputSqlErrors = $true
        IncludeSqlUserErrors = $true
        AbortOnError = $true
    }


    $generatedSql = [System.Text.StringBuilder]::new()
    [void]$generatedSql.AppendLine("exec [deploy].[usp_set_plan_task_configuration] ")
    [void]$generatedSql.AppendLine("   @plan_name = '$PlanName', ")
    [void]$generatedSql.AppendLine("   @plantask_data = '$PlanTask_Json' ")

    $query = $generatedSql.ToString()
    Write-Verbose "Executing query:"
    Write-Verbose $query

    if ( $PlanTask_Json) {
        # dev-info: the -NoEnumerate below keeps the "array" idea from the json, otherwise we'll get null or single item
        $numPlanTasks = ($PlanTask_Json | ConvertFrom-Json -NoEnumerate).Count
    } else {
        $numPlanTasks = 0
    }
    
    $SQLResult = Get-SQLQueryResults @sqlParameters -Query $query -numTotalParam $numPlanTasks -PrintDetailedResults:$PrintDetailedResults

    return $SQLResult
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionPlanTaskConfiguration.Tests.ps1">
# Dev-note: Very basic test because function does not really do a lot -&gt; Potentially pull apart reusable pieces into new function and rewrite part of the module to be more modulair

BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionPlanTaskConfiguration" {

    Context "UnitTests" {

        BeforeAll {


            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
            $resultset = [PSCustomObject]@{
                Total        = 1
                Inserted     = 1
                Updated      = 0
                Deleted      = 0
            }
            
            Mock Get-SQLQueryResults { $resultset }
        }

        It "Should invoke Get-SQLQueryResults function" {

            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                PlanName = 'dummy'
                PlanTask_Json = '{"dummy_key": "dummy_value"}'
            }

            # This is needed, else: unable to pass Json into -Like statement
            $PlanTask_Json = '{"dummy_key": "dummy_value"}'
                    
            Set-IngestionPlanTaskConfiguration @params -ErrorAction "Stop"
            Should -Invoke -CommandName Get-SQLQueryResults -Times 1 -ParameterFilter {
                $Query -like "*deploy*set_plan_task_configuration*" -and 
                $Query -like "*@plan_name = 'dummy'*" -and
                $Query -like "*@plantask_data = '$PlanTask_Json'*"
            }

        }

        It "Should return object from Get-SQLQueryResults"{
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                PlanName = 'dummy'
                PlanTask_Json = '{"dummy_key": "dummy_value"}'
            }

            $Result = Set-IngestionPlanTaskConfiguration @params -ErrorAction "Stop"
            $Result.Total       | Should -Be 1
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 0
            $Result.Deleted     | Should -Be 0

        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceCheckConfiguration.ps1">
function Set-IngestionSourceCheckConfiguration {

    [CmdletBinding()]
    param (
    [Parameter(Mandatory = $true)]    
    [String] $ServerInstance,

    [Parameter(Mandatory = $true)]    
    [String] $Database,

    [Parameter(Mandatory = $true)]    
    [String] $AccessToken,

    [Parameter(Mandatory = $true)]    
    [String] $FileLayout,                   # File_Layout used to link checks to a certain dataset
    
    [Parameter(Mandatory = $true)]    
    [String] $ChecksDataArrayJson,          # Array of Json-objects 

    [Parameter(Mandatory = $false)]
    [Switch] $PrintDetailedResults

    )

    $sqlParameters = @{
        ServerInstance = $ServerInstance 
        Database = $Database 
        AccessToken = $AccessToken
        
        OutputSqlErrors = $true
        IncludeSqlUserErrors = $true
        AbortOnError = $true
    }

    # Create a string for the new SQL-script to be executed
    $generatedSql = [System.Text.StringBuilder]::new()
    [void]$generatedSql.AppendLine("exec [deploy].[usp_set_source_check_configuration] ")
    [void]$generatedSql.AppendLine("   @file_layout = '$FileLayout', ")
    [void]$generatedSql.AppendLine("   @checks_data = '$ChecksDataArrayJson' ")

    # Make sure the script is in string format (to prevent errors)
    $Query = $generatedSql.ToString()
    # Write-Verbose "Executing query:"
    # Write-Information $Query

    # Execute the query that you just generated
    $SQLResult = Get-SQLQueryResults @sqlParameters -Query $Query -numTotalParam ($ChecksDataArrayJson | ConvertFrom-Json).Count -PrintDetailedResults:$PrintDetailedResults

    return $SQLResult
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceCheckConfiguration.Tests.ps1">
# Dev-note: Very basic test because function does not really do a lot -&gt; Potentially pull apart reusable pieces into new function and rewrite part of the module to be more modulair

BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionSourceCheckConfiguration" {

    Context "UnitTests" {

        BeforeAll {


            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
            $resultset = [PSCustomObject]@{
                Total        = 1
                Inserted     = 1
                Updated      = 0
                Deleted      = 0
            }
            
            Mock Get-SQLQueryResults { $resultset }
        }

        It "Should invoke Get-SQLQueryResults function" {

            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                FileLayout = 'dummy'
                ChecksDataArrayJson = '{"dummy_key": "dummy_value"}'
            }

            # This is needed, else: unable to pass Json into -Like statement
            $ChecksDataArrayJson = '{"dummy_key": "dummy_value"}'
                    
            Set-IngestionSourceCheckConfiguration @params -ErrorAction "Stop"
            Should -Invoke -CommandName Get-SQLQueryResults -Times 1 -ParameterFilter {
                $Query -like "*deploy*set_source_check_configuration*" -and 
                $Query -like "*@file_layout = 'dummy'*" -and
                $Query -like "*@checks_data = '$ChecksDataArrayJson'*"
            }

        }

        It "Should return object from Get-SQLQueryResults"{
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                FileLayout = 'dummy'
                ChecksDataArrayJson = '{"dummy_key": "dummy_value"}'
            }

            $Result = Set-IngestionSourceCheckConfiguration @params -ErrorAction "Stop"
            $Result.Total       | Should -Be 1
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 0
            $Result.Deleted     | Should -Be 0

        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceColumnConfiguration.ps1">
function Set-IngestionSourceColumnConfiguration {

    [CmdletBinding()]
    param (
    [Parameter(Mandatory = $true)]    
    [String] $ServerInstance,

    [Parameter(Mandatory = $true)]    
    [String] $Database,

    [Parameter(Mandatory = $true)]    
    [String] $AccessToken,

    [Parameter(Mandatory = $true)]    
    [String] $FileLayout,
    
    [Parameter(Mandatory = $true)]    
    [String] $ColumnDataJson,

    [Parameter(Mandatory = $false)]
    [Switch] $PrintDetailedResults 

    )

    $sqlParameters = @{
        ServerInstance = $ServerInstance 
        Database = $Database 
        AccessToken = $AccessToken
        
        OutputSqlErrors = $true
        IncludeSqlUserErrors = $true
        AbortOnError = $true
    }

    $generatedSql = [System.Text.StringBuilder]::new()
    [void]$generatedSql.AppendLine("exec [deploy].[usp_set_source_column_configuration] ")
    [void]$generatedSql.AppendLine("   @file_layout = '$FileLayout', ")
    [void]$generatedSql.AppendLine("   @column_data = '$ColumnDataJson' ")

    $Query = $generatedSql.ToString()
    # Write-Verbose "Executing query:"
    # Write-Information $Query
    
    
    $SQLResult = Get-SQLQueryResults @sqlParameters -Query $Query -numTotalParam ($ColumnDataJson | ConvertFrom-Json).Count -PrintDetailedResults:$PrintDetailedResults
    

    return $SQLResult
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceColumnConfiguration.Tests.ps1">
# Dev-note: Very basic test because function does not really do a lot -&gt; Potentially pull apart reusable pieces into new function and rewrite part of the module to be more modulair

BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionSourceColumnConfiguration" {

    Context "UnitTests" {

        BeforeAll {


            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
            $resultset = [PSCustomObject]@{
                Total        = 1
                Inserted     = 1
                Updated      = 0
                Deleted      = 0
            }
            
            Mock Get-SQLQueryResults { $resultset }
        }

        It "Should invoke Get-SQLQueryResults function" {

            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                FileLayout = 'dummy'
                ColumnDataJson = '{"dummy_key": "dummy_value"}'
            }

            # This is needed, else: unable to pass Json into -Like statement
            $ColumnDataJson = '{"dummy_key": "dummy_value"}'
                    
            Set-IngestionSourceColumnConfiguration @params -ErrorAction "Stop"
            Should -Invoke -CommandName Get-SQLQueryResults -Times 1 -ParameterFilter {
                $Query -like "*deploy*set_source_column_configuration*" -and 
                $Query -like "*@file_layout = 'dummy'*" -and
                $Query -like "*@column_data = '$ColumnDataJson'*"
            }

        }

        It "Should return object from Get-SQLQueryResults"{
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                FileLayout = 'dummy'
                ColumnDataJson = '{"dummy_key": "dummy_value"}'
            }

            $Result = Set-IngestionSourceColumnConfiguration @params -ErrorAction "Stop"
            $Result.Total       | Should -Be 1
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 0
            $Result.Deleted     | Should -Be 0

        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceConfiguration.ps1">
function Set-IngestionSourceConfiguration {

    [CmdletBinding()]
    param (
    [Parameter(Mandatory = $true)]    
    [String] $ServerInstance,

    [Parameter(Mandatory = $true)]    
    [String] $Database,

    [Parameter(Mandatory = $true)]    
    [String] $AccessToken,

    [Parameter(Mandatory = $true)]    
    [String] $FileLayout,
    
    [Parameter(Mandatory = $true)]    
    [String] $MatchPattern,
    [Parameter(Mandatory = $true)]    
    [String] $FileKind,    

    [Parameter(Mandatory = $true)]    
    [String] $Extension,
    [Parameter(Mandatory = $false)]    
    [String] $ColumnDelimiter,
    [Parameter(Mandatory = $false)]    
    [String] $RowDelimiter,
    [Parameter(Mandatory = $false)]    
    [String] $EscapeCharacter,
    [Parameter(Mandatory = $false)]    
    [String] $QuoteCharacter,
    [Parameter(Mandatory = $true)]    
    [bool] $HeaderLine,
    [Parameter(Mandatory = $false)]    
    [String] $SheetName,
    [Parameter(Mandatory = $false)]    
    [String] $DataRange,
    [Parameter(Mandatory = $true)]    
    [String] $Encoding,
    [Parameter(Mandatory = $false)]    
    [Int16] $SkipFirstLines,
    [Parameter(Mandatory = $false)]
    [Switch] $PrintDetailedResults,
    [Parameter(Mandatory= $false)]
    [String] $Preconditions
    )

    $sqlParameters = @{
        ServerInstance = $ServerInstance 
        Database = $Database 
        AccessToken = $AccessToken
        
        OutputSqlErrors = $true
        IncludeSqlUserErrors = $true
        AbortOnError = $true
    }

    $file_layout = "'$FileLayout'"
    $match_pattern = "'$MatchPattern'"
    $file_kind = "'$FileKind'"
    $extension = ($Extension) ? ("'" + $Extension + "'") : "null"
    $column_delimiter = ($ColumnDelimiter) ? ($ColumnDelimiter) : "null" # Doesn't work when enclosed with single quotes, so leave them out
    $row_delimiter = ($RowDelimiter) ? ("'" + $RowDelimiter + "'") : "null"
    $escape_character = ($EscapeCharacter) ? ("'" + $EscapeCharacter + "'") : "null"
    $quote_character = ($QuoteCharacter) ? ("'" + $QuoteCharacter + "'") : "null"
    $headers = ($HeaderLine)
    $skip_first_lines = ($SkipFirstLines) ? ($SkipFirstLines) : "null"
    $preconditions = ($Preconditions) ? ("'" +  ($Preconditions  -replace "'", "''") + "'") : "null"

    $generatedSql = [System.Text.StringBuilder]::new()
    [void]$generatedSql.AppendLine("exec [deploy].[usp_set_source_configuration] ")
    [void]$generatedSql.AppendLine("   @file_layout = $file_layout, ")
    [void]$generatedSql.AppendLine("   @file_pattern = $match_pattern, ")
    [void]$generatedSql.AppendLine("   @file_kind = $file_kind, ")
    [void]$generatedSql.AppendLine("   @file_extension = $extension, ")
    [void]$generatedSql.AppendLine("   @column_delimiter = '$column_delimiter', ")
    [void]$generatedSql.AppendLine("   @row_delimiter = $row_delimiter, ")
    [void]$generatedSql.AppendLine("   @escape_character = $escape_character, ")
    [void]$generatedSql.AppendLine("   @quote_character = $quote_character, ")
    [void]$generatedSql.AppendLine("   @header = $headers, ")
    [void]$generatedSql.AppendLine("   @encoding = '$Encoding', ")
    [void]$generatedSql.AppendLine("   @skip_first_lines = $skip_first_lines, ")
    [void]$generatedSql.AppendLine("   @source_conditions = $Preconditions ")

    $Query = $generatedSql.ToString()
    # Write-Verbose "Executing query:"
    # Write-Information $Query

    $SQLResult = Get-SQLQueryResults @sqlParameters -Query $Query -PrintDetailedResults:$PrintDetailedResults

    return $SQLResult
}

</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionSourceConfiguration.Tests.ps1">
# Dev-note: Very basic test because function does not really do a lot -&gt; Potentially pull apart reusable pieces into new function and rewrite part of the module to be more modulair

BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionSourceConfiguration" {

    Context "UnitTests" {

        BeforeAll {


            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
            $resultset = [PSCustomObject]@{
                Total        = 1
                Inserted     = 1
                Updated      = 0
                Deleted      = 0
            }
            
            Mock Get-SQLQueryResults { $resultset }
        }

        It "Should invoke Get-SQLQueryResults function" {

            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                FileLayout = 'dummy'
                MatchPattern = '(dummy)'
                FileKind = 'dummy'
                Extension = '.dummy'
                ColumnDelimiter = 'd'
                RowDelimiter = 'd'
                EscapeCharacter = 'd'
                QuoteCharacter = 'd'
                HeaderLine = $true
                SheetName = 'dummy'
                Encoding = 'utf-dummy'
                SkipFirstLines = -1
            }

                    
            Set-IngestionSourceConfiguration @params -ErrorAction "Stop"
            Should -Invoke -CommandName Get-SQLQueryResults -Times 1 -ParameterFilter {
                $Query -like "*deploy*set_source_configuration*" -and 
                $Query -like "*@file_layout = 'dummy'*" -and
                $Query -like "*@file_pattern = '(dummy)'*" -and
                $Query -like "*@file_kind = 'dummy'*" -and
                $Query -like "*@file_extension = '.dummy'*" -and
                $Query -like "*@column_delimiter = 'd'*"  -and
                $Query -like "*@row_delimiter = 'd'*"  -and
                $Query -like "*@escape_character = 'd'*" -and
                $Query -like "*@quote_character = 'd'*" -and
                $Query -like "*@header = True*" -and
                $Query -like "*@encoding = 'utf-dummy'*"
                $query -like "*@skip_first_lines = -1*"
                $query -like "*@source_conditions = 'dummy'*"

            }

        }

        It "Should return object from Get-SQLQueryResults"{
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                FileLayout = 'dummy'
                FileKind = 'dummy'
                MatchPattern = '(dummy)'
                Extension = '.dummy'
                ColumnDelimiter = 'd'
                RowDelimiter = 'd'
                EscapeCharacter = 'd'
                QuoteCharacter = 'd'
                HeaderLine = $true
                SheetName = 'dummy'
                Encoding = 'utf-dummy'
                SkipFirstLines = -1
                Preconditions = 'dummy'
            }

            $Result = Set-IngestionSourceConfiguration @params -ErrorAction "Stop"
            $Result.Total       | Should -Be 1
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 0
            $Result.Deleted     | Should -Be 0

        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionTaskConfiguration.ps1">
function Set-IngestionTaskConfiguration {

    [CmdletBinding()]
    param (
        [Parameter(Mandatory = $true)]    
        [String] $ServerInstance,

        [Parameter(Mandatory = $true)]    
        [String] $Database,

        [Parameter(Mandatory = $true)]    
        [String] $AccessToken,

        [Parameter (Mandatory = $true)]
        [String] $TaskName,


        [Parameter (Mandatory = $true, HelpMessage = "Task category is required. Valid values are 'INGEST' and 'PREPROCESS'")]    
        [ValidateSet('INGEST', 'PREPROCESS')]
        [String] $TaskCategory,

        [Parameter (Mandatory = $true, HelpMessage = "Task description is required.")]    
        [String] $TaskDescription,

        [Parameter (Mandatory = $true)]
        [String] $TaskType,

        [Parameter (Mandatory = $true)]
        [String] $WorkerName,

        [Parameter (Mandatory = $false)]
        [String] $FileLayout,

        [Parameter (Mandatory = $true)]
        [AllowEmptyString()]
        [String] $SourceFolder,

        [Parameter (Mandatory = $true)]
        [String] $ContainerName,

        [Parameter (Mandatory = $false)]
        [String] $TableName,

        [Parameter (Mandatory = $false)]
        [AllowEmptyString()]
        [psobject] $TargetOptions = @{},

        [Parameter (Mandatory = $false)]
        [AllowEmptyString()]
        [String] $PreprocessPattern = $null,

        [Parameter (Mandatory = $false)]
        [AllowEmptyString()]
        [psobject] $PreprocessWorkerOptions = $null,

        [Parameter(Mandatory = $false)]
        [Switch] $PrintDetailedResults
    )

    #region additional checks: INGEST
    if ($TaskCategory -eq "INGEST") {
        if (-not $FileLayout) {
            Throw "FileLayout is required for INGEST task category"
        }
        if (-not $TableName) {
            Throw "TableName is required for INGEST task category"
        }
    }
    #endregion

    #region additional checks: PREPROCESS
    if ($TaskCategory -eq "PREPROCESS") {
        if (-not $PreprocessPattern) {
            Throw "PreprocessPattern is required for PREPROCESS task category"
        }
    }
    #endregion


    $sqlParameters = @{
        ServerInstance = $ServerInstance 
        Database = $Database 
        AccessToken = $AccessToken
        
        OutputSqlErrors = $true
        IncludeSqlUserErrors = $true
        AbortOnError = $true
    }

    $table_name = ($TableName) ? ("'" + $TableName.trim() + "'") : "null"
    $source_folder = ($SourceFolder) ? ("'" + $SourceFolder+ "'") : "null"
    $file_layout = ($FileLayout) ? ("'" + $FileLayout.trim() + "'") : "null"

    if ($TargetOptions){
        $TargetOptions = "'" + ($TargetOptions | ConvertTo-Json -Depth 5 ) + "'" 
    } else {
        $TargetOptions = "null"
    }
    if ($PreprocessWorkerOptions){
        $PreprocessWorkerOptions = "'" + ($PreprocessWorkerOptions | ConvertTo-Json -Depth 5) + "'"
    } else {
        $PreprocessWorkerOptions = "null"
    }

    
    $generatedSql = [System.Text.StringBuilder]::new()
    [void]$generatedSql.AppendLine("exec [deploy].[usp_set_task_configuration] ")
    [void]$generatedSql.AppendLine("   @task_name = '$TaskName', ")
    [void]$generatedSql.AppendLine("   @task_category = '$TaskCategory', ")
    [void]$generatedSql.AppendLine("   @task_type = '$TaskType', ")
    [void]$generatedSql.AppendLine("   @worker_name = $WorkerName, ")
    [void]$generatedSql.AppendLine("   @task_description = '$TaskDescription', ")
    [void]$generatedSql.AppendLine("   @table_name = $table_name, ")
    [void]$generatedSql.AppendLine("   @target_options = $TargetOptions, ")
    [void]$generatedSql.AppendLine("   @file_layout = $file_layout, ")
    [void]$generatedSql.AppendLine("   @source_folder = $source_folder, ")
    [void]$generatedSql.AppendLine("   @container_name = '$ContainerName', ")
    [void]$generatedSql.AppendLine("   @preprocess_pattern = '$PreprocessPattern', ")
    [void]$generatedSql.AppendLine("   @preprocess_worker_options = $PreprocessWorkerOptions, ")
    [void]$generatedSql.AppendLine("   @enabled = $true ")

    $Query = $generatedSql.ToString()
    Write-Verbose "Executing query:"
    Write-Verbose $Query

    $SQLResult = Get-SQLQueryResults @sqlParameters -Query $Query -PrintDetailedResults:$PrintDetailedResults

    return $SQLResult
}
</file>
<file name="src\powershell\modules\DataConfig\public\Set-IngestionTaskConfiguration.Tests.ps1">
# Dev-note: Very basic test because function does not really do a lot -&gt; Potentially pull apart reusable pieces into new function and rewrite part of the module to be more modulair

BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # load depending functions (they need to be loaded before they can be mocked)
    #. $PSScriptRoot/Set-MbbAzIngestionBatchjobs.ps1
    #. $PSScriptRoot/Set-MbbAzIngestionDatasets.ps1
    #. $PSScriptRoot/Get-MbbAzDetokenizedString.ps1


    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Set-IngestionTaskConfiguration" {

    Context "UnitTests" {

        BeforeAll {


            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
            
            $resultset = [PSCustomObject]@{
                Total        = 1
                Inserted     = 1
                Updated      = 0
                Deleted      = 0
            }
            
            Mock Get-SQLQueryResults { $resultset }
        }

        It "Should invoke Get-SQLQueryResults function" {

            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                TaskName = 'dummy'
                TaskCategory = 'INGEST'
                TaskDescription = 'dummy dummy'
                TaskType = 'dummy'
                WorkerName = 'dummyworker'
                FileLayout = 'dummy_file'
                SourceFolder = '/dummy/dummy'
                ContainerName = 'dummy_container'
                TableName = 'dummy_table'
                TargetOptions = $null
            }

                    
            Set-IngestionTaskConfiguration @params -ErrorAction "Stop"
            Should -Invoke -CommandName Get-SQLQueryResults -Times 1 -ParameterFilter {
                $Query -like "*deploy*set_task_configuration*" -and 
                $Query -like "*@task_name = 'dummy'*" -and
                $Query -like "*@task_type = 'dummy'*"-and
                $Query -like "*@worker_name = dummyworker*" -and
                $Query -like "*@task_description = 'dummy dummy'*"  -and
                $Query -like "*@table_name = 'dummy_table'*"  -and
                $Query -like "*@target_options = null*"  -and
                $Query -like "*@file_layout = 'dummy_file'*" -and
                $Query -like "*@source_folder = '/dummy/dummy'*" -and
                $Query -like "*@container_name = 'dummy_container'*" -and
                $Query -like "*@enabled = True*"

            } 

        }

        It "Should return object from Get-SQLQueryResults"{
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                TaskName = 'dummy'
                TaskCategory = 'INGEST'
                TaskDescription = 'dummy dummy'
                TaskType = 'dummy'
                WorkerName = 'dummyworker'
                FileLayout = 'dummy_file'
                SourceFolder = '/dummy/dummy'
                ContainerName = 'dummy_container'
                TableName = 'dummy_table'
                TargetOptions = $null
            }

            $Result = Set-IngestionTaskConfiguration @params -ErrorAction "Stop"
            $Result.Total       | Should -Be 1
            $Result.Inserted    | Should -Be 1
            $Result.Updated     | Should -Be 0
            $Result.Deleted     | Should -Be 0

        }

        It "Should fail if the PREPROCESS task does not have a PreprocessPattern" {
            $params = @{
                ServerInstance = 'dummy'
                Database = 'dummy'
                AccessToken = 'dummy'
                TaskName = 'dummy'
                TaskCategory = 'PREPROCESS'
                TaskDescription = 'dummy dummy'
                TaskType = 'dummy'
                WorkerName = 'dummyworker'
                SourceFolder = '/dummy/dummy'
                ContainerName = 'dummy_container'
            }
            {Set-IngestionTaskConfiguration @params -ErrorAction "Stop"} | Should -Throw "*PreprocessPattern*"
        }

    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Start-IngestionSparkSessionJobs.ps1">
function Start-IngestionSparkSessionJobs {
    &lt;#
    .SYNOPSIS
    Invoke a code statement against a Spark Session and return a jobObject or statementObject

    .DESCRIPTION
    Function uses the PSSynapseSparkSession or gets such an object using the SessionName
    It launches a code statement to the object. This can be done as a job or not
        -job: Do not wait for the statement to execute before continuing -&gt; return jobObject
        -else: Wait for the statement to complete before continuing      -&gt; return statementObject

    .NOTES
    - Need to define either the name of the session OR pass a PSSynapseSparkSession object
    #&gt;

    [CmdletBinding()]

    param(
        [Parameter(Mandatory=$true, HelpMessage="Code to execute on the spark session")]
        [string] $Code, 

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse workspace where to invoke the session" )]
        [string] $SynapseWorkspace,

        [Parameter(Mandatory=$true, HelpMessage="Name of the Synapse spark pool to invoke a session from" )]
        [string] $SynapseSparkPool,
        
        [Parameter(Mandatory=$false, HelpMessage="PSSynapseSparkSession object")]
        [psobject] $SessionObj,

        [Parameter(Mandatory=$false, HelpMessage="Name of the spark session")]
        [string] $SessionName,

        [Parameter(Mandatory=$false, HelpMessage="Run the code as a job (-AsJob) or not")]
        [boolean] $Job
    )

    # Local scripts settings
    Set-StrictMode -Version "latest"

    #region step 1: Execute data quality checks
        # Either Session or SessionName needs to be defined
        if (-not $SessionObj -and -not $SessionName ){
            throw "Need to define either a SessionObject or give the name of a Session"
        }

    #endregion

    #region step 2: Get spark session object
        if (-not $SessionObj){
            $SessionObj = Get-IngestionSparkSessionObject -SynapseWorkspace $SynapseWorkspace -SynapseSparkPool $SynapseSparkPool -SessionName $SessionName -WaitForSession $true -SetUpSession $true
        }

        # Make sur ethe type of the object is correct        
        if ($SessionObj.GetType().Name -ne "PSSynapseSparkSession"){
            throw "Given session object is not of type 'PSSynapseSparkSession'"
        }
    #endregion

    #region step 3: Execute the code-block on the spark session
        if ($Job){
            $jobObj = $SessionObj | Invoke-AzSynapseSparkStatement -Language 'PySpark' -Code $Code -AsJob
            return $jobObj
        }
        else{
            $statementObj = $SessionObj | Invoke-AzSynapseSparkStatement -Language 'PySpark' -Code $Code
            return $statementObj
        }
    #endregion
}
</file>
<file name="src\powershell\modules\DataConfig\public\Start-IngestionSparkSessionJobs.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Start-IngestionSparkSessionJobs" {

    Context "UnitTests" {

        BeforeAll {

            # mock session object
            $sparkObjectGet = [hashtable]@{
                State = 'idle'
                Name = 'Unittest'
                Id = '123'
                AppId = 'application_0000000000000_0000'
                LivyInfo = @{
                    StartingAt = Get-Date
                }
            }

            $mockSession = New-MockObject -Type Microsoft.Azure.Commands.Synapse.Models.PSSynapseSparkSession -Properties $sparkObjectGet
            mock Get-IngestionSparkSessionObject {$mockSession}
        }

        Context "Execute functions with usable spark session"{
            BeforeAll {

                # mock the output when -AsJob is used
                $jobOutput = @([PSCustomObject]@{
                    Id = '123'
                    State = "waiting"
                })

                # mock the output when -AsJob is not used
                $statementOutput = @{
                    Output = @{
                        Status = 'ok'
                    }
                    Id = 2
                }

                # mock function and return different result based on code-statement
                mock Invoke-AzSynapseSparkStatement { 
                    Param([string]$Code) 
                    if ($Code -eq 'statement') {return $statementOutput} 
                    elseif ($Code -eq 'job') {return $jobOutput} 
                }
                # }return $jobOutput}
                # mock Invoke-AzSynapseSparkStatement { Param([string]$AsJob=$false) return $statementOutput}
            }

            It "Should get a new session object when sessionName is passed"{
                Start-IngestionSparkSessionJobs -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -Code "localtest" -SessionName "localtest"

                Should -Invoke Get-IngestionSparkSessionObject
            }
            It "Should run the code as a job"{
                $result = Start-IngestionSparkSessionJobs -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -Code "job" -SessionName "localtest" -Job $true

                Should -Invoke Invoke-AzSynapseSparkStatement -Times 1 -ParameterFilter {$AsJob -eq $true}
                $result | Should -Be $jobOutput
            }
            It "Should run the code as a statement"{
                $result = Start-IngestionSparkSessionJobs -SynapseWorkspace 'dev-dap-syn-core' -SynapseSparkPool 'devdapsspcore' -Code "statement" -SessionName "localtest" -Job $false

                Should -Invoke Invoke-AzSynapseSparkStatement -Times 0 -ParameterFilter {$AsJob -eq $true}
                $result | Should -Be $statementOutput
            }
        }
    }
}

</file>
<file name="src\powershell\modules\DataConfig\public\Update-IngestionDeltaTableConfiguration.ps1">
function Update-IngestionDeltaTableConfiguration {
    &lt;#
    .SYNOPSIS
        Function to create a configuration file with missing delta tabes

    .DESCRIPTION
        The function creates a configuration file for all delta tables that are not currently present in the silver lake database.
        The function checks if there exists a folder in the silver container with the name of the table.
        If this is not the case, the metadata for the delta table (table name, column names and datatypes...) is added to a configuration file.
        Later in the process, the Set-IngestionDeltaTableConfiguration will deploy the configuration file

    .NOTES
        At the time of writing, the function will only add the tables that are not in the silver database yet.
        In the future, it might be useful to include all tables and execute validation checks. This can help flag
            issues like schema drift, column renames, etc.

    #&gt;
    [CmdletBinding()]

    param(
        # Metadata for the delta table
        [Parameter(Mandatory=$true, HelpMessage="Name of the delta table to be created" )]
        [string] $TableName,

        [Parameter(Mandatory=$true, HelpMessage="List of JSON objects containing column metadata")]
        [psobject] $ColumnData,

        [Parameter(Mandatory=$false, HelpMessage="Metadata description of what the table resembles" )]
        [string] $TableDescription,

        [Parameter(Mandatory=$true, HelpMessage="Container name inside of the storage account (e.g. silver)")]
        [string] $ContainerName,

        # Logical parameters needed for function logic
        [Parameter(Mandatory=$false, HelpMessage="Name of the environment the table needs to be deployed to" )]
        [string] $TargetEnvironment,

        [Parameter(Mandatory=$false, HelpMessage="A switch to turn on or off when you don't need to check the existence of delta tables in the container. When not provided, precheck will always be executed")]
        [switch] $CheckTableExistence,

        [Parameter(Mandatory=$false, HelpMessage="Configuration file containing the list of delta table objects to deploy to the delta lake")]
        [string] $DeltaTableConfigurationFile, # "$($env:REPOROOT)/configuration/delta_tables/delta_tables.json"

        [Parameter(Mandatory=$false, HelpMessage="Additional options-object for the delta table to be assigned during deployment")]
        [psobject] $TargetOptions = @{}

    )

    $StorageAccount = "$($TargetEnvironment)dapstdala1"             # Name of the storage account that will store the tables
    
    # Use a regex-function to validate the existence of the delta table
    $Regex = "^($($TableName)\/_delta_log)$"
    # Only the silver container can contain delta tables in the current set-up
    # Therefore, if the container is not 'silver', an error will be thrown
    if ($ContainerName -ne 'silver'){
        throw "Only 'silver' container can contain delta tables, not $($ContainerName)"
    }

    if (-not (Test-Path $DeltaTableConfigurationFile)){
        Write-Information "Creating new configuration file"
        New-Item -Path $DeltaTableConfigurationFile -ItemType File -Force
        # Create a custom PowerShell object
        $jsonObject = New-Object PSObject
        # Add table_configurations property as an empty array
        $jsonObject | Add-Member -MemberType NoteProperty -Name table_configurations -Value @()
        # Convert the object to JSON format
        $jsonString = $jsonObject | ConvertTo-Json
        # Save the JSON string to a file
        $jsonString | Out-File -FilePath $DeltaTableConfigurationFile
    }

    # (See Notes)
    # First the function checks on whether the delta table already exists in the silver database.
    # This is done by checking if, in the 'silver' container, there is a folder with the name of the table.
    # If this is the case, the table already exists and there is no need to redeploy the table.
    
    # $ContainerContent = New-Object Collections.Generic.List[string]
    [bool]$FoundFlag = $false
    if ($CheckTableExistence){
        Write-Information "Checking for existence of delta table $($TableName) in container $($ContainerName)..."

        # Set the context of the Azure Storage Account
        $Context = New-AzStorageContext -StorageAccountName $StorageAccount -UseConnectedAccount
        # Get all blobs inside a container according to a specific filter
        $Blobs = Get-AzStorageBlob -Context $Context -Container $ContainerName
    
        # If there are no blobs inside the container, the boolean will be false
        if ([bool]$Blobs){
            # For each found blob, check if it matches the regex pattern
            foreach($Blob in $Blobs.Name){
                # If there is a match, the delta table exists in the lake database
                if ($Blob -match $Regex){
                    $FoundFlag = $true
                    # $ContainerContent.Add($Blob)
                    break
                }
            }
        }
    }

    if((-Not $FoundFlag) -or (-not $CheckTableExistence)){
        if (-not $CheckTableExistence){
            # User-friendly output based on which scenario is happening in the IF-structure
            # Write-Warning "Existence of delta table for $TableName disabled. Continuing with caution..."
            Write-Information "Adding metadata for delta table $($TableName) to configuration file..."
        }
        else{
            Write-Information "Table $($TableName) not found. Adding metadata to configuration file..."
        }
    
        # Convert the column-data to a JSON parameter
        # Replace the " by ' in the colum data: Necessary for later deployments
        $JsonColumnData =  ($ColumnData | ConvertTo-Json -Compress).Replace('"', "'")
        # $TargetOptions = ($TargetOptions | ConvertTo-Json)
        # Create a parameter object containing the parameters of the DeployDeltaTable script 
        $params = @{
            table_name = $TableName
            container_name = $ContainerName
            column_info = $JsonColumnData
            storage_account = $StorageAccount
            table_description = $TableDescription
            target_options = $TargetOptions
        }

        # Open the configuration file and add the parameter object to the file
        $deltaConfigFileContent = Get-Content $DeltaTableConfigurationFile | ConvertFrom-Json
        $tableConfigurations = $deltaConfigFileContent.table_configurations
        # Check if the object already exists in the configuration
        $tableNameObjects = $tableConfigurations | Where-Object { $_.table_name -eq $TableName }
        # If it does not exist, add it
        if ($tableNameObjects -eq $null) {
            # If not found, add a new object
            $deltaConfigFileContent.table_configurations += $params
            $deltaConfigFileContent | ConvertTo-Json -Depth 5 | Out-File $DeltaTableConfigurationFile  
            Compare-IngestionMetadataFromJsonFile $DeltaTableConfigurationFile -schema "DeltaTableData.schema.json"
        }    
        else{
            Write-Information "Configuration for table $($TableName) already exists"
        }
    }
    else{
        Write-Information "Delta table for table $TableName found in $($ContainerName). No new deployment needed."
    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Update-IngestionDeltaTableConfiguration.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Update-IngestionDeltaTableConfiguration" {

    Context "UnitTests" {

        BeforeAll {
            mock Get-AzContext {
                '{ "subscription": { "id": "xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"  } }' | ConvertFrom-Json
            }
        }

        Context "Skip configuration update when table exists"{
            BeforeAll{
                $storageObject = @([PSCustomObject]@{
                    Name = "table_name/_delta_log"
                })
                mock New-AzStorageContext { }
                mock Get-AzStorageBlob { $storageObject }

                mock Write-Information { Param([string]$messageData) } # Write-Host "info: $messageData"
                mock New-Item { }
                mock Out-File { }
                
            }

            It "Table exists in silver table. Skip configuration"{

                Update-IngestionDeltaTableConfiguration `
                -TableName 'table_name' `
                -ColumnData "[{}]" `
                -TableDescription 'test description' `
                -ContainerName 'silver' `
                -TargetEnvironment 'dev' `
                -DeltaTableConfigurationFile 'mock' `
                -CheckTableExistence `
                -TargetOptions @{} `
                -WarningAction SilentlyContinue

                Should -Invoke New-AzStorageContext -Times 1
                Should -Invoke Get-AzStorageBlob -Times 1
                Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -eq "Delta table for table table_name found in silver. No new deployment needed." }
                Should -Invoke New-Item -Times 1 -ParameterFilter { $path -eq 'mock' &amp;&amp; $ItemType -eq 'File'}
            }

        }

        Context "Add configuration to an empty configuration file"{
            BeforeAll{

                # Mock the functions that are called during the Update-IngestionDeltaTableConfiguration function call
                ## Write-Information and Write-Warning: Write the message to the host for manual validation
                mock Write-Information { Param([string]$messageData) } # Write-Host "info: $messageData"
                mock Write-Warning { Param([string]$messageData) } # Write-Host "info: $messageData"

                # Mock Azure-native functions to call storage contents
                $storageObject = @([PSCustomObject]@{
                    Name = "table_name/_delta_log"
                })
                mock New-AzStorageContext { }
                mock Get-AzStorageBlob { $storageObject }

                # This is the object that will try to be added to the configuration file
                $newObject = @{
                    table_description = 'test description'
                    table_name = 'table_name_2'
                    column_info = "'[{}]'"
                    storage_account = 'devdapstdala1'
                    container_name = 'silver'
                    target_options = @{}
                }
                
                # The configuration file is an empty file
                $configurationContent = [PSCustomObject]@{
                    table_configurations = @()
                }

                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/user.json'  
              
            }

            It "Should add configuration when configuration file is empty and CheckTableExistence is disabled"{
                # ----------------------------------------------------------
                ## Call the function with parameters 
                Update-IngestionDeltaTableConfiguration `
                -TableName 'table_name_2' `
                -ColumnData "[{}]" `
                -TableDescription 'test description' `
                -ContainerName 'silver' `
                -TargetEnvironment 'dev' `
                -DeltaTableConfigurationFile 'TestDrive:/user.json' `
                -TargetOptions @{} `
                -WarningAction SilentlyContinue
                # ----------------------------------------------------------
                
                
                # ----------------------------------------------------------
                ## Assert functions: Validate that certain methods have (not) been called during the function-call
                
                ### Do not call: New-AzStorageContext (-not CheckTableExistence)
                Should -Not -Invoke New-AzStorageContext

                ### Do call: Write-Information and Write-Warning when 
                Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -eq "Adding metadata for delta table table_name_2 to configuration file..." }
                # Should -Invoke Write-Warning -Times 1 -ParameterFilter { $message -eq "Existence of delta table for table_name_2 disabled. Continuing with caution..." }

                ### Validate: Check that the output file has the correct schema
                Compare-IngestionMetadataFromJsonFile 'TestDrive:/user.json' -schemaFile "DeltaTableData.schema.json"

                ### Validate that there is only 1 object in the file
                $configuration_contents = Get-Content 'TestDrive:/user.json' -Raw
                $objectCount =  (($configuration_contents | ConvertFrom-Json).table_configurations).Count
                $objectCount | Should -Be 1
            }

            It "Should add configuration when configuration file is empty and CheckTableExistence is enabled"{
                # ----------------------------------------------------------
                ## Call the function with parameters 
                Update-IngestionDeltaTableConfiguration `
                -TableName 'table_name_2' `
                -ColumnData "[{}]" `
                -TableDescription 'test description' `
                -ContainerName 'silver' `
                -TargetEnvironment 'dev' `
                -DeltaTableConfigurationFile 'TestDrive:/user.json' `
                -TargetOptions @{} `
                -CheckTableExistence `
                -WarningAction SilentlyContinue
                # ----------------------------------------------------------
                
                
                # ----------------------------------------------------------
                ## Assert functions: Validate that certain methods have (not) been called during the function-call
                
                ### Do not call: Write-Warning (CheckTableExistence disabled)
                # Should -Not -Invoke Write-Warning

                ### Do call: Write-Information with warning
                Should -Invoke New-AzStorageContext -Times 1
                Should -Invoke Get-AzStorageBlob -Times 1
                Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -eq "Table table_name_2 not found. Adding metadata to configuration file..." }

                ### Validate: Check that the output file has the correct schema
                Compare-IngestionMetadataFromJsonFile 'TestDrive:/user.json' -schemaFile "DeltaTableData.schema.json"

                ### Validate that there is only 1 object in the file
                $configuration_contents = Get-Content 'TestDrive:/user.json' -Raw
                $objectCount =  (($configuration_contents | ConvertFrom-Json).table_configurations).Count
                $objectCount | Should -Be 1
            }

        }


        Context "Add configuration to an non-empty configuration file"{
            BeforeAll{

                # Mock the functions that are called during the Update-IngestionDeltaTableConfiguration function call
                ## Write-Information and Write-Warning: Write the message to the host for manual validation
                mock Write-Information { Param([string]$messageData) } # Write-Host "info: $messageData"
                mock Write-Warning { Param([string]$messageData) } # Write-Host "info: $messageData"

                # Mock Azure-native functions to call storage contents
                $storageObject = @([PSCustomObject]@{
                    Name = "table_name_3/_delta_log"
                })
                mock New-AzStorageContext { }
                mock Get-AzStorageBlob { $storageObject }

                # This is the object that will try to be added to the configuration file
                $newObject = @{
                    table_description = 'test description'
                    table_name = 'table_name_2'
                    column_info = "'[{}]'"
                    storage_account = 'devdapstdala1'
                    container_name = 'silver'
                    target_options = @{}
                }
    
                # This is the object that will already be in the configuration file
                $existingObject = @{
                    table_description = 'test description'
                    table_name = 'table_name'
                    column_info = "'[{}]'"
                    storage_account = 'devdapstdala1'
                    container_name = 'silver'
                    target_options = @{}
                }
                $configurationContent = [PSCustomObject]@{
                    table_configurations = @(
                        $existingObject
                    )
                }
            }

            It "Should add configuration when metadata not in configuration and CheckTableExistence is disabled"{
                # The configuration file is a non-empty file
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/user.json'
                # ----------------------------------------------------------
                ## Call the function with parameters 
                Update-IngestionDeltaTableConfiguration `
                -TableName 'table_name_2' `
                -ColumnData "[{}]" `
                -TableDescription 'test description' `
                -ContainerName 'silver' `
                -TargetEnvironment 'dev' `
                -DeltaTableConfigurationFile 'TestDrive:/user.json' `
                -TargetOptions @{} `
                -WarningAction SilentlyContinue
                # ----------------------------------------------------------
                
                
                # ----------------------------------------------------------
                ## Assert functions: Validate that certain methods have (not) been called during the function-call
                
                ### Do not call: New-AzStorageContext (CheckTableExistence)
                Should -Not -Invoke New-AzStorageContext

                ### Do call: Write-Information and Write-Warning when 
                Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -eq "Adding metadata for delta table table_name_2 to configuration file..." }
                # Should -Invoke Write-Warning -Times 1 -ParameterFilter { $message -eq "Existence of delta table for table_name_2 disabled. Continuing with caution..." }

                ### Validate: Check that the output file has the correct schema
                Compare-IngestionMetadataFromJsonFile 'TestDrive:/user.json' -schemaFile "DeltaTableData.schema.json"

                ### Validate that there is only 1 object in the file
                $configuration_contents = Get-Content 'TestDrive:/user.json' -Raw
                $objectCount =  (($configuration_contents | ConvertFrom-Json).table_configurations).Count
                $objectCount | Should -Be 2
            }

            It "Should add configuration when metadata not in configuration and CheckTableExistence is enabled"{
                # The configuration file is a non-empty file
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/user.json'
                # ----------------------------------------------------------
                ## Call the function with parameters 
                Update-IngestionDeltaTableConfiguration `
                -TableName 'table_name_2' `
                -ColumnData "[{}]" `
                -TableDescription 'test description' `
                -ContainerName 'silver' `
                -TargetEnvironment 'dev' `
                -DeltaTableConfigurationFile 'TestDrive:/user.json' `
                -TargetOptions @{} `
                -CheckTableExistence `
                -WarningAction SilentlyContinue
                # ----------------------------------------------------------
                
                
                # ----------------------------------------------------------
                ## Assert functions: Validate that certain methods have (not) been called during the function-call
                
                ### Do not call: Write-Warning (CheckTableExistence disabled)
                # Should -Not -Invoke Write-Warning

                ### Do call: Write-Information with warning
                Should -Invoke New-AzStorageContext -Times 1
                Should -Invoke Get-AzStorageBlob -Times 1
                Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -eq "Table table_name_2 not found. Adding metadata to configuration file..." }

                ### Validate: Check that the output file has the correct schema
                Compare-IngestionMetadataFromJsonFile 'TestDrive:/user.json' -schemaFile "DeltaTableData.schema.json"

                ### Validate that there is only 1 object in the file
                $configuration_contents = Get-Content 'TestDrive:/user.json' -Raw
                $objectCount =  (($configuration_contents | ConvertFrom-Json).table_configurations).Count
                $objectCount | Should -Be 2
            }

            It "Should not add configuration when metadata in configuration"{
                $configurationContent | ConvertTo-Json -Depth 5 | Out-File 'TestDrive:/user.json'
                # ----------------------------------------------------------
                ## Call the function with parameters 
                Update-IngestionDeltaTableConfiguration `
                -TableName 'table_name' `
                -ColumnData "[{}]" `
                -TableDescription 'test description' `
                -ContainerName 'silver' `
                -TargetEnvironment 'dev' `
                -DeltaTableConfigurationFile 'TestDrive:/user.json' `
                -TargetOptions @{} `
                -CheckTableExistence `
                -WarningAction SilentlyContinue
                # ----------------------------------------------------------
                
                
                # ----------------------------------------------------------
                ## Assert functions: Validate that certain methods have (not) been called during the function-call
                
                ### Do not call: Write-Warning (CheckTableExistence disabled)
                # Should -Not -Invoke Write-Warning

                ### Do call: Write-Information with warning
                Should -Invoke New-AzStorageContext -Times 1
                Should -Invoke Get-AzStorageBlob -Times 1
                Should -Invoke Write-Information -Times 1 -ParameterFilter { $message -eq "Configuration for table table_name already exists" }


                ### Validate that no new configuration has been added to the configuration file
                $configuration_contents = Get-Content 'TestDrive:/user.json' -Raw
                $objectCount =  (($configuration_contents | ConvertFrom-Json).table_configurations).Count
                $objectCount | Should -Be 1
            }

        }
    }
}
</file>
<file name="src\powershell\modules\DataConfig\public\Wait-AvailableJobs.ps1">

function Wait-AvailableJobs {

    &lt;#
    .SYNOPSIS
    Given an array of PSLongRunningJobs, wait until at most $maxConcurrentJobs are still active

    .DESCRIPTION
    When executing long running jobs, it is good practice to not overload the system.
    This script introduces a "pause" on new jobs being introduced to the system.

    The user passes an array of PSLongRunningJobs, and the script will wait until less then $maxConrrurentJobs
    are still active (in state 'running' or 'waiting'). Whenever this is the case, an updated jobs-array will
    be returned containing all the still active jobs.

    Between every "ping" to the system, a wait time of StatusUpdateSeconds is introduced to not overload the system

    .OUTPUTS
    Return a list of active PSLongRunningJobs objects

    .PARAMETER StartDate
    ETC time of when the process started. The time will be offset against the local time

    #&gt;

    [CmdletBinding()]

    param(
        [Parameter(Mandatory=$true, HelpMessage="Maximum number of jobs that can run at the same time")]
        [int] $MaxConcurrentJobs,

        [Parameter(Mandatory=$false, HelpMessage="Array of PSLongRunningJob-objects")]
        [array] $Jobs = @(),
        
        [Parameter(Mandatory=$false, HelpMessage="ETC Time when the process began, for information purposes only")]
        [datetime] $StartDate = (Get-Date),

        [Parameter(Mandatory=$false, HelpMessage="Time to wait between status updates")]
        [int] $StatusUpdateSeconds = 30
    )

    # Local scripts settings
    Set-StrictMode -Version "latest"
    [timespan] $ToLocalTimeOffset  = (Get-TimeZone).BaseUtcOffset


#region
    # Get the list of PSLongRunningJobs that are still running or waiting
    if ($Jobs){
        $Jobs = [array] @($Jobs | Where-Object { $_.State -in ('running', 'waiting') })
        # If there are more/equal number of jobs than $MaxConcurrentJobs, wait for $StatusUpdateSeconds and check again
        while ($Jobs.Count -ge $MaxConcurrentJobs) {
            # Write user information
            $duration = New-Timespan –Start $StartDate.Add($ToLocalTimeOffset).ToString('HH:mm:ss') –End (Get-Date).Add($ToLocalTimeOffset).ToString('HH:mm:ss')
            Write-Information "  In Progress... running time: $($duration.ToString('hh\:mm\:ss')) | $($MaxConcurrentJobs) jobs running, next check in $($StatusUpdateSeconds) seconds..."    
            
            # Wait for $StatusUpdateSeconds
            Start-Sleep -Seconds $StatusUpdateSeconds
                
            # Get the list of PSLongRunningJobs that are still running or waiting
            $Jobs = [array]  @($Jobs | Where-Object { $_.State -in ('running', 'waiting') })
        }
        Write-Information "Jobs in progress: $($Jobs.Count)/$MaxConcurrentJobs | $($MaxConcurrentJobs - $Jobs.Count) slot(s) available"    
    }
#endregion

    return $Jobs
}
</file>
<file name="src\powershell\modules\DataConfig\public\Wait-AvailableJobs.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}
Describe "Wait-AvailableJobs" {
    Context "UnitTests" {
        # Mocking the Get-Date cmdlet to control the current time during tests
        BeforeAll{
            Mock Get-Date { return [datetime]::Parse("2023-10-01 12:00:00") }
        }
        It "Should return an null when an null is passed" {

            # Call the function under test
            $result = Wait-AvailableJobs -maxConcurrentJobs 5 -Jobs $null

            # Expect the return value to be empty
            $result | Should -Be $null
        }

        It "Should return an empty array when an empty array is passed" {

            # Call the function under test 
            # Force an array to be returned
            $result = @(Wait-AvailableJobs -maxConcurrentJobs 5 -Jobs @())

            # Expect the returned array to be empty
            $result.Count | Should -Be 0
        }

        It "Should return only the Running jobs" {
            # Define function parameters
            $jobs = @(
                [PSCustomObject]@{ State = 'running' },
                [PSCustomObject]@{ State = 'completed' }
            )

            # Call the function under test
            $result = [array] ((Wait-AvailableJobs -Jobs $jobs -maxConcurrentJobs 5) ?? @())
            
            # Expect only one item to be returned with state = Running
            $result.Count | Should -Be 1
            $result[0].State | Should -Be 'running'
        }

        It "Should wait until the number of running jobs is less than maxConcurrentJobs" {
            # Define function parameters
            # Define a list of initial jobs
            $job1 = [PsCustomObject] @{State = "running"}
            $job2 = [PsCustomObject] @{State = "running"}
            $job3 = [PsCustomObject] @{State = "running"}
            $jobs = @($job1, $job2, $job3)

            # Mock the Start-Sleep function to simulate time passing without actually sleeping
            # Use the Start-Sleep mock to change the state of 2 of the running jobs to "COMPLETED"
            Mock -CommandName Start-Sleep -MockWith {
                # Simulate changing the state of 2 jobs to 'Completed'
                $job1.State = 'rompleted'
                $job2.State = 'rompleted'
                # Leave job3 as 'running'
            }

            # Call the function under test
            $remainingJobs = [array] @(Wait-AvailableJobs -Jobs $jobs -maxConcurrentJobs 2 -StatusUpdateSeconds 1)

            # Assert: Check that only 1 job remains in the jobs array
            $remainingJobs.Count | Should -Be 1
            $remainingJobs[0].State | Should -Be 'running'
        }

        It "Should return an empty array if all jobs are completed" {
            # Define function parameters
            $jobs = @(
                [PSCustomObject]@{ State = 'completed' },
                [PSCustomObject]@{ State = 'completed' }
            )

            # Call the function under test
            $result = [array] ((Wait-AvailableJobs -Jobs $jobs -maxConcurrentJobs 1) ?? @())
            
            # If all jobs are complete, expect the result to be an empty array
            $result.Count | Should -Be 0
        }
    }
}
</file>
<file name="src\powershell\modules\DataConfig\schemas\DeltaTableData.schema.json">
{
    "$schema": "http://json-schema.org/draft-07/schema#",
    "type": "object",
    "title": "Ingestion configuration files schema",
    "description": "Schema for Keyrus ingestion framework configuration files",
    "required": [],
    "optional": [
      "table_configurations"
    ],
    "additionalProperties": false,
    "properties": {
        "table_configurations": {
            "type": "array",
            "description": "array of delta tables to deploy",
            "items": {
                "type": "object",
                "description": "An explanation about the purpose of this instance.",
                "required": [
                    "table_description",
                    "table_name",
                    "column_info",
                    "storage_account",
                    "container_name"
                ],
                "optional": [
                    "target_options"
                ],
                "properties": {
                    "table_description": {
                    "type": "string",
                    "description": "Description of the delta table in the Synapse workspace"
                    },
                    "table_name": {
                    "type": "string",
                    "description": "Name of te delta table to deploy to the Synapse workspace"
                    },
                    "column_info": {
                    "type": "string",
                    "description": "Json object containing the names of the columns and their data types"
                    },
                    "storage_account": {
                    "type": "string",
                    "description": "Name of the storage account where the silver database is stored",
                    "enum": ["devdapstdala1", "intdapstdala1", "tstdapstdala1", "accdapstdala1", "prddapstdala1"]
                    },
                    "container_name": {
                    "type": "string",
                    "description": "Name of the lake database where to deploy the delta table to",
                    "enum": ["silver"]
                    },
                    "target_options": {
                        "type": "object",
                        "description": "Additional options for the target table",
                        "additionalItems": false,
                        "items": {
                            "type": "object",
                            "required": [
                            ],
                            "optional": [
                            "partitioning"
                            ],
                            "properties": {
                                "partitioning": {
                                    "type": "array",
                                    "description": "Properties used during the creation of the target_table deployment",
                                    "additionalProperties": false,
                                    "items": {
                                        "type": "object",
                                        "required": [ 
                                            "name",
                                            "sequence"
                                        ],
                                        "optional": [
                                            "datePart"
                                        ],
                                        "properties": {
                                            "name": {
                                            "type": "string",
                                            "description": "Name of the column on which to deploy the partition"
                                            },
                                            "sequence": {
                                            "type": "integer",
                                            "description": "Sequence number of the partition for this column"
                                            },
                                            "datePart": {
                                            "type": "string",
                                            "description": "Part of the date that needs to be partitioned on",
                                            "enum": ["year", "month", "day"]
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
             }
        }
    }
}
</file>
<file name="src\powershell\modules\DataConfig\schemas\IngestionMetadata.schema.json">
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "type": "object",
  "title": "Ingestion configuration files schema",
  "description": "Schema for Keyrus ingestion framework configuration files",
  "required": [],
  "optional": [
    "plans", 
    "datasets",
    "table_configurations"
],
  "additionalProperties": false,
  "properties": {
    "plans": {
      "type": "array",
      "description": "array of plan definitions resources, with each one or more task_groups",
      "additionalItems": false,
      "items": {
        "type": "object",
        "description": "An explanation about the purpose of this instance.",
        "required": [
            "name"
        ],
        "optional": [
            "description", 
            "environments", 
            "enabled", 
            "task_groups"
        ],
        "properties": {
          "name": {
            "type": "string",
            "description": "name of the plan"
          },
          "description": {
            "type": "string",
            "description": "freetext description of what tasks this plan will run"
          },
          "environments": {
            "type": "string",
            "description": "Optional regex filter which will be used to evaluate which environments this plan should be deployed too. Omit for 'any' (.*)",
            "default": ".*"
          },
          "enabled": {
            "type": "boolean",
            "description": "Enable this plan, or not",
            "default": true
          },
          "task_groups": {
            "type": "array",
            "description": "array of task_group definitions, with each one or more tasks",
            "additionalItems": false,
            "items": {
              "type": "object",
              "required": [
                "name"
            ],
              "optional": [
                "tasks"
            ],
              "properties": {
                "name": {
                  "type": "string",
                  "description": "name of this task group",
                  "enum": ["PREPROCESS","INGEST", "DUMMY"] 
                },
                "tasks": {
                  "type": "array",
                  "description": "array of tasks which will be executed (in parallel and which no interdependencies possible) in this task group",
                  "additionalItems": false,
                  "items": {
                    "type": "object",
                    "required": [
                        "name"
                    ],
                    "optional": [
                        "sequence", 
                        "enabled"
                    ],
                    "properties": {
                      "name": {
                        "type": "string",
                        "description": "name of this task"
                      },
                      "sequence": {
                        "type": "integer",
                        "description": "Sequence number, optional and only used managing/representation",
                        "default": 1
                      },
                      "enabled": {
                        "type": "boolean",
                        "description": "Enable this task, or not",
                        "default": true
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "datasets": {
      "type": "array",
      "description": "array of plan definitions resources, with each one or more task_groups",
      "items": {
        "type": "object",
        "description": "An explanation about the purpose of this instance.",
        "required": [
            "name", 
            "kind",
            "task_type",
            "worker"
        ],
        "optional": [
          "description",
          "ingestion", 
          "checks",
          "columns"
        ],
        "additionalProperties": false,
        "properties": {
          "name": {
            "type": "string",
            "description": "name of the dataset, used for creating the tasks and other main identifiers"
          },
          "description": {
            "type": "string",
            "description": "freetext description of this dataset"
          },
          "kind": {
            "type": "string",
            "description": "description of the dataset's file type",
            "enum":  ["csv", "json", "parquet", "zip"] 
          },
          "task_type": {
            "type": "string",
            "description": "Describes which tool/service will be used to execute the task",
            "enum": ["SPARK_NOTEBOOK", "SYNAPSE_PIPELINE"]
          },
          "worker": {
            "type": "string",
            "description": "Depending on the used service, diffeent workers exist to execute the specific task",
            "enum": ["IngestionWorker", "DummyWorker", "FilWorker", "pl_unzip_worker", "pl_dummy_worker"]
          },
          "ingestion": {
            "type": "object",
            "description": "Properties used during the 'ingestion' phase from landing area to ODS",
            "additionalProperties": false,
            "required": [
              "target_table",
              "source_folder",
              "match_pattern",
              "extension"
            ],
            "optional": [
              "target_options",
              "container",
              "encoding",
              "column_delimiter",
              "row_delimiter",
              "escape_character",
              "quote_character",
              "header_line",
              "skip_first_lines",
              "preconditions"
            ],
            "properties": {
              "target_table": {
                "type": "string",
                "description": "Synapse table on where to drop the to-be-ingested files"
              },
              "target_options":{
                "type": "object",
                "description": "Properties used during the creation of the target_table deployment",
                "additionalProperties": false,
                "required": [
                ],
                "optional": [
                  "partitioning",
                  "extract_date",
                  "retention_time"
                ],
                "properties": {
                  "extract_date": {
                    "type": "object",
                    "description": "Properties used to define technical field 't_extract_date'",
                    "additionalProperties": false,
                    "required": [ 
                      "column_name"
                    ],
                    "optional": [
                      "regex_expression",
                      "extract_date_format"
                    ],
                    "properties": {
                      "column_name": {
                        "type": "string",
                        "description": "Name of the column to use to extract the timestamp"
                      },
                      "regex_expression": {
                        "type": "string",
                        "description": "Regex expression that will allow to extract the timestamp from the given column"
                      },
                      "extract_date_format": {
                        "type": "string",
                        "description": "Format of the timestamp"
                      }
                    }
                  },
                  "partitioning": {
                    "type": "array",
                    "description": "Properties used during the creation of the target_table deployment",
                    "additionalProperties": false,
                    "items": {
                      "type": "object",
                      "required": [ 
                        "name",
                        "sequence"
                      ],
                      "optional": [
                        "datePart"
                      ],
                      "properties": {
                        "name": {
                          "type": "string",
                          "description": "Name of the column on which to deploy the partition"
                        },
                        "sequence": {
                          "type": "integer",
                          "description": "Sequence number of the partition for this column"
                        },
                        "datePart": {
                          "type": "string",
                          "description": "Part of the date that needs to be partitioned on",
                          "enum": ["year", "month", "day"]
                        }
                      }
                    }
                  },
                  "retention_time":{
                    "type":"integer",
                    "description": "Properties used during the vacuum process to determine how long the data should be retained"
                  }
                }
              },
              "source_folder": {
                "type": "string",
                "description": "Folder relative to the ftpRoot folder set in the environment_configuration, but without a leading slash, eg 'myfolder/'"
              },
              "match_pattern": {
                "type": "string",
                "description": "A simple 'contains' style of match pattern, no wildcards, eg 'some_file' would match 'xxxx_some_file_123.csv'"
              },
              "extension": {
                "type": "string",
                "description": "Extension excluding the dot, eg 'csv'",
                "enum": ["csv", "json", "txt", "parquet", "gz", "zip", "fil"]
              },
              "container": {
                "type": "string",
                "description": "Azure container where the dataset is expected to be found",
                "default": "landing"
              },
              "column_delimiter": {
                "type": "string",
                "maxLength": 1,
                "description": "A single character with the column_delimiter, or empty for 'none'. IMPORTANT: because of a limitation in Azure Data Factory, we'll use a tab character because 'empty' column_delimiter is not supported.",
                "default": ","
              },
              "row_delimiter": {
                "type": "string",
                "maxLength": 5,
                "description": "A character with the row_delimiter, or empty for 'none'. IMPORTANT: because of a limitation in Azure Data Factory, we'll use a tab character because 'empty' row_delimiter is not supported.",
                "default": ""
              },
              "escape_character": {
                "type": "string",
                "maxLength": 1,
                "description": "Optionally provide the escape character, or empty for 'none' (default)",
                "default": ""
              },
              "quote_character": {
                "type": "string",
                "maxLength": 1,
                "description": "Optionally provide the quote character, or empty for 'none' (default)",
                "default": ""
              },
              "header_line": {
                "type": "boolean",
                "description": "Optionally mark if the file has a header line with the columns names (default), or not",
                "default": true
              },
              "encoding": {
                "type": "string",
                "description": "",
                "enum": ["UTF-8", "ANSI"],
                "default": "UTF-8"
              },
              "skip_first_lines": {
                "type": "integer",
                "description": "Total number of lines that need to be skipped at the beginning of the source file",
                "default": 0
              }
              , "preconditions": {
                "type": "array",
                "description": "preconditions to allow the task to start processing",
                "items": {
                  "type": "object",
                  "description": "The allowed precondition configuration items.",
                  "required": [
                      "name", 
                      "type",
                      "frequency"
                    ],
                    "optional": [
                      "expected_file_mask",
                      "enabled",
                      "description"
                  ],
                  "properties": {
                      "name" : {
                            "type": "string"
                          , "description": "Descriptive name of the validation to perform (free text)"
                      }
                    , "type" : {
                          "type": "string"
                        , "description": "verification type currently only and default file_exists"
                        , "enum": ["file_exists"]
                      }
                    , "enabled" : {
                        "type":"boolean"
                      , "description": "perform the check? (default= true)"
                      , "default": true
                      }
                    , "description" : {
                          "type":"string"
                        , "description": "The description to this check (in addition to the name)"
                      }
                    , "expected_file_mask" : {
                        "type":"string"
                      , "description": "The filemask that must be found in the found file. The expected context is deduced from the last loaded file"
                      } 
                    , "frequency" : {
                        "type":"string"
                      , "description": "Frequency expections between previous and current file. Will influence the definition of the expected file mask. for file_Exists it "
                      , "enum": ["daily","hourly"]
                      , "default" : "daily"
                      }
                  }
                }
              }
            }
          },
          "checks": {
            "type": "array",
            "description": "Checks that need to be executed for the dataset",
            "additionalItems": false,
            "items": {
              "type": "object",
              "required": [
                "name"
            ],
              "optional": [
                "enabled",
                "config_params"
            ],
              "properties": {
                "name": {
                  "type": "string",
                  "description": "Name of the check that needs to be executed",
                  "enum": ["header", "default_replace", "primary_keys", "data_type", "landing_rows","duplicate_primary_keys","replace_primary_key"]
                },
                "enabled": {
                  "type": "boolean",
                  "description": "Is the check enabled or disabled?",
                  "default": true
                },
                "config_params": {
                  "type": "string",
                  "description": "Flat json object, saved as a string"
                }
              }
            }
          },
          "columns": {
            "type": "array",
            "description": "Column metadata to control ingestion process",
            "additionalItems": false,
            "items": {
              "type": "object",
              "required": [
                "sequence", 
                "name"
              ],
              "optional": [
                "dimension", 
                "sink_name", 
                "data_type",
                "column_info"
              ],
              "properties": {
                "sequence": {
                  "type": "integer",
                  "description": "Sequence number of the column"
                },
                "name": {
                  "type": "string",
                  "description": "Name of the column as it appears in the source (ignored for dataset with kind set as 'fixedlength_file')"
                },
                "dimension": {
                  "type": "string",
                  "description": "Describes the dimension of the column",
                  "enum": ["PK", "SCD2"],
                  "default": "SCD2"
                },
                "sink_name": {
                  "type": "string",
                  "description": "If the source name needs to be changed, the sink name specifies that new column name"
                },
                "data_type": {
                  "type": "string",
                  "description": "The expected data type of the column",
                  "oneOf": [
                      { "pattern": "^array$"}, 
                      { "pattern": "^binary$"},
                      { "pattern": "^boolean$"},
                      { "pattern": "^date$"},
                      { "pattern": "^string$"},
                      { "pattern": "^timestamp$"},
                      { "pattern": "^float$"},
                      { "pattern": "^byte$"},
                      { "pattern": "^integer$"},
                      { "pattern": "^long_integer$"},
                      { "pattern": "^decimal"},
                      { "pattern": "^varchar"}
                  ],
                  "default": "string"
                },
                "column_info": {
                  "type": "object",
                  "description": "Column metadata to control ingestion process",
                  "additionalItems": false,
                  "items": {
                    "type": "object",
                    "required": [
                    ],
                    "optional": [
                      "locale",
                      "format",
                      "thousand_sepatator",
                      "decimal_separator",
                      "optional"
                      ,"replace_value"
                    ],
                    "properties": {
                      "locale":{
                        "type": "string",
                        "description": "indication of the locale setting of the file (en-US, fr-FR, etc)",
                        "default": "en-US"
                      },
                      "format": {
                        "type": "string",
                        "description": "format of a date or timestamp (yyyy-MM-dd, dd-MM-yyyy, etc)",
                        "default": "yyyy-MM-dd"
                      },
                      "thousand_sepatator":{
                        "type": "string",
                        "description": "Separator value used for numbers in between thousands",
                        "enum": [",", "."],
                        "default": ","
                      },
                      "decimal_separator": {
                        "type": "string",
                        "description": "Separator value used for decimal numbers",
                        "enum": [",", "."]
                      },
                      "optional": {
                        "type": "boolean",
                        "description": "Boolean value indicating if a column is optional"
                      },
                      "replace_value": {
                        "type": "string",
                        "description": "The name that will be used to replace a null value in a primary key"
                      }
                    }
                  }
                }
              }
            }
          }
        }
      }
    },
    "preprocess": {
      "type": "array",
      "description": "array of preprocess definition resources",
      "items": {
        "type": "object",
        "required": [
          "name", 
          "task_type",
          "worker_pipeline",
          "source_pattern"
        ],
        "optional": [
          "description",
          "source_container",
          "source_folder",
          "worker_properties"
        ],
        "additionalProperties": false,
        "properties": {
          "name": {
            "type": "string",
            "description": "name of the preprocessing task, used for creating the tasks and other main identifiers"
          },
          "description": {
            "type": "string",
            "description": "freetext description of this task"
          },
          "task_type": {
            "type": "string",
            "description": "Type of the worker for this task, should be 'SYNAPSE_PIPELINE' for this type of task",
            "enum": ["SYNAPSE_PIPELINE"] 
          },
          "worker_pipeline": {
            "type": "string",
            "description": "Name of the synapse pipeline to run for exucuting the preprocessing task"
          },
          "source_pattern": {
            "type": "string",
            "description": "A simple 'contains' style of match pattern, no wildcards, eg 'some_file' would match 'xxxx_some_file_123.csv'"
          },
          "source_container":{
            "type": "string",
            "description": "Name of the container where the source files are located, if not not specified then 'preprocess' is used"
          },
          "source_folder": {
            "type": "string",
            "description": "Optional: path relative to the source_countainer to filter for source files, does not support wildcards. If not provided then only match_pattern is used for filtering."
          },
          "worker_properties": {
            "type": "object",
            "description": "Properties used for the preprocessing task",
            "additionalProperties": true,
            "required": [],
            "optional": [],
            "properties": {
            }
          }
        }
      }
    }
  }
}

</file>
<file name="src\powershell\modules\Networking\.build\build.ps1">
[CmdletBinding()]
param (
    [Parameter(HelpMessage = "Name of the module")]
    [String] $ModuleName = "Networking",

    [Parameter(HelpMessage = "Version number in Major.Minor.Path format")]
    [String] $BuildVersion,

    [Parameter(HelpMessage = "Base directory with the module folder")]
    [String] $Workingdir,   

    [Parameter(HelpMessage = "Directory for the localFeed PSRepository")]
    [String] $LocalFeedPath,   

    [Parameter(HelpMessage = "Set this switch to enable cleaning the module folder before packaging")]
    [Switch] $CleanFolder

)

#local settings
Set-StrictMode -Version "latest"
#Requires -Version 7.2

#region script info
$inputParameters = $MyInvocation.MyCommand.Parameters.Keys | Where-Object { $_ -notin [System.Management.Automation.PSCmdlet]::CommonParameters}
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "----------------------------------------------------------------------"
Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
Write-Information "  Parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}
Write-Information "PSVersion: $($PSVersiontable.PSVersion.ToString())"
Write-Information "----------------------------------------------------------------------"
#endregion script info

Write-Output ""
Write-Output "Package providers:"
Get-PackageProvider
Write-Output "----------------------------------------------------------------------"


#unload the module if currently loaded
$currentModules = Get-Module -Name $ModuleName -ErrorAction "Ignore"
if( $currentModules) {
    Write-Output "Unloading currently loaded '$ModuleName' modules ..." 
    $currentModules.Version | ForEach-Object { Write-Information "  $_"}
    $currentModules | Remove-Module | Out-Null
} else {
    Write-Output "No currently loaded '$ModuleName' modules (OK)" 
}
#uninstall the package if currently installed
$currentPackages = Get-Package -Name $ModuleName -ErrorAction "Ignore"
if( $currentPackages) {
    Write-Output "Uninstalling currently installed packages ..."
    $currentPackages.Version | ForEach-Object { Write-Information "  $_"}
    $currentPackages | Uninstall-Package | Out-Null
} else {
    Write-Output "No currently installed '$ModuleName' packages (OK)" 
}

#build and check path to the module folder
$modulePath = Join-Path $Workingdir $ModuleName
Write-Information ""
Write-Information "Using modulePath: $modulePath"
if ( -Not (Test-Path $modulePath -PathType "Container") ) {
    Throw "Module '$ModuleName' is not a subfolder of '$WorkingDir'"
} else {
    Write-Information "  modulePath: OK, folder exists"
    $modulePath = (Resolve-Path $modulePath).Path
    Write-Information "  resolved to: $modulePath"
}

#build and check path to the module manifest
$psdFile = $ModuleName + ".psd1"
Write-Information "Using psdFile: $psdFile"
$manifestPath = Join-Path $modulePath $psdFile
Write-Information "Using manifestPath: $manifestPath"
if (-not (test-Path $manifestPath) ) {
    Throw "Manifest does not exist at: $manifestPath"
}

#region Prepare localFeed PSRepository

    Write-Output "Preparing localFeed PSRepository ..."

    #check and resolve localFeedPath
    if (-not (Test-Path $localFeedPath)) {
        New-Item -Path $localFeedPath -ItemType "Directory" | Out-Null
    }
    $LocalFeedPathResolved = ($LocalFeedPath | Resolve-Path -ErrorAction Ignore)
    if ( -not $LocalFeedPathResolved ) {
        Throw "Could not resolve LocalFeedPath: '$LocalFeedPath'"
    } else {
        $LocalFeedPathResolved = $LocalFeedPathResolved.Path
        Write-Information "  Resolved LocalFeedPath to: $LocalFeedPathResolved"
    }

    #check or create "localFeed" PSRepository
    $localFeed = Get-PSRepository -Name localFeed -ErrorAction "Ignore"
    if ( $localFeed ) {
        Write-Output "  Found localFeed PSRepository"
        # $localFeed 
        # $localFeed.ScriptSourceLocation
        # $localFeed.ScriptPublishLocation
        # Write-Output "(members)"
        # $localFeed | Get-Member
    } else {
        Write-Output "  localFeed PSRepository not found"
    }

    if ( $localFeed -and ($localFeed.SourceLocation -ne $LocalFeedPathResolved) ) {
        Write-Warning "  localFeed repository has wrong path ($($localFeed.SourceLocation)), removing it ..."
        #Write-Output "Unregister existing localFeed repository ..."
        Unregister-PSRepository -Name "localFeed"
        $localFeed = $null
    }
    if ( -not $localFeed) {
        Write-Output " Registering 'localFeed' PSRepository with path: $LocalFeedPathResolved ..."
        Register-PSRepository -name localFeed -sourceLocation $LocalFeedPathResolved -publishLocation $LocalFeedPathResolved -InstallationPolicy Trusted
    } else {
        Write-Output "  PSRepository 'localFeed' already present with path: $LocalFeedPathResolved"
    }

# Remove existing package with current version (main to avoid conflict when (re)building version 0.0.0 locally)
$existingPackages = Get-ChildItem -Path $LocalFeedPathResolved -Filter "${ModuleName}.${BuildVersion}.nupkg"
if( -not $existingPackages) {
    Write-Information "  Deleting existing package files : none found [OK]"
} else {
    Write-Information "  Deleting existing package files ..."
    $existingPackages.FullName | ForEach-Object { Write-Information "    $_"}
    $existingPackages  | Remove-Item | Out-Null
}

#endregion

#region create the manifest

    Write-Output ""
    Write-Output "Prepare the module and manifest..."

    # remove the files not required for the nuget package
    if ( $CleanFolder) {                
        Write-Information "  Remove unneeded files for nuget package..."
        Get-ChildItem -Path $modulePath -Include "*.Tests.ps1" -Recurse | Remove-Item
        Get-ChildItem -Path $modulePath -Filter ".build" -Directory | Remove-Item -Recurse
        Get-ChildItem -Path $modulePath -Filter ".test_config" -Directory | Remove-Item -Recurse
        Get-ChildItem -Path $modulePath -Filter "coverage.xml" | Remove-Item
        Get-ChildItem -Path $modulePath -Filter "testResults_unit.xml" | Remove-Item
    } else {
        Write-Information "  (...skipping CleanFolder section...)"
    }

    # Update build version in manifest
    Write-Information "  Updating version number in manifest..."
    $manifestContent = Get-Content -Path $manifestPath -Raw
    $manifestContent = $manifestContent -replace '&lt;ModuleVersion&gt;', $BuildVersion

    # Find all of the public functions and add them as cmdlets
    $publicFuncFolderPath = Join-Path $modulePath public
    $itemParams = @{
        Path = $publicFuncFolderPath
        File = $true
        Recurse = $true
        Filter = '*.ps1'
        Exclude = '*.Tests.ps1'
    }
    if ((Test-Path -Path $publicFuncFolderPath) -and ($publicFunctionNames = Get-ChildItem @itemParams | Select-Object -ExpandProperty BaseName)) {
        $funcStrings = "'$($publicFunctionNames -join "',$([System.Environment]::NewLine)  '")'"
        $funcStrings = $([System.Environment]::NewLine) + "  " + $funcStrings + $([System.Environment]::NewLine)
    } else {
        $funcStrings = $null
    }
    ## Add all public functions to FunctionsToExport attribute
    $manifestContent = $manifestContent -replace "'&lt;FunctionsToExport&gt;'", $funcStrings

    Write-Information "  "
    Write-Information "  functions to export:"
    $funcStrings | ForEach-Object { Write-output "    $_" }

    ## save updated manifest
    $manifestContent | Set-Content -Path "$manifestPath"
    Write-Information "  Updated manifest: $manifestPath"

#endregion

# create nuget package
Write-Output ""
Write-Output "Create and publish nuget package to localFeed repository..."
Publish-Module -path $modulePath -Repository localFeed -NuGetApiKey 'dummy' -Force

# list available packages
Write-Output ""
Write-Output "List available packages in localFeed repository..."
#Find-Package -Source "localFeed" |  Select-Object Version, ProviderName, Source, FullPath
Find-Package -Source "localFeed" |  ForEach-Object {Write-Output "  $($_.Version) -- $($_.ProviderName)"}

# install the package
Write-Output ""
Write-Output "Install the module from localFeed..."
Install-Package -Name $ModuleName -Source "localFeed" -ProviderName "PowerShellGet" -Force |  ForEach-Object {Write-Output "  $($_.Version)"}

# review installed modules
$modules = Get-Module -Name $ModuleName -ListAvailable | Select-Object Name, Version, RepositorySourceLocation, Path
Write-Output ""
Write-Output "Available module versions:"
$modules |  ForEach-Object {Write-Output "  $($_.Version)"}

# check that the "current" version is there
Write-Output ""
Write-Output "Check that current version is available..."
$currentModule = $modules | Where-Object {$_.Version.ToString() -eq $BuildVersion }
if ( -Not $currentModule) {
    Throw "Module with version '$BuildVersion' not found in available modules"
} else {
    Write-Information "  Found available module '$ModuleName' with version '$BuildVersion' (OK)"
}
 
# import the module
Write-Output ""
Write-Output "Import the module ..."
Import-Module -Name $ModuleName
Get-Module -Name $ModuleName |  ForEach-Object {Write-Output "  $($_.Version)"}


</file>
<file name="src\powershell\modules\Networking\.build\unit_tests.ps1">

[CmdletBinding()]
param (

)

# --------------------------------------------------------------
# SECTION: Prerequisites
# --------------------------------------------------------------

#requires -Modules Az.KeyVault
#requires -Modules Az.Accounts
#requires -Modules Az.Resources
#requires -Modules Az.Synapse
#requires -Modules @{ ModuleName="Pester"; ModuleVersion="5.5.0" }

# install pester if needed  
# note Joachim: this should actually be done upfront, so we can simple use a 'requires' statement in this script
# Write-Output "Available pester versions:"
# $pesterRequiredVersion = "5.5"
# $pesterCheckRegex = "^" + $pesterRequiredVersion.Replace(".", "\.")
# (Get-Module Pester -ListAvailable).Path | ForEach-Object {"  $_"}
# if(-not (Get-Module Pester -ListAvailable | where-Object Version -match $pesterCheckRegex)){
#     Write-Output "Installing Pester $pesterRequiredVersion ..."
#     Install-Module -Name Pester -RequiredVersion $pesterRequiredVersion -Force -Scope CurrentUser -AllowClobber
# } else {
#     Write-Output "Pester 5.5 already installed"
# }

# load the correct one
# Import-Module Pester -MinimumVersion $pesterRequiredVersion -MaximumVersion "$pesterRequiredVersion.999999"
# just to be sure
Write-Output ""
Write-Output "Using pester version: $((Get-Module Pester).Version)"

Write-Output ""
Write-Output "Module to test:"
$modulePath = resolve-path ( Join-Path "$PSScriptRoot" "..")
$moduleName = (Split-Path $modulePath -Leaf)  + '.psm1'
Write-Output "  modulePath: $modulePath"
Write-Output "  moduleName: $moduleName"

# preload the module a first time
Import-Module -Name (Join-Path  $modulePath $moduleName ) -Force


# --------------------------------------------------------------
# SECTION: Pester configuration
# --------------------------------------------------------------

# start with default configuration
$pesterConfig = [PesterConfiguration]::Default

# create testResults.xml output
$pesterConfig.TestResult.Enabled = $true
$pesterConfig.TestResult.OutputFormat = "NUnitXml"
$pesterConfig.TestResult.OutputPath = Join-Path $modulePath "testResults_unit.xml"

# create code coverage output
$pesterConfig.CodeCoverage.Enabled = $True
$pesterConfig.CodeCoverage.OutputFormat = "JaCoCo"
$pesterConfig.CodeCoverage.OutputPath = Join-Path $modulePath "coverage.xml"
$pesterConfig.CodeCoverage.Path = "$modulePath/[p]*/*.ps1"  #  public, private

# filters (the fullname filter is a powershell "LIKE" statement, not regex)
# important: the FullName and Tag filters work are combined as "OR", not as "AND"
$pesterConfig.Filter.FullName = "*.UnitTests.*"
$pesterConfig.Filter.Tag = 'UnitTest'
$pesterConfig.Filter.ExcludeTag = "Draft"

# run parameters
$pesterConfig.Run.Path = Join-Path $modulePath "public" 
$pesterConfig.Run.Exit = $False
$pesterConfig.Run.PassThru = $False

# --------------------------------------------------------------
# SECTION: Run tests
# --------------------------------------------------------------
Write-Output "Run tests..."

Invoke-Pester -Configuration $pesterConfig
</file>
<file name="src\powershell\modules\Networking\public\Remove-NetworkingSqlServerFirewallRule.ps1">
&lt;#
.SYNOPSIS
Wrapper around Az.Sql functions Remove-AzSqlServerFirewallRule

.DESCRIPTION
This function add some functionalities to removing a Firewall rule for a SQL Server in a specific Resource Group
    * Create a list of existing firewall rules
    * Only Remove a firewall rule when it exists -&gt; Avoid throwing an error
   
.EXAMPLE
    PS&gt; Remove-NetworkingSqlServerFirewallRule -ServerName "myserver" ResourceGroupName "my-rg" -FirewallRuleName "rule0"

#&gt;
function Remove-NetworkingSqlServerFirewallRule {

    [CmdletBinding()]
    param (
        [Parameter( Mandatory = $true, HelpMessage="Name of the Resource Group where the SQL Server is located")]
        [ValidateNotNullOrEmpty()]
        [String] $ResourceGroupName,
 
        [Parameter( Mandatory = $true, HelpMessage="Name of the SQL Server (xxx), not the full host server (xxx.database.windows.net)")]
        [ValidateNotNullOrEmpty()]
        [String] $ServerName,

        [Parameter( Mandatory = $true, HelpMessage="Reference name of the to-be removed firewall rule")]
        [ValidateNotNullOrEmpty()]
        [String] $FirewallRuleName
   
    )

    ## START REGION: List script name and parameters
    $scriptName = $script:MyInvocation.MyCommand.Path
    Write-Output "*** Starting script: $scriptName ***"

    #region print parameters
    $inputParameters = @( 
        "ResourceGroupName"
        "ServerName"
        "FirewallRuleName"
    )

    # $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)   
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
    foreach ($param in $inputParameters) {
        Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
    }
    #ENDREGION


    # Get list of existing firewall rules for server
    $existingRules = @( Get-AzSqlServerFirewallRule `
                        -ResourceGroupName $ResourceGroupName `
                        -ServerName $ServerName `
                        -WhatIf:$false # Completeness, not necessary to add this parameter
                        )

    # Throw warning and continue when no firewall rules are found 
    if (-Not $existingRules) {
        Write-Information "Found no existing firewall rules"
        # rule doesn't exist, throw warning and continue
        Write-Warning "No rule found with name '$FirewallRuleName'"
    } 
    
    # If firewall rules exist for the server
    else {
        Write-Information "Found $($existingRules.Count) existing firewall rule(s)"
        $existingRules.FirewallRuleName | Write-Verbose
        # Filter: Get list of existing fireall rules with name $FirewallRuleName
        $existingRule = $existingRules | Where-Object {$_.FirewallRuleName -eq $FirewallRuleName}
        
        # If rule exists
        if( $existingRule) {
            # rule exists, remove and surpress output
            $null = Remove-AzSqlServerFirewallRule  `
                -ResourceGroupName $ResourceGroupName `
                -ServerName $ServerName `
                -FirewallRuleName $FirewallRuleName `
                -Force
        } 
        # If rule does not exist
        else {
            # rule doesn't exist, throw warning and continue
            Write-Warning "No rule found with name '$FirewallRuleName'"
        }
    }

}
</file>
<file name="src\powershell\modules\Networking\public\Remove-NetworkingSqlServerFirewallRule.Tests.ps1">
BeforeAll {
    # load function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # set path to files with test assignments
    $script:TestFilePath = Join-Path $PSScriptRoot "../.test_config"

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest

}

Describe "Remove-NetworkingSqlServerFirewallRule" {
    
    # General context expected by the Pester Module
    Context "UnitTests" {

        # Group: There are multiple mocked firewall rules
        Context "Multiple existing rules" {
            # Test the result when there are multiple existing firewall rules

            BeforeAll {
                # Mock function from Az.Sql module
                mock Remove-AzSqlServerFirewallRule {}

                #  Mock function from Az.Sql module
                # If function Get-AzSqlServerFirewallRule gets called, return the 2 objects defined below
                mock Get-AzSqlServerFirewallRule {

                    $rule1 = New-Object -TypeName "Microsoft.Azure.Commands.Sql.FirewallRule.Model.AzureSqlServerFirewallRuleModel" 
                    $rule1.FirewallRuleName = "testrule1"
                    $rule1.StartIpAddress = "startIp"
                    $rule1.EndIpAddress = "endIp" 

                    $rule2 = New-Object -TypeName "Microsoft.Azure.Commands.Sql.FirewallRule.Model.AzureSqlServerFirewallRuleModel" 
                    $rule2.FirewallRuleName = "testrule2"
                    $rule2.StartIpAddress = "startIp"
                    $rule2.EndIpAddress = "endIp" 

                    return @( $rule1, $rule2 )
                }   
                
            }
            
            It "test existing rule"  { 

                # When the FirewallRuleName exists, the function Remove-AzSqlServerFirewallRule should be called exactly once
                Remove-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule1" `
                    -InformationAction $PesterInformationAction
                            
                Should -Invoke Remove-AzSqlServerFirewallRule -Times 1 -Exactly

            }

            It "test non-existing rule"  { 

                # When the FirewallRuleName not exist, the function Remove-AzSqlServerFirewallRule should not be called
                Remove-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "another rule" `
                    -InformationAction $PesterInformationAction
                            
                Should -Invoke Remove-AzSqlServerFirewallRule -Times 0 -Exactly
            }
        }

        # Group: Add one firewall rule as a mock
        Context "Single existing rule" {
            # Test the result when there is 1 existing firewall rule

            BeforeAll {

                # Mock function from Az.Sql module
                mock Remove-AzSqlServerFirewallRule {}

                #  Mock function from Az.Sql module
                # If function Get-AzSqlServerFirewallRule gets called, return the object defined below
                mock Get-AzSqlServerFirewallRule {
                    $rule1 = New-Object -TypeName "Microsoft.Azure.Commands.Sql.FirewallRule.Model.AzureSqlServerFirewallRuleModel" 
                    $rule1.FirewallRuleName = "testrule1"
                    $rule1.StartIpAddress = "startIp"
                    $rule1.EndIpAddress = "endIp" 

                    return $rule1
                
                }
            }
            
            It "test existing rule"  { 
                    
                # When the FirewallRuleName exists, the function Remove-AzSqlServerFirewallRule should be called exactly once
                Remove-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule1" `
                    -InformationAction $PesterInformationAction
                            
                Should -Invoke Remove-AzSqlServerFirewallRule -Times 1 -Exactly

            }

            It "test non-existing rule"  { 

                # When the FirewallRuleName not exist, the function Remove-AzSqlServerFirewallRule should not be called
                Remove-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "another rule" `
                    -InformationAction $PesterInformationAction
                            
                Should -Invoke Remove-AzSqlServerFirewallRule -Times 0 -Exactly
            }
        }    

        # Group: There are no existing firewall rules
        Context "No existing rules" {
            # Test the result when there are no existing firewall rules
            BeforeAll {

                # Mock function from Az.Sql module
                mock Remove-AzSqlServerFirewallRule {}

                #  Mock function from Az.Sql module
                # If function Get-AzSqlServerFirewallRule gets called, return an empty object-list
                mock Get-AzSqlServerFirewallRule {
                    
                    return $null
                
                }
            }
            
            It "test non-existing rule"  { 

                Remove-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "another rule" `
                    -InformationAction $PesterInformationAction
                            
                Should -Invoke Remove-AzSqlServerFirewallRule -Times 0 -Exactly
            }
            
        }
    }
}



</file>
<file name="src\powershell\modules\Networking\public\Set-NetworkingSqlServerFirewallRule.ps1">
&lt;#
.SYNOPSIS
Wrapper around New-AzSqlServerFirewallRule and Set-AzSqlServerFirewallRule to simplify "create if exists"

.DESCRIPTION
This function add some functionalities to creating a Firewall rule for a SQL Server in a specific Resource Group
    * Create a list of existing firewall rules
    * Create the requested firewall rule when it does not exist already
    * Update the requested firewall rule when it does already exist
   

.EXAMPLE
    PS&gt; Set-NetworkingSqlServerFirewallRule -ServerName "myserver" ResourceGroupName "my-rg" -RuleName "rule0" -StartIpAddress "10.0.0.0" -EndIpAddress "10.0.255.255"
    
#&gt;
function Set-NetworkingSqlServerFirewallRule {

    [CmdletBinding(SupportsShouldProcess)]
    param (
        [Parameter( Mandatory = $true, HelpMessage='Name of the Resource Group where the SQL Server is located')]
        [ValidateNotNullOrEmpty()]
        [String] $ResourceGroupName,
 
        [Parameter( Mandatory = $true, HelpMessage="Name of the SQL Server (xxx), not the full host server (xxx.database.windows.net)")]
        [ValidateNotNullOrEmpty()]
        [String] $ServerName,

        [Parameter( Mandatory = $true, HelpMessage="Reference name of the to-be created firewall rule")]
        [ValidateNotNullOrEmpty()]
        [String] $FirewallRuleName,

        [Parameter( Mandatory = $true, HelpMessage="Start of the IP-adress range that needs to be added (e.g. xxx.0.0.0)")]
        [ValidateNotNullOrEmpty()]
        [String] $StartIpAddress,

        [Parameter( Mandatory = $true,  HelpMessage="End of the IP-adress range that needs to be added (e.g. xxx.0.255.255)")]
        [ValidateNotNullOrEmpty()]
        [String] $EndIpAddress
   
    )

    ## START REGION: List script name and parameters
    $scriptName = $script:MyInvocation.MyCommand.Path
    Write-Output "*** Starting script: $scriptName ***"

    #region print parameters
    $inputParameters = @( 
        "ResourceGroupName"
        "ServerName"
        "FirewallRuleName"
        "StartIpAddress"
        "EndIpAddress"
    )

    # $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)   
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name) with parameters:"
    foreach ($param in $inputParameters) {
        Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
    }
    #ENDREGION

    # Get list of existing firewall rules on the SQL server
    $existingRules = @( Get-AzSqlServerFirewallRule `
                        -ResourceGroupName $ResourceGroupName `
                        -ServerName $ServerName `
                        -WhatIf:$false # Completeness, not necessary to add this parameter
                        )

    # Create an empty array when no firewall rules exist 
    if (-Not $existingRules) {
        Write-Information "Found no existing firewall rules"
        $existingRule = @()
    } 
    # Create a list of the existing firewall rules when there are existing ones
    # Filter the list on the FirewallRuleName that needs to be added
    else {
        Write-Information "Found $($existingRules.Count) existing firewall rule(s)"
        $existingRules.FirewallRuleName | Write-Verbose
        $existingRule = $existingRules | Where-Object {$_.FirewallRuleName -eq $FirewallRuleName}
    }   
    
    # If there is an existing firewall rule with the same name: UPDATE
    if( $existingRule) {
        # rule exists, update if different
        if( ( $existingRule.StartIpAddress -eq $StartIpAddress ) -and ( $existingRule.EndIpAddress -eq $EndIpAddress) ) {
            Write-Information "Rule already exists with same definition"
        } else {
            # If called: The process should be initiated.
            # Satefy precaution
            if($PSCmdlet.ShouldProcess("$ServerName", "Update existing FirewallRule")) {
            Write-Information "Updating existing rule ..."
            $null = Set-AzSqlServerFirewallRule `
                -ResourceGroupName $ResourceGroupName `
                -ServerName $ServerName `
                -FirewallRuleName $FirewallRuleName `
                -StartIpAddress $StartIpAddress `
                -EndIpAddress $EndIpAddress
            }
        }

    } 

    # If there is no existing firewall rule with the same name: CREATE
    else {
        # rule doesn't exist, create
        # If called: The process should be initiated.
        # Satefy precaution
        if($PSCmdlet.ShouldProcess("$ServerName", "Create new FirewallRule")) {
            Write-Information "Creating new rule..."
            $null = New-AzSqlServerFirewallRule `
                -ResourceGroupName $ResourceGroupName `
                -ServerName $ServerName `
                -FirewallRuleName $FirewallRuleName `
                -StartIpAddress $StartIpAddress `
                -EndIpAddress $EndIpAddress
        }
    }
}
</file>
<file name="src\powershell\modules\Networking\public\Set-NetworkingSqlServerFirewallRule.Tests.ps1">
BeforeAll {
    # Import the function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    $script:DebugPreference = $true

    Set-StrictMode -Version Latest
}

Describe "Set-NetworkingSqlServerFirewallRule" {
    # General context expected by the Pester Module
    Context "UnitTests" {
        
        # Group: There are multiple mocked firewall rules
        Context "Multiple existing rules" {
            # Test the result when there are multiple existing firewall rules

            BeforeAll {
                
                # Mock functions from Az.Sql module
                mock New-AzSqlServerFirewallRule {}
                mock Set-AzSqlServerFirewallRule {}

                #  Mock function from Az.Sql module
                # If function Get-AzSqlServerFirewallRule gets called, return the 2 objects defined below
                mock Get-AzSqlServerFirewallRule {
                    $rule1 = New-Object -TypeName "Microsoft.Azure.Commands.Sql.FirewallRule.Model.AzureSqlServerFirewallRuleModel" 
                    $rule1.FirewallRuleName = "testrule1"
                    $rule1.StartIpAddress = "startIp"
                    $rule1.EndIpAddress = "endIp" 

                    $rule2 = New-Object -TypeName "Microsoft.Azure.Commands.Sql.FirewallRule.Model.AzureSqlServerFirewallRuleModel" 
                    $rule2.FirewallRuleName = "testrule2"
                    $rule2.StartIpAddress = "startIp"
                    $rule2.EndIpAddress = "endIp" 

                    return @( $rule1, $rule2 )
                 }   
                
            }
            
            It "test existing rule"  { 

                # When the FirewallRuleName exists, the function Set-AzSqlServerFirewallRule and New-AzSqlServerFirewallRule should not be called
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule1" `
                    -StartIpAddress "startIp" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 0 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 0 -Exactly

            }

            It "test new rule"  { 
                # When the FirewallRuleName does not exist, the function New-AzSqlServerFirewallRule should be called once
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "another rule" `
                    -StartIpAddress "startIp" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 1 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 0 -Exactly
            }

            It "test updated rule"  { 

                # When the FirewallRuleName exists, the function Set-AzSqlServerFirewallRule should be called once
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule1" `
                    -StartIpAddress "something else" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 0 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 1 -Exactly
            }
    

        }

        # Group: Add one firewall rule as a mock
        Context "Single existing rule" {
            # Test the result when there is 1 existing firewall rule

            BeforeAll {
   
                # Mock functions from Az.Sql module
                mock New-AzSqlServerFirewallRule {}
                mock Set-AzSqlServerFirewallRule {}

                #  Mock function from Az.Sql module
                # If function Get-AzSqlServerFirewallRule gets called, return the object defined below
                mock Get-AzSqlServerFirewallRule {
                    $rule = New-Object -TypeName "Microsoft.Azure.Commands.Sql.FirewallRule.Model.AzureSqlServerFirewallRuleModel" 
                    $rule.FirewallRuleName = "testrule1"
                    $rule.StartIpAddress = "startIp"
                    $rule.EndIpAddress = "endIp" 
                    return $rule
                }   
                
            }
            
            It "test existing rule"  { 

                # When the FirewallRuleName exists, the function Set-AzSqlServerFirewallRule and New-AzSqlServerFirewallRule should not be called
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule1" `
                    -StartIpAddress "startIp" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 0 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 0 -Exactly

            }

            It "test new rule"  { 

                # When the FirewallRuleName does not exist, the function New-AzSqlServerFirewallRule should be called once
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "another rule" `
                    -StartIpAddress "startIp" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 1 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 0 -Exactly
            }

            It "test updated rule"  { 

                # When the FirewallRuleName exists, the function Set-AzSqlServerFirewallRule should be called once
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule1" `
                    -StartIpAddress "something else" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 0 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 1 -Exactly
            }
    

        }

        # Group: There are no existing rules
        Context "No existing rules" {
            # Test the result when there are no existing firewall rules
            BeforeAll {
   
                # Mock functions from Az.Sql module
                mock New-AzSqlServerFirewallRule {}
                mock Set-AzSqlServerFirewallRule {}

                #  Mock function from Az.Sql module
                # If function Get-AzSqlServerFirewallRule gets called, return an empty object-list
                mock Get-AzSqlServerFirewallRule {
                    return $null
                }   
                
            }
            
            It "test new rule"  { 
                # When rule does not exist, New-AzSqlServerFirewallRule should be called
                Set-NetworkingSqlServerFirewallRule `
                    -ResourceGroupName "dummyResourceGroup" `
                    -ServerName "dummyServer" `
                    -FirewallRuleName "testrule" `
                    -StartIpAddress "startIp" `
                    -EndIpAddress "endIp" `
                    -InformationAction $PesterInformationAction
                              
                Should -Invoke New-AzSqlServerFirewallRule -Times 1 -Exactly
                Should -Invoke Set-AzSqlServerFirewallRule -Times 0 -Exactly

            }

        }


        
    }

}



</file>
<file name="src\powershell\modules\TokenModule\.build\build.ps1">
&lt;#

.SYNOPSIS
    Module that contains all functions that have to do with tokenization and detokenization

.DESCRIPTION
    Tokenization is the practice of adding meaningless string-values to a larger string (text, json...). These values are called tokens, and should start and end with a specific (set of) character(s).
    Detokenization is the practice of replacing the tokens defined above with the correct value depending on certain parameters/variables/conditions... 
    To make the practice of tokenization a little easier, this module contains a set of functions that can be called to more easily create and replace tokens.

#&gt;

[CmdletBinding()]
param (
    [Parameter(HelpMessage = "Name of the module")]
    [string] $ModuleName = "TokenModule",

    [Parameter(HelpMessage = "Version number in Major.Minor.Path format")]
    [string] $BuildVersion,

    [Parameter(HelpMessage = "Base directory with the module folder")]
    [string] $Workingdir,   

    [Parameter(HelpMessage = "Directory for the localFeed PSRepository")]
    [string] $LocalFeedPath,   

    [Parameter(HelpMessage = "Set this switch to enable cleaning the module folder before packaging")]
    [switch] $CleanFolder

)

#local settings
Set-StrictMode -Version "latest"
#Requires -Version 7.2

# ---------------------------------------------------------------------------------------------------------
# Write information for script user

$scriptName = $script:MyInvocation.MyCommand.Path
Write-Output "*** Starting script: $scriptName ***"

# Start region: Print all variables

$inputParameters = @( 
    "ModuleName"
    "BuildVersion"
    "WorkingDir"
    "LocalFeedPath"
)

# $inputParameters = $MyInvocation.MyCommand.Parameters.Keys is dynamic, but includes all the standard ones (debug, verbose, *action, ...)
# Print list of input parameters  
$maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
Write-Information "Start script: $($MyInvocation.MyCommand.Name) with parameters:"
foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
}

Write-Information "PSVersion: $($PSVersiontable.PSVersion.ToString())"
Write-Information "----------------------------------------------------------------------"
#endregion script info
# ---------------------------------------------------------------------------------------------------------


# ---------------------------------------------------------------------------------------------------------

Write-Output ""
Write-Output "Package providers:"
Get-PackageProvider
Write-Output "----------------------------------------------------------------------"


#unload the module if currently loaded
$currentModules = Get-Module -Name $ModuleName -ErrorAction "Ignore"
if( $currentModules) {
    Write-Output "Unloading currently loaded '$($ModuleName)' modules ..." 
    $currentModules.Version | ForEach-Object { Write-Information "  $_"}
    $currentModules | Remove-Module | Out-Null
} else {
    Write-Output "No currently loaded '$($ModuleName)' modules (OK)" 
}
#uninstall the package if currently installed
$currentPackages = Get-Package -Name $ModuleName -ErrorAction "Ignore"
if( $currentPackages) {
    Write-Output "Uninstalling currently installed packages ..."
    $currentPackages.Version | ForEach-Object { Write-Information "  $_"}
    $currentPackages | Uninstall-Package | Out-Null
} else {
    Write-Output "No currently installed '$($ModuleName)' packages (OK)" 
}

#build and check path to the module folder
$modulePath = Join-Path $Workingdir $ModuleName
Write-Information ""
Write-Information "Using modulePath: $modulePath"
if ( -Not (Test-Path $modulePath -PathType "Container") ) {
    Throw "Module $($ModuleName) is not a subfolder of '$($WorkingDir)'"
} else {
    Write-Information "  modulePath: OK, folder exists"
    $modulePath = (Resolve-Path $modulePath).Path
    Write-Information "  resolved to: $($modulePath)"
}

#build and check path to the module manifest
$psdFile = $ModuleName + ".psd1"
Write-Information "Using psdFile: $psdFile"
$manifestPath = Join-Path $modulePath $psdFile
Write-Information "Using manifestPath: $manifestPath"
if (-not (test-Path $manifestPath) ) {
    Throw "Manifest does not exist at: $manifestPath"
}

#region Prepare localFeed PSRepository

    Write-Output "Preparing localFeed PSRepository ..."

    #check and resolve localFeedPath
    if (-not (Test-Path $localFeedPath)) {
        New-Item -Path $localFeedPath -ItemType "Directory" | Out-Null
    }
    $LocalFeedPathResolved = ($LocalFeedPath | Resolve-Path -ErrorAction Ignore)
    if ( -not $LocalFeedPathResolved ) {
        Throw "Could not resolve LocalFeedPath: '$LocalFeedPath'"
    } else {
        $LocalFeedPathResolved = $LocalFeedPathResolved.Path
        Write-Information "  Resolved LocalFeedPath to: $LocalFeedPathResolved"
    }

    #check or create "localFeed" PSRepository
    $localFeed = Get-PSRepository -Name localFeed -ErrorAction "Ignore"
    if ( $localFeed ) {
        Write-Output "  Found localFeed PSRepository"
        # $localFeed 
        # $localFeed.ScriptSourceLocation
        # $localFeed.ScriptPublishLocation
        # Write-Output "(members)"
        # $localFeed | Get-Member
    } else {
        Write-Output "  localFeed PSRepository not found"
    }

    if ( $localFeed -and ($localFeed.SourceLocation -ne $LocalFeedPathResolved) ) {
        Write-Warning "  localFeed repository has wrong path ($($localFeed.SourceLocation)), removing it ..."
        #Write-Output "Unregister existing localFeed repository ..."
        Unregister-PSRepository -Name "localFeed"
        $localFeed = $null
    }
    if ( -not $localFeed) {
        Write-Output " Registering 'localFeed' PSRepository with path: $LocalFeedPathResolved ..."
        Register-PSRepository -name localFeed -sourceLocation $LocalFeedPathResolved -publishLocation $LocalFeedPathResolved -InstallationPolicy Trusted
    } else {
        Write-Output "  PSRepository 'localFeed' already present with path: $LocalFeedPathResolved"
    }

# Remove existing package with current version (main to avoid conflict when (re)building version 0.0.0 locally)
$existingPackages = Get-ChildItem -Path $LocalFeedPathResolved -Filter "${ModuleName}.${BuildVersion}.nupkg"
if( -not $existingPackages) {
    Write-Information "  Deleting existing package files : none found [OK]"
} else {
    Write-Information "  Deleting existing package files ..."
    $existingPackages.FullName | ForEach-Object { Write-Information "    $_"}
    $existingPackages  | Remove-Item | Out-Null
}

#endregion

#region create the manifest

    Write-Output ""
    Write-Output "Prepare the module and manifest..."

    # remove the files not required for the nuget package
    if ( $CleanFolder) {                
        Write-Information "  Remove unneeded files for nuget package..."
        Get-ChildItem -Path $modulePath -Include "*.Tests.ps1" -Recurse | Remove-Item
        Get-ChildItem -Path $modulePath -Filter ".build" -Directory | Remove-Item -Recurse
        Get-ChildItem -Path $modulePath -Filter ".test_config" -Directory | Remove-Item -Recurse
        Get-ChildItem -Path $modulePath -Filter "coverage.xml" | Remove-Item
        Get-ChildItem -Path $modulePath -Filter "testResults_unit.xml" | Remove-Item
    } else {
        Write-Information "  (...skipping CleanFolder section...)"
    }

    # Update build version in manifest
    Write-Information "  Updating version number in manifest..."
    $manifestContent = Get-Content -Path $manifestPath -Raw
    $manifestContent = $manifestContent -replace '&lt;ModuleVersion&gt;', $BuildVersion

    # Find all of the public functions and add them as cmdlets
    $publicFuncFolderPath = Join-Path $modulePath public
    $itemParams = @{
        Path = $publicFuncFolderPath
        File = $true
        Recurse = $true
        Filter = '*.ps1'
        Exclude = '*.Tests.ps1'
    }
    if ((Test-Path -Path $publicFuncFolderPath) -and ($publicFunctionNames = Get-ChildItem @itemParams | Select-Object -ExpandProperty BaseName)) {
        $funcStrings = "'$($publicFunctionNames -join "',$([System.Environment]::NewLine)  '")'"
        $funcStrings = $([System.Environment]::NewLine) + "  " + $funcStrings + $([System.Environment]::NewLine)
    } else {
        $funcStrings = $null
    }
    ## Add all public functions to FunctionsToExport attribute
    $manifestContent = $manifestContent -replace "'&lt;FunctionsToExport&gt;'", $funcStrings

    Write-Information "  "
    Write-Information "  functions to export:"
    $funcStrings | ForEach-Object { Write-output "    $_" }

    ## save updated manifest
    $manifestContent | Set-Content -Path "$manifestPath"
    Write-Information "  Updated manifest: $($manifestPath)"

#endregion

# create nuget package
Write-Output ""
Write-Output "Create and publish nuget package to localFeed repository..."
Publish-Module -path $modulePath -Repository localFeed -NuGetApiKey 'dummy' -Force

# list available packages
Write-Output ""
Write-Output "List available packages in localFeed repository..."
#Find-Package -Source "localFeed" |  Select-Object Version, ProviderName, Source, FullPath
Find-Package -Source "localFeed" |  ForEach-Object {Write-Output "  $($_.Version) -- $($_.ProviderName)"}

# install the package
Write-Output ""
Write-Output "Install the module from localFeed..."
Install-Package -Name $ModuleName -Source "localFeed" -ProviderName "PowerShellGet" -Force |  ForEach-Object {Write-Output "  $($_.Version)"}

# review installed modules
$modules = Get-Module -Name $ModuleName -ListAvailable | Select-Object Name, Version, RepositorySourceLocation, Path
Write-Output ""
Write-Output "Available module versions:"
$modules |  ForEach-Object {Write-Output "  $($_.Version)"}

# check that the "current" version is there
Write-Output ""
Write-Output "Check that current version is available..."
$currentModule = $modules | Where-Object {$_.Version.ToString() -eq $BuildVersion }
if ( -Not $currentModule) {
    Throw "Module with version '$BuildVersion' not found in available modules"
} else {
    Write-Information "  Found available module '$($ModuleName)' with version '$($BuildVersion)' (OK)"
}
 
# import the module
Write-Output ""
Write-Output "Import the module ..."
Import-Module -Name $ModuleName
Get-Module -Name $ModuleName |  ForEach-Object {Write-Output "  $($_.Version)"}


</file>
<file name="src\powershell\modules\TokenModule\.build\unit_tests.ps1">

[CmdletBinding()]
param (

)

# --------------------------------------------------------------
# SECTION: Prerequisites
# --------------------------------------------------------------

#requires -Modules Az.KeyVault
#requires -Modules Az.Accounts
#requires -Modules Az.Resources
#requires -Modules Az.Synapse
#requires -Modules @{ ModuleName="Pester"; ModuleVersion="5.5.0" }

# install pester if needed  
# note Joachim: this should actually be done upfront, so we can simple use a 'requires' statement in this script
# Write-Output "Available pester versions:"
# $pesterRequiredVersion = "5.5"
# $pesterCheckRegex = "^" + $pesterRequiredVersion.Replace(".", "\.")
# (Get-Module Pester -ListAvailable).Path | ForEach-Object {"  $_"}
# if(-not (Get-Module Pester -ListAvailable | where-Object Version -match $pesterCheckRegex)){
#     Write-Output "Installing Pester $pesterRequiredVersion ..."
#     Install-Module -Name Pester -RequiredVersion $pesterRequiredVersion -Force -Scope CurrentUser -AllowClobber
# } else {
#     Write-Output "Pester 5.5 already installed"
# }
# load the correct one
#Import-Module Pester -MinimumVersion $pesterRequiredVersion -MaximumVersion "$pesterRequiredVersion.999999"
# just to be sure
Write-Output ""
Write-Output "Using pester version: $((Get-Module Pester).Version)"

Write-Output ""
Write-Output "Module to test:"
$modulePath = resolve-path ( Join-Path "$PSScriptRoot" "..")
$moduleName = (Split-Path $modulePath -Leaf)  + '.psm1'
Write-Output "  modulePath: $modulePath"
Write-Output "  moduleName: $moduleName"

# preload the module a first time
Import-Module -Name (Join-Path  $modulePath $moduleName ) -Force


# --------------------------------------------------------------
# SECTION: Pester configuration
# --------------------------------------------------------------

# start with default configuration
$pesterConfig = [PesterConfiguration]::Default

# create testResults.xml output
$pesterConfig.TestResult.Enabled = $true
$pesterConfig.TestResult.OutputFormat = "NUnitXml"
$pesterConfig.TestResult.OutputPath = Join-Path $modulePath "testResults_unit.xml"

# create code coverage output
$pesterConfig.CodeCoverage.Enabled = $True
$pesterConfig.CodeCoverage.OutputFormat = "JaCoCo"
$pesterConfig.CodeCoverage.OutputPath = Join-Path $modulePath "coverage.xml"
$pesterConfig.CodeCoverage.Path = "$modulePath/[p]*/*.ps1"  #  public, private

# filters (the fullname filter is a powershell "LIKE" statement, not regex)
# important: the FullName and Tag filters work are combined as "OR", not as "AND"
$pesterConfig.Filter.FullName = "*.UnitTests.*"
$pesterConfig.Filter.Tag = 'UnitTest'
$pesterConfig.Filter.ExcludeTag = "Draft"

# run parameters
$pesterConfig.Run.Path = Join-Path $modulePath "public" 
$pesterConfig.Run.Exit = $False
$pesterConfig.Run.PassThru = $False

# --------------------------------------------------------------
# SECTION: Run tests
# --------------------------------------------------------------
Write-Output "Run tests..."

Invoke-Pester -Configuration $pesterConfig
</file>
<file name="src\powershell\modules\TokenModule\public\Get-DetokenizedString.ps1">
&lt;#

.SYNOPSIS
    Replaces tokenized values using by the value of variables with the same name.

.DESCRIPTION
    If the variable is not set, the function throws an error and just removes the token (see second example below).
    The function first looks into session variables, then if none is found, then it will look into environment variables

    Limitations in this version:
    * will throw a (non-terminating) error when no such variable is found and keep the token

    TODO: 
    * support a parameter to define behaviour on missing variable (error/warning/none, stop/keep/drop token)
    * allow custom token identifiers (i.e. something else than [])

.PARAMETER StringValue
    The string from which tokens need to be replaced

.PARAMETER StartToken
    The opening string defining the start of any token, default: "["

.PARAMETER EndToken
    The closing string defining the start of any token, default: "]"

.EXAMPLE

    PS&gt;$TokenizedTest = "This is a [modifier] text."
    PS&gt;$modifier = "very short" 
    PS&gt;$Result = Get-DetokenizedString $TokenizedTest
    PS&gt;Write-Output $Result
    This is a very short text.

.EXAMPLE

    PS&gt;$TokenizedTest = "This is a [modifier] text with a [missing] variable."
    PS&gt;$Result = Get-DetokenizedString $TokenizedTest
    ReplaceTokensWithVariables:   Could not find replacement variable for literal: [missing]
    PS&gt;Write-Output $Result
    This is a very short text with a  variable.
#&gt;
function Get-DetokenizedString {
     
    [CmdletBinding()]
    param (
        [parameter(Mandatory=$true, Position=0)]
        [String]$stringValue,

        [parameter(Mandatory=$false)]
        [String]$StartToken="[",

        [parameter(Mandatory=$false)]
        [String]$EndToken="]",

        [parameter(Mandatory=$false)]
        [ValidateSet('Error','Warning')]
        [String]$MissingVariableAction="Error"
        
    )

    # search pattern: '&lt;StartToken&gt;&lt;UnlimitedSetOfCharacters&gt;&lt;EndToken&gt;'
    $regex = [System.Text.RegularExpressions.Regex]::Escape($StartToken) + '(\w+)' + [System.Text.RegularExpressions.Regex]::Escape($EndToken)
    Write-Debug "regex: $regex"
    Write-Debug "stringValue: $stringValue"
    $regexMatches = [regex]::Matches($stringValue,$regex) 
    Foreach ( $match in $regexMatches) {
        Write-Debug "  Match: $match"
        $ToReplace = $match.groups[0].value
        Write-Debug ("  look for variable: " + '$' + $match.groups[1].value)
        # look if there is a session variable with lookup name we need, if not look into env: variables
        $ReplaceVar = (Get-ChildItem Variable: | Where-Object {$_.Name -eq $match.groups[1].value})
        $ReplaceVar = $ReplaceVar ?? (Get-ChildItem env: | Where-Object {$_.Name -eq $match.groups[1].value})

        if ( -not $ReplaceVar -or ($null -eq $ReplaceVar.Value)) {
            switch ( $MissingVariableAction) {
                "Error" {
                    Write-Error "Could not find replacement variable for literal: $ToReplace"
                }
                "Warning" {
                    Write-Warning "Could not find replacement variable for literal: $ToReplace"
                }
            }
            
        } else {
            $stringValue = $stringValue.replace($ToReplace,$ReplaceVar.Value)
            Write-Verbose "  Replaced '$ToReplace' by '$($ReplaceVar.Value)'"
        }

    }
    return $stringValue
}
</file>
<file name="src\powershell\modules\TokenModule\public\Get-DetokenizedString.Tests.ps1">
BeforeAll {
    # Import the function under test
    . $PSCommandPath.Replace('.Tests.ps1', '.ps1')

    # informationAction preference for the tests
    if ( Test-Path env:PESTER_INFORMATION_ACTION ) {
        $script:PesterInformationAction = $env:PESTER_INFORMATION_ACTION
    }
    else {
        $script:PesterInformationAction = $InformationPreference
    }

    Set-StrictMode -Version Latest
}

Describe "Get-DetokenizedString" {

    $Tokens = @(
        @{StartToken = '[' ; EndToken = ']'}, 
        @{StartToken = '#{'; EndToken = '}#'}
    )

    Context "UnitTests" {
        
        Context "&lt;StartToken&gt;&lt;EndToken&gt;" -Foreach $Tokens {

            Context "Basic tests" {

                BeforeAll {
                    $script:test_var = "basic"
                }
                
                It "Word replacement"  { 

                    $testString = "This is a ${StartToken}test_var${EndToken} test"   
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a basic test"     

                }

                It "Part of word replacement" { 

                    $testString = "This is a${StartToken}test_var${EndToken}test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is abasictest"     
                    
                }

                It "Double variable"  { 

                    $testString = "This is a ${StartToken}test_var${EndToken} ${StartToken}test_var${EndToken} test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a basic basic test"     

                }             

                It "Case insensitve" -TestCases $Tokens { 

                    $testString = "This is a ${StartToken}test_var${EndToken} test"
                    $TEST_VAR = "basic"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a basic test"     

                    $testString = "This is a ${StartToken}TEST_VAR${EndToken} test"
                    $test_var = "basic"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a basic test"     

                    #just get rid of the unused variable warnings
                    $null = $TEST_VAR

                }    
    
                It "Double token" { 

                    $testString = "This is a ${StartToken}${StartToken}test_var${EndToken}${EndToken} test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a ${StartToken}basic${EndToken} test"     
    
                }     

                It "Multiple variables test" {

                    $test_var1 = "very"
                    $test_var2 = "basic"

                    $testString = "This is a ${StartToken}test_var1${EndToken} ${StartToken}test_var2${EndToken} test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a very basic test"     

                    #just get rid of the unused variable warnings
                    $null = $test_var1 + $test_var2
                }  

                It "Empty variable test" {

                    $test_var1 = "very"
                    $test_var2 = ""

                    $testString = "This is a ${StartToken}test_var1${EndToken} ${StartToken}test_var2${EndToken} test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a very  test"     

                    #just get rid of the unused variable warnings
                    $null = $test_var1 + $test_var2        
                }  

                It "Missing variable test" {

                    $test_var1 = "very"
                    $test_var2 = $null

                    $testString = "This is a ${StartToken}test_var1${EndToken} ${StartToken}test_var2${EndToken} test"

                    #should NOT throw a terminating error
                    { Get-DetokenizedString -stringValue $testString -ErrorAction SilentlyContinue -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction } | 
                    Should -Not -Throw

                    #should raise a non-terminating error
                    Get-DetokenizedString -stringValue $testString -ErrorAction SilentlyContinue -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction -ErrorVariable err
                    $err.Count | Should -Be 1
                    $err[0].Exception.Message | Should -Be "Could not find replacement variable for literal: ${StartToken}test_var2${EndToken}"

                    #should keep the token
                    $result = Get-DetokenizedString -stringValue $testString -ErrorAction SilentlyContinue -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a very ${StartToken}test_var2${EndToken} test"     

                    #just get rid of the unused variable warnings
                    $null = $test_var1 + $test_var2        

                }

                It "Environment variables test" {

                    $env:test_var1 = "very"
                    $test_var2 = "basic"

                    $testString = "This is a ${StartToken}test_var1${EndToken} ${StartToken}test_var2${EndToken} test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a very basic test"     

                    #just get rid of the unused variable warnings
                    $null = $env:test_var1 + $test_var2
                    $env:test_var1 = $null
                }

                It "Session and environment variables overlap test" {

                    $env:test_var1 = "very"
                    $test_var1 = "basic"

                    $testString = "This is a ${StartToken}test_var1${EndToken} test"
                    $result = Get-DetokenizedString -stringValue $testString -StartToken $StartToken -EndToken $EndToken -InformationAction $PesterInformationAction
                    $result | Should -Be "This is a basic test"     

                    #just get rid of the unused variable warnings
                    $null = $test_var1 + $env:test_var1
                    $env:test_var1 = $null
                }

            }

        }

    }

}



</file>
<file name="src\sql\metadb\functions\meta.fn_add_json_property.sql">
CREATE FUNCTION [meta].[fn_add_json_property] (
    @json [nvarchar](max),@property_path [nvarchar](100),@property_value [nvarchar](max)
) RETURNS nvarchar(max)
AS
begin
	/*
    Helper function, wrapping around JSON_MODIFY() to build json values.

    It expects the @json input to be an existing single json object, or null/empty string.
    
    * Replace null or empty string by empty object {}, then add or update the property
    * Allow adding null properties
    * Allow implicit path ("$.")

    A future enhancement could be to automatically build missing elements from the path
    in @property_value. For example, when the path is '$.object.property' then we could 
    automaticlaly add the 'object' if it doesn't exist, before creating the 'property'.

    NOTE: the function cannot throw exceptions. When errors are detected in the input
    parameters, an string with the error message is returned like "error: not valid json". 
    This return value is not valid json. This can be detected by (trying to) store it
    in a json type variable, or by checking the varchar value with isjson().

    Example usage:

        select meta.fn_add_json_property('{"property": "value"}','foo','bar');

            returns:
            {
                "property": "value",
                "foo": "bar"
            }

        
        -- Capture errors with a "json" type variable, this will case an exception

        declare @result json;
        select @result = meta.fn_add_json_property('invalid','foo','bar');

            result:
            Msg 13609, Level 16, State 9, Line 2
            JSON text is not properly formatted. Unexpected character 'E' is found at position 0.
        
        -- Capture errors with a isjson() check

        declare @result nvarchar(max);
        select @result = meta.fn_add_json_property('invalid','foo','bar');
        select isjson(@result) as valid_json;

            result:
            0
  
	Credits for the trick to add NULL properties from:
	https://stackoverflow.com/questions/59038499/add-new-key-with-value-null-to-existing-json-object
	the first JSON_MODIFY adds a key with empty string value, the second one updates it, now it can be set to null

*/

    -- check incoming json
    declare @startjson json
    if @json is NULL or trim(@json) = ''
        set @startjson = '{}';
    ELSE
    BEGIN
        if not (isjson(@json) = 1)
            --cant throw exceptions in a functin so we'll return something invalid
            return 'Error: Provided value is not valid json';
        if not (isjson(@json, object) = 1)
            return 'Error: Provided value is not valid json object';
        set @startjson = @json;
    END

    -- check the @property_name value

    --do nothing if the property_name is null
	if @property_path is null
		return @json;

    --assume root property if property_name is not a valid path
    if @property_path not like '$.%'
        set @property_path = '$.' + @property_path;

    --check if the property_value is json ?
    declare @isjson bit = ISJSON(@property_value)

    --first modify adds the property with empty string value
    set @json = JSON_MODIFY( -- the inner JSON_MODIFY adds a key with empty string value
				isnull(nullif(@json,''),'{}'),
				'lax ' + @property_path,
				''
			)

    --second modify updates the property with the real value, including null
    --if the @property_value string contains JSON, will consider it as such
    if ISJSON(@property_value) = 1
    	set @json = JSON_MODIFY (
            @json,
			'strict ' + @property_path,
			JSON_QUERY(@property_value)  
        );
    --if not json, add as a string
    else            
    	set @json = JSON_MODIFY (
            @json,
			'strict ' + @property_path,
			@property_value  
        );
    
    return @json;

end


</file>
<file name="src\sql\metadb\roles\role_ais_integration.sql">
-- ======================
-- ROLE CREATION
-- ======================
CREATE Role [role_ais_integration] authorization dbo
GO

-- ======================
-- Permissions
-- ======================

grant exec on meta.usp_ais_log_landing to role_ais_integration -- "as dbo" avoids the drop/recreate
GO
grant exec on meta.usp_ais_get_landing_configuration to role_ais_integration -- "as dbo" avoids the drop/recreate
GO
grant select on meta.ais_landing_configuration to role_ais_integration -- "as dbo" avoids the drop/recreate
GO
grant select on meta.ais_landing_log to role_ais_integration -- "as dbo" avoids the drop/recreate


</file>
<file name="src\sql\metadb\roles\role_ais_integration_admin.sql">
-- ======================
-- ROLE CREATION
-- ======================
CREATE Role [role_ais_integration_admin] authorization dbo
GO

-- ======================
-- Permissions
-- ======================

grant insert,update,delete on meta.ais_landing_configuration to role_ais_integration_admin as dbo -- "as dbo" avoids the drop/recreate
GO
grant insert,update,delete on meta.ais_landing_log to role_ais_integration_admin as dbo -- "as dbo" avoids the drop/recreate
GO

</file>
<file name="src\sql\metadb\roles\role_dap_core_engineer.sql">
-- ======================
-- ROLE CREATION
-- ======================
CREATE Role [role_dap_core_engineer] authorization dbo
GO

-- ======================
-- Permissions
-- ======================
-- IMPERSONATE
GRANT IMPERSONATE ON USER::[dbo] TO [role_dap_core_engineer]
GO

-- EXECUTE
GRANT EXECUTE TO [role_dap_core_engineer]
GO

-- CREATE TABLE
GRANT CREATE TABLE TO [role_dap_core_engineer]
GO

-- ALTER
GRANT ALTER TO [role_dap_core_engineer]
GO

-- VIEW DEFINITION
GRANT VIEW DEFINITION TO [role_dap_core_engineer]
GO


-- Update, insert, delete
GRANT UPDATE, INSERT, DELETE TO [role_dap_core_engineer]
GO

-- ======================
-- ADD ROLES TO...
-- ======================
-- Add role to db_datareader
ALTER ROLE db_datareader ADD MEMBER [role_dap_core_engineer]
GO
</file>
<file name="src\sql\metadb\roles\role_dap_ingestion_engineer.sql">
-- ======================
-- ROLE CREATION
-- ======================
CREATE Role [role_dap_ingestion_engineer] authorization dbo
GO

-- ======================
-- Permissions
-- ======================
-- VIEW DEFINITION
GRANT VIEW DEFINITION TO [role_dap_ingestion_engineer]
GO

-- ======================
-- ADD ROLES TO...
-- ======================
-- Add role to db_datareader
ALTER ROLE db_datareader ADD MEMBER [role_dap_ingestion_engineer]
GO
</file>
<file name="src\sql\metadb\roles\role_syn_integration.sql">
/*

A role for the synapse managed identity (xxx-syn-core and xxx-syn-interf) so these
can run the meta stored procedures as needed.

*/
CREATE Role [role_syn_integration] authorization dbo
GO


-- dev info: "as dbo" avoids the drop/recreate when deploying the dacpac

grant exec on schema::meta to role_syn_integration as dbo
GO


</file>
<file name="src\sql\metadb\schemas\deploy.sql">
-- Create deploy-schema in SQL
CREATE SCHEMA [deploy];
</file>
<file name="src\sql\metadb\schemas\meta.sql">
-- Create meta-schema in SQL
CREATE SCHEMA [meta];
</file>
<file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_plan_configuration.sql">
CREATE PROCEDURE [deploy].[usp_set_plan_configuration]
	
	@plan_name          varchar(100),
	@plan_description   varchar(500),
	@enabled         	bit

AS
begin


-- 1: Quality checks: Make sure the necessary values are not null 
if @plan_name is null or LTRIM(rtrim(@plan_name)) = ''
	throw 50000, 'plan_name cannot be null or empty', 1;

if @plan_description is null or LTRIM(rtrim(@plan_description)) = ''
	throw 50000, 'plan_description cannot be null or empty', 1;

if @enabled is null 
	throw 50000, 'enabled cannot be null', 1;


-- 2. Merge data into meta.plan_configuration table
with input_data as (
	select 
		ltrim(rtrim(@plan_name)) as plan_name,
		ltrim(rtrim(@plan_description)) as plan_description,
		@enabled as [enabled]
)
MERGE
	INTO meta.plan_configuration as t
USING input_data s on (
	t.plan_name = s.plan_name 
	)
WHEN MATCHED and (
	   t.enabled &lt;&gt; s.enabled
	or t.plan_description &lt;&gt; s.plan_description
	)
THEN
	UPDATE SET t.enabled = s.enabled,
			   t.plan_description = s.plan_description
WHEN NOT MATCHED BY TARGET THEN 
	insert (plan_name,		plan_description,	[enabled]) 
	values (s.plan_name,	s.plan_description,	s.enabled)
OUTPUT 
    $ACTION as [action], 
    ISNULL(INSERTED.plan_name, DELETED.plan_name) AS plan_name,
	ISNULL(INSERTED.plan_description, DELETED.plan_description) AS plan_description,
	ISNULL(INSERTED.enabled, DELETED.enabled) AS [enabled]
;

end

GO
</file>
<file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_plan_task_configuration.sql">
CREATE PROCEDURE [deploy].[usp_set_plan_task_configuration]
	
    @plan_name varchar(100),
	@plantask_data nvarchar(max)

AS
begin

-- 1. Quality checks: Make sure the necessary values are not null 
if @plan_name is null or LTRIM(rtrim(@plan_name)) = ''
	throw 50000, 'plan_name cannot be null or empty', 1;

-- 2. Merge data into meta.plan_task_configuration table
-- 2.1. Create an empty table with the expected schema
declare @input_data table ( 
	[plan_name]		varchar(100)	not null,
	[task_group]	varchar(100)	not null, 
	[task_name]		varchar(100)	not null, 
	[task_sequence] int				not null,
	[enabled]		bit				not null,
	primary key ( [task_group], [task_name] )
);

-- 2.2. Insert the data into the empty table
-- Dev-Info: A json-object (@checks_data) is passed to the stored procedure
-- 			The individual keys-values are passed to the SQL table columns with openjson
-- 			Note that this approach is slightly different from, for example, 
--			stored procedure deploy.usp_set_plan_configuration because of this json-object 
insert into @input_data
select 
	@plan_name,
	nullif(ltrim(rtrim(task_group)),''),
	nullif(ltrim(rtrim(task_name)),''),
	task_sequence,
	[enabled]
from openjson ( @plantask_data)
WITH (   
    task_group		varchar(100)  '$.task_group',  
    task_name		varchar(100) '$.task_name',
    task_sequence   int '$.task_sequence',
	[enabled]		bit '$.enabled'
);

-- 2.3. Merge the input_data table into the meta.plan_task_configuration table
merge
	into meta.plan_task_configuration as t
		
using @input_data as s on (
	t.plan_name = s.plan_name 
	and t.task_group = s.task_group
	and t.task_name = s.task_name
	)
when matched and (
	   t.task_sequence &lt;&gt; s.task_sequence --not null
	or t.enabled &lt;&gt; s.enabled     --not null
	)
then
	update set 
		t.task_sequence = s.task_sequence,
		t.enabled = s.enabled

when not matched by target then 
	insert (  plan_name,   task_sequence,   task_name,   task_group,   [enabled]) 
	values (s.plan_name, s.task_sequence, s.task_name, s.task_group, s.enabled)

when not matched by source and t.plan_name = @plan_name then
	delete

OUTPUT 
    $ACTION as action, 
    ISNULL(INSERTED.plan_name, DELETED.plan_name) AS plan_name,
    ISNULL(INSERTED.task_group, DELETED.task_group) AS task_group,
    ISNULL(INSERTED.task_name, DELETED.task_name) AS task_name,
    ISNULL(INSERTED.task_name, DELETED.task_name) AS is_enabled
;

end

GO
</file>
<file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_source_check_configuration.sql">
CREATE PROCEDURE [deploy].[usp_set_source_check_configuration]
	@file_layout		varchar(100),		-- Name of the dataset
	@checks_data		nvarchar(max)		-- Array of JSON objects of Datachecks
AS
BEGIN

-- 1. Quality checks: Make sure the necessary values are not null 
if @file_layout is null or LTRIM(rtrim(@file_layout)) = ''
	throw 50406, 'file_layout cannot be NULL or empty', 406			-- 406: Not acceptable


-- 2. Merge data into meta.source_check_configuration table
-- 2.1. Create an empty table with the expected schema
DECLARE @input_data TABLE (
	check_name		varchar(50)		NOT NULL,
	file_layout		varchar(100)	NOT NULL,
	[enabled]		bit 			NOT NULL,
	config_params	varchar(max)	NULL
	
	PRIMARY KEY (check_name, file_layout)
);

-- 2.2. Insert the data into the empty table
-- Dev-Info: A json-object (@checks_data) is passed to the stored procedure
-- 			The individual keys-values are passed to the SQL table columns with openjson
-- 			Note that this approach is slightly different from, for example, 
--			stored procedure deploy.usp_set_plan_configuration because of this json-object 
INSERT INTO @input_data
SELECT
	NULLIF(LTRIM(RTRIM(check_name)),''),
	@file_layout,
	[enabled],
	config_params
from openjson (@checks_data)
WITH (
	check_name		varchar(50)		'$.name',
	[enabled]		bit				'$.enabled',
	config_params	varchar(max)	'$.config_params'
);

-- 2.3. Merge the input_data table into the meta.source_check_configuration table
MERGE
	INTO meta.source_check_configuration as t
USING @input_data as s ON(
	t.file_layout = s.file_layout
	AND t.check_name = s.check_name
	AND t.enabled = s.enabled
	AND t.config_params = s.config_params
)
WHEN matched AND(
	(ISNULL( NULLIF(t.config_params, s.config_params), NULLIF(s.config_params, t.config_params) ) IS NOT NULL)
)
THEN
	UPDATE SET
		t.check_name = s.check_name,
		t.enabled = s.enabled,
		t.config_params = s.config_params
	

WHEN not matched BY TARGET THEN
	INSERT( check_name, file_layout, [enabled], config_params )
	VALUES (s.check_name, s.file_layout, s.enabled, s.config_params)

WHEN not matched BY source and t.file_layout = @file_layout THEN
	delete

-- This is the table that will be returned to show what has been done
OUTPUT
	$ACTION as action, -- Was it a delete or insert?
	ISNULL(INSERTED.check_name, DELETED.check_name) AS check_name,
	ISNULL(INSERTED.file_layout, DELETED.file_layout) AS file_layout,
	ISNULL(INSERTED.enabled, DELETED.enabled) AS [enabled],
	ISNULL(INSERTED.config_params, DELETED.config_params) AS config_params;

END

GO
</file>
<file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_source_column_configuration.sql">
CREATE PROCEDURE [deploy].[usp_set_source_column_configuration]
	@file_layout varchar(100),
	@column_data nvarchar(max)

AS
begin

if @file_layout is null or LTRIM(rtrim(@file_layout)) = ''
	throw 50000, 'file_layout cannot be null or empty', 1;


-- 2. Merge data into meta.source_column_configuration table
-- 2.1. Create an empty table with the expected schema
declare @input_data table ( 
	file_layout 		varchar(100) 	not null,
	column_sequence 	int 			not null primary key,
	source_column_name 	varchar(100) 	not null, 
	sink_column_name 	varchar(100) 	not null ,
    dimension 			nvarchar(5) 	not null,
	data_type 			nvarchar(20) 	not null,
	column_info			nvarchar(max)		null
);

-- 2.2. Insert the data into the empty table
-- Dev-Info: A json-object (@checks_data) is passed to the stored procedure
-- 			The individual keys-values are passed to the SQL table columns with openjson
-- 			Note that this approach is slightly different from, for example, 
--			stored procedure deploy.usp_set_plan_configuration because of this json-object 
insert into @input_data
select 
	@file_layout,
	column_sequence,
	nullif(ltrim(rtrim(source_column_name)),''),
	nullif(ltrim(rtrim(sink_column_name)),''),
    nullif(ltrim(rtrim(dimension)),''),
    nullif(ltrim(rtrim(data_type)),''),
	nullif(ltrim(rtrim(column_info)),'')
from openjson ( @column_data)
WITH (   
    column_sequence   int '$.column_sequence' ,
    source_column_name varchar(100)  '$.source_column_name',  
    sink_column_name varchar(100) '$.sink_column_name',
    dimension varchar(100) '$.dimension',
    data_type varchar(20) '$.data_type',
	column_info varchar(max)	'$.column_info'
);

-- 2.3. Merge the input_data table into the meta.source_column_configuration table
merge
	into meta.source_column_configuration as t
using @input_data as s on (
	t.file_layout = s.file_layout
	and t.column_order = s.column_sequence
	)
when matched and (
	-- looks complicated but this works for nullable columns: true is both are not null AND different or when ONE of them is null, but NOT when both are null
	   (ISNULL( NULLIF(t.source_name, s.source_column_name), NULLIF(s.source_column_name, t.source_name) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.sink_name, s.sink_column_name), NULLIF(s.sink_column_name, t.sink_name) ) IS NOT NULL)
    or (ISNULL( NULLIF(t.dimension, s.dimension), NULLIF(s.dimension, t.dimension) ) IS NOT NULL)
    or (ISNULL( NULLIF(t.data_type, s.data_type), NULLIF(s.data_type, t.data_type) ) IS NOT NULL)
    or (ISNULL( NULLIF(t.column_info, s.column_info), NULLIF(s.column_info, t.column_info) ) IS NOT NULL)
			
)
then
	update set 
		t.source_name = s.source_column_name,
		t.sink_name = s.sink_column_name,
        t.column_order = s.column_sequence,
        t.dimension = s.dimension,
		t.data_type = s.data_type,
		t.column_info = s.column_info

when not matched by target then
	insert (   file_layout,   column_order,   	 source_name,   	   sink_name, 		   dimension,   data_type, 	 column_info) 
	values ( s.file_layout, s.column_sequence, s.source_column_name, s.sink_column_name, s.dimension, s.data_type, s.column_info)

when not matched by source and t.file_layout = @file_layout then  
	delete

OUTPUT 
    $ACTION as action, 
    ISNULL(INSERTED.file_layout, 	DELETED.file_layout) 	AS file_layout,
    ISNULL(INSERTED.column_order, 	DELETED.column_order) 	AS column_order,
    ISNULL(INSERTED.source_name, 	DELETED.source_name) 	AS source_name,
    ISNULL(INSERTED.sink_name, 		DELETED.sink_name) 		AS sink_name,
	ISNULL(INSERTED.dimension, 		DELETED.dimension) 		AS dimension,
	ISNULL(INSERTED.data_type, 		DELETED.data_type) 		AS data_type,
	ISNULL(INSERTED.column_info, 	DELETED.column_info) 	AS column_info
;

end

GO
</file>
<file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_source_configuration.sql">
CREATE PROCEDURE [deploy].[usp_set_source_configuration]
	@file_layout        varchar(100),
	@file_pattern		varchar(100),
	@file_kind			varchar(10),
	@file_extension     varchar(10),
	@column_delimiter   varchar(5),
    @row_delimiter      varchar(5),
	@escape_character   varchar(5),
	@quote_character    varchar(5),
	@header				bit,
	@encoding           varchar(10),
	@skip_first_lines   integer,
    @source_conditions  nvarchar(max)

AS
begin

-- 1. Quality checks: Make sure the necessary values are not null 
if @file_layout is null or LTRIM(rtrim(@file_layout)) = ''
	throw 50000, 'file_layout cannot be null or empty', 1;

if @file_pattern is  null or LTRIM(rtrim(@file_pattern)) = ''
	throw 50000, 'file_pattern cannot be null or empty', 1;

--prevent error in ADF: if quote_character is specified, then escape_character is also required
if isnull(ltrim(rtrim(@quote_character)),'') &lt;&gt; ''
	and isnull(ltrim(rtrim(@escape_character)),'') = ''
	throw 50000, 'if a quote_character is specified, then the escape_character cannot be null or empty', 1;


-- 2. Merge data into meta.source_configuration table
with input_data as (
	select 
		ltrim(rtrim(@file_layout)) 								as file_layout,
		ltrim(rtrim(@file_pattern)) 							as file_pattern,
		ltrim(rtrim(@file_kind)) 								as file_kind,
		ltrim(rtrim(@file_extension)) 							as file_extension,
		ltrim(rtrim(@column_delimiter)) 						as column_delimiter,
		ltrim(rtrim(@row_delimiter)) 							as row_delimiter,
		nullif(ltrim(rtrim(@escape_character)),'') 				as escape_character,
		nullif(ltrim(rtrim(@quote_character)),'') 				as quote_character,
		case when @header  = 1 then 'True' else 'False' end 	as header,
		nullif(ltrim(rtrim(@encoding)),'') 						as [encoding],
		nullif(ltrim(rtrim(@skip_first_lines)),'') 				as skip_first_lines,
		nullif(ltrim(rtrim(@source_conditions)),'') 			as source_conditions

)
MERGE
	INTO meta.source_configuration as t
USING input_data as s on (
	t.file_layout = s.file_layout 
	)
WHEN MATCHED  and (
	-- looks complicated but this works for nullable columns: true is both are not null AND different or when ONE of them is null, but NOT when both are null
	-- the (isnull(nullif(x,y),nullif(y,x)) IS NOT NULL) reacts the same as " IS DISTINCT FROM " (to be refactored to " IS DISTINCT FROM " )
	   (ISNULL( NULLIF(t.file_pattern, s.file_pattern), NULLIF(s.file_pattern, t.file_pattern) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.file_kind, s.file_kind), NULLIF(s.file_kind, t.file_kind) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.file_extension, s.file_extension), NULLIF(s.file_extension, t.file_extension) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.column_delimiter, s.column_delimiter), NULLIF(s.column_delimiter, t.column_delimiter) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.row_delimiter, s.row_delimiter), NULLIF(s.row_delimiter, t.row_delimiter) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.escape_character, s.escape_character), NULLIF(s.escape_character, t.escape_character) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.quote_character, s.quote_character), NULLIF(s.quote_character, t.quote_character) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.header, s.header), NULLIF(s.header, t.header) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.[encoding], s.[encoding]), NULLIF(s.[encoding], t.[encoding]) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.skip_first_lines, s.skip_first_lines), NULLIF(s.skip_first_lines, t.skip_first_lines) ) IS NOT NULL)
	or (t.source_conditions IS DISTINCT FROM s.source_conditions)

)
THEN
	UPDATE SET 
		t.file_pattern 		= s.file_pattern,
		t.file_kind 		= s.file_kind,
		t.file_extension 	= s.file_extension,
		t.column_delimiter 	= s.column_delimiter,
		t.row_delimiter 	= s.row_delimiter,
		t.escape_character 	= s.escape_character,
		t.quote_character 	= s.quote_character,
		t.header 			= s.header,
		t.[encoding] 		= s.[encoding],
		t.skip_first_lines 	= s.skip_first_lines,
		t.source_conditions	= s.source_conditions

WHEN NOT MATCHED BY TARGET THEN 
	insert (  file_layout,   file_pattern,   file_kind,   file_extension,   column_delimiter,   row_delimiter,   escape_character,   quote_character,   header,	  [encoding], 	  skip_first_lines, source_conditions)
	values (s.file_layout, s.file_pattern, s.file_kind, s.file_extension, s.column_delimiter, s.row_delimiter, s.escape_character, s.quote_character, s.header,  s.[encoding], 	s.skip_first_lines, s.source_conditions)
OUTPUT 
    $ACTION as action, 
    ISNULL(INSERTED.file_layout, DELETED.file_layout) AS file_layout
;

end
GO
</file>
<file name="src\sql\metadb\stored_procedures\deploy\deploy.usp_set_task_configuration.sql">
CREATE PROCEDURE [deploy].[usp_set_task_configuration]

    @task_name                  nvarchar(max),
    @task_category              varchar(20) = 'INGEST',  -- backwards compatibility for now
    @task_type                  nvarchar(max),
    @worker_name                nvarchar(max),
    @task_description           nvarchar(max),
    @table_name                 nvarchar(max),
    @target_options             nvarchar(max),
    @file_layout                nvarchar(max),
    @source_folder              nvarchar(max),
    @container_name             nvarchar(max),
    @preprocess_pattern         nvarchar(100) = null, -- backwards compatibility for now
    @preprocess_worker_options  nvarchar(max) = null, -- backwards compatibility for now
    @enabled                    bit

AS
begin


--#region 1. Quality checks: Make sure the necessary values are not null 
if @task_name is null or LTRIM(rtrim(@task_name)) = ''
	throw 50000, 'task_name cannot be null or empty', 1;

set @task_category = upper(trim(@task_category));
if @task_category is null or @task_category = ''
	throw 50000, 'task_category cannot be null or empty', 1;
if @task_category not in ('INGEST', 'PREPROCESS')
    throw 50000, 'task_category must be "INGEST" or "PREPROCESS"', 1;

if @task_type is null or LTRIM(rtrim(@task_type)) = ''
    throw 50000, 'task_type cannot be null or empty', 1;

if @worker_name is null or LTRIM(rtrim(@worker_name)) = ''
	throw 50000, 'worker_name cannot be null or empty', 1;

if @task_description is null or LTRIM(rtrim(@task_description)) = ''
	throw 50000, 'task_description cannot be null or empty', 1;


if @container_name is null or LTRIM(rtrim(@container_name)) = ''
	throw 50000, 'container_name cannot be null or empty', 1;

if @enabled is null 
	throw 50000, 'enabled cannot be null', 1;

if @task_category = 'INGEST'
begin
    if @file_layout is null or LTRIM(rtrim(@file_layout)) = ''
        throw 50000, 'file_layout cannot be null or empty for INGEST tasks', 1;
end

if @task_category = 'PREPROCESS'
BEGIN
    set @file_layout = nullif(trim(@file_layout), '');
    if @preprocess_pattern is null
        throw 50000, 'preprocess_pattern cannot be null for PREPROCESS tasks', 1;
end
--#endregion


--#region 2. Merge data into meta.task_configuration table
;with input_data as (
	select 
		ltrim(rtrim(@task_name))        as task_name,
        @task_category                  as task_category,
		ltrim(rtrim(@task_type))        as task_type,
        ltrim(rtrim(@worker_name))      as worker_name,
		ltrim(rtrim(@task_description)) as task_description,
        ltrim(rtrim(@table_name))       as table_name,
        nullif(ltrim(rtrim(@target_options)), '')   as target_options,
		ltrim(rtrim(@file_layout))      as file_layout,
        ltrim(rtrim(@source_folder))    as source_folder,
		ltrim(rtrim(@container_name))   as container_name,
        @preprocess_pattern             as preprocess_pattern, -- no trimming, could be spaces are valid part of the pattern
        @preprocess_worker_options      as preprocess_worker_options,
		@enabled as [enabled]
)

MERGE
	INTO meta.task_configuration as t
USING input_data s on (
	t.task_name = s.task_name 
	)
WHEN MATCHED and (
	-- looks complicated but this works for nullable columns: true is both are not null AND different or when ONE of them is null, but NOT when both are null
	   (ISNULL( NULLIF(t.enabled, s.enabled), NULLIF(s.enabled, t.enabled) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.task_description, s.task_description), NULLIF(s.task_description, t.task_description) ) IS NOT NULL)
    or (ISNULL( NULLIF(t.task_type, s.task_type), NULLIF(s.task_type, t.task_type) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.worker_name, s.worker_name), NULLIF(s.worker_name, t.worker_name) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.table_name, s.table_name), NULLIF(s.table_name, t.table_name) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.target_options, s.target_options), NULLIF(s.target_options, t.target_options) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.file_layout, s.file_layout), NULLIF(s.file_layout, t.file_layout) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.source_folder, s.source_folder), NULLIF(s.source_folder, t.source_folder) ) IS NOT NULL)
	or (ISNULL( NULLIF(t.container_name, s.container_name), NULLIF(s.container_name, t.container_name) ) IS NOT NULL)
    or (t.preprocess_pattern        IS DISTINCT FROM s.preprocess_pattern)
    or (t.preprocess_worker_options IS DISTINCT FROM s.preprocess_worker_options)

	)
THEN
	UPDATE SET  t.enabled                   = s.enabled,
                t.task_category             = s.task_category,
                t.task_name                 = s.task_name,
                t.task_type                 = s.task_type,
                t.worker_name               = s.worker_name,
                t.task_description          = s.task_description,
                t.table_name                = s.table_name,
                t.target_options            = s.target_options,
                t.file_layout               = s.file_layout,
                t.source_folder             = s.source_folder,
                t.container_name            = s.container_name,
                t.preprocess_pattern        = s.preprocess_pattern,
                t.preprocess_worker_options = s.preprocess_worker_options
WHEN NOT MATCHED BY TARGET THEN 
	insert (task_name,		  task_category,   task_description,	    task_type,      worker_name,    table_name,     target_options,   file_layout,    source_folder,      container_name,    preprocess_pattern,   preprocess_worker_options,  [enabled]) 
	values (s.task_name,	s.task_category, s.task_description,	    s.task_type,    s.worker_name,  s.table_name, s.target_options, s.file_layout,  s.source_folder,    s.container_name , s.preprocess_pattern, s.preprocess_worker_options, s.enabled)
OUTPUT 
    $ACTION as [action], 
    ISNULL(INSERTED.task_name,                 DELETED.task_name)                 AS task_name,
    ISNULL(INSERTED.task_category,             DELETED.task_category)             AS task_category,
	ISNULL(INSERTED.enabled,                   DELETED.enabled)                   AS [enabled],
    ISNULL(INSERTED.task_type,                 DELETED.task_type)                 AS task_type,
    ISNULL(INSERTED.worker_name,               DELETED.worker_name)               AS worker_name,
    ISNULL(INSERTED.task_description,          DELETED.task_description)          AS task_description,
    ISNULL(INSERTED.table_name,                DELETED.table_name)                AS table_name,
    ISNULL(INSERTED.target_options,            DELETED.target_options)            AS target_options,
    ISNULL(INSERTED.file_layout,               DELETED.file_layout)               AS file_layout,
    ISNULL(INSERTED.source_folder,             DELETED.source_folder)             AS source_folder,
    ISNULL(INSERTED.container_name,            DELETED.container_name)            AS container_name,
    ISNULL(INSERTED.preprocess_pattern,        DELETED.preprocess_pattern)        AS preprocess_pattern,
    ISNULL(INSERTED.preprocess_worker_options, DELETED.preprocess_worker_options) AS preprocess_worker_options
;
--#endregion

end

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_ais_get_landing_configuration.sql">
/*

    About the "locking" : we want to avoid that the landing automation starts processing 
    a certain dataset while the previous one is still not completed.

    So we will lock the configurations returned to the caller for the specified number of
    hours. Next usp_ais_get_landing_configuration calls will not return these configurations
    until the lock is either released by logging the result of the landing activity
    (using meta.usp_ais_log_landing) or the lock has expired.

    To ensure backwards compatibility and to allow engineers to also run the stored procedure
    without locking effects (as they might be debugging or testing), the lock_hours parameter
    defaults to 0. 

*/
create procedure meta.usp_ais_get_landing_configuration 
    -- mandatory: timestamp to be used for the drop folders
	@drop_timestamp datetime,
    -- optional filters
    @source_system varchar(50) = null,
    @source_environment varchar(20) = null,
    @interface_name varchar(50) = null,
    @correlation_id varchar(50) = null,
    -- if lock_hours is set, the procedure will lock the return configuration item for the given number of hours 
    -- (based on @drop_timestamp)
    @lock_hours int = 0
as
BEGIN

    -- set the debug variable during development or debugging
    declare @debug bit  = 0;
 
    declare @error_message nvarchar(max);

    -- format the drop timestamp eg 20241231_172059
    declare @drop_ts_string char(15) = FORMAT( @drop_timestamp, 'yyyyMMdd_HHmmss');

    -- get current environment
    declare @env varchar(20) = (SELECT value FROM STRING_SPLIT(db_name(), '-', 1) where ordinal = 1)
    if @env is null 
    BEGIN
        set @error_message = concat( 'Could not determine current environment from database name [', db_name(), ']');
        throw 50000, @error_message, 1;
    END

    -- create lock period
    if @lock_hours is null or @lock_hours &lt; 0
    begin
        set @error_message = concat( 'Invalid @lock_hours argument given: ', @lock_hours, '. Should be NOT NULL and positive');
        throw 50000, @error_message, 1;
    end
    else
    BEGIN

        declare @lock_until datetime2 = dateadd(hour, @lock_hours, @drop_timestamp);
        if @debug=1
        BEGIN
            print 'setting lock...'
            print '  lock_hours     = ' + convert(varchar, @lock_hours)
            print '  drop timestamp = ' + convert(varchar, @drop_timestamp, 121)
            print '  lock_until     = ' + convert(varchar, @lock_until, 121)
        END

        update meta.ais_landing_configuration
        set t_locked_until = @lock_until
        where is_enabled = 1
            and ( @source_system is null or @source_system = source_system )
            and ( @source_environment is null or @source_environment = source_environment )
            and ( @interface_name is null or @interface_name = interface_name )
            and ( t_locked_until is null or t_locked_until &lt; @drop_timestamp )
    END

    -- return enabled configurations
    select 
         source_system
        ,source_environment
        ,interface_name
		,source_path
        ,@env + 'dapstdala1' as target_storage
        ,@env + '-dap-rg-dala' as target_rg
        ,'landing' as target_container
        ,concat(source_system, '/', source_environment, '/', interface_name, '/' , @drop_ts_string , '/'   )as target_path -- can add container here as well?
        ,isnull(t_last_file_ts, '1900-01-01') as files_from_ts
        ,@drop_timestamp as drop_ts
        ,t_locked_until as locked_until
    from meta.ais_landing_configuration
    where is_enabled = 1
        and ( @source_system is null or @source_system = source_system )
        and ( @source_environment is null or @source_environment = source_environment )
        and ( @interface_name is null or @interface_name = interface_name )
        -- if lock_hours is set, limit to the ones we just marked
        and ( @lock_hours = 0 or t_locked_until = @lock_until )

END

GO

</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_ais_log_landing.sql">
/*

    Log information about (successfully) landed files.

    The purpose is threefold:
    * have a log about which file was landed when
    * update the "last_file_ts" watermark in the ais_landing_configuration table
    * logging the end of the landing process for an interface, releases the "lock" (if any) on the configuration 
      record.

    We use json parameter fields so we can over time dynamically and easily change what information is passed
    (without always having to change the column lists in calling and caller logic)

    NOTE:
    * @json_parameters, after validating that it's proper json, is converted to json datatype for logging,
      but most of the other logic is still doing classic openjson() etc on the varchar(max). Could probably
      be simplified but no time now...

    Parameters:

    @log_type:
        Success or Error indicator, accepted values 'SUCCESS' or 'ERROR', default is 'SUCCESS'

    @drop_timestamp: 
        The drop timestamp is used as an identifier to link this landing log execution to a "run" of the delivery processes.

    @source_system, @source_environment, @interface_name
        The triple key identifies the dataset which is being landed, there is a unique record in the configuration table
        for each combination of the three fields

    @json_parameters:
        A json object with following properties:
        - last_file_ts    | datetime | optional  | (max) UTC timestamp of the file(s) which were just landed. If not provided, max data from 
                                                    the files_dropped list will be used
        - error_message   | string   | optional  | error message or description
        - target_folder   | string   | mandatory | file path where file(s) were landed, only the folder part eg 'igt/dev/pdm/20230502_091030/'
        - pipeline_guid   | string   | optional  | guid of the pipeline execution, useful for debugging
        Example: 
        {
            "last_file_ts" : "2024-06-07 12:34:56",
            "target_folder" : "igt/dev/pdm/20230502_091030/""
            "pipeline_guid" : "xxxxxx-xxxx-xxxx-xxxx-xxxxx"
        }

    @files_dropped : 
        An array of JSON objects with information about the files dropped by this execution.
        The maximum "filedate" of the provided information will be used to update the "last_file_ts" watermark value in the
        ais_landing_configuration table.

        Each object should have following properties:
        - filename   | string    | mandatory | file path where file(s) were landed, only the folder part eg 'igt/dev/pdm/20230502_091030/'
        - filedate   | datetime  | optional  | UTC timestamp of the file

        Example: 
        [
            { "filename": "myfile.txt" },
            { "filename": "anotherfile.txt" }
        ]

    @correlation_id : 
        optional value which can be used to link the landing logs to across "applications", can be left to null

*/
create procedure meta.usp_ais_log_landing
    --mandatory
    @drop_timestamp datetime2,
    @source_system varchar(50),
    @source_environment varchar(20),
    @interface_name varchar(50),
    @json_parameters nvarchar(max),
    --optional
    @log_type varchar(20) = 'SUCCESS',
    @files_dropped nvarchar(max) = null,
    @correlation_id varchar(50) = null

as
BEGIN

    declare 
        @error_message       nvarchar(max),
        @proc_name           sysname = OBJECT_SCHEMA_NAME(@@PROCID) + '.' + OBJECT_NAME(@@PROCID),
        @proc_start          datetime2 = (SELECT SYSUTCDATETIME() AT TIME ZONE 'UTC' AT TIME ZONE 'Central Europe Standard Time'),
        @log_properties_base nvarchar(max) = null,
        @log_properties      nvarchar(max) = null,
        @num_files           int = null
        ;

    -- generate a correlation_id if not provided
    if @correlation_id is NULL
        set @correlation_id = NEWID()      

    -- generate set of log_properties which we'll allways use
    set @log_properties_base = meta.fn_add_json_property(@log_properties_base, 'correlation_id', @correlation_id);

    -- log the start of the proc
    set @log_properties = meta.fn_add_json_property(@log_properties_base, 'proc_start', convert(varchar(50), @proc_start, 121));
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'proc_executed_by', (select suser_sname()) );
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'drop_timestamp', convert(varchar(50), @drop_timestamp, 121));
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'source_system', @source_system);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'source_environment', @source_environment);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'interface_name', @interface_name);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'log_type', @log_type);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'json_parameters', @json_parameters);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'files_dropped', @files_dropped);
    exec meta.usp_proc_log 
        @log_type         = 'info'  ,
        @proc_name        = @proc_name,
        @statement_name   = 'proc_start',
        @message          = 'procedure started',
        @extra_parameters = @log_properties,
        @query_text       = null,
        @correlation_id   = @correlation_id

    -- check that combination of source_system + source_environment + interface_name exists in configuration table
    if not exists (
        select *
        from meta.ais_landing_configuration
        where   @source_system = source_system 
            and @source_environment = source_environment
            and @interface_name = interface_name    
    )    
    begin
        set @error_message = concat ( 'There is no configuration record for source_system [', @source_system, '], source_environment [', @source_environment, '], interface_name [', @interface_name, ']');
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;        
        throw 50000, @error_message, 1
    end 
    -- if @json_parameters is provided, it needs to be valid json
    if @json_parameters is not null and not((select isjson( @json_parameters, OBJECT )) = 1)
    BEGIN
        set @error_message = 'Provided json_parameters value is not a valid json object';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
        throw 500000,@error_message, 1
    end;
    --convert to json datatype, gets rid of whitespaces etc
    declare @json_parameters_json json = @json_parameters


    -- check that the log_type is valid
    set @log_type = upper(trim(@log_type));
    if @log_type is null or @log_type not in ('FAIL', 'SUCCESS')
    BEGIN
        set @error_message = 'Provided log_type value is not valid, accepted values: FAIL or SUCCESS';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
        throw 500000,@error_message, 1      
    end;

    -- check that the @files_dropped parameter is either valid array or null
    if @files_dropped is not null
    begin
        if isjson(@files_dropped, array) = 0
        begin
            set @error_message = 'Provided @files_dropped parameter is not a valid json array';
            exec meta.usp_proc_log 
                @log_type = 'error', 
                @proc_name = @proc_name, 
                @statement_name = 'input_validation', 
                @message = @error_message,
                @extra_parameters = @log_properties_base, 
                @query_text = null,
            @correlation_id   = @correlation_id;
            throw 50000, @error_message, 1;
        end
    end

    -- release the lock on the configuration record
        UPDATE meta.ais_landing_configuration 
        set 
            t_locked_until = null
        where   @source_system = source_system 
            and @source_environment = source_environment
            and @interface_name = interface_name    
        -- log the action in proc_log
        set @log_properties = meta.fn_add_json_property(@log_properties_base, 'source_system', @source_system);
        set @log_properties = meta.fn_add_json_property(@log_properties     , 'source_environment', @source_environment);
        set @log_properties = meta.fn_add_json_property(@log_properties     , 'interface_name', @interface_name);
        exec meta.usp_proc_log 
            @log_type         = 'info'  ,
            @proc_name        = @proc_name,
            @statement_name   = 'release_lock',
            @message          = 'released the configuration lock',
            @extra_parameters = @log_properties,
            @query_text       = null,
            @correlation_id   = @correlation_id;

    -- if the log_type was 'FAIL', then we just log this end exit
    -- (not further processing is needed)
    if @log_type = 'FAIL'
    begin
        -- create the landig_log entry
        insert into meta.ais_landing_log (
             log_type
            ,log_ts
            ,source_system
            ,source_environment
            ,interface_name
            ,last_file_ts
            ,drop_ts
            ,json_parameters
            ,files_dropped
        )
        SELECT  
             @log_type
            ,(SELECT SYSUTCDATETIME() AT TIME ZONE 'UTC' AT TIME ZONE 'Central Europe Standard Time') as log_ts
            ,@source_system as source_system
            ,@source_environment as source_environment 
            ,@interface_name as interface_name
            ,null as last_file_ts
            ,@drop_timestamp as drop_ts
            ,convert(varchar(max),@json_parameters_json)
            ,@files_dropped as files_dropped
        ;    
        -- log the action in proc_log
        set @log_properties = meta.fn_add_json_property(@log_properties_base, 'source_system', @source_system);
        set @log_properties = meta.fn_add_json_property(@log_properties     , 'source_environment', @source_environment);
        set @log_properties = meta.fn_add_json_property(@log_properties     , 'interface_name', @interface_name);
        set @log_properties = meta.fn_add_json_property(@log_properties     , 'num_files', @num_files);
        exec meta.usp_proc_log 
            @log_type         = 'info'  ,
            @proc_name        = @proc_name,
            @statement_name   = 'success',
            @message          = 'logged the landing error successfully',
            @extra_parameters = @log_properties,
            @query_text       = null,
            @correlation_id   = @correlation_id;
        return;
    end

    -- extract the required fields from @json_parameters
    declare @json_parameters_table table ( 
        last_file_ts  datetime2    null,
        target_folder varchar(500) null, 
        pipeline_guid varchar(100) null
    );
    insert into @json_parameters_table
    select 
        try_convert(datetime2, last_file_ts) as last_file_ts,
        nullif(ltrim(rtrim(target_folder)),'') as target_folder,
        nullif(ltrim(rtrim(pipeline_guid)), '') as pipeline_guid
    from openjson ( @json_parameters)
    WITH (   
        last_file_ts  varchar(100) 'lax $.last_file_ts',  
        target_folder varchar(500) 'lax $.target_folder',
        pipeline_guid varchar(100) 'lax $.pipeline_guid'
    );

    -- evaluate they are present and valid
    if (select count(*) from @json_parameters_table) = 0 
    begin
        set @error_message = 'No data in provided json parameters';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
            throw 50000, @error_message, 1;
    end
    if (select count(*) from @json_parameters_table) &gt; 1 
    begin
        set @error_message = 'Invalid data in provided json parameters';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
            throw 50000, @error_message, 1;
    end
    if (select target_folder from @json_parameters_table) is null
    begin
        set @error_message = 'target_folder property in json parameters is missing or invalid';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
            throw 50000, @error_message, 1;
    end
    if (select pipeline_guid from @json_parameters_table) is null
    begin
        set @error_message = 'pipeline_guid property in json parameters is missing or invalid';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
            throw 50000, @error_message, 1;
    end
    
    -- store the json parameters into variables if we need them as columns
    declare 
        @last_file_ts datetime2,
        @pipeline_guid as varchar(100)
    select 
        @last_file_ts = last_file_ts,
        @pipeline_guid = pipeline_guid
    from @json_parameters_table;

    -- extract the required fields from @files_dropped
    declare @files_dropped_table table ( 
        filename      varchar(500) null, 
        filedate      datetime2    null
    );
    insert into @files_dropped_table
    select 
        nullif(ltrim(rtrim(filename)),'') as filename,
        try_convert(datetime2, filedate) as filedate
    from openjson ( @files_dropped)
    WITH (   
        filename  varchar(100) 'lax $.filename',  
        filedate varchar(500) 'lax $.filedate'
    );    

    -- determine number of files dropped
    if @files_dropped is not null
    begin
        select @num_files = count(*) from @files_dropped_table;
    end

    -- if last_file_ts was not provided, then the entries in @files_dropped must have (valid) filedates
    if @last_file_ts is null and @num_files &gt; 0
    begin
        if exists ( select * from @files_dropped_table where filedate is null)
        BEGIN
            set @error_message = 'The entries in @files_dropped must have a "filedate" property unless last_file_ts is provided';
            exec meta.usp_proc_log 
                @log_type = 'error', 
                @proc_name = @proc_name, 
                @statement_name = 'input_validation', 
                @message = @error_message,
                @extra_parameters = @log_properties_base, 
                @query_text = null,
                @correlation_id   = @correlation_id;
            throw 50000, @error_message, 2;
        END
        select @last_file_ts  = max(filedate) from @files_dropped_table
    end

    -- if any files were dropped, save the @drop_timestamp as the "last_drop_ts"
    declare @last_drop_ts datetime2;
    if @num_files &gt; 0
        set @last_drop_ts = @drop_timestamp;
      
    -- insert landing_log entry
    insert into meta.ais_landing_log (
         log_type
        ,log_ts
        ,source_system
        ,source_environment
        ,interface_name
        ,last_file_ts
        ,drop_ts
        ,json_parameters
        ,files_dropped
    )
    SELECT  
         @log_type
        ,(SELECT SYSUTCDATETIME() AT TIME ZONE 'UTC' AT TIME ZONE 'Central Europe Standard Time') as log_ts
        ,@source_system as source_system
        ,@source_environment as source_environment 
        ,@interface_name as interface_name
        ,@last_file_ts as last_file_ts
        ,@drop_timestamp as drop_ts
        ,convert(varchar(max),@json_parameters_json)
        ,@files_dropped as files_dropped;

    --update the configuration item
    update meta.ais_landing_configuration set 
         t_last_file_ts = case when @num_files &gt; 0 then @last_file_ts else t_last_file_ts end
        ,t_last_drop_ts = case when @num_files &gt; 0 then @last_drop_ts else t_last_drop_ts end
        ,t_last_sync_ts = @drop_timestamp
    where   @source_system = source_system 
        and @source_environment = source_environment
        and @interface_name = interface_name    
    if @@rowcount = 0
    begin
        set @error_message = 'No record was updated in configuration table';
        exec meta.usp_proc_log 
            @log_type = 'error', 
            @proc_name = @proc_name, 
            @statement_name = 'input_validation', 
            @message = @error_message,
            @extra_parameters = @log_properties_base, 
            @query_text = null,
            @correlation_id   = @correlation_id;
        throw 50000, @error_message, 3;
    end

    -- log the action in proc_log
    set @log_properties = meta.fn_add_json_property(@log_properties_base, 'source_system', @source_system);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'source_environment', @source_environment);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'interface_name', @interface_name);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 'num_files', @num_files);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 't_last_file_ts', @last_file_ts);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 't_last_drop_ts', @last_drop_ts);
    set @log_properties = meta.fn_add_json_property(@log_properties     , 't_last_sync_ts', @drop_timestamp);
    exec meta.usp_proc_log 
        @log_type         = 'info'  ,
        @proc_name        = @proc_name,
        @statement_name   = 'success',
        @message          = 'configuration table updated successfully',
        @extra_parameters = @log_properties,
        @query_text       = null,
        @correlation_id   = @correlation_id;

end

GO

--exec tSQLt.run 't_usp_ais_log_landing'


</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_end_plan.sql">
/*

DESCRIPTION: 
End a plan with the current status depending on a given success flag.
If success_flag = 1: Current Status = 'SUCCESS'
If success_flag = 0: Current Status = 'FAILED'

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in the logging tables
	- current_status needs to be 'IN PROGRESS' to put it on 'SUCCESS'

*/

CREATE PROCEDURE [meta].[usp_end_plan]
	--mandatory
	@plan_id		int,					-- ID of the plan
	@success_flag   bit,					-- 1 if success, 0 if failure
	@run_id			int,					-- ID of the run

	--optional
	@pipeline_id    varchar(40)  = null,	-- ID of the Synapse pipeline
	@comment        varchar(max) = null		-- Optional comment

AS
BEGIN

	-- step 1 : validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if ( @plan_id is null)
	begin
		raiserror('plan_id parameter is mandatory', 11, 102 );
		return -1;
	end

	if ( @run_id is null)
	begin
		raiserror('run_id parameter is mandatory', 11, 102 );
		return 102;
	end

	if ( @success_flag is null  )
	begin
		raiserror('success_flag parameter is mandatory', 11, 103 );
		return -1; 
	end


	-- Step 2 : check if parameters exist in the logging tables (-3)
	if not exists ( select run_id from meta.log_runs where run_id = @run_id )
	begin
		raiserror ('The run with run_id ''%i'' could not be located in LOG_RUNS table', 11, 301, @run_id );
		return 301; 
	end

	if not exists ( select plan_id from meta.log_plans where plan_id = @plan_id )
	begin
		raiserror('The plan with plan_id ''%i'' could not be located in LOG_PLANS table', 11, 302, @plan_id );
		return 302; 
	end

	if not exists ( select plan_id, run_id from meta.log_run_plans where plan_id = @plan_id and run_id = @run_id)
	begin
		raiserror('The plan with plan_id ''%i'' could not be located in LOG_RUN_PLANS table for run_id ''%i''', 11, 303,  @plan_id, @run_id );
		return 303;
	end


	-- Step 3: Validate that the current status of the plan = 'in progress' when success_flag = 1
	declare @current_status nvarchar(20);
	select 
		@current_status  = isnull(current_status, 'NULL')
	from meta.log_plans where plan_id = @plan_id;

	if (@success_flag = 1 and  @current_status &lt;&gt; 'IN PROGRESS')
	begin
		-- a "success" log can only happen when previous status was "IN PROGRESS"
		raiserror('The plan with plan_id ''%i'' has an invalid current_status ''%s''', 16, 302, @plan_id, @current_status );
		return -3; 
	end

	if (@success_flag = 1)
	begin
		-- Count the number of unsuccessful tasks
		declare @unsuccessful_tasks int;
		set @unsuccessful_tasks = (select count(*) from meta.log_tasks where plan_id = @plan_id and coalesce(current_status, 'INCOMPLETE') &lt;&gt; 'SUCCEEDED')
		-- A plan can only be successful if all tasks have been successful
		if @unsuccessful_tasks &lt;&gt; 0
		begin
			raiserror('The plan with plan_id ''%i'' has still ''%i'' unsuccessful tasks', 16, 302, @plan_id, @unsuccessful_tasks );
			return -3;
		end 
	end


	-- step 4 : update meta.LOG_PLANS table
	begin try

		declare @result nvarchar(20);
		if (@success_flag = 1)
			set @result = 'SUCCEEDED';
		else 
			set @result = 'FAILED';

		update [meta].[log_plans] 
			set
				current_status = @result,
				-- azure sql database runs in UTC, but we want CEST
				end_ts = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time',
				--replace comment on success, append on failure
				comment = case when @success_flag=1 then @comment else isnull(comment + char(13),'') + @comment end
		where plan_id =  @plan_id 
		
		;

		if (@success_flag = 0)
			begin
				-- Set the 'IN PROGRESS' tasks to 'FAILED'
				update meta.log_tasks 
				set current_status = 'FAILED'
				where current_status = 'IN PROGRESS' and plan_id = @plan_id
				
				-- Set the remaining (unsuccessful) tasks to 'CANCELLED'
				update meta.log_tasks 
				set current_status = 'CANCELLED'
				where current_status &lt;&gt; 'SUCCEEDED' and current_status &lt;&gt; 'FAILED' and plan_id = @plan_id
			end
		-- all OK
		RETURN 0
	
	end try
	begin catch
		
		--rethrow the exception
		throw
		--return error code
		return -9
		;
			
	end catch

END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_end_run.sql">
/*

DESCRIPTION: 
End a run with the current status depending on a given success flag.
If success_flag = 1: Current Status = 'SUCCESS'
If success_flag = 0: Current Status = 'FAILED'

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in the logging tables
	- current_status needs to be 'IN PROGRESS' to put it on 'SUCCESS'

*/


CREATE PROCEDURE [meta].[usp_end_run]
	--mandatory
	@run_id			int,					-- ID of the run
	@success_flag	bit,					-- 1 if success, 0 if failure

	-- optional
	@pipeline_id	nvarchar(40) = null,			-- Synapse pipeline ID
	@comment		nvarchar(max) = null	-- Optional comment

AS

BEGIN

	-- step 1 : validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if ( @run_id is null)
	begin
		raiserror('run_id parameter is mandatory', 11, 102 );
		return -1;
	end
	if ( @success_flag is null  )
	begin
		raiserror('success_flag parameter is mandatory', 11, 103 );
		return -1; 
	end


	-- Step 2 : check if parameters exist in the logging tables (-3)
	if not exists ( select run_id from meta.log_runs where run_id = @run_id )
	begin
		raiserror ('The run with run_id ''%i'' could not be located in LOG_TASKS table', 11, 301, @run_id );
		return 301; 
	end

	-- Step 3: Validate that the current status of the run = 'in progress' when success_flag = 1
	declare 
		@current_status nvarchar(20);

	select @current_status  = isnull(current_status, 'NULL')
	from meta.log_runs 
	where run_id = @run_id;

	if @success_flag = 1 and  @current_status &lt;&gt; 'IN PROGRESS' 
	begin
		-- a "success" log can only happen when previous status was "IN PROGRESS"
		raiserror('The run with run_id ''%i'' has an invalid current_status ''%s''', 16, 302, @run_id, @current_status );
		return -3; 
	end


	-- step 4 : update meta.LOG_RUNS table
	begin try

		declare @result nvarchar(20);
		if @success_flag = 1  
			set @result = 'SUCCEEDED';
		else 
			set @result = 'FAILED';

		update [meta].[log_runs] 
			set
				current_status = @result,
				-- azure sql database runs in UTC, but we want CEST
				end_ts = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time',
				--replace comment on success, append on failure
				comment = case when @success_flag=1 then @comment else isnull(comment + char(13),'') + @comment end
		where run_id =  @run_id 
		
		;
		-- all OK
		RETURN 0
	
	end try

	begin catch
		
		--rethrow the exception
		throw
		--return error code
		return -9
		;
			
	end catch

END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_end_task.sql">
/*

DESCRIPTION: 
End a task with the current status depending on a given success flag.
If success_flag = 1: Current Status = 'SUCCESS'
If success_flag = 0: Current Status = 'FAILED'

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in the logging tables
	- current_status needs to be 'IN PROGRESS' to put it on 'SUCCESS'

*/

CREATE PROCEDURE [meta].[usp_end_task]
--mandatory
	@task_id              	int,					-- ID of the task to end
	@plan_id              	int,					-- ID of the plan executing the task
	@success_flag          	bit,					-- 1 if success, 0 of failure
--optional
	@pipeline_id            varchar(40)  = null,	-- ID of the Synapse pipeline
	@comment               	varchar(max) = null,	-- Optional comment
	@max_values             varchar(50) = null		-- ??
	--@RECORDCOUNT_INS       int = null,			-- Number of records inserted in the table
	--@RECORDCOUNT_UPD       int = null,			-- Number of records updated in the table
	--@RECORDCOUNT_DEL       int = null,			-- Number of records deleted from the table
	--@RECORDCOUNT_REJ       int = null,			-- Number of records rejected by the table
	--@RECORDCOUNT_STG       int = null				-- Number of records moved to raw

AS
BEGIN

	---- step 1 : validate mandatory input parameters
	---- check mandatory parameter: cannot be empty or NULL
	if ( @task_id is null)
	begin
		raiserror('task_id parameter is mandatory', 11, 101);
		return 101;
	end
	if ( @plan_id is null)
	begin
		raiserror('plan_id parameter is mandatory', 11, 102 );
		return 102;
	end
	if ( @success_flag is null  )
	begin
		raiserror('success_flag parameter is mandatory', 11, 103 );
		return 103; 
	end


	-- Step 2 : check if parameters exist in the logging tables (-3)
	if not exists ( select task_id from meta.log_tasks where task_id = @task_id )
	begin
		raiserror ('The task with task_id ''%i'' could not be located in LOG_TASKS table', 11, 301, @task_id );
		return 301; 
	end

	if not exists ( select plan_id from meta.log_plans where plan_id = @plan_id )
	begin
		raiserror('The task with plan_id ''%i'' could not be located in LOG_PLANS table', 11, 302, @plan_id );
		return 302; 
	end
	
	if not exists ( select plan_id, task_id from meta.log_plan_tasks where plan_id = @plan_id and task_id = @task_id)
	begin
		raiserror('The task with task_id ''%i'' could not be located in LOG_PLAN_TASKS table for plan_id ''%i''', 11, 303,  @task_id, @plan_id );
		return 303;
	end
	

	-- Step 3: Validate that the current status of the run = 'in progress' when success_flag = 1
	declare 
		@current_status nvarchar(20),
		@prev_pipeline_id varchar(50);
	select 
		@current_status  = isnull(current_status, 'NULL'),
		@prev_pipeline_id = isnull(pipeline_id, 'NULL')
	from meta.log_tasks where task_id = @task_id
	;

	if @success_flag = 1 and  @current_status &lt;&gt; 'IN PROGRESS' 
	begin
		-- a "success" log can only happen when previous status was "IN PROGRESS"
		raiserror('The task with task_id ''%i'' has an invalid current_status ''%s''', 16, 302, @task_id, @current_status );
		return -3; 
	end

	/*
	if @success_flag = 0 and @prev_pipeline_id &lt;&gt; @pipeline_id and  @current_status &lt;&gt; 'IN PROGRESS' 
	begin
		-- a "fail" log with a different execution GUID is only OK when previous status was "IN PROGRESS"
		raiserror('The task with task_id ''%i'' has an invalid current_status ''%s''', 16, 303, @task_id, @current_status );
		return -3; 
	end
	*/

	-- step 4 : update meta.log_tasks table
	begin try

		declare @result nvarchar(20);
		if @success_flag = 1  
			set @result = 'SUCCEEDED';
		else 
			set @result = 'FAILED';

		update [meta].[log_tasks] 
			set
				-- Record_Count_Ins = @RECORDCOUNT_INS,
				-- Record_Count_Upd = @RECORDCOUNT_UPD,
				-- Record_Count_Del = @RECORDCOUNT_DEL,
				-- Record_Count_Rej = @RECORDCOUNT_REJ,
				-- Record_Count_Stg = @RECORDCOUNT_STG,
				last_plan_id = @plan_id,
				pipeline_id = isnull(@pipeline_id, @prev_pipeline_id),
				current_status = @result,
				-- azure sql database runs in UTC, but we want CEST
				end_ts = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time',
				--replace comment on success, append on failure
				comment = case when @success_flag=1 then @comment else isnull(comment + char(13),'') + @comment end,
				max_value = @max_values
		where task_id = @task_id 
		
		;
		-- all OK
		RETURN 0
	
	end try
	begin catch
		
		--rethrow the exception
		throw
		--return error code
		return -9
		;
			
	end catch

END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_plans.sql">
/*

=======================================================
Author: Joachim Baert, Simon Plancke
Create date: 2024-08-28
Alter date:  2024-09-03
Description: Return a table of PENDING plans for @run_id and @plan_name
=======================================================

:param @run_id		[int]: 		The ID of the run that is initiating the plan
:param @plan_name	[varchar]: 	The name of the plan the run is initiating

=======================================================
*/

CREATE PROCEDURE [meta].[usp_get_plans]
	-- mandatory
	@run_id 	int,
	@plan_name 	nvarchar(100)

AS

BEGIN

	DECLARE @errormessage nvarchar(max)

	-- Step 1: validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if (@run_id is null)
	begin
		set @errormessage = 'run_id parameter is mandatory';
		throw 50000, @errormessage, 100;
	end;

	if (@plan_name is null)
	begin
		set @errormessage = 'plan_name parameter is mandatory';
		throw 50000, @errormessage, 100;
	end;

	--Step 2: Check if IDs exist in logging tables
	if (@run_id not in (select run_id from meta.log_runs))
	begin
		set @errormessage = 'run_id does not exist in LOG_RUNS table';
		throw 50000, @errormessage, 100;
	end;

	if (@run_id not in (select run_id from meta.log_run_plans))
	begin
		set @errormessage = 'run_id does not exist in LOG_RUN_PLANS table';
		throw 50000, @errormessage, 100;
	end;

	-- Step 3: Check if Plan Name exists in logging tables
	if (@plan_name not in (select plan_name from meta.log_run_plans where run_id= @run_id))
	begin
		set @errormessage =  'plan_name does not exist in LOG_RUN_PLANS table';
		throw 50000, @errormessage, 100;
	end;


	-- Select the plans to start for @run_id and @plan_name
	SELECT lp.plan_id, lp.plan_name
	FROM 	meta.log_plans as lp 
			inner join meta.log_run_plans as lrp 	on (lrp.plan_id = lp.plan_id)
	WHERE 	1=1
			AND lrp.run_id = @run_id
			AND lp.current_status in ('PENDING')
			AND lp.plan_name = @plan_name



END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_plan_metadata.sql">
/*

=======================================================
Author: Joachim Baert, Simon Plancke
Create date: 2024-08-28
Alter date:  2024-09-03
Description: Return a list of task objects to run for a plan and task_group
=======================================================

:param @run_id				[int]: 		The ID of the run that is initiating the plan
:param @plan_id 			[int]: 		The ID of the plan that is being run 
:param @task_group 			[varchar]: 	The name of the task group that is being run
:param @task_type 			[varchar]: 	The type of tasks that need to be returned (SYNAPSE_PIPELINE vs SPARK_NOTEBOOK)
:param @results_as_select 	[bit]: 		Boolean value, indicating whether a return value is needed (default = 1)

:return @num_tasks			[int]:		The total number of tasks to execute
:return @tasks				[varchar]:	A list of json objects, containing all the tasks to be executed.
=======================================================

DETAILED DESCRIPTION
	The stored procedure will return 2 things:
		A list of json objects, where each object resembles a task that needs to be run
		The total number of tasks that need to be run for the plan
	
	IF the task_type == 'SPARK_NOTEBOOK' or task_type == 'SYNAPSE_PIPELINE' 
		-&gt; Call usp: meta.usp_get_task_metadata
		-&gt; Filter the tasks on task_type and task_group and return the value for @tasks
	&lt;else if case can be added when new task_type options are introduced that require different metadata&gt;
	ELSE 
		-&gt; Return the total number of tasks to execute
	
=======================================================
*/


CREATE PROCEDURE [meta].[usp_get_plan_metadata]
	--mandatory
	@run_id int,
	@plan_id int,
	@task_group nvarchar(10),
	@task_type nvarchar(20),

	--optional
	@results_as_select bit = 1,

	--output
	@num_tasks int = null output,
	@tasks nvarchar(max) = null output

AS
BEGIN

	declare @errormsg nvarchar(max)

	-- step 1 validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if ( @run_id is null)
	begin;
		throw 50000, 'run_id parameter is mandatory', 100 ;
	end
	if ( @plan_id is null)
	begin;
		throw 50000, 'plan_id parameter is mandatory', 101 ;
	end
	if ( @task_group is null )
	begin;
		throw 50000, 'task_group parameter is mandatory', 102 ;
	end
	if ( @task_type is null )
	begin;
		throw 50000, 'task_type parameter is mandatory', 103 ;
	end

	--Step 2: Check if IDs exist in logging tables
	if ( @run_id not in (select run_id from meta.log_runs) )
	begin;
		raiserror ('The task with run_id ''%i'' could not be located in LOG_RUNS table', 11, 301, @run_id );
		return 301; 
	end

	if ( @plan_id not in (select plan_id from meta.log_plans) )
	begin;
		raiserror('The task with plan_id ''%i'' could not be located in LOG_PLANS table', 11, 302, @plan_id );
		return 302;
	end

	if not exists ( select plan_id, run_id from meta.log_run_plans where plan_id = @plan_id and run_id = @run_id)
	begin
		raiserror('The task with plan_id ''%i'' could not be located in LOG_RUN_PLANS table for run_id ''%i''', 11, 303,  @plan_id, @run_id );
		return 303;
	end


	-- IF: Case is foreseen
	-- Dev-Info: If there would be a need, a separate case could be made for the different task_types, each returning different metadata
    if @task_type in ('SPARK_NOTEBOOK', 'SYNAPSE_PIPELINE')
    begin
		
		-- Execute stored procedure 'meta.usp_get_tasks' and return the list of tasks as @tasks 
        EXEC meta.usp_get_tasks 
			@run_id = @run_id, 
			@plan_id = @plan_id, 
			@task_group = @task_group, 
			@task_type = @task_type, 
			@tasks = @tasks output, 
			@num_tasks = @num_tasks output

		-- Return the tasks 
		if @results_as_select = 1
			select
				@tasks		as tasks,
				@num_tasks	as num_tasks

			return
    end
	
	-- ELSE: Return number of tasks that are pending for the plan, task_group, and task_type
    else
	begin 
		set @num_tasks = (
			SELECT count(*)
			FROM meta.log_plan_tasks as b
				inner join meta.log_tasks as j on ( j.task_id = b.task_id)
			where 1=1
				and b.task_group = @task_group
				and b.plan_id = @plan_id
				and b.task_type = @task_type
				and j.current_status in ('PENDING')
				)

		-- Return the number of tasks
		if @results_as_select = 1
			select
				@num_tasks as num_tasks
	end

end

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_preprocessing_info.sql">
/*

=======================================================
Author: Simon Plancke
Create date: 2024-12-11
Alter date:  -
Description: Return a plan_name and task_filter to determine which tasks to run
=======================================================

:param @trigger_file_path	[varchar]: 	 path to the file that triggered the Synapse Blob Trigger
:param @trigger_file_name   [varhcar]: 	The name of the file that triggered the Synapse Blob Trigger

:return @plan_name			[varchar]:	The name of the plan to run
:return @task_filter		[varchar]:	SQL-regex filter to determine which plans to log
=======================================================
*/


CREATE PROCEDURE [meta].[usp_get_preprocessing_info]
	-- mandatory
	@trigger_file_path 	nvarchar(max),

	-- optional
	@trigger_file_name 	nvarchar(100) = null,

	-- result
	@task_filter varchar(100) = null output,
	@plan_name			varchar(100) = null output

AS

BEGIN
	
	DECLARE @error_message nvarchar(max);
	DECLARE @preprocessing_count int;

	-- Check valid parameters
	if (@trigger_file_path is null)
	begin
		set @error_message =  'trigger_file_path is mandatory and cannot be null' ;
		throw 50000, @error_message, 1;
	end

	begin try
		-- Set plan_name
		set @plan_name = 'preprocessing'

		-- Get number of tasks that match with the trigger_file_path and are enabled in task_config and plan_task_config
		set @preprocessing_count = (
			SELECT count(*)
			FROM meta.plan_task_configuration
			WHERE   1=1
					AND plan_name = @plan_name
					AND [enabled] = 1
					AND task_name in (
						SELECT task_name
						FROM meta.task_configuration
						WHERE   1=1
								AND @trigger_file_path like concat('%', preprocess_pattern, '%')
								AND task_category = 'PREPROCESS'
								AND [enabled] = 1
						)
		)

		-- If too little or too many tasks, throw error
		if (@preprocessing_count &lt;&gt; 1)
		begin
			set @error_message =  concat('Only allowed to match with one task, but matched with ', @preprocessing_count, ' tasks')  ;
			throw 50000, @error_message, 1;
		end

		-- If onyl one task, return the name of the task
		set @task_filter = (
			SELECT task_name
			FROM meta.plan_task_configuration as plantaskconfig
			WHERE   1=1
					AND plan_name = @plan_name
					AND [enabled] = 1
					AND task_name in (
						SELECT task_name
						FROM meta.task_configuration
						WHERE   1=1
								AND @trigger_file_path like preprocess_pattern
								AND task_category = 'PREPROCESS'
								AND [enabled] = 1
						)
		)	

		-- return output values
		select @task_filter as task_filter, @plan_name as plan_name

	end try

	begin catch
		
		--rethrow the exception
		throw
		--return error code
		return -9
		;
			
	end catch
END

GO;
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_tasks.sql">
/*

=======================================================
Author: Joachim Baert, Simon Plancke
Create date: 2024-08-28
Alter date:  2024-09-03
Description: Return all pending tasks for plan_id, task_group, and task_type as a list of json objects
=======================================================

:param @run_id				[int]: 		The ID of the run that is initiating the plan
:param @plan_id 			[int]: 		The ID of the plan that is being run 
:param @task_group 			[varchar]: 	The name of the task group that is being run
:param @task_type 			[varchar]: 	The type of tasks that need to be returned (SYNAPSE_PIPELINE vs SPARK_NOTEBOOK)
:param @results_as_select 	[bit]: 		Boolean value, indicating whether a return value is needed (default = 1)

:return @num_tasks			[int]:		The total number of tasks to execute
:return @tasks				[varchar]:	A list of json objects, containing all the tasks to be executed.
=======================================================
*/

CREATE PROCEDURE [meta].[usp_get_tasks]
	--mandatory
	@run_id					int,
	@plan_id				int,
	@task_group				nvarchar(10),
	@task_type				nvarchar(20),
	--optional
	@results_as_select		bit = 1,

	--result
	@num_tasks				int = null output,
	@tasks					nvarchar(max) = null output

AS
BEGIN

	declare @errormsg nvarchar(max)

	-- step 1 validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if ( @run_id is null)
	begin;
		throw 50000, 'run_id parameter is mandatory', 100 ;
	end
	if ( @plan_id is null)
	begin;
		throw 50000, 'plan_id parameter is mandatory', 101 ;
	end
	if ( @task_group is null )
	begin;
		throw 50000, 'task_group parameter is mandatory', 102 ;
	end
	if ( @task_type is null )
	begin;
		throw 50000, 'task_type parameter is mandatory', 103 ; 
	end

	-- step 2: check if IDs exist
	if ( @run_id not in (select run_id from meta.log_runs) )
	begin;
		set @errormsg = concat('run_id parameter "', @run_id, '" does not exist in the LOG_RUNS table');
		throw 50000, @errormsg, 200;
	end

	if ( @plan_id not in (select plan_id from meta.log_plans) )
	begin;
		set @errormsg = concat('plan_id parameter "', @plan_id, '" does not exist in the LOG_PLANS table');
		throw 50000, @errormsg, 201;
	end

	--Joachim: I've removed this check because it's a valid scenario when a plan is started with no tasks
	-- if ( @plan_id not in (select plan_id from meta.log_plan_tasks where plan_id = @plan_id) )
	-- begin;
	-- 	set @errormsg = concat('plan_id parameter "', @plan_id, '" does not exist in the [log_plan_tasks] table');
	-- 	throw 50000, @errormsg, 201;
	-- end

	-- step 3: Return values
	if @results_as_select = 1
	-- Return list of tasks and total number of tasks to execute
	begin
		set @tasks = (
			SELECT 
				lpt.*,
				lt.current_status
			FROM meta.log_plan_tasks as lpt
			inner join meta.log_tasks as lt on ( lt.task_id = lpt.task_id)
			where 1=1
				and lpt.task_group = @task_group
				and lpt.plan_id = @plan_id
				and lpt.task_type = @task_type
				-- and lpt.Original_plan_id = @plan_id
				and lt.current_status in ('PENDING')
			order by task_id
			FOR JSON PATH

		);
		
		set @num_tasks = (
			SELECT count(*)
			FROM meta.log_plan_tasks as lpt
			inner join meta.log_tasks as lt on ( lt.task_id = lpt.task_id)
			where 1=1
				and lpt.task_group = @task_group
				and lpt.plan_id = @plan_id
				and lpt.task_type = @task_type
				--and lpt.Original_plan_id = @plan_id
				and lt.current_status in ('PENDING')
		)
	end

	else
	-- Return total number of tasks to execute
	begin
		set @num_tasks = (
			SELECT count(*)
			FROM meta.log_plan_tasks as lpt
			inner join meta.log_tasks as lt on ( lt.task_id = lpt.task_id)
			where 1=1
				and lpt.task_group = @task_group
				and lpt.plan_id = @plan_id
				--and lpt.Original_plan_id = @plan_id
				and lt.current_status in ('PENDING')
			)
		;

	end


	-- all OK
	RETURN 0

END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_task_last_loaded_file.sql">
/*

DESCRIPTION: 
Retrieve the last successfully processed extended_filename for a given task.
Returns empty dataset when there's no previous sucessfull execution

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in the logging tables
	- current_status needs to be 'SUCCEEDED'

*/

CREATE PROCEDURE [meta].[usp_get_task_last_loaded_file]
    @task_name        varchar(50)

as
begin

    declare @error_message nvarchar(max);

    if @task_name is null
    begin
        set @error_message =  '@task_name is mandatory and cannot be null' ;
        throw 50000, @error_message, 1;
    end

    if (@task_name not in (select task_name from meta.task_configuration where task_name = @task_name))
    begin	
        set @error_message =   concat('task_name "',@task_name,'" not found in TASK_CONFIGURATION table' );
        throw 50000, @error_message, 100;
    end;

    --select the as complete possible filename to be used
    SELECT  [filename] = extended_filename 
    FROM    meta.log_files 
    where   task_id in (
        select max(task_id)
        from  meta.log_tasks  x
        where x.current_status  = 'SUCCEEDED'
        and task_name         = @task_name
    )


end;
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_get_task_metadata.sql">
/*

DESCRIPTION
Get all the metadata needed to execute a task

CHECKS
	- Mandatory parameters are not empty
	- IDs exist in the logging tables

*/


create proc [meta].[usp_get_task_metadata]
    @task_id        int

as
begin

declare @error_message nvarchar(max);

if @task_id is null
begin
	set @error_message =  '@task_id is mandatory and cannot be null' ;
	throw 50000, @error_message, 1;
end

if (@task_id not in (select task_id from meta.log_tasks where task_id= @task_id))
begin	
	set @error_message =   'task_id does not exist in LOG_TASKS table' ;
	throw 50000, @error_message, 100;
end;

--------------------------------------------------------------------------------------------
-- retrieve worker_name and task_name from task_configuration
--------------------------------------------------------------------------------------------
declare @worker_name   	varchar(100),
        @task_name  	varchar(100);
select 
	@worker_name = tc.worker_name,
	@task_name = tc.task_name
from meta.task_configuration tc
inner join meta.log_tasks lt on (lt.task_name = tc.task_name)
where lt.task_id = @task_id

--------------------------------------------------------------------------------------------
-- the task doesn't exist, or is not configured as ADF_worker_name type,
--------------------------------------------------------------------------------------------
if @worker_name is null
begin
	set @error_message = concat( 'Could not locate worker_name information for task_id ', @task_id);
	throw 50000, @error_message, 1 ;
end



--------------------------------------------------------------------------------------------
-- create json answer for worker_name: IngestionWorker &amp; FilWorker
--------------------------------------------------------------------------------------------

if @worker_name IN ('IngestionWorker', 'FilWorker')
begin
	-- ==================
	-- DECLARE
	-- ==================
	---- Declare file_layout for easy look-up (similar keys)
	declare @file_layout				nvarchar(100)

	---- For JSON-objects
	declare @task_config				nvarchar(max)
	declare @source_config				nvarchar(max)
	declare @source_column_config		nvarchar(max)
	declare @source_check_config		nvarchar(max)

	-- ==================
	-- SET
	-- ==================
	---- Set file_layout so it can be used for easy look-up
	set @file_layout = (select file_layout
						from meta.task_configuration
						where task_name = @task_name)

	---- Set the JSON-objects to be returned
	set @task_config = (
		select task_name, task_type, worker_name, file_layout, source_folder, container_name, table_name, target_options, [enabled]
		from meta.task_configuration
		where file_layout = @file_layout
		FOR JSON PATH
	)

	set @source_config = (	
		select 	file_pattern
			, 	file_extension
			, 	file_kind
			, 	column_delimiter
			, 	row_delimiter
			, 	escape_character
			, 	quote_character
			, 	header
			, 	skip_first_lines
			,   source_conditions
		from meta.source_configuration
		where file_layout  = @file_layout
		FOR JSON PATH
	)

	set @source_column_config =(	
		select source_name, sink_name, column_order, dimension, data_type, column_info
		from meta.source_column_configuration
		where file_layout  = @file_layout
		FOR JSON PATH
	)

	set @source_check_config = (	
		SELECT check_name, config_params
		FROM meta.source_check_configuration
		WHERE file_layout = @file_layout
		AND [enabled] = 1
		FOR JSON PATH
	)

	-- ==================
	-- RESULT
	-- ==================
	SELECT 	@task_config as task_config,
			@source_config as source_config,
			@source_column_config as source_column_config,
			@source_check_config as source_check_config

	return
end

--------------------------------------------------------------------------------------------
-- create json answer for worker_name: DummyWorker
--------------------------------------------------------------------------------------------

-- this is an empty test worker_name used only for testing the execution
-- framework, it doesn't need any configuration
else if @worker_name = 'DummyWorker'
begin
	return 
end


--------------------------------------------------------------------------------------------
-- create json answer for worker_name: pl_unzip_worker
--------------------------------------------------------------------------------------------

else if @worker_name = 'pl_unzip_worker'
begin
	-- ==================
	-- DECLARE
	-- ==================
	---- Declare file_layout for easy look-up (similar keys)
	declare @file_layout_unzip				nvarchar(100)

	---- For JSON-objects
	declare @task_config_unzip				nvarchar(max)
	declare @source_config_unzip			nvarchar(max)
	declare @source_check_config_unzip		nvarchar(max)

	-- ==================
	-- SET
	-- ==================
	---- Set file_layout so it can be used for easy look-up
	set @file_layout_unzip = (select file_layout
						from meta.task_configuration
						where task_name = @task_name)

	---- Set the JSON-objects to be returned
	set @task_config_unzip = (
		select task_name, task_type, worker_name, file_layout, source_folder, container_name, table_name, [enabled]
		from meta.task_configuration
		where file_layout = @file_layout_unzip
		FOR JSON PATH
	)

	set @source_config_unzip = (	
		select file_pattern, file_extension, column_delimiter, row_delimiter, escape_character, quote_character, header
		from meta.source_configuration
		where file_layout  = @file_layout_unzip
		FOR JSON PATH
	)

		set @source_check_config_unzip = (	
		SELECT check_name, config_params 
		FROM meta.source_check_configuration
		WHERE file_layout = @file_layout_unzip
		AND [enabled] = 1
		FOR JSON PATH
	)

	-- ==================
	-- RESULT
	-- ==================
	SELECT 	@task_config_unzip as task_config,
			@source_config_unzip as source_config,
			@source_check_config_unzip as source_check_config

	return
end


--------------------------------------------------------------------------------------------
-- task is configured as an ADF_worker_name, but the target_name is not supported here
--------------------------------------------------------------------------------------------


set @error_message = concat( 'Unsupported target_name: ''', @worker_name, '''');
throw 50000, @error_message, 1 ;

end

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_new_file.sql">
/*

=======================================================
Description: Prepare the logging tables to ingest a new file
=======================================================

:param @task_id 			[int]: 		The ID of the task that is being run
:param @plan_id 			[int]: 		The ID of the plan that is being run
:param @filename 			[varchar]: 	The name of the file that is being run
:param @extended_filename	[varchar]: 	The extended name of the file that is being run : filename + '_' + timestamp_folder\n"
:param @info_message			[varchar]:	The path where the file is located in the landing zone

	optional
:param @no_select 	[bit]: 		Boolean indication of whether the stored procedure should return a final select statement

:return @file_id 	[data type]: [description of return value (as a recordset)]
=======================================================

DETAILED DESCRIPTION
	The stored procedure will log a new file in meta.log_file
	
	IF the file is new :
		It will create a new record 
	
	ELSE If the extended_filename already exists for this job:
		It will update the existing record by setting the info field to null

=======================================================
*/


CREATE PROCEDURE [meta].[usp_new_file]
(
	-- mandatory
	@task_id int,
	@plan_id int,
	@filename nvarchar(max),
	@extended_filename nvarchar(max),
	@info_message nvarchar(max),

	-- optional
	@no_select bit = 0 -- set this boolean to 1 to avoid returning the output resultset
)
AS
BEGIN
	
	declare 
		@errormsg nvarchar(max),
		@file_id int

	-- Step 1: Make sure all mandatory parameters are provided
	if ( @filename is null)
	begin
		set @errormsg = 'filename parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @task_id is null)
	begin
		set @errormsg = 'task_id parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @plan_id is null)
	begin
		set @errormsg = 'plan_id parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @extended_filename is null)
	begin
		set @errormsg = 'extended_filename parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @info_message is null)
	begin
		set @errormsg = 'info_message parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	-- Step 2: Make sure file is not already being tracked
	-- check if extended_filename already have some tracked progres, because then we throw an error
	if ( @extended_filename in (select [extended_filename] from meta.log_files where archive_status ='SUCCEEDED') )
	begin
		set @errormsg = concat ( 'extended_filename ''', @extended_filename, ''' already exists in the log_files table with recorded progress');
		throw 50000, @errormsg, 201;
	end

	-- Step 3: Check if IDs are in log tables
	if not exists ( select task_id from meta.log_tasks where task_id = @task_id )
	begin
		raiserror ('The task with task_id ''%i'' could not be located in LOG_TASKS table', 11, 301, @task_id );
		return 301; 
	end

	if not exists ( select plan_id from meta.log_plans where plan_id = @plan_id )
	begin
		raiserror('The task with plan_id ''%i'' could not be located in LOG_PLANS table', 11, 302, @plan_id );
		return 302; 
	end
	
	if not exists ( select plan_id, task_id from meta.log_plan_tasks where plan_id = @plan_id and task_id = @task_id)
	begin
		raiserror('The task with task_id ''%i'' could not be located in LOG_PLAN_TASKS table for plan_id ''%i''', 11, 303,  @task_id, @plan_id );
		return 303;
	end

	-- Step 4: Start logging features
	-- check if extended_filename already exists for this job (if yes, we will reuse the entry)
	declare @existing_file_id int = (
		select [file_id] from meta.log_files where extended_filename = @extended_filename and (COALESCE(archive_status, 'NOT INGESTED') &lt;&gt; 'SUCCEEDED')
		);

	-- If the file is new, the existing_file_id should be NULL -&gt; Create a new record
	if @existing_file_id is null
	begin
		-- create a new record
		INSERT INTO meta.log_files (task_id, plan_id, [filename], extended_filename, registration_ts,landing_ts,landing_info,landing_status)
		VALUES (@task_id, @plan_id, @filename, @extended_filename, getdate(), getdate(),@info_message,'SUCCEEDED');
		set @file_id = SCOPE_IDENTITY();
	end

	-- If the file is not new, the existing_file_id cannot be NULL -&gt; update existing record
	else
	begin
		-- update the existing record
		set @file_id = @existing_file_id;
		update meta.log_files 
		set
            upload_ts       = getdate(),
            raw_info      	= null,
            silver_info   	= null,
            archive_info  	= null,
            raw_status   	= null,
            silver_status   = null,
            archive_status  = null,
            raw_ts       	= null,
            silver_ts       = null,
            archive_ts      = null
			-- record_count_init = null,
			-- record_count_stg  = null,
			-- record_count_ins  = null,
			-- record_count_del  = null,
			-- record_count_upd  = null,
			-- record_count_rej  = null,
			-- copied_message	 = null,
		where [file_id] = @file_id
	end

	-- return the result as a recordset
	if not (@no_select = 1)
	begin
		select @file_id as [file_id], @task_id as task_id, @plan_id as plan_id, @filename as [filename], @extended_filename as extended_filename;
	end

END
GO



</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_new_run.sql">
/*

=======================================================
Author: Joachim Baert, Simon Plancke
Create date: 2024-08-28
Alter date:  2024-09-03
Description: Prepare the logging tables to start a new run of the ingestion framework
=======================================================

:param @pipeline_id [varchar]: 	The ID of the Synapse pipeline that started the run
:param @new_plan 	[bit]: 		Boolean indication of whether the run is logging a new plan or rerunning an old (failed) plan
:param @plan_name 	[varchar]: 	The name of the plan that is being run
:param @debug 		[bit]: 		Boolean indication of whether the stored procedure is being run in debug-mode
:param @no_select 	[bit]: 		Boolean indication of whether the stored procedure should return a final select statement
:param @task_filter	[varchar]:  (Regex-like) string value that will filter the tasks of a plan, allowing for task-subset executions

:return @run_id 	[data type]: [description of return value]
=======================================================

DETAILED DESCRIPTION
	The stored procedure will log a new run in meta.log_runs
	
	IF parameter new_plan = 1
		Log a new plan in meta.log_plans
		Log a new row in meta.log_run_plans 
		For each task of the plan: Log new tasks in meta.log_tasks
		For each task: Log a new row in meta.log_plan_tasks
	
	ELSE IF parameter new_plan = 0:
		Get the plan_id of the last plan with name @plan_name
		IF current_status = 'SUCCEEDED':
			Do nothing
			Synapse pipeline will get the list of 'PENDING' plans which should be empty
		ELSE IF current_status = 'FAILED' or 'CANCELLED'
			Log a new entry in meta.log_run_plans with the new run ID and the previous plan ID
			Reset the current_status to 'PENDING' for failed/cancelled plan and tasks 

=======================================================
*/

CREATE   PROCEDURE [meta].[usp_new_run]

	@pipeline_id	nvarchar(40),
	@new_plan		bit = 1,
	@plan_name		nvarchar(100),
	
	@run_id			int = null OUTPUT,

	@task_filter	nvarchar(100) = null,
	@debug			bit = 0,
	@no_select		bit = 1

AS

BEGIN
	-- CHECK MANDATORY PARAMETERS
	-- check mandatory parameter: 'pipeline_id' cannot be empty or NULL
	if ( @pipeline_id is null or @pipeline_id = '')
	begin
		raiserror('pipeline_id parameter is mandatory', 11, 101 );
		return -1;
	end


	-- check mandatory parameter: 'plan_name' cannot be empty or NULL
	if ( @plan_name is null or @plan_name = '')
	begin
		raiserror('plan_name parameter is mandatory', 11, 102 );
		return -1;
	end

	-- CHECK METADATA CONFIGURATION
	-- plan_name needs to exist in the meta.plan_configuration table
	if ( @plan_name not in (select plan_name from meta.plan_configuration) )
	begin
		raiserror('plan_name parameter "%s" does not exist in the PLAN_CONFIGURATION table', 11, 201, @plan_name);
		return -2; 
	end

	-- plan_name needs to be [enabled] to allow execution 
	if ( @plan_name not in (select plan_name from meta.plan_configuration where [enabled] = 1) )
	begin
		raiserror('plan_name parameter "%s" is not enabled in the PLAN_CONFIGURATION table', 11,202, @plan_name );
		return -2; 
	end

	begin try

		-- Azure sql database runs in UTC, but we want CEST
		declare @registration_ts datetime = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time';

		begin tran;

		-- Add new run to meta.log_runs with status 'PENDING'
		insert into meta.log_runs(
			pipeline_id, new_plan, plan_name, current_status, registration_ts
		)
		select	@pipeline_id,
				@new_plan,
				@plan_name,
				'PENDING',
				@registration_ts

		-- Get the ID of the run that has been inserted into the meta.log_runs
		set @run_id = SCOPE_IDENTITY();

		-- If new_plan = True -&gt; Add new rows to logging tables
		if @new_plan = 1
		begin
			
			-- Insert a new plan in table meta.log_plans with status PENDING
			insert into meta.log_plans(
				pipeline_id,
				plan_name,
				registration_ts,
				current_status,
				run_id
			)
			select	@pipeline_id,
					@plan_name,
					@registration_ts,
					'PENDING',
					@run_id

			-- Get the ID of the plan that has been inserted into the meta.log_plans
			declare @plan_id int = SCOPE_IDENTITY();

			-- Insert a new run-plan instance in meta.log_run_plans
			insert into meta.log_run_plans
			values (@run_id, @plan_id, @plan_name, @run_id)

			-- Insert a set of new tasks in meta.log_tasks with status 'PENDING'
			-- Only insert the enabled tasks for the plan that is being initialized
			DECLARE @task_filter_string nvarchar(100);
			set @task_filter_string = CASE WHEN @task_filter IS NULL then '%' ELSE @task_filter END;
			
			insert into meta.log_tasks (
				task_name, 
				plan_id, 
				current_status, 
				registration_ts 
			)
			select 
				plan_task.task_name, 
				@plan_id as plan_id,
				'PENDING' as current_status,
				@registration_ts as registration_ts
			from meta.plan_task_configuration as plan_task
			inner join meta.task_configuration as task  on plan_task.task_name = task.task_name
			where plan_task.plan_name = @plan_name
				and task.[enabled] = 1
				and plan_task.enabled=1
				and task.task_name like @task_filter_string
			order by plan_task.task_sequence
			;

			-- Insert a set of new plan-task instances in meta.log_plan_tasks
			-- Select the relevant metadata from the meta.configuration tables
			with plan_task_data as (
				select 	distinct plan_task.task_name, 
						plan_task.task_sequence, 
						plan_task.task_group, 
						task.task_type
				from meta.plan_task_configuration as  plan_task
				inner join meta.task_configuration as task 		on ( task.task_name = plan_task.task_name )
				where plan_name = ( select plan_name from meta.log_plans where plan_id = @plan_id )
			)

	
			insert into meta.log_plan_tasks (
				plan_id, 
				plan_name, 
				task_id, 
				task_group, 
				task_type, 
				task_name, 
				worker_name, 
				task_sequence, 
				original_plan_id
			)
			SELECT 	@plan_id, 
					plan_name, 
					plan_task_log_data.task_id, 
					plan_task_log_data.task_group, 
					plan_task_log_data.task_type, 
					plan_task_log_data.task_name, 
					plan_task_log_data.worker_name, 
					plan_task_data.task_sequence, 
					plan_task_log_data.plan_id as original_plan_id
			FROM (
				-- First select all the configuration data that is relevant for logging
				SELECT 
					task.task_id,
					task.plan_id,
					task.task_name,
					task.current_status,
					[plan].plan_name, 
					plan_task.task_group,
					task_config.task_type,
					task_config.worker_name
				from meta.log_tasks 					as task
				inner join meta.log_plans 				as [plan]			on ( task.plan_id = [plan].plan_id )
				inner join meta.plan_task_configuration as plan_task	on ( [plan].plan_name = plan_task.plan_name and task.task_name = plan_task.task_name )
				inner join meta.task_configuration 		as task_config	on ( plan_task.task_name = task_config.task_name )
				inner join meta.plan_configuration 		as plan_config	on ( plan_task.plan_name = plan_config.plan_name )
			) as plan_task_log_data
			-- Filter on the tasks within the plan
			inner join plan_task_data on (plan_task_data.task_name = plan_task_log_data.task_name)
			where 1=1
				-- Filter on the tasks that were initialized for the current plan_id
				and plan_task_log_data.plan_id = @plan_id
				-- Filter on tasks having status 'PENDING'
				and current_status in ('PENDING')
			order by 	plan_task_data.task_sequence ASC, 
						plan_task_log_data.task_name DESC, 
						plan_task_log_data.task_id ASC
			;

		end

		if @new_plan = 0
		begin

			-- Create a temporary table
			create table #latest_plan (
				run_id int,
				plan_id int,
				plan_name varchar(50),
				original_run_id int
			);

			-- Get the last plan-instance that failed for @plan_name
			with latest_plan_cte as (
				select  top 1 @run_id as run_id, plan_id, plan_name, run_id as original_run_id
				from	meta.log_plans
				where	1=1
						and run_id &lt; @run_id
						and current_status in ('FAILED')
						and plan_name = @plan_name
				order by original_run_id desc
			)
			-- Add the instance to a temporary table
			insert into #latest_plan
			select * from latest_plan_cte;

			-- Add the new run-plan to meta.log_run_plans
			insert into meta.log_run_plans(
				run_id,
				plan_id,
				plan_name,
				original_run_id
			)
			select * from #latest_plan;

			-- Reset the status of the plan to 'PENDING'
			update meta.log_plans
			set current_status = 'PENDING',
				start_ts = NULL,
				end_ts = NULL
			where plan_id = (select plan_id from #latest_plan);

			-- Reset the status of the FAILED/CANCELLED tasks to 'PENDING'
			update meta.log_tasks
			set current_status = 'PENDING',
				start_ts = NULL,
				end_ts = NULL
			where 	1=1
					AND plan_id = (select plan_id from #latest_plan)
					AND current_status in ('FAILED', 'CANCELLED')

		end;
		
		--successful transaction
		commit tran;

		--output the new run info
		if (@no_select = 0)
		begin
			select * from meta.log_runs where run_id = @run_id;
		end

		RETURN 0

	-- successful execution
	end try

	begin catch
		rollback tran;
		declare @MSG varchar(max) = 'Scheduling plan failed -- message: ' + ERROR_MESSAGE();
		raiserror( @MSG , 11, 1 );
		return -9
		;
			
	end catch

END;


GO



</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_proc_log.sql">
create procedure meta.usp_proc_log 
	--mandatory
    @proc_name        sysname,
	--optional
	@log_type         varchar(20) = 'info',
    @statement_name   varchar(100)  = null,
	@message          nvarchar(max) = null,
	@extra_parameters nvarchar(max) = null,
    @query_text       nvarchar(max) = null,
    @correlation_id   varchar(50)   = null
as
begin

	declare @error_message nvarchar(max);

    -- check log_type
    set @log_type = trim(@log_type);
	if @log_type is null or @log_type = ''
	begin
		set @error_message = '@log_type property cannot be null or empty';
		throw 50000, @error_message, 1;
	end
	set @log_type = lower(@log_type);
    if @log_type not in ( 'info', 'debug', 'error', 'warning', 'query')
	begin
		set @error_message = concat('@log_type property [', isnull(@log_type, 'NULL'), '] is not valid, accepted values are: ''info'',''debug'',''error'',''warning'',''query''  ');
		throw 50000, @error_message, 1;
	end

    -- check proc_name
    set @proc_name = trim(@proc_name);
	if @proc_name is null or @proc_name = '' 
	begin
		set @error_message = '@proc_name property cannot be null or empty';
		throw 50000, @error_message, 1;
	end    

	-- timestamp in local time
	declare @timestamp datetime2;
	set @timestamp = (SELECT SYSUTCDATETIME() AT TIME ZONE 'UTC' AT TIME ZONE 'Central Europe Standard Time')
	
	insert into meta.proc_log (
		[log_ts],
		[log_type],
		[proc_name],
		[statement_name],
		[message],
		[extra_parameters],
		[query_text],
        correlation_id
	) select
		 @timestamp as log_ts
		,@log_type as log_type
		,@proc_name as proc_name
		,@statement_name as statement_name
		,@message as message
		,@extra_parameters as extra_parameters
		,@query_text as query_text
        ,@correlation_id

end

GO
/*
exec meta.usp_proc_log 

	@log_type       = 'warning'  ,
    @proc_name        = 'x',
    @statement_name   = null,
	@message          = null,
	@extra_parameters = null,
    @query_text       = null

select * from meta.proc_log
*/
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_start_plan.sql">
/*

DESCRIPTION
Set the status of a plan to "IN PROGRESS"

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in logging tables
	- Current Status cannot be 'IN PROGRESS', 'ABORTED' or 'SUCCEEDED'
	- Compare original_run_id and run_id

*/

CREATE PROCEDURE [meta].[usp_start_plan]
	-- mandatory
	@plan_id   		int,
	@run_id			int,

	-- optional
	@pipeline_id    varchar(40) = null

AS
BEGIN

	-- step 1 validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if ( @plan_id is null)
	begin
		raiserror('plan_id parameter is mandatory', 16, 102 );
		return -1;
	end

	if ( @run_id is null)
	begin
		raiserror('run_id parameter is mandatory', 16, 102 );
		return -1;
	end

	-- Step 2: Make sure file is not already being tracked
	if not exists ( select run_id from meta.log_runs where run_id = @run_id )
	begin
		raiserror ('The run with run_id ''%i'' could not be located in LOG_RUNS table', 11, 301, @run_id );
		return 301; 
	end

	if not exists ( select plan_id from meta.log_plans where plan_id = @plan_id )
	begin
		raiserror('The plan with plan_id ''%i'' could not be located in LOG_PLANS table', 11, 302, @plan_id );
		return 302; 
	end

	if not exists ( select plan_id, run_id from meta.log_run_plans where plan_id = @plan_id and run_id = @run_id)
	begin
		raiserror('The plan with plan_id ''%i'' could not be located in LOG_RUN_PLANS table for run_id ''%i''', 11, 303,  @plan_id, @run_id );
		return 303;
	end

	--step 3: Plan can only be set to 'IN PROGRESS' if the current_status is PENDING
	declare @current_status nvarchar(15) = ( select current_status from [meta].[log_plans] where plan_id = @plan_id);
	if @current_status in ( 'ABORTED', 'IN PROGRESS', 'SUCCEEDED')
	begin
		raiserror('Cannot start the Plan, current_status is "%s"', 16, 3, @current_status );
		return -3; 
	end

	-- step 4: The Run that initially tried initiating the plan cannot have a higher ID than the one trying to execute the plan now
	-- declare @Original_run_id int = ( select run_id from meta.log_plans where plan_id = @plan_id );
	-- if ( @run_id &lt; @Original_run_id )
	-- begin
	-- 	raiserror('run_id value "%i" cannot be lower than the Original_run_id value "%i" in LOG_RUN table', 16, 203, @run_id, @Original_run_id );
	-- 	return 203;
	-- end


	-- step 5: Set Plan to 'IN PROGRESS'
	begin try

		declare @plan_name nvarchar(100);

		select @plan_name = plan_name
		from meta.log_plans a 
		where plan_id = @plan_id;

		--update LOG_PLANS
		update  [meta].[log_plans] 
		-- azure sql db runs in UTC, but we want CEST
		set start_ts = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time', 
			current_status = 'IN PROGRESS',
			end_ts = null,
			comment = null
		where plan_id = @plan_id
		;

		-- all OK
		RETURN 0
	
	end try
	begin catch
		
		declare @msg varchar(max) = 'Logging plan failed during update step -- message: ' + ERROR_MESSAGE()
		raiserror( @msg , 16, 9 );
		return -9
		;
			
	end catch

END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_start_run.sql">
/*

DESCRIPTION
Set the status of a run to "IN PROGRESS"

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in logging tables
	- Current Status cannot be 'IN PROGRESS', 'ABORTED' or 'SUCCEEDED'
	- 


*/


CREATE PROCEDURE [meta].[usp_start_run]

	-- mandatory
	@run_id			int,

	-- optional
	@pipeline_id	nvarchar(40) = null

AS

BEGIN

	-- step 1 validate input parameters
	-- check mandatory parameter: cannot be empty or NULL
	if ( @run_id is null)
	begin
		raiserror('run_id parameter is mandatory', 16, 102 );
		return -1;
	end

	-- step 2: Make sure IDs exist in logging tables
	if ( @run_id not in (select run_id from meta.log_runs) )
	begin
		raiserror('run_id parameter does not exist in the LOG_RUNS table', 16, 202 );
		return -2; 
	end

	-- step 3: Run can only be set to 'IN PROGRESS' if the current_status is PENDING
	declare @current_status nvarchar(15) = ( select current_status from [meta].[log_runs] where run_id = @run_id);
	if @current_status in ( 'ABORTED', 'IN PROGRESS', 'SUCCEEDED')
	begin
		raiserror('Cannot start the Plan, current_status is "%s"', 16, 3, @current_status );
		return -3; 
	end

	-- step 4: Set run to 'IN PROGRESS'
	begin try

		--update LOG_PLANS
		update  [meta].[log_runs] 
		-- azure sql db runs in UTC, but we want CEST
		set start_ts = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time', 
			current_status = 'IN PROGRESS',
			end_ts = null,
			comment = null
		where run_id = @run_id
		;

		-- all OK
		RETURN 0
	
	end try
	begin catch
		
		declare @msg varchar(max) = 'Logging run failed during update step -- message: ' + ERROR_MESSAGE()
		raiserror( @msg , 16, 9 );
		return -9
		;
			
	end catch


END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_start_task.sql">
/*

DESCRIPTION
Set the status of a task to "IN PROGRESS"

CHECKS
	- Mandatory parameters cannot be NULL
	- IDs need to exist in logging tables
	- Current Status cannot be 'IN PROGRESS', 'ABORTED' or 'SUCCEEDED'
	- Compare original_run_id and run_id

*/

CREATE PROCEDURE [meta].[usp_start_task]
	--mandatory
	@task_id     int,
	@plan_id   int,

	-- optional
	@pipeline_id     varchar(40) = null

AS
BEGIN


	-- step 1: validate input parameters
	-- check mandatory parameter: cannot be empty or NULL (10*)
	if ( @task_id is null)
	begin
		raiserror('task_id parameter is mandatory', 16, 101 );
		return 101;
	end
	if ( @plan_id is null)
	begin
		raiserror('plan_id parameter is mandatory', 16, 102 );
		return 102;
	end

	-- step 2: IDs need to exist in logging tables
	if ( @task_id not in (select task_id from meta.log_tasks) )
	begin
		raiserror('task_id parameter does not exist in the LOG_TASKS table', 16, 201 );
		return 201; 
	end
	if ( @plan_id not in (select plan_id from meta.log_plans) )
	begin
		raiserror('plan_id parameter does not exist in the LOG_PLANS table', 16, 202 );
		return 202;
	end

	--step 3 Current Status cannot be 'IN PROGRESS', 'ABORTED' or 'SUCCEEDED'
	declare @current_status nvarchar(15) = ( select current_status from [meta].[log_tasks] where task_id = @task_id );
	if @current_status in ( 'ABORTED', 'IN PROGRESS', 'SUCCEEDED')
	begin
		raiserror('Cannot start the task, current_status is "%s"', 16, 301, @current_status );
		return 301;
	end


	-- step 4 Compare original_plan_id and plan_id
	declare @original_plan_id int = ( select plan_id from meta.log_tasks where task_id = @task_id );
	if ( @plan_id &lt; @original_plan_id )
	begin
		raiserror('plan_id value "%i" cannot be lower than the original_plan_id value "%i" in LOG_TASKS table', 16, 401, @plan_id, @original_plan_id );
		return 401;
	end


	begin try
		-- Set current status to 'IN PROGRESS'
		-- Get the name of the task from the task_configuration table
		declare @task_name nvarchar(100);

		select @task_name = b.task_name 
		from meta.log_tasks a 
		inner join meta.task_configuration b on (a.task_name = b.task_name)
		where task_id = @task_id;

		--update log_tasks
		update  [meta].[log_tasks] 
		-- azure sql db runs in UTC, but we want CEST
		set start_ts = convert(datetimeoffset, getdate()) at time zone 'Central Europe Standard Time', 
			current_status = 'IN PROGRESS',
			end_ts = null,
			attempts = isnull(attempts,0) + 1,
			last_plan_id = @plan_id,
			pipeline_id = @pipeline_id,
			comment = null
		where task_id = @task_id
		;
	
		-- all OK
		RETURN 0
	
	end try
	begin catch
		
		declare @msg varchar(max) = 'Task logging failed during update step -- message: ' + ERROR_MESSAGE()
		raiserror( @msg , 16, 900 );
		;
			
	end catch

END;

GO
</file>
<file name="src\sql\metadb\stored_procedures\meta\meta.usp_update_file_activity.sql">
/*

=======================================================
Description: Update the metadata of a file_record
=======================================================

:param @extended_filename	[varchar]: 	The extended name of the file that is being run : filename + '_' + timestamp_folder\n"
:param @activity 			[varchar]: 	This field indicate the location where the file is loaded : accepted values: 'RAW', 'SILVER', 'ARCHIVE'
:param @success 			[bit]:		Boolean indication whether the loading was successful 
:param @info_message 		[varchar]: 	The path where the file is located in the landing zone


=======================================================

DETAILED DESCRIPTION
	The stored procedure will update the file in meta.log_file
	
	IF @activity = 'RAW' :
		update the values of the raw fields to track that the file has been loaded into the raw layer

	IF @activity = 'SILVER'
		update the values of the SILVER fields to track that the file has been loaded into the SILVER layer

	ELSE IF @activity = 'ARCHIVE'
		update the values of the ARCHIVE fields to track that the file has been loaded into the ARCHIVE layer

=======================================================
*/
--Dev-note: In the future Success parameter is not used to indicate successful or unsuccessful file loading. 
--Might be relevant to add the error throw for the specific file when an error occurs, and set the status to FAILED.
CREATE PROCEDURE [meta].[usp_update_file_activity]
(
	-- mandatory
    @extended_filename nvarchar(150),
	@activity varchar(50), -- accepted values: 'RAW', 'SILVER', 'ARCHIVE'
	@success bit,
    @info_message varchar(max)
	
	-- optional
	-- @source_folder nvarchar(max) = NULL,
	-- @sink_container nvarchar(max) = NULL,
	-- @sink_folder nvarchar(max) = NULL,
	-- @record_count_init int = NULL,
	-- @record_count_stg  int = NULL,
	-- @record_count_ins  int = NULL,
	-- @record_count_del  int = NULL,
	-- @record_count_upd  int = NULL,
	-- @record_count_rej  int = NULL,
	-- @message		   nvarchar(max) = NULL

)
AS
BEGIN
    declare @errormsg nvarchar(max);

	-- step 1: Mandatory parameters are not NULL
	if ( @activity is null)
	begin
		set @errormsg = 'activity parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @extended_filename is null)
	begin
		set @errormsg = 'extended_filename parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @success is null)
	begin
		set @errormsg = 'success parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	if ( @info_message is null)
	begin
		set @errormsg = 'info_message parameter is mandatory';
		throw 50000, @errormsg, 101;		
	end

	-- step 2: validation of values
	--Make sure file is already in the raw when we are trying to load it into the silver layer
	if ( @activity = 'SILVER' and 'SUCCEEDED' &lt;&gt;(select raw_status from meta.log_files where @extended_filename=[extended_filename]))
	begin
		set @errormsg = concat ( 'extended_filename ''', @extended_filename, ''' is trying to be loaded into the silver layer without being in the raw layer');
		throw 50000, @errormsg, 201;
	end

	--Make sure file is already being tracked
	if ( @extended_filename not in (select [extended_filename] from meta.log_files) )
	begin
		set @errormsg = concat ( 'extended_filename ''', @extended_filename, ''' does not exist in the log_files table');
		throw 50000, @errormsg, 201;
	end

	-- step 3: Validate that the activity is in ('RAW', 'SILVER', 'ARCHIVE')
	if ( @activity not in ( 'RAW', 'SILVER', 'ARCHIVE' ) )
	begin
		set @errormsg = concat ( 'activity parameter provided ''', @activity, ''' is not valid, accepted values are ''RAW'', ''SILVER'' and ''ARCHIVE''');
		throw 50000, @errormsg, 201;
	end

	-- STEP 4: Start activity update
	begin try
		
		declare @result nvarchar(20);
		if @success = 1  
			set @result = 'SUCCEEDED';
		else 
			set @result = 'FAILED';


		if ( @activity = 'RAW')
		begin
			update meta.log_files
			set raw_info = @info_message,
                raw_status = @result,
                raw_ts = getdate()
			where	extended_filename = @extended_filename
		end

		else if ( @activity = 'SILVER')
		begin
            update meta.log_files
			set silver_info = @info_message,
                silver_status = @result,
                silver_ts = getdate()
			where	extended_filename = @extended_filename
		end

		else if ( @activity = 'ARCHIVE')
		begin
            update meta.log_files
			set archive_info = @info_message,
                archive_status = @result,
                archive_ts = getdate()
			where	extended_filename = @extended_filename
		end

		;
	end try

	begin catch
		--rethrow the exception
		throw
		--return error code
		return -9
		;
	end catch
END;
GO



</file>
<file name="src\sql\metadb\tables\meta\meta.ais_landing_configuration.sql">
create table meta.ais_landing_configuration (

     source_system          varchar(50)  not null -- eg "igt"
    ,source_environment     varchar(20)  not null -- eg "prod"
    ,interface_name         varchar(50)  not null -- eg "pdm"
	,source_path            varchar(200) not null -- eg "PDM\Archive" or "IGT\Archive\xxx"
    ,is_enabled             bit          not null -- set disabled to NOT receive anything
    ,t_last_file_ts         datetime2        null -- timestamp of last loaded file (next drop will contain files newer then this timestamp)
    ,t_last_drop_ts         datetime2        null -- timestamp of last AIS interface run which effetively landed any files
    ,t_last_sync_ts         datetime2        null -- timestamp of last AIS interface run, whether it landed files or not
    ,t_locked_until         datetime2        null -- CET timestamp. if not null, the interface is locked until this timestamp (eg. during a drop)

);
GO
alter table meta.ais_landing_configuration add constraint pk__meta__ais_landing_configuration
    primary key clustered ( source_system, source_environment, interface_name)

</file>
<file name="src\sql\metadb\tables\meta\meta.ais_landing_log.sql">
create table meta.ais_landing_log (
     log_id               int          not null identity(1,1) 
    ,log_ts               datetime2    not null -- timestamp of when the log entry was created (CEST local time)
    ,log_type             varchar(20)      null
    
    ,source_system        varchar(50)  not null -- eg "igt"
    ,source_environment   varchar(20)  not null -- eg "dev"
    ,interface_name       varchar(50)  not null -- eg "pdm"

    ,last_file_ts         datetime2        null
    ,drop_ts              datetime2        null
    ,json_parameters      nvarchar(max)    null
    ,files_dropped        nvarchar(max)    null

)
GO
alter table meta.ais_landing_log add constraint pk__meta__ais_landing_log
    primary key clustered ( log_id )

</file>
<file name="src\sql\metadb\tables\meta\meta.log_files.sql">
-- Drop and create logging table for files
-- Primary Key: (file_id)

--DROP TABLE IF EXISTS [meta].[log_files]


CREATE TABLE [meta].[log_files] (
	[file_id]           	int                 NOT NULL identity(1,1), -- ID of the file; Auto-generated by SQL when creating new instance
	[task_id]				int                 NOT NULL,				-- ID of the task that is processing the file
	[plan_id]		    	int                 NOT NULL,				-- ID of the plan that is processing the file
	[registration_ts]   	datetime            	NULL,				-- Timestamp of when the file was first registered

 	[filename]         		nvarchar(150)       NOT NULL,				-- The name of the file on landing
    [extended_filename]   	nvarchar(200)       NOT NULL,				-- Filename_Folder: Add the name of the folder where the file was found -&gt; We expect this to be a timestamp
	[upload_ts]           	datetime            	NULL,				-- Timestamp on when the file was uploaded to landing 

	[landing_ts]      		datetime       			NULL,				-- Timestamp of the last ingestion in the landing folder
	[landing_status]      	nvarchar(150)       	NULL,				-- Status of the file:SUCCEEDED, FAILED, NULL
	[landing_info]      	nvarchar(max)        	NULL,				-- Location of the file in the landing folder
	
	[raw_ts]      			datetime       			NULL,				-- Timestamp of the last ingestion in the raw folder
	[raw_status]      		nvarchar(150)       	NULL,				-- Status of the file:SUCCEEDED, FAILED, NULL
	[raw_info]      		nvarchar(max)        	NULL,				-- Location of the file in the raw folder
	
	[silver_ts]      		datetime       			NULL,				-- Timestamp of the last ingestion in the silver folder
	[silver_status]      	nvarchar(150)       	NULL,				-- Status of the file:SUCCEEDED, FAILED, NULL
	[silver_info]			nvarchar(max)        	NULL,				-- Location of the file in the silver folder
	
	[archive_ts]			datetime       			NULL,				-- Timestamp of the last ingestion in the archive folder
	[archive_status]      	nvarchar(150)       	NULL,				-- Status of the file:SUCCEEDED, FAILED, NULL
	[archive_info]      	nvarchar(max)        	NULL,				-- Location of the file in the archive folder

	-- [record_count_init]  int     				null,				-- total number of records found in the file
	-- [record_count_stg]   int						null, 				-- records staged (not including rejected)
	-- [record_count_ins]   int						null, 				-- records inserted into final table
	-- [record_count_del]	int						null, 				-- records deleted from final table
	-- [record_count_upd]	int						null, 				-- records updated in final table (type 1 or 2)
	-- [record_count_rej]   int						null  				-- records rejected during staging
) 

GO

ALTER TABLE [meta].[log_files] ADD CONSTRAINT [PK__META__LOG_FILES] 
	PRIMARY KEY CLUSTERED ( [file_id] ASC ) with (DATA_COMPRESSION = PAGE); 

GO
ALTER TABLE [meta].[log_files] ADD CONSTRAINT [uc__meta__log_files__extended_filename]
	UNIQUE ( [extended_filename] ) 

GO


</file>
<file name="src\sql\metadb\tables\meta\meta.log_plans.sql">
-- Drop and create logging table for plans
-- Primary Key: (Plan_ID)

-- DROP TABLE IF EXISTS meta.log_plans

CREATE TABLE meta.log_plans 
(
-- batch configuration
	[plan_id]             	int           		NOT NULL identity(1,1),		-- The ID of the plan -&gt; Auto-completed by SQL
	[pipeline_id]           varchar(40)   		NOT NULL,					-- The ID of the Synapse pipeline that initiated the plan
	[plan_name]           	nvarchar(100) 		NOT NULL,					-- The name of the plan
	[registration_ts]     	datetime      		NOT NULL,					-- Timestamp of when the plan was first registrated
	[run_id]				INT					NOT NULL,					-- ID of the run that first initiated the plan
-- status tracking	
	[start_ts]	        	datetime          		NULL,					-- Timestamp of when the plan was started
	[end_ts]	            datetime          		NULL,					-- Timestamp of when the plan ended
	[current_status]	    nvarchar(15)      	NOT	NULL,					-- Status of the plan: Pending, In progress, succeeded, failed, aborted
	[last_pipeline_id]		int						NULL,					-- The ID of the run that last tried to execute the plan
	[comment]	            nvarchar(max)     		NULL					-- Optional comment

) ;
GO

ALTER TABLE meta.log_plans ADD CONSTRAINT META__PK__LOG_PLANS
	PRIMARY KEY CLUSTERED (plan_id) WITH ( DATA_COMPRESSION = PAGE )  ;
GO


ALTER TABLE meta.log_plans ADD CONSTRAINT CK__META__LOG_PLANS__CURRENT_STATUS 
	CHECK (([current_status]='ABORTED' OR [current_status]='IN PROGRESS' OR [current_status]='CANCELLED' OR [current_status]='SUCCEEDED' OR [current_status]='FAILED' OR [current_status]='PENDING'))

GO
</file>
<file name="src\sql\metadb\tables\meta\meta.log_plan_tasks.sql">
-- Drop and create logging table for plan_tasks
-- Primary Key: (Plan_ID, Task_ID)

-- DROP TABLE IF EXISTS [meta].[log_plan_tasks]

CREATE TABLE [meta].[log_plan_tasks] (

	[plan_id]           int           	NOT NULL,		-- ID of the plan
	[plan_name]         nvarchar(100) 	NOT NULL,		-- Name of the plan
	[task_id]           int            	NOT NULL,		-- ID of the task
	[task_name]         nvarchar(100)  	NOT NULL,		-- Name of the task
	[task_group]		nvarchar(100) 	NOT NULL,		-- Task group of the task
	[task_type]			nvarchar(100)	NOT NULL,		-- Executor of the task: SYNAPSE_PIPELINE or SPARK_NOTEBOOK
	[worker_name]		nvarchar(100)	NOT NULL,		-- Name of the worker that will execute the task
	[task_sequence]     int            	NOT NULL,		-- The index of the task in the plan
	[original_plan_id]  int             NOT NULL		-- The ID of the plan that first tried to execute the task
) ;

GO

ALTER TABLE meta.log_plan_tasks ADD CONSTRAINT PK__META__LOG_PLAN_TASKS
	PRIMARY KEY CLUSTERED ( plan_id, task_id ) 
	WITH (DATA_COMPRESSION = PAGE);

GO
</file>
<file name="src\sql\metadb\tables\meta\meta.log_runs.sql">
-- Drop and create logging table for pipeline runs
-- Primary Key: (run_id)

-- DROP TABLE IF EXISTS meta.log_runs

CREATE TABLE meta.log_runs
(
-- batch configuration
	[run_id]              	int           	NOT NULL identity(1,1),		-- ID of the run -&gt; Auto-generated by SQL
	[pipeline_id]         	nvarchar(40)   	NOT NULL,					-- ID of the Synapse pipeline that initiated the run
	[new_plan]             	bit             NOT NULL,					-- Boolean: Are we starting a new plan?
    [plan_name]           	varchar(100)   	NOT NULL,					-- Name of the plan to be executed
	[current_status] 		nvarchar(100)	NOT NULL,					-- Current status of the run: Pending, In progress, Success, Failed...
	[registration_ts]		datetime		NOT NULL,					-- Timestamp when run was first registered

	[start_ts]	        	datetime          	NULL,					-- Timestamp when run started
	[end_ts]	            datetime          	NULL,					-- Timestamp when run ended
	[comment]	            nvarchar(max)     	NULL					-- Optional comment

) ;
GO

ALTER TABLE meta.log_runs ADD CONSTRAINT META__PK__LOG_RUNS
	PRIMARY KEY CLUSTERED (run_id) WITH ( DATA_COMPRESSION = PAGE )  ;
GO
</file>
<file name="src\sql\metadb\tables\meta\meta.log_run_plans.sql">
-- Drop and create logging table for files
-- Primary Key: (Plan_ID, Run_ID)

-- DROP TABLE IF EXISTS meta.log_run_plans

CREATE TABLE meta.log_run_plans (

	[run_id]             	int           	NOT NULL,		-- ID of the run
	[plan_id]             	int            	NOT NULL,		-- ID of the plan
	[plan_name]           	nvarchar(100)  	NOT NULL,		-- Name of the plan
	[original_run_id]   	int             NOT NULL,		-- ID of the run that originally initiated the plan
) ;

GO

ALTER TABLE meta.log_run_plans ADD CONSTRAINT PK__META__LOG_RUN_PLANS
	PRIMARY KEY CLUSTERED ( plan_id, run_id ) 
	WITH (DATA_COMPRESSION = PAGE);

GO
</file>
<file name="src\sql\metadb\tables\meta\meta.log_tasks.sql">
-- Drop and create logging table for tasks
-- Primary Key: (Task_ID)

-- DROP TABLE IF EXISTS meta.log_tasks

CREATE TABLE meta.log_tasks 
(
-- job configuration
	[task_id]					int           	NOT NULL IDENTITY(1,1),			-- ID of the task -&gt; Auto-generated by SQL
	[task_name]					nvarchar(100) 	NOT NULL,						-- Name of the task
	[registration_ts]			datetime      	NOT NULL,						-- Timestamp when task was first registered
	[plan_id]					int           	NOT NULL,						-- ID of the first plan that tried executing the task

-- status tracking
	[start_ts]					datetime			NULL,						-- Timestamp of when the task started
	[end_ts]					datetime			NULL,						-- Timestamp of when the task ended
	[current_status]			nvarchar(15)	NOT	NULL,						-- Current Status of the task: Pending, In progress, success, failure...
	[comment]					nvarchar(max)		NULL,						-- Optional comment
	[max_value]					nvarchar(50)		NULL,						-- ??

	-- Record_Count_Stg			int					NULL, 						-- records staged (not including rejected)
	-- Record_Count_Ins			int					NULL, 						-- records inserted into final table
	-- Record_Count_Del			int					NULL, 						-- records deleted from final table
	-- Record_Count_Upd			int					NULL, 						-- records updated in final table (type 1 or 2)
	-- Record_Count_Rej         int					NULL, 						-- records rejected during staging
	[attempts]				    int					NULL,						-- Number of attempts made to execute the task
	[last_plan_id]              int					NULL,						-- Last plan id that tried executing the task
	[pipeline_id]               varchar(40)			NULL, 				-- ssis/df run guid
	[archive_flg]				nvarchar(1)			NULL						-- Set to Y if the data was archived (applicable to landing data only)
)  ;
GO

ALTER TABLE meta.log_tasks ADD CONSTRAINT PK__META__LOG_TASKS 
	PRIMARY KEY (task_id) 
	WITH (DATA_COMPRESSION = PAGE);
GO

ALTER TABLE meta.log_tasks ADD CONSTRAINT CK__META__LOG_TASKS__CURRENT_STATUS 
	CHECK (([current_status]='ABORTED' OR [current_status]='IN PROGRESS' OR [current_status]='CANCELLED' OR [current_status]='SUCCEEDED' OR [current_status]='FAILED' OR [current_status]='PENDING'));

GO
</file>
<file name="src\sql\metadb\tables\meta\meta.plan_configuration.sql">
-- Drop and create configuration table for plans
-- Primary Key: (Plan_Name)

-- DROP TABLE IF EXISTS meta.plan_configuration

CREATE TABLE meta.plan_configuration
(
	[plan_name]			nvarchar(100)	not null,	-- Name of the plan
	[plan_description]	nvarchar(max)	not null,	-- Description of the plan
	[enabled]			bit				not null	-- Boolean: Is the plan enabled?
);

GO 

ALTER TABLE meta.plan_configuration
ADD CONSTRAINT PK__META__PLAN_CONFIGURATION 
	PRIMARY KEY (plan_name) 
	WITH (DATA_COMPRESSION = PAGE);
GO
</file>
<file name="src\sql\metadb\tables\meta\meta.plan_task_configuration.sql">
-- Drop and create configuration table for plan_task combintations
-- Primary Key: (Plan_Name, Task_Group, Task_Name)

-- DROP TABLE IF EXISTS meta.plan_task_configuration

CREATE TABLE meta.plan_task_configuration
(
	[plan_name]		nvarchar(100)	not null,		-- Name of the plan
	[task_name]		nvarchar(100)	not null,		-- Name of the task
	[task_sequence]	nvarchar(100)	not null,		-- Execution sequence of the task in the plan
	[task_group]	nvarchar(100)	not null,		-- Task group of the task
	[enabled]		bit				not null		-- Boolean: Is the plan_task combination enabled?
);

GO 

ALTER TABLE meta.plan_task_configuration
ADD CONSTRAINT PK_META__PLAN_TASK_CONFIGURATION 
	PRIMARY KEY (	
		[plan_name] ASC,
		[task_group] ASC,
		[task_name] ASC) 
	WITH (DATA_COMPRESSION = PAGE)
GO


</file>
<file name="src\sql\metadb\tables\meta\meta.proc_log.sql">
create table meta.proc_log (
     log_id             int            not null identity(1,1) 
    ,log_ts             datetime2      not null -- timestamp of when the log entry was created in CEST local time
    ,correlation_id     varchar(50)        null
	,[log_type]         varchar(20)        null -- INFO, DEBUG, ERROR, WARNING, QUERY
	,[proc_name]        sysname            null -- name of the stored procedure
    ,[statement_name]   varchar(100)       null -- name of statement or location in the proc
	,[message]          nvarchar(max)      null -- freetext message
	,[extra_parameters] nvarchar(max)      null -- json
    ,[query_text]       nvarchar(max)      null -- placeholder to store dynamically generated queries
);
GO
alter table meta.proc_log add constraint pk__meta__proc_log
    primary key clustered ( log_id )

</file>
<file name="src\sql\metadb\tables\meta\meta.source_check_configuration.sql">
-- Drop and create configuration table to assign the right checks to each task
-- Primary Key: (Error_Name)

-- DROP TABLE IF EXISTS meta.source_check_configuration

-- TO DO: Write test to see if database exists / is created

CREATE TABLE meta.source_check_configuration
(
    [check_name]            nvarchar(100)   NOT NULL,       -- Name of the check
    [file_layout]           nvarchar(100)   NOT NULL,       -- Layout of the file: Used to identify which sources are paired to which checks
	[enabled]			    bit				NOT NULL,	    -- Boolean: Is the task_check combination enabled?
	[config_params]			nvarchar(max)	NULL			-- Column with flat json-objects stored as strings to save the needed parameters regarding configuration checks
);

GO 

-- Create PK
ALTER TABLE meta.source_check_configuration
ADD CONSTRAINT PK__META__TASK_CHECK_CONFIGURATION
	PRIMARY KEY (check_name, file_layout)
	WITH (DATA_COMPRESSION = PAGE); -- Compress data to save space
GO
</file>
<file name="src\sql\metadb\tables\meta\meta.source_column_configuration.sql">
-- Drop and create configuration table for data source columns
-- Primary Key: (File_Layout, Column_Order)

-- DROP TABLE IF EXISTS meta.source_column_configuration

CREATE TABLE meta.source_column_configuration
(
	[file_layout]		varchar(100)	not null,		-- Layout of the file: Used to identify which sources are paired to which tasks
	[source_name]		varchar(100)	not null,		-- Column names of the source files
	[sink_name]			varchar(100)	not null,		-- Column names of the delta table table
	[column_order]		int				not null,		-- Order of the column in the source file (necessary when no headers)
	[dimension]			varchar(5)		not null,		-- Dimension of the column: Business Key, Slowly changing dimension, ...
	[data_type]			varchar(20)		not null,		-- Expected data type of the values in the column
	[column_info]		varchar(max)		null		-- Additional information needed for columns
);

GO 

ALTER TABLE meta.source_column_configuration ADD CONSTRAINT pk__meta__source_column_configuration 
	PRIMARY KEY (file_layout, column_order) 
	WITH (DATA_COMPRESSION = PAGE);
GO
</file>
<file name="src\sql\metadb\tables\meta\meta.source_configuration.sql">
-- Drop and create configuration table for data sources
-- Primary Key: (File_Layout)

-- DROP TABLE IF EXISTS meta.source_configuration

CREATE TABLE meta.source_configuration
(
	[file_layout]		nvarchar(100)		not null,	-- Layout of the file: Used to identify which sources are paired to which tasks
	[file_pattern]		nvarchar(100)		not null,	-- Expected file_name pattern: What should the name of the file look like (eg. qualifio_*)
	[file_kind]			nvarchar(10)		not null,	-- Kind of the file: csv, json, parquet, zip, etc. (see enum in powershell/modules/DataConfig/schemas/IngestionMetadata.schema.json)
	[file_extension]	nvarchar(10)		not null,	-- Expected extension of the file: csv, json, ...
	[column_delimiter]	nvarchar(5)			not null,	-- Column delimiter of the file content
	[row_delimiter]		nvarchar(5)			null,		-- Row delimiter of the file content
	[escape_character]	nvarchar(5)			null,		-- Escape character of the file content
	[quote_character]	nvarchar(5)			null,		-- Quote characted of the file content
	[header]			nvarchar(5)			not null,	-- Boolean: Do we expect the files to have headers?
	[encoding]			nvarchar(10)		not null,	-- Encoding of the file: UTF-8, ANSI...
	[skip_first_lines]	integer				null,		-- Integer indicating on which line to start the ingestion
	[source_conditions] nvarchar(max)		NULL		-- Optional PreCond Values: LoadOrder sequence, etc
)

GO 

ALTER TABLE meta.source_configuration ADD CONSTRAINT pk__meta__source_configuration 
	PRIMARY KEY (file_layout) 
	WITH (DATA_COMPRESSION = PAGE);
GO
</file>
<file name="src\sql\metadb\tables\meta\meta.task_configuration.sql">
-- Drop and create configuration table for tasks
-- Primary Key: (Task_Name)

CREATE TABLE meta.task_configuration
(
	[task_name]			        nvarchar(100)	NOT NULL,		-- Name of the task
	[task_category]             varchar(20)         NULL,       -- Category of the task: INGEST or PREPROCESS
	[task_description]	        nvarchar(max)	NOT NULL,		-- Description of the task
	[task_type]			        nvarchar(100)	NOT NULL,		-- Executor of the task: SYNAPSE_PIPELINE, SPARK_NOTEBOOK
	[worker_name]		        nvarchar(100)	NOT NULL,		-- Worker that will execute the task
	[file_layout]		        nvarchar(100)	    NULL,		-- Ingestion tasks only: Layout of the file: Used to identify which sources are paired to which sources
	[source_folder]		        nvarchar(100)		NULL,		-- Location of where we expect to find the file
	[container_name]	        nvarchar(50)	NOT NULL,		-- Data Lake container of where we expect to find the file
	[table_name]		        nvarchar(100)		NULL,		-- Name of the table into where the file needs to be ingested
	[target_options]	        nvarchar(max)		NULL,		-- Potential additional values needed for target location (partitioning columns, etc.)
	[preprocess_pattern]        nvarchar(100)		NULL,		-- Preprocessing tasks only: filter pattern
	[preprocess_worker_options] nvarchar(max)       NULL,		-- Preprocessing tasks only: worker-specific parameters and options (json)
	[enabled]			        bit				NOT NULL		-- Boolean: Is the task enabled?
);

GO

ALTER TABLE meta.task_configuration ADD CONSTRAINT PK__META__TASK_CONFIGURATION 
	PRIMARY KEY (task_name) 
	WITH (DATA_COMPRESSION = PAGE);

GO
</file>
<file name="src\sql\metadb_test\schemas\test.sql">
CREATE SCHEMA [test] authorization dbo;

</file>
<file name="src\sql\metadb_test\stored_procedures\test.usp_reset_database.sql">
﻿/*

Reset the database by deleting all non-configuration data

	- truncate all 'lnd', 'stg', 'err', 'whs', 'deploy' tables
	- truncate all deploy and err tables
	- truncate batch/job/file tracking

A triple safety guard (helps to) prevent(s) accidential execution:
	- will only work on dev/test databases  (name like "gm6dwh-sndxx-sqldb-meta" or equal to "gm6dwh-int-sqldb-meta" or "gm6dwh-tst-sqldb-meta")
	- need to provide database explicitly as a parameter (must match the database)
	- default mode is "preview", set the @sure parameter to 1 to execute the truncates

*/
CREATE PROCEDURE [test].[usp_reset_database]
	@database_name sysname,
	@sure bit = 0
AS
begin

---------------------------------------------
-- safeguard : only in dev/test databases
---------------------------------------------

declare @database sysname = db_name();

if not ( @database like 'dev-dap-sqldb-core-meta' or @database like 'int-dap-sqldb-core-meta')
begin;
	throw 50000, 'Error, use of usp_reset_database not allowed not this database', 1;
end

---------------------------------------------
-- safeguard : compare current db with parameter
---------------------------------------------

if not ( @database = @database_name)
begin;
	throw 50000, 'Error, provided database_name value does not match current database', 2;
end

---------------------------------------------
-- preview mode only
---------------------------------------------
if not ( @sure = 1 )
begin
	print '------------------------------------------------------'
	print 'Preview mode - print but not executing statements '
	print '  (set  @sure parameter to 1 to avoid preview mode)'
	print '------------------------------------------------------'
end

---------------------------------------------
-- specific tables in meta schemas
---------------------------------------------

print 'truncate table meta.log_runs';
if @sure = 1 
	truncate table meta.log_runs;
print 'truncate table meta.log_plans';
if @sure = 1 
	truncate table meta.log_plans;
print 'truncate table meta.log_tasks';
if @sure = 1 
	truncate table meta.log_tasks;
print 'truncate table meta.log_files';
if @sure = 1 
	truncate table meta.log_files;
print 'truncate table meta.log_run_plans';
if @sure = 1 
	truncate table meta.log_run_plans;
print 'truncate table meta.log_plan_tasks';
if @sure = 1 
	truncate table meta.log_plan_tasks;
end
</file>
<file name="src\sql\metadb_test\stored_procedures\test.usp_run_tsqlt_tests.sql">
﻿/*

Run test classes classified as the given type

The test classes to run for each type are determined by evaluating the test class name

Valid types and classes which will be run:
	
	unit         : t_usp_*  , t_vw_*
	integration  : ti_*
	design       : t_design_*
	consistency  : tc_*

*/
create procedure test.usp_run_tsqlt_tests 
	@test_type varchar(100),
	@debug int = 0
as
begin

	----------------------------------------------------------------------------------------------------------------
	-- VALIDATION
	----------------------------------------------------------------------------------------------------------------

	--validate test type
	declare @errorMsg nvarchar(max) = concat( 'Invalid value for TestType parameter (', isnull( '''' + @test_type + '''', 'null'), '). Valid values for @test_type are : unit, integration, design, consistency.');
	if  ISNULL( @test_type, '') not in (  'unit', 'integration', 'design', 'consistency' )
	begin;
		throw 50000, @ErrorMsg, 101;
	end

	----------------------------------------------------------------------------------------------------------------
	-- DETERMINE TESTCLASSES TO EXECCUTE
	----------------------------------------------------------------------------------------------------------------

	--table will hold the class names (schema names)
	declare @TestClasses table ( TestClass sysname );

	--unit tests
	if @test_type = 'unit'
	begin
		set nocount on
		insert into @TestClasses
		select SchemaName as TestClass from (
			select schema_name(major_id) as SchemaName 
			from sys.extended_properties where [name] = 'tSQLt.TestClass' and class_desc  = 'SCHEMA'	
		) t
		where SchemaName like 't[_]usp[_]%'
			or SchemaName like 't[_]vw[_]%'
			or SchemaName like 't[_]fn[_]%'
		set nocount off
	end

	--integration
	else if @test_type = 'integration'
	begin
		set nocount on
		insert into @TestClasses
		select SchemaName as TestClass from (
			select schema_name(major_id) as SchemaName 
			from sys.extended_properties where [name] = 'tSQLt.TestClass' and class_desc  = 'SCHEMA'	
		) t
		where SchemaName like 'ti[_]%'
		set nocount off
	end

	-- design
	else if @test_type = 'design'
	begin
		set nocount on
		insert into @TestClasses
		select SchemaName as TestClass from (
			select schema_name(major_id) as SchemaName 
			from sys.extended_properties where [name] = 'tSQLt.TestClass' and class_desc  = 'SCHEMA'	
		) t
		where SchemaName like 't[_]design[_]%'
		set nocount off
	end

	-- consistency
	else if  @test_type = 'consistency'
	begin
		set nocount on
		insert into @TestClasses
		select SchemaName as TestClass from (
			select schema_name(major_id) as SchemaName 
			from sys.extended_properties where [name] = 'tSQLt.TestClass' and class_desc  = 'SCHEMA'	
		) t
		where SchemaName like 'tc[_]%'
		set nocount off
	end

	else throw 50000, '@test_type parameter did not match valid types', 102
	;

	----------------------------------------------------------------------------------------------------------------
	-- TEST EXECUTION
	----------------------------------------------------------------------------------------------------------------

	set nocount on;

	--temporary table to hold test results
	declare @results table (
		[Id] 			[int] 			NOT NULL,
		[Class] 		[nvarchar](max) NOT NULL,
		[TestCase] 		[nvarchar](max) NOT NULL,
		[Name] 			[nvarchar](max) NOT NULL,
		[TranName] 		[nvarchar](max) NOT NULL,
		[Result] 		[nvarchar](max) 	NULL,
		[Msg] 			[nvarchar](max) 	NULL,
		[TestStartTime] [datetime] 		NOT NULL,
		[TestEndTime] 	[datetime] 			NULL
		);

	--loop and run tests
	declare @TestClass sysname = ''
	while @TestClass is not null
	begin
		select @TestClass = min(TestClass) from @TestClasses where TestClass &gt; @TestClass;
		if @TestClass is null
			break;
		begin try
			if @debug &gt; 0
				print 'tSQLt.run ''' + @TestClass + '''...';
			exec tSQLt.Run @TestClass , 'tSQLt.NullTestResultFormatter';
		end try
		begin catch
			--ignore and continue
		end catch
		insert into @results
		select * from tSQLt.TestResult;

	end

	--restore results from all runs
	truncate table tSQLt.TestResult;
	begin try
		set identity_insert tSQLt.TestResult ON;
		insert into tSQLt.TestResult (
			[Id] ,[Class], [TestCase], [TranName], [Result], [Msg], [TestStartTime], [TestEndTime] 
		) select 
			[Id] ,[Class], [TestCase], [TranName], [Result], [Msg], [TestStartTime], [TestEndTime] 
		from @results;
		set identity_insert tSQLt.TestResult OFF;
	end try
	begin catch
		set identity_insert tSQLt.TestResult OFF;
		throw;
	end catch

	--results as XML
	--exec tSQLt.XmlResultFormatter;

	-- output as console output
	exec tSQLt.DefaultResultFormatter;

	-- list the classes with errors
	declare @ClassesWithErrors nvarchar(max) = null
	select distinct @ClassesWithErrors = isnull(@ClassesWithErrors + ',','') + Class 
		from tSQLt.TestResult
		where not ( Result = 'Success')

 	--return execution summary as a dataset
	select 
		COUNT(Id) as NumTests,
		SUM(case when Result = 'Success' then 1 else 0 end) as NumSuccess,
		SUM(case when Result = 'Success' then 0 else 1 end) as NumFailed,
		@ClassesWithErrors as ClassesWithErrors
	from tSQLt.TestResult

	return 0;

end

GO


--exec test.usp_run_tsqlt_tests @test_type = 'unit'

</file>
<file name="src\sql\metadb_test\t_design_tests\class_t_design_rules.sql">
CREATE SCHEMA [t_design_rules]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_design_rules'

</file>
<file name="src\sql\metadb_test\t_design_tests\t_design_rules -- schema name convention.sql">
CREATE PROCEDURE [t_design_rules].[test schema name convention]
AS
BEGIN
    
/*
all schema names must be in (all) lowercase
*/

declare @NAMES varchar(250) = null;

with testdata as (

	select SCHEMA_NAME
	from INFORMATION_SCHEMA.SCHEMATA 
	where 1=1
		and SCHEMA_OWNER = 'meta'
		and SCHEMA_NAME &lt;&gt; 'tSQLt'
		and SCHEMA_NAME not like 't[_]%'
		and SCHEMA_NAME not like 't_[_]%'
		and LOWER(SCHEMA_NAME) &lt;&gt; SCHEMA_NAME collate LATIN1_GENERAL_CS_AS

)

select @NAMES = isnull(@NAMES + ', ' , '') + SCHEMA_NAME from testdata;

declare @ERROR_MSG varchar(250) = 'The following schemas are not in full lowercase as per design rule: ' + @NAMES;

exec tSQLt.AssertEquals  @Expected = null, @Actual = @NAMES, @Message = @ERROR_MSG;



END
</file>
<file name="src\sql\metadb_test\t_design_tests\t_design_rules -- table name convention.sql">
CREATE PROCEDURE [t_design_rules].[test table name convention]
AS
BEGIN
    
/*
all table names must be in (all) lowercase
*/

declare @NAMES varchar(250) = null;
declare @NUM_FAIL int = 0;

with testdata as (

	select distinct tbl.TABLE_SCHEMA + '.' + tbl.TABLE_NAME as TABLE_NAME
	from INFORMATION_SCHEMA.TABLES tbl
	where 1=1
		--exclude test schemas:
		and tbl.TABLE_SCHEMA not in ('test','tSQLt')
		and tbl.TABLE_TYPE = 'BASE TABLE'
		and tbl.TABLE_NAME &lt;&gt; lower(tbl.TABLE_NAME) collate latin1_general_cs_as
		--exclude SSDT table:
		and TABLE_NAME &lt;&gt; '__RefactorLog'
)

select 
	@NAMES = isnull(@NAMES + ', ' , '') + TABLE_NAME ,
	@NUM_FAIL = @NUM_FAIL + 1
from testdata;

declare @ERROR_MSG varchar(250) = 'The following tables are not in full lowercase as per design rule: ' + @NAMES;

exec tSQLt.AssertEquals  @Expected = 0, @Actual = @NUM_FAIL, @Message = @ERROR_MSG;


END

GO

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_fn_add_json_property\class.sql">
CREATE SCHEMA [t_fn_add_json_property]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_fn_add_json_property'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_fn_add_json_property\test_normal_execution.sql">
/*if object_id('[t_fn_add_json_property].[test_normal_execution]') is not null 
    drop PROC [t_fn_add_json_property].[test_normal_execution]
GO
*/
CREATE PROCEDURE [t_fn_add_json_property].[test_normal_execution]
AS

BEGIN

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @startJson json;
    declare @expectedValue json;
    declare @actualValue json;
    -- tSQlt asserts use variant datatype, json is not supported, so we'll convert to varchar for assertions:
    declare @expectedValueString varchar(1000);
    declare @actualValueString varchar(1000);
    ----------------------------------------------------------------------------------------------------------------
    -- act &amp; assert
    ----------------------------------------------------------------------------------------------------------------

    -- add property to null string
    set @expectedValue = '{"foo": "bar"}';
    set @actualValue = (select meta.fn_add_json_property(null, 'foo', 'bar'));
    set @expectedValueString = convert(varchar(1000),@expectedValue)
    set @actualValueString = convert(varchar(1000),@actualValue)
    exec tSQLt.AssertEquals @Expected = @expectedValueString, @Actual = @actualValueString , @Message = 'test: adding property to null string failed.'

    -- add property to empty string
    set @expectedValue = '{"foo": "bar"}';
    set @actualValue = (select meta.fn_add_json_property('', 'foo', 'bar'));
    set @expectedValueString = convert(varchar(1000),@expectedValue)
    set @actualValueString = convert(varchar(1000),@actualValue)
    exec tSQLt.AssertEquals @Expected = @expectedValueString, @Actual = @actualValueString , @Message = 'test: adding property to empty string failed.'

    -- add property to json string with explicit path   
    set @startJson = '{"test": {}}'
    set @expectedValue = '{"test": {"foo": "bar"}}';
    set @actualValue = (select meta.fn_add_json_property(convert(nvarchar(max),@startJson), '$.test.foo', 'bar'));
    set @expectedValueString = convert(varchar(1000),@expectedValue)
    set @actualValueString = convert(varchar(1000),@actualValue)
    exec tSQLt.AssertEquals @Expected = @expectedValueString, @Actual = @actualValueString , @Message = 'test: adding property with explicit path failed.'  

END

GO

--exec tSQLt.Run '[t_fn_add_json_property].[test_normal_execution]'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_fn_add_json_property\test_should_fail_when_invalid_json.sql">
/*
if object_id('[t_fn_add_json_property].[test_should_fail_when_invalid_json]') is not null 
    drop PROC [t_fn_add_json_property].[test_should_fail_when_invalid_json]
GO
*/
CREATE PROCEDURE [t_fn_add_json_property].[test_should_fail_when_invalid_json]
AS

BEGIN

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValueString varchar(1000);
    declare @actualValueString varchar(1000);

    ----------------------------------------------------------------------------------------------------------------
    -- act &amp; assert
    ----------------------------------------------------------------------------------------------------------------
  
    -- add property to invalid start string
    set @expectedValueString = 'error%not valid json%';
    set @actualValueString = (select meta.fn_add_json_property('invalid', 'foo', 'bar'));
    exec tSQLt.AssertLike @ExpectedPattern = @expectedValueString, @Actual = @actualValueString , @Message = 'test: adding property to invalid json failed.'


END

GO

--exec tSQLt.Run '[t_fn_add_json_property].[test_should_fail_when_invalid_json]'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\class_t_usp_ais_get_landing_configuration.sql">
CREATE SCHEMA [t_usp_ais_get_landing_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_ais_get_landing_configuration'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\setup_t_usp_ais_get_landing_configuration.sql">
CREATE PROCEDURE [t_usp_ais_get_landing_configuration].[setup]
AS
BEGIN

    exec tSQLt.FakeTable	'meta.ais_landing_configuration'
    exec tSQLt.FakeTable	'meta.ais_landing_log', @Identity=1
    exec tSQLt.FakeTable	'meta.proc_log', @Identity=1

    insert into meta.ais_landing_configuration 
            (source_system, source_environment, interface_name, source_path, is_enabled, t_last_file_ts, t_locked_until)
        values  
            ('igt'     , 'acc' , 'pdm'           , 'PDM/Archive' , 1   , null                 , null),
            ('igt'     , 'prod', 'pdm'           , 'PDM/Archive' , 1   , null                 , null),
            ('igt'     , 'dev' , 'something_else', 'blah'        , 1   , '2024-06-01 13:14:15', '2024-01-01'),
            ('disabled', 'blah', 'foo'           , 'bar'         , 0   , null                 , '2024-01-01')
        ;

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\test_locking.sql">
/*
if object_id('[t_usp_ais_get_landing_configuration].[test_normal_execution]') is not null
    DROP PROCEDURE [t_usp_ais_get_landing_configuration].[test_normal_execution]
GO
*/
CREATE PROCEDURE [t_usp_ais_get_landing_configuration].[test_locking]
AS

BEGIN

    /*

    About the test:

    We'll do three calls of the proc, with lock_hours = 1

    We start with 1 disabled and 3 enabled records, of which two have no lock and one has an old, expired lock
    * the first time, we expect 3 records returned, with the "locked_until" set to @drop_timestamp + 1 HOUR
    * the second time we simulate few minutes later, we expect no results (everything is still locked)
    * the third time we simulate the next day, we expect again 3 results (locks have expired)

    */

    -- enabled @debug during test development only
    declare @debug bit = 0;

    ----------------------------------------------------------------------------------------------------------------
    -- assemble (1)
    ----------------------------------------------------------------------------------------------------------------

    declare @drop_ts datetime = '2024-06-25 12:11:56.933'

    create table t_usp_ais_get_landing_configuration.actual  (
        source_system      varchar(100),
        source_environment varchar(100),
        interface_name     varchar(100),
        source_path        varchar(200),
        target_storage     varchar(100),
        target_rg          varchar(100),
        target_container   varchar(100),
        target_path        varchar(500),
        files_from_ts      datetime2,
        drop_ts            datetime2,
        locked_until       datetime2
    );

    ----------------------------------------------------------------------------------------------------------------
    -- act (1)
    ----------------------------------------------------------------------------------------------------------------

    -- get configuration with lock_hours = 1
    insert into t_usp_ais_get_landing_configuration.actual
        exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts, @lock_hours = 1
    if @debug=1
        select 'actual' as actual, * from t_usp_ais_get_landing_configuration.actual

    ----------------------------------------------------------------------------------------------------------------
    -- assert (1)
    ----------------------------------------------------------------------------------------------------------------

    -- the original setup data contains 3 enabled records, 2 have no lock (null) and 1 has an expired lock
    -- we expect all three returned, with an updated lock

    declare @expectedValue varchar(50);
    declare @actualValue varchar(50);

    -- check recordcount (the fake config table contains a disabled record which should be ignored)
    set @expectedValue = 3
    set @actualValue = (select count(*) from t_usp_ais_get_landing_configuration.actual)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'

    -- check that the lock was set
    set @expectedValue = 3
    declare @lock_until datetime2 = dateadd(hour, 1, @drop_ts);
    if @debug=1
    begin        
        select 'config' as configuration, *, @lock_until as expected_lock from meta.ais_landing_configuration
    end
    set @actualValue = (select count(*) from meta.ais_landing_configuration where t_locked_until = @lock_until);
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'

    ----------------------------------------------------------------------------------------------------------------
    -- assemble (2)
    ----------------------------------------------------------------------------------------------------------------

    -- few minutes later
    set @drop_ts = '2024-06-25 12:15:00'

    -- clear the results table
    truncate table t_usp_ais_get_landing_configuration.actual

    ----------------------------------------------------------------------------------------------------------------
    -- act (2)
    ----------------------------------------------------------------------------------------------------------------

    -- get configuration with lock_hours = 1
    insert into t_usp_ais_get_landing_configuration.actual
        exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts, @lock_hours = 1
    if @debug=1
        select 'actual' as actual, * from t_usp_ais_get_landing_configuration.actual

    ----------------------------------------------------------------------------------------------------------------
    -- assert (2)
    ----------------------------------------------------------------------------------------------------------------

    -- we ran the procedure again, but because all enabled records are now "locked", we expected no results

        set @expectedValue = 0
        set @actualValue = (select count(*) from t_usp_ais_get_landing_configuration.actual)
        exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'


    ----------------------------------------------------------------------------------------------------------------
    -- assemble (3)
    ----------------------------------------------------------------------------------------------------------------

    -- few hours later
    set @drop_ts = '2024-06-26 00:00:000'

    -- clear the results table
    truncate table t_usp_ais_get_landing_configuration.actual

    ----------------------------------------------------------------------------------------------------------------
    -- act (3)
    ----------------------------------------------------------------------------------------------------------------

    -- get configuration with lock_hours = 1
    insert into t_usp_ais_get_landing_configuration.actual
        exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts, @lock_hours = 1
    if @debug=1
        select 'actual' as actual, * from t_usp_ais_get_landing_configuration.actual

    ----------------------------------------------------------------------------------------------------------------
    -- assert (3)
    ----------------------------------------------------------------------------------------------------------------

    -- we ran the procedure again "few hours later", past the previous locks so we expect again 3 recors with updated locks
    set @expectedValue = 3
    set @actualValue = (select count(*) from t_usp_ais_get_landing_configuration.actual)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'

    -- check that the lock was set
    set @expectedValue = 3
    set @lock_until = dateadd(hour, 1, @drop_ts);
    if @debug=1
    begin        
        select 'config' as configuration, *, @lock_until as expected_lock from meta.ais_landing_configuration
    end
    set @actualValue = (select count(*) from meta.ais_landing_configuration where t_locked_until = @lock_until);
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'


END

GO
/*
exec tSQLt.Run 't_usp_ais_get_landing_configuration.test_normal_execution'
*/
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_get_landing_configuration\test_normal_execution.sql">
/*
if object_id('[t_usp_ais_get_landing_configuration].[test_normal_execution]') is not null
    DROP PROCEDURE [t_usp_ais_get_landing_configuration].[test_normal_execution]
GO
*/
CREATE PROCEDURE [t_usp_ais_get_landing_configuration].[test_normal_execution]
AS

BEGIN

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @drop_ts datetime = '2024-06-25 12:11:56.933'
    declare @drop_ts_string varchar(50) = '20240625_121156'

    declare @env varchar(50) = (select left( db_name(), 3));

    create table t_usp_ais_get_landing_configuration.expected (
        source_system      varchar(100),
        source_environment varchar(100),
        interface_name     varchar(100),
        source_path        varchar(200),
        target_storage     varchar(100),
        target_rg          varchar(100),
        target_container   varchar(100),
        target_path        varchar(500),
        files_from_ts      datetime2,
        drop_ts            datetime2,
        locked_until       datetime2
    )

    create table t_usp_ais_get_landing_configuration.actual (
        source_system      varchar(100),
        source_environment varchar(100),
        interface_name     varchar(100),
        source_path        varchar(200),
        target_storage     varchar(100),
        target_rg          varchar(100),
        target_container   varchar(100),
        target_path        varchar(500),
        files_from_ts      datetime2,
        drop_ts            datetime2,
        locked_until       datetime2
    )

    insert into t_usp_ais_get_landing_configuration.expected (    
        source_system, 
        source_environment, 
        interface_name, 
        source_path, 
        target_storage, 
        target_rg, 
        target_container, 
        target_path, 
        files_from_ts, 
        drop_ts, 
        locked_until
    )
    select
         source_system      
        ,source_environment 
        ,interface_name     
        ,source_path        
        ,@env + 'dapstdala1' as target_storage     
        ,@env + '-dap-rg-dala' as target_rg
        ,'landing' as target_container   
        ,source_system + '/' + source_environment + '/' + interface_name + '/' + @drop_ts_string + '/' as target_path
        ,isnull(t_last_file_ts,'1900-01-01') as files_from_ts
        ,@drop_ts as drop_ts
        ,@drop_ts as locked_until
    from meta.ais_landing_configuration
    where is_enabled = 1
    --select * from t_usp_ais_get_landing_configuration.expected

    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    insert into t_usp_ais_get_landing_configuration.actual
        exec meta.usp_ais_get_landing_configuration @drop_timestamp = @drop_ts
    --select * from t_usp_ais_get_landing_configuration.actual

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValue varchar(50);
    declare @actualValue varchar(50);

    -- check recordcount (the fake config table contains a disabled record which should be ignored)
    set @expectedValue = 3
    set @actualValue = (select count(*) from t_usp_ais_get_landing_configuration.actual)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'

    -- check results
    exec tSQLt.AssertEqualsTable @Expected = 't_usp_ais_get_landing_configuration.expected', @Actual = 't_usp_ais_get_landing_configuration.actual' , @Message = 'Expected results table check failed.'




END

GO

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\class_t_usp_ais_log_landing.sql">
CREATE SCHEMA [t_usp_ais_log_landing]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_ais_log_landing'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\setup_t_usp_ais_log_landing.sql">
CREATE PROCEDURE [t_usp_ais_log_landing].[setup]
AS
BEGIN

    exec tSQLt.FakeTable	'meta.ais_landing_configuration'
    exec tSQLt.FakeTable	'meta.ais_landing_log', @Identity=1
    exec tSQLt.FakeTable	'meta.proc_log', @Identity=1

    insert into meta.ais_landing_configuration 
            (source_system, source_environment, interface_name, source_path, is_enabled, t_last_file_ts)
        values  
            ('igt', 'acc', 'pdm', 'PDM/Archive' , 1, null),
            ('igt', 'prod', 'pdm', 'PDM/Archive', 1, null),
            ('igt', 'dev', 'something_else','blah', 1, '2024-06-01 13:14:15'),
            ('disabled', 'blah', 'foo','bar', 0, null)
        ;

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_bad_log_type.sql">
/*
if object_id('[t_usp_ais_log_landing].[test_bad_logtype]') is not null
    DROP PROCEDURE [t_usp_ais_log_landing].[test_bad_logtype]
GO
*/
CREATE PROCEDURE [t_usp_ais_log_landing].[test_bad_logtype]
AS

BEGIN

    /*

    This test validates that the log_type validation works

    * should throw an error during parameter validation
    * should not update the last_file_ts nor the last_drop_ts in config table
    * should not update the last_sync_ts in the config table

    */

    -- set this to 1 during test development to output additional information
    declare @TEST_DEBUG bit  = 0;


    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @drop_ts            varchar(50) = '2024-06-25 12:11:56.9330000', 
            @drop_ts_string     varchar(50) = '20240625_121156',
            @correlation_id     varchar(20) = 'unit_test',
            @source_system      varchar(50) = 'igt',
            @source_environment varchar(50) = 'prod',
            @interface_name     varchar(50) = 'pdm',
            @error_message      varchar(100) = 'error message from the unit test'
            ;

    update meta.ais_landing_configuration 
        set t_last_file_ts = '2000-01-01', t_last_drop_ts = '2000-01-01', t_last_sync_ts = '2000-01-01' 
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name;

    if @TEST_DEBUG = 1 
    BEGIN   
        select 'before', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
    END

    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    declare @error_message_raised nvarchar(max);
    declare @json_parameters nvarchar(max) = '{ "error_message": "' + @error_message + '"}'
    
    -- run with bad log_type_value
    begin try
        exec meta.usp_ais_log_landing
            @log_type = 'BAD',
            @drop_timestamp = @drop_ts,
            @source_system = @source_system,
            @source_environment = @source_environment,
            @interface_name = @interface_name,
            @json_parameters = @json_parameters,
            @files_dropped = null,
            @correlation_id = @correlation_id
    end TRY
    begin CATCH
        set @error_message_raised = ERROR_MESSAGE()
    end CATCH

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValue varchar(50),
            @actualValue varchar(50),
            @expectedDate datetime2,
            @actualDate datetime2;

    -- verify that the exception was thrown
    exec tSQLt.AssertLike @ExpectedPattern = '%Provided log_type value is not valid%', @Actual =  @error_message_raised

    -- verify that the configuration did not change
    set @expectedDate =  convert(date,'2000-01-01');
    set @actualDate = (select t_last_file_ts from meta.ais_landing_configuration where source_system = @source_system and source_environment = @source_environment and interface_name = @interface_name);
    exec tSQLt.AssertEquals @Expected = @expectedDate, @Actual = @actualDate
    set @actualDate = (select t_last_drop_ts from meta.ais_landing_configuration where source_system = @source_system and source_environment = @source_environment and interface_name = @interface_name);
    exec tSQLt.AssertEquals @Expected = @expectedDate, @Actual = @actualDate
    set @actualDate = (select t_last_sync_ts from meta.ais_landing_configuration where source_system = @source_system and source_environment = @source_environment and interface_name = @interface_name);
    exec tSQLt.AssertEquals @Expected = @expectedDate, @Actual = @actualDate

    -- verify that the ais_landing_log was not made
    set @expectedValue = 0;
    set @actualValue = (select count(1) from meta.ais_landing_log);
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue;

    if @TEST_DEBUG = 1 
    BEGIN   
        select 'after', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
        select * from meta.ais_landing_log;
        select * from meta.proc_log;
    END    
 
END

GO
/*
if object_id('t_usp_ais_log_landing.[test_bad_logtype]') is NULL 
    throw 50000, 'test procedure doesn''t exist', 1;
exec tSQLt.Run 't_usp_ais_log_landing.[test_bad_logtype]';
*/

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_fail.sql">
/*
if object_id('[t_usp_ais_log_landing].[test_normal_execution_fail]') is not null
    DROP PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_fail]
GO
*/
CREATE PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_fail]
AS

BEGIN

    /*

    This test validates that logging a failed landing works correctly

    * should log this event
    * should not update the last_file_ts nor the last_drop_ts in config table
    * should not update the last_sync_ts in the config table

    */

    -- set this to 1 during test development to output additional information
    declare @TEST_DEBUG bit  = 0;


    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @drop_ts            varchar(50) = '2024-06-25 12:11:56.9330000', 
            @drop_ts_string     varchar(50) = '20240625_121156',
            @correlation_id     varchar(20) = 'unit_test',
            @source_system      varchar(50) = 'igt',
            @source_environment varchar(50) = 'prod',
            @interface_name     varchar(50) = 'pdm',
            @error_message      varchar(100) = 'error message from the unit test'
            ;

    update meta.ais_landing_configuration 
        set t_last_file_ts = '2000-01-01', t_last_drop_ts = '2000-01-01', t_last_sync_ts = '2000-01-01' 
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name;

    if @TEST_DEBUG = 1 
    BEGIN   
        select 'before', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
    END
    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    declare @json_parameters nvarchar(max) = '{ "error_message": "' + @error_message + '"}'
    
    -- run with 'FAIL' log_type
    exec meta.usp_ais_log_landing
        @log_type = 'FAIL',
        @drop_timestamp = @drop_ts,
        @source_system = @source_system,
        @source_environment = @source_environment,
        @interface_name = @interface_name,
        @json_parameters = @json_parameters,
        @files_dropped = null,
        @correlation_id = @correlation_id

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------


    if @TEST_DEBUG = 1 
    BEGIN   
        select 'after', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
        select * from meta.ais_landing_log;
        select * from meta.proc_log;
    END    

   declare @expectedValue varchar(50),
            @actualValue varchar(50),
            @expectedDate datetime2,
            @actualDate datetime2;

    -- verify that the configuration did not change
    set @expectedDate =  convert(date,'2000-01-01');
    set @actualDate = (select t_last_file_ts from meta.ais_landing_configuration where source_system = @source_system and source_environment = @source_environment and interface_name = @interface_name);
    exec tSQLt.AssertEquals @Expected = @expectedDate, @Actual = @actualDate
    set @actualDate = (select t_last_drop_ts from meta.ais_landing_configuration where source_system = @source_system and source_environment = @source_environment and interface_name = @interface_name);
    exec tSQLt.AssertEquals @Expected = @expectedDate, @Actual = @actualDate
    set @actualDate = (select t_last_sync_ts from meta.ais_landing_configuration where source_system = @source_system and source_environment = @source_environment and interface_name = @interface_name);
    exec tSQLt.AssertEquals @Expected = @expectedDate, @Actual = @actualDate

    -- verify that the ais_landing_log was made
    set @expectedValue = 1;
    set @actualValue = (select count(1) from meta.ais_landing_log );
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue;
    set @actualValue = (select count(1) from meta.ais_landing_log where log_type = 'FAIL');
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue;
 
 
END

GO
/*
if object_id('t_usp_ais_log_landing.[test_normal_execution_fail]') is NULL 
    throw 50000, 'test procedure doesn''t exist', 1;
exec tSQLt.Run 't_usp_ais_log_landing.[test_normal_execution_fail]';
*/
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_filedates.sql">
/*
if object_id('[t_usp_ais_log_landing].[test_normal_execution_filedates]') is not null
    DROP PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_filedates]
GO
*/
CREATE PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_filedates]
AS

BEGIN

    -- set this to 1 during test development to output additional information
    declare @TEST_DEBUG bit  = 0

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @drop_ts            varchar(50) = '2024-06-25 12:11:56.9330000', 
            @drop_ts_string     varchar(50) = '20240625_121156',
            @correlation_id     varchar(20) = 'unit_test',
            @source_system      varchar(50) = 'igt',
            @source_environment varchar(50) = 'prod',
            @interface_name     varchar(50) = 'pdm'
            --,@last_file_ts       varchar(50) = '2020-10-02 07:00:00.0000000'
            ;

    if @TEST_DEBUG = 1
    begin
        select 'before', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
    END

    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    declare @json_parameters nvarchar(max) =  
        '{ 
            "target_folder": "xxx/yyy/20240502_123456/", 
            "pipeline_guid": "123-456-789-zzzz"
        }';
    declare @files_dropped nvarchar(max) = 
        '[
            {
                "filename": "pdm_001.zip",
                "filedate": "2020-10-02 07:00:00.0000000"
            },
            {
                "filename": "pdm_002.zip",
                "filedate": "2020-10-03 09:00:00.0000000"
            }
        ]'
    
    exec meta.usp_ais_log_landing
        @drop_timestamp = @drop_ts,
        @source_system = @source_system,
        @source_environment = @source_environment,
        @interface_name = @interface_name,
        @json_parameters = @json_parameters,
        @files_dropped = @files_dropped,
        @correlation_id = @correlation_id

    if @TEST_DEBUG = 1
    begin
        select 'after', * from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
        select 'after', * from meta.proc_log
        select 'after', * from meta.ais_landing_log
    END

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValue varchar(50);
    declare @actualValue varchar(50);

    -- check the landing log
    set @expectedValue = 1
    set @actualValue = (select count(*) from meta.ais_landing_log)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'landing_log recordcount check failed.'

    -- check in config table
    declare @expected_last_file_ts varchar(50) = '2020-10-03 09:00:00.0000000',
            @actual_last_file_ts varchar(50),
            @actual_drop_ts varchar(50),
            @actual_sync_ts varchar(50)
            ;
    select @actual_last_file_ts = t_last_file_ts,
           @actual_drop_ts  = t_last_drop_ts,
           @actual_sync_ts = t_last_sync_ts
        from meta.ais_landing_configuration
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name;

    exec tSQLt.AssertEquals @Expected = @expected_last_file_ts, @Actual = @actual_last_file_ts , @Message = 'Found unexpected t_last_file_ts in configuration table'
    exec tSQLt.AssertEquals @Expected = @drop_ts, @Actual = @actual_drop_ts , @Message = 'Found unexpected t_last_drop_ts in configuration table'
    exec tSQLt.AssertEquals @Expected = @drop_ts, @Actual = @actual_sync_ts , @Message = 'Found unexpected t_last_sync_ts in configuration table'
    



END

GO

--exec tSQLt.Run 't_usp_ais_log_landing.[test_normal_execution_filedates]'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_last_file_ts.sql">
/*
DROP PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_last_file_ts]
GO
*/
CREATE PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_last_file_ts]
AS

BEGIN

    -- set this to 1 during test development to output additional information
    declare @TEST_DEBUG bit  = 0

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @drop_ts            varchar(50) = '2024-06-25 12:11:56.9330000', 
            @drop_ts_string     varchar(50) = '20240625_121156',
            @correlation_id     varchar(20) = 'unit_test',
            @source_system      varchar(50) = 'igt',
            @source_environment varchar(50) = 'prod',
            @interface_name     varchar(50) = 'pdm',
            @last_file_ts       varchar(50) = '2020-10-02 07:00:00.0000000'
            ;

    if @TEST_DEBUG = 1
    begin
        select 'before', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
    END

    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    declare @json_parameters nvarchar(max) =  
        '{ 
            "last_file_ts": "' + @last_file_ts + '", 
            "target_folder": "xxx/yyy/20240502_123456/", 
            "pipeline_guid": "123-456-789-zzzz"
        }';
    declare @files_dropped nvarchar(max) = 
        '[
            {
                "filename": "pdm_xxxxxx.zip",
                "foo": "bar"
            }
        ]'
    
    exec meta.usp_ais_log_landing
        @drop_timestamp = @drop_ts,
        @source_system = @source_system,
        @source_environment = @source_environment,
        @interface_name = @interface_name,
        @json_parameters = @json_parameters,
        @files_dropped = @files_dropped,
        @correlation_id = @correlation_id

    if @TEST_DEBUG = 1
    begin
        select 'after', * from meta.ais_landing_configuration
            where source_system = @source_system
              and source_environment = @source_environment
              and interface_name = @interface_name;
        select 'proc_log', * from meta.proc_log
        select 'ais_landing_log', * from meta.ais_landing_log
    END

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValue varchar(50);
    declare @actualValue varchar(50);

    -- check the landing log
    set @expectedValue = 1
    set @actualValue = (select count(*) from meta.ais_landing_log)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'landing_log recordcount check failed.'

    -- check in config table
    declare @expected_last_file_ts varchar(50) = @last_file_ts,
            @actual_last_file_ts varchar(50),
            @actual_drop_ts varchar(50),
            @actual_sync_ts varchar(50)
            ;
    select @actual_last_file_ts = t_last_file_ts,
           @actual_drop_ts  = t_last_drop_ts,
           @actual_sync_ts = t_last_sync_ts
        from meta.ais_landing_configuration
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name;

    exec tSQLt.AssertEquals @Expected = @expected_last_file_ts, @Actual = @actual_last_file_ts , @Message = 'Found unexpected t_last_file_ts in configuration table'
    exec tSQLt.AssertEquals @Expected = @drop_ts, @Actual = @actual_drop_ts , @Message = 'Found unexpected t_last_drop_ts in configuration table'
    exec tSQLt.AssertEquals @Expected = @drop_ts, @Actual = @actual_sync_ts , @Message = 'Found unexpected t_last_sync_ts in configuration table'
    



END

GO

--exec tSQLt.Run 't_usp_ais_log_landing.[test_normal_execution_last_file_ts]'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_no_files.sql">
/*
if object_id('[t_usp_ais_log_landing].[test_normal_execution_no_files]') is not null
    DROP PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_no_files]
GO
*/
CREATE PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_no_files]
AS

BEGIN

    /*

    This test validates that logging a "no files landed this time event" behaves correctly.

    * should log this event
    * should not update the last_file_ts nor the last_drop_ts in config table
    * should update the last_sync_ts in the config table

    This test verifies the behaviour when  was already loaded at least once before (so there is a last_file_ts and last_drop_ts)

    */

    -- set this to 1 during test development to output additional information
    declare @TEST_DEBUG bit  = 0;


    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValue varchar(50);
    declare @actualValue varchar(50);

    declare @drop_ts            varchar(50) = '2024-06-25 12:11:56.9330000', 
            @drop_ts_string     varchar(50) = '20240625_121156',
            @correlation_id     varchar(20) = 'unit_test',
            @source_system      varchar(50) = 'igt',
            @source_environment varchar(50) = 'prod',
            @interface_name     varchar(50) = 'pdm',
            @last_file_ts       varchar(50) = '2024-01-01 00:00:00.0000000',
            @last_drop_ts       varchar(50) = '2024-02-02 00:00:00.0000000'
            ;

    -- the test expects one record in the config table
    set @expectedValue = 1;
    set @actualValue = ( 
        select count(*) 
        from meta.ais_landing_configuration  
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name
            );
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue, @Message = 'Test prerequisite check failed.'

    -- set the test record to a normal situation (i.e. not first time)
    update meta.ais_landing_configuration set 
         t_last_file_ts = @last_file_ts ,
         t_last_drop_ts = @last_drop_ts ,
         t_last_sync_ts = @last_drop_ts 
    where source_system = @source_system
        and source_environment = @source_environment
        and interface_name = @interface_name

    if @TEST_DEBUG = 1
    begin
        select 'before', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
                and source_environment = @source_environment
                and interface_name = @interface_name
    END

    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    -- do a logging with no files delivered

    declare @json_parameters nvarchar(max) =  
        '{ 
            "target_folder": "xxx/yyy/20240502_123456/", 
            "pipeline_guid": "123-456-789-zzzz"
        }';
    declare @files_dropped nvarchar(max) = '[]'

    exec meta.usp_ais_log_landing
        @drop_timestamp = @drop_ts,
        @source_system = @source_system,
        @source_environment = @source_environment,
        @interface_name = @interface_name,
        @json_parameters = @json_parameters,
        @files_dropped = @files_dropped,
        @correlation_id = @correlation_id

    if @TEST_DEBUG = 1
    begin
        select 'after', * from meta.ais_landing_configuration
            where source_system = @source_system
                and source_environment = @source_environment
                and interface_name = @interface_name
        select 'after', * from meta.ais_landing_log
        select 'after', * from meta.proc_log
    END

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    --check log - expect 1 record
    set @expectedValue = 1;
    set @actualValue = (select count(*) from meta.ais_landing_log)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'

    -- check in config table
    declare 
        @expected_last_file_ts varchar(50) = @last_file_ts, --should remain unchanged
        @expected_drop_ts varchar(50)      = @last_drop_ts, --should remain unchanged
        @expected_sync_ts varchar(50)      = @drop_ts,      --should be updated
        @actual_last_file_ts varchar(50),
        @actual_drop_ts varchar(50),
        @actual_sync_ts varchar(50)
        ;
    select @actual_last_file_ts = t_last_file_ts,
           @actual_drop_ts  = t_last_drop_ts,
           @actual_sync_ts = t_last_sync_ts
        from meta.ais_landing_configuration
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name;

    exec tSQLt.AssertEquals @Expected = @expected_last_file_ts, @Actual = @actual_last_file_ts , @Message = 'Found unexpected t_last_file_ts in configuration table';
    exec tSQLt.AssertEquals @Expected = @expected_drop_ts     , @Actual = @actual_drop_ts      , @Message = 'Found unexpected t_last_drop_ts in configuration table';
    exec tSQLt.AssertEquals @Expected = @expected_sync_ts     , @Actual = @actual_sync_ts      , @Message = 'Found unexpected t_last_sync_ts in configuration table';

END

GO
/*
if object_id('t_usp_ais_log_landing.[test_normal_execution_no_files]') is NULL 
    throw 50000, 'test procedure doesn''t exist', 1;
exec tSQLt.Run 't_usp_ais_log_landing.[test_normal_execution_no_files]';
*/

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_ais_log_landing\test_normal_execution_no_files_first_time.sql">
/*
if object_id('[t_usp_ais_log_landing].[test_normal_execution_no_files_first_time]') is not null
    DROP PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_no_files_first_time]
GO
*/
CREATE PROCEDURE [t_usp_ais_log_landing].[test_normal_execution_no_files_first_time]
AS

BEGIN

    /*

    This test validates that logging a "no files landed this time event" behaves correctly.

    * should log this event
    * should not update the last_file_ts nor the last_drop_ts in config table
    * should update the last_sync_ts in the config table

    This test verifies the behaviour when the interface wes never loaded before (so there is no last_file_ts, last_drop_ts, or last_sync_ts)

    */

    -- set this to 1 during test development to output additional information
    declare @TEST_DEBUG bit  = 0;


    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValue varchar(50);
    declare @actualValue varchar(50);

    declare @drop_ts            varchar(50) = '2024-06-25 12:11:56.9330000', 
            @drop_ts_string     varchar(50) = '20240625_121156',
            @correlation_id     varchar(20) = 'unit_test',
            @source_system      varchar(50) = 'igt',
            @source_environment varchar(50) = 'prod',
            @interface_name     varchar(50) = 'pdm',
            @last_file_ts       varchar(50) = null,
            @last_drop_ts       varchar(50) = null
            ;

    -- the test expects one record in the config table
    set @expectedValue = 1;
    set @actualValue = ( 
        select count(*) 
        from meta.ais_landing_configuration  
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name
            );
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue, @Message = 'Test prerequisite check failed.'

    -- set the test record to "first time" situation
    update meta.ais_landing_configuration set 
         t_last_file_ts = @last_file_ts ,
         t_last_drop_ts = @last_drop_ts ,
         t_last_sync_ts = @last_drop_ts 
    where source_system = @source_system
        and source_environment = @source_environment
        and interface_name = @interface_name

    if @TEST_DEBUG = 1
    begin
        select 'before', * 
            from meta.ais_landing_configuration
            where source_system = @source_system
                and source_environment = @source_environment
                and interface_name = @interface_name
    END

    ----------------------------------------------------------------------------------------------------------------
    -- act
    ----------------------------------------------------------------------------------------------------------------

    -- do a logging with no files delivered

    declare @json_parameters nvarchar(max) =  
        '{ 
            "target_folder": "xxx/yyy/20240502_123456/", 
            "pipeline_guid": "123-456-789-zzzz"
        }';
    declare @files_dropped nvarchar(max) = '[]'

    exec meta.usp_ais_log_landing
        @drop_timestamp = @drop_ts,
        @source_system = @source_system,
        @source_environment = @source_environment,
        @interface_name = @interface_name,
        @json_parameters = @json_parameters,
        @files_dropped = @files_dropped,
        @correlation_id = @correlation_id

    if @TEST_DEBUG = 1
    begin
        select 'after', * from meta.ais_landing_configuration
            where source_system = @source_system
                and source_environment = @source_environment
                and interface_name = @interface_name
        select 'after', * from meta.ais_landing_log
        select 'after', * from meta.proc_log
    END

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    --check log - expect 1 record
    set @expectedValue = 1;
    set @actualValue = (select count(*) from meta.ais_landing_log)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue , @Message = 'recordcount check failed.'

    -- check in config table
    declare 
        @expected_last_file_ts varchar(50) = @last_file_ts, --should remain unchanged
        @expected_drop_ts varchar(50)      = @last_drop_ts, --should remain unchanged
        @expected_sync_ts varchar(50)      = @drop_ts,      --should be updated
        @actual_last_file_ts varchar(50),
        @actual_drop_ts varchar(50),
        @actual_sync_ts varchar(50)
        ;
    select @actual_last_file_ts = t_last_file_ts,
           @actual_drop_ts  = t_last_drop_ts,
           @actual_sync_ts = t_last_sync_ts
        from meta.ais_landing_configuration
        where source_system = @source_system
            and source_environment = @source_environment
            and interface_name = @interface_name;

    exec tSQLt.AssertEquals @Expected = @expected_last_file_ts, @Actual = @actual_last_file_ts , @Message = 'Found unexpected t_last_file_ts in configuration table';
    exec tSQLt.AssertEquals @Expected = @expected_drop_ts     , @Actual = @actual_drop_ts      , @Message = 'Found unexpected t_last_drop_ts in configuration table';
    exec tSQLt.AssertEquals @Expected = @expected_sync_ts     , @Actual = @actual_sync_ts      , @Message = 'Found unexpected t_last_sync_ts in configuration table';

END

GO
/*
if object_id('t_usp_ais_log_landing.[test_normal_execution_no_files_first_time]') is NULL 
    throw 50000, 'test procedure doesn''t exist', 1;
exec tSQLt.Run 't_usp_ais_log_landing.[test_normal_execution_no_files_first_time]';
*/
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\class_t_usp_end_plan.sql">
CREATE SCHEMA [t_usp_end_plan]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_end_plan'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\fail on pending tasks for successful plan.sql">
CREATE PROCEDURE [t_usp_end_plan].[fail on pending tasks for successful plan]
    @DEBUG int = 0
AS

BEGIN

    -- Declare and set ID params
    DECLARE @plan_id int; 
    DECLARE @run_id int;


    -- TEST 2: Success Flag = 0
    SET @plan_id = 5
    SET @run_id = 5

	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = 'The plan with plan_id 5 has still 3 unsuccessful tasks' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = @run_id,
		                            @plan_id = @plan_id,
		                            @success_flag = 1

END


</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\failed plan should cancel tasks.sql">
CREATE PROCEDURE [t_usp_end_plan].[failed plan should cancel tasks]
    @DEBUG int = 0
AS

BEGIN

    -- Declare and set ID params
    DECLARE @plan_id int; 
    DECLARE @run_id int;


    -- TEST 2: Success Flag = 0
    SET @plan_id = 3
    SET @run_id = 3

    CREATE TABLE [t_usp_end_plan].expected_log_tasks (
        [task_id]             	int,
        [task_name]           	nvarchar(100),
        [plan_id]               int,
        [current_status]        nvarchar(15)
    )

    CREATE TABLE [t_usp_end_plan].actual_log_tasks (
        [task_id]             	int,
        [task_name]           	nvarchar(100),
        [plan_id]               int,
        [current_status]        nvarchar(15)
    )

    CREATE TABLE [t_usp_end_plan].expected_log_plans (
        [plan_id]             	int,
        [plan_name]           	nvarchar(100),
        [run_id]				int,
        [current_status]        nvarchar(15)
    )

    CREATE TABLE [t_usp_end_plan].actual_log_plans (
        [plan_id]             	int,
        [plan_name]           	nvarchar(100),
        [run_id]				int,
        [current_status]        nvarchar(15)
    )



    insert into [t_usp_end_plan].expected_log_plans(plan_id, plan_name, run_id, current_status)
    values
        (@plan_id, 'unit_test_end_plan_success', @run_id, 'FAILED')


    insert into [t_usp_end_plan].expected_log_tasks(task_id, task_name, plan_id, current_status)
    values
        (1, 'progress_task',    3, 'CANCELLED'),
        (2, 'pending_task',     3, 'CANCELLED'),
        (3, 'successful_task',  3, 'SUCCEEDED'),
        (4, 'empty_task',       3, 'CANCELLED')


    EXEC meta.usp_end_plan  @plan_id = @plan_id,
                        @run_id = @run_id,
                        @success_flag = 0

	INSERT INTO [t_usp_end_plan].actual_log_plans ( plan_id, plan_name, run_id, current_status )
	SELECT lp.plan_id, lp.plan_name, lrp.run_id, lp.current_status
	FROM	meta.log_plans					as lp
			inner join meta.log_run_plans	as lrp on (lp.plan_id = lrp.plan_id)
	WHERE lp.plan_id = @plan_id

    INSERT INTO [t_usp_end_plan].actual_log_tasks ( task_id, task_name, plan_id, current_status )
	SELECT  task_id, task_name, plan_id, current_status
	FROM	meta.log_tasks
	WHERE   plan_id = @plan_id

    -- Assert: Check if expect = actual
    exec tSQLt.AssertEqualsTable @Expected = '[t_usp_end_plan].expected_log_plans', @Actual = '[t_usp_end_plan].actual_log_plans' , @Message = 'Plan-log test failed'
    exec tSQLt.AssertEqualsTable @Expected = '[t_usp_end_plan].expected_log_tasks', @Actual = '[t_usp_end_plan].actual_log_tasks' , @Message = 'Task-log test failed'


END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\setup_t_usp_end_plan.sql">
CREATE PROCEDURE [t_usp_end_plan].[setup]
AS
BEGIN
    exec tSQLt.FakeTable	'meta.log_runs' 		--, @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans'		--, @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks' 		-- @Identity=1
	exec tSQLt.FakeTable	'meta.log_run_plans'	--, @Identity = 1
    -- exec tSQLt.FakeTable	'meta.log_plan_tasks', 	-- @Identity=1

	--log_runs
	insert into meta.log_runs ( run_id, pipeline_id, new_plan, plan_name, current_status, registration_ts )
	values 	
		( 1, 'xxx', 1, 'unit_test_end_plan_pending', 'PENDING', GETDATE()),
		( 2, 'xxx', 1, 'unit_test_end_plan_success', 'IN PROGRESS', GETDATE()),
		( 3, 'xxx', 1, 'unit_test_end_plan_fail', 'IN PROGRESS', GETDATE()),
		( 4, 'xxx', 1, 'unit_test_end_plan_failed_plan', 'FAILED', GETDATE()),
		( 5, 'xxx', 1, 'unit_test_end_plan_success_2', 'IN PROGRESS', GETDATE())


	--log_plans
	insert into meta.log_plans ( plan_id, pipeline_id, plan_name, registration_ts, current_status )
	values 
		( 1, 'xxx', 'unit_test_end_plan_pending', GETDATE(), 'PENDING'),
		( 2, 'xxx', 'unit_test_end_plan_success', GETDATE(), 'IN PROGRESS'),
		( 3, 'xxx', 'unit_test_end_plan_fail', GETDATE(), 'IN PROGRESS'),
		( 4, 'xxx', 'unit_test_end_plan_failed_plan', GETDATE(), 'FAILED'),
		( 5, 'xxx', 'unit_test_end_plan_success_2', GETDATE(), 'IN PROGRESS')

	--log_run_plans
	insert into meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id )
	values
		(1, 1, 'unit_test_end_plan_pending', 1),
		(2, 2, 'unit_test_end_plan_success', 2),		
		(3, 3, 'unit_test_end_plan_fail', 3),
		(4, 4, 'unit_test_end_plan_failed_plan', 4),
		(5, 5, 'unit_test_end_plan_success_2', 5)


	insert into meta.log_tasks(task_id, task_name, registration_ts, plan_id, current_status)
	values
		(1, 'progress_task', 	GETDATE(), 3,'IN PROGRESS'),
		(2, 'pending_task', 	GETDATE(), 3,'PENDING'),
		(3, 'successful_task', 	GETDATE(), 3,'SUCCEEDED'),
		(4, 'empty_task', 		GETDATE(), 3, NULL),
		(5, 'progress_task', 	GETDATE(), 5,'IN PROGRESS'),
		(6, 'pending_task', 	GETDATE(), 5,'PENDING'),
		(7, 'successful_task', 	GETDATE(), 5,'SUCCEEDED'),
		(8, 'empty_task', 		GETDATE(), 5, NULL)



END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_end_plan].[test should fail on bad params]
    @DEBUG int = 1
AS

BEGIN

    -- Fail: No run_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%run_id%mandatory%' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = null,
                                @plan_id = 1,
		                        @success_flag = 1

    -- Fail: No plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_id%mandatory%' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = 1,
                                @plan_id = null,
		                        @success_flag = 1

	-- Fail: No success_flag
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%success_flag%mandatory%' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = 1,
		                        @plan_id = 1,
		                        @success_flag = null


    -- Fail: invalid run_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_runs%' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = 0,
		                        @plan_id = 1,
		                        @success_flag = 1

	-- Fail: invalid plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_plans%' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = 1,
		                        @plan_id = 0,
		                        @success_flag = 1


    -- Fail: Current_Status != 'IN PROGRESS'
	DECLARE @run_id int, @plan_id int
	SET @plan_id = 1 --(select plan_id from meta.log_plans where plan_name = 'unit_test_end_plan_pending')
	SET @run_id = 1 --(select run_id from meta.log_runs where plan_name = 'unit_test_end_plan_pending')

	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%invalid%Current_Status%' ;
	EXECUTE [meta].[usp_end_plan] 	@run_id = @plan_id,
		                        @plan_id = @plan_id,
		                        @success_flag = 1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\test should fail on failed plan.sql">
CREATE PROCEDURE [t_usp_end_plan].[test should fail on failed plan]
    @DEBUG int = 0
AS

BEGIN

    -- Declare and set ID params
    DECLARE @plan_id int; 
    DECLARE @run_id int;

    SET @plan_id = 4--(select plan_id from meta.log_plans where plan_name = 'unit_test_end_plan_failed_plan')
    SET @run_id = 4 --(select run_id from meta.log_runs where plan_name = 'unit_test_end_plan_failed_plan')

    EXEC tSQLt.ExpectException  @ExpectedMessagePattern = '%plan%Current%Status%FAILED%' ;
    EXEC [meta].[usp_end_plan] 	    @run_id = @run_id,
		                        @plan_id = @plan_id,
		                        @success_flag = 1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_plan\test should succeed.sql">
CREATE PROCEDURE [t_usp_end_plan].[test should succeed]
    @DEBUG int = 0
AS

BEGIN

    -- Declare and set ID params
    DECLARE @plan_id int; 
    DECLARE @run_id int;

    SET @plan_id = 2 --(select plan_id from meta.log_plans where plan_name = 'unit_test_end_plan_success')
    SET @run_id = 2 --(select run_id from meta.log_runs where plan_name = 'unit_test_end_plan_success')

    -- Prepare expected and actual tables
    CREATE TABLE [t_usp_end_plan].expected (
        [plan_id]             	int,
        [plan_name]           	nvarchar(100),
        [run_id]				int,
        [current_status]        nvarchar(15)
    )

    CREATE TABLE [t_usp_end_plan].actual (
        [plan_id]             	int,
        [plan_name]           	nvarchar(100),
        [run_id]				int,
        [current_status]        nvarchar(15)
    )


    -- TEST 1: Success Flag = 1
    insert into [t_usp_end_plan].expected( plan_id, plan_name, run_id, current_status )
    values
        (@plan_id, 'unit_test_end_plan_success', @run_id, 'SUCCEEDED')


    EXEC meta.usp_end_plan  @plan_id = @plan_id,
                        @run_id = @run_id,
                        @success_flag = 1

	INSERT INTO [t_usp_end_plan].actual( plan_id, plan_name, run_id, current_status )
	SELECT lp.plan_id, lp.plan_name, lrp.run_id, lp.current_status
	FROM	meta.log_plans					as lp
			inner join meta.log_run_plans	as lrp on (lp.plan_id = lrp.plan_id)
	WHERE lp.plan_id = @plan_id


    -- Assert: Check if expect = actual
    exec tSQLt.AssertEqualsTable @Expected = '[t_usp_end_plan].expected', @Actual = '[t_usp_end_plan].actual' , @Message = 'Test 1 failed'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_run\class_t_usp_end_run.sql">
CREATE SCHEMA [t_usp_end_run]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_end_run'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_run\setup_t_usp_end_run.sql">
CREATE PROCEDURE [t_usp_end_run].[setup]
AS
BEGIN
    exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    --exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    --exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	--exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    --exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1

	--log_runs
	insert into meta.log_runs ( pipeline_id, new_plan, plan_name, current_status, registration_ts )
	values ( 'xxx', 1, 'unit_test', 'PENDING', GETDATE())
END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_run\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_end_run].[test should fail on bad params]
    @DEBUG int = 1
AS

BEGIN

    -- Fail: No run_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%run_id%mandatory%' ;
	EXECUTE [meta].[usp_end_run] 	@run_id = null,
		                        @success_flag = 1

	-- Fail: No success_flag
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%success_flag%mandatory%' ;
	EXECUTE [meta].[usp_end_run] 	@run_id = 1,
		                        @success_flag = null


    -- Fail: invalid run_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_runs%' ;
	EXECUTE [meta].[usp_end_run] 	@run_id = 0,
		                        @success_flag = 1


    -- Fail: current_status != 'IN PROGRESS'
	DECLARE @run_id int;
	SET @run_id = (select TOP 1 run_id from meta.log_runs where current_status = 'PENDING')

	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%invalid%current_status%' ;
	EXECUTE [meta].[usp_end_run] 	@run_id = @run_id,
		                        @success_flag = 1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_task\class_t_usp_end_task.sql">
CREATE SCHEMA [t_usp_end_task]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_end_task'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_task\setup_t_usp_end_task.sql">
CREATE PROCEDURE [t_usp_end_task].[setup]
AS
BEGIN
    --exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	--exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1

	--log_plans
	insert into meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status )
	values ( 'xxx', 'unit_test', GETDATE(), 'PENDING')

	declare @Plan_ID int = scope_identity()

	--log_tasks
	insert into meta.log_tasks ( task_name, registration_ts, plan_id, current_status) 
	values 
		( 'Task', GETDATE(), @Plan_ID, 'PENDING')

    --declare @Task_ID int = scope_identity()

	--log_plan_tasks
	insert into meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name, task_group, task_type, worker_name, task_sequence, original_plan_id )
	select 
		plan_id, 'unit_test', task_id, task_name, 'unit_test', LEFT(task_name,charindex(' ', task_Name)), 'xxx', task_id, plan_id
	from meta.log_tasks

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_end_task\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_end_task].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN

	-- Fail: No plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_id%mandatory%' ;
	EXECUTE [meta].[usp_end_task] 	@plan_id = null,
		                        @task_id = 1,
		                        @success_flag = 1

	-- Fail: No task_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_id%mandatory%' ;
	EXECUTE [meta].[usp_end_task] 	@plan_id = 1,
		                        @task_id = null,
		                        @success_flag = 1


	-- Fail: No success_flag
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%success_flag%mandatory%' ;
	EXECUTE [meta].[usp_end_task] 	@plan_id = 1,
		                        @task_id = 1,
		                        @success_flag = null


    -- Fail: invalid plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_plans%' ;
	EXECUTE [meta].[usp_end_task] 	@plan_id = 0,
		                        @task_id = 1,
		                        @success_flag = 1

	-- Fail: invalid plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_tasks%' ;
	EXECUTE [meta].[usp_end_task] 	@plan_id = 1,
		                        @task_id = 0,
		                        @success_flag = 1

	DECLARE @task_id int, @plan_id int
	SET @task_id = (select TOP 1 task_id from meta.log_tasks where current_status = 'PENDING')
	SET @plan_id = (select plan_id from meta.log_plan_tasks where task_id = @task_id)

	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%invalid%current_status%' ;
	EXECUTE [meta].[usp_end_task] 	@plan_id = @plan_id,
		                        @task_id = @plan_id,
		                        @success_flag = 1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plans\class_t_usp_get_plans.sql">
CREATE SCHEMA [t_usp_get_plans]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_get_plans'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plans\setup_t_usp_get_plans.sql">
CREATE PROCEDURE [t_usp_get_plans].[setup]
AS
BEGIN
    exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    -- exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    -- exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    exec tSQLt.FakeTable    'meta.plan_configuration'

	--log_runs
	insert into meta.log_runs ( pipeline_id, new_plan, plan_name, current_status, registration_ts )
	values ( 'xxx', 1, 'unit_test', 'PENDING', GETDATE())

    declare @Run_ID int = scope_identity()

    --log_plans
	insert into meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status )
	values  ( 'xxx', 'unit_test', GETDATE(), 'PENDING'),
            ( 'xxx', 'unit_test', GETDATE(), 'IN PROGRESS'),
            ( 'xxx', 'unit_test', GETDATE(), 'FAILED')

   --log_plan_tasks
	insert into meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id)
	select @run_id, plan_id, plan_name, @run_id
    from meta.log_plans

    -- plan_configuration
    insert into meta.plan_configuration( plan_name)
    values ('unit_test')

END;
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plans\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_get_plans].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN

    -- Fail: No run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%run_id%mandatory%' ;
    EXECUTE meta.usp_get_plans  @run_id = null,
	                        @plan_name = 'unit_test'

    -- Fail: No Plan_Name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%Plan_Name%mandatory%' ;
    EXECUTE meta.usp_get_plans  @run_id = 1,
	                        @Plan_Name = null

    -- Fail: Invalid run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_runs%' ;
    EXECUTE meta.usp_get_plans  @run_id = 0,
	                        @Plan_Name = 'unit_test'

    -- Fail: Invalid Plan_Name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_configuration%' ;
    EXECUTE meta.usp_get_plans  @run_id = 1,
	                        @Plan_Name = 'unit_test'


END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plan_metadata\class_t_usp_get_plan_metadata.sql">
CREATE SCHEMA [t_usp_get_plan_metadata]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_get_plan_metadata'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plan_metadata\setup_t_usp_get_plan_metadata.sql">
CREATE PROCEDURE [t_usp_get_plan_metadata].[setup]
AS
BEGIN
    exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1

	--log_runs
	insert into meta.log_runs ( pipeline_id, new_plan, plan_name, current_status, registration_ts )
	values ( 'xxx', 1, 'unit_test', 'PENDING', GETDATE())

    declare @Run_ID int = scope_identity()

    --log_plans
	insert into meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status )
	values ( 'xxx', 'unit_test', GETDATE(), 'PENDING')

	declare @plan_id int = scope_identity()

	--log_tasks
	insert into meta.log_tasks ( task_name, registration_ts, plan_id, current_status) 
	values 
		( 'Task', GETDATE(), @plan_id, 'PENDING')


	--log_plan_tasks
	insert into meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name, task_group, task_type, worker_name, task_sequence, original_plan_id)
	select 
		plan_id, 'unit_test', task_id, task_name, 'unit_test', LEFT(task_name,charindex(' ', task_name)), 'xxx', task_id, plan_id
	from meta.log_tasks
END


</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_plan_metadata\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_get_plan_metadata].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN

    -- Fail: No run_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%run_id%mandatory%' ;
	EXECUTE [meta].[usp_get_plan_metadata] 	@run_id = null,
		                                @plan_id = 1,
		                                @task_group = 'DUMMY',
										@task_type = 'DUMMY'

	-- Fail: No plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_id%mandatory%' ;
	EXECUTE [meta].[usp_get_plan_metadata] 	@run_id = 1,
		                                @plan_id = null,
		                                @task_group = 'DUMMY',
										@task_type = 'DUMMY'


	-- Fail: No task_group
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_group%mandatory%' ;
	EXECUTE [meta].[usp_get_plan_metadata] 	@run_id = 1,
		                                @plan_id = 1,
		                                @task_group = null,
										@task_type = 'DUMMY'
	
	-- Fail: No task_type
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_type%mandatory%' ;
	EXECUTE [meta].[usp_get_plan_metadata] 	@run_id = 1,
		                                @plan_id = 1,
		                                @task_group = 'DUMMY',
										@task_type = null


    -- Fail: invalid run_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_runs%' ;
	EXECUTE [meta].[usp_get_plan_metadata] 	@run_id = 0,
		                                @plan_id = 1,
		                                @task_group = 'DUMMY',
										@task_type = 'DUMMY'

	-- Fail: invalid plan_id
	EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_plans%' ;
	EXECUTE [meta].[usp_get_plan_metadata] 	@run_id = 1,
		                                @plan_id = 0,
		                                @task_group = 'DUMMY',
										@task_type = 'DUMMY'
	
	

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\class_t_usp_get_preprocessing_info.sql">
CREATE SCHEMA [t_usp_get_preprocessing_info]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_get_preprocessing_info'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\setup_t_usp_get_preprocessing_info.sql">
CREATE PROCEDURE [t_usp_get_preprocessing_info].[setup]
AS
BEGIN
	-- Mock relevant configuration tables
	exec tSQLt.FakeTable	'meta.plan_task_configuration'
	exec tSQLt.FakeTable	'meta.task_configuration'


	-- Add fake data to mocked tables
	-- task_configuration
	insert into meta.task_configuration ( task_name, task_category, preprocess_pattern, [enabled])
	values 	( 'unittest_task_success', 'PREPROCESS', 'parent/%/source_name1/', 1),
			( 'unittest_task_disable', 'PREPROCESS', 'parent/%/source_name2/', 0),
			( 'unittest_task_fail', 'PREPROCESS', 'parent/%/source_name3/', 1),
			( 'unittest_task_fail2', 'PREPROCESS', 'parent/%/source_name3/', 1),
			( 'unittest_task_unparented','PREPROCESS', 'parent/%/source_name4/', 1)

	-- plan_task_configuration
	insert into meta.plan_task_configuration ( plan_name, task_name, [enabled] )
	values 	( 'preprocessing', 'unittest_task_success', 1 ),
			( 'preprocessing', 'unittest_task_fail', 1),
			( 'preprocessing', 'unittest_task_fail2', 1),
			( 'preprocessing', 'unittest_task_disable', 0 )

END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\test functionalities of the stored proc.sql">
CREATE PROCEDURE [t_usp_get_preprocessing_info].[test functionalities of the stored proc]
    @DEBUG int = 0
AS

BEGIN
    -- declare variables
	declare @expected  nvarchar(4000);
    declare @actual nvarchar(4000);
    declare @if_error int;

    -- Failure if no tasks are retured because not enabled
	begin try
        set @if_error = 0
        EXEC meta.usp_get_preprocessing_info
            @trigger_file_path = 'parent/20241212_121212/source_name2/',
            @trigger_file_name = 'filename'
	end try
    -- Validate that error matches pattern
	begin catch
        set @expected = '%match%one%0%tasks%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: zero matches'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: zero matches'



    -- Failure if too many source_path matches
	begin try
        set @if_error = 0
        EXEC meta.usp_get_preprocessing_info	
            @trigger_file_path = 'parent/20241212_121212/source_name3/',
            @trigger_file_name = 'filename'
	end try
    -- Validate that error matches pattern
	begin catch
        set @expected = '%match%one%task%2%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: multiple matches'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: multiple matches'



    -- Failure if task not exists in plan
	begin try
        set @if_error = 0
        EXEC meta.usp_get_preprocessing_info	
            @trigger_file_path = 'parent/20241212_121212/source_name4/',
            @trigger_file_name = 'filename'
	end try
    -- Validate that error matches pattern
	begin catch
        set @expected = '%match%one%task%0%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: No plan'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: No plan'



    -- Failure if task no match w source_path
	begin try
        set @if_error = 0
        EXEC meta.usp_get_preprocessing_info	
            @trigger_file_path = 'parent/20241212_121212/source_name5/',
            @trigger_file_name = 'filename'
	end try
    -- Validate that error matches pattern
	begin catch
        set @expected = '%match%one%task%0%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: No match'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: No match'



    -- Success if match and enabled
    CREATE TABLE #actual(
        [task_filter] varchar(100),
        [plan_name]          varchar(100)
    )

    CREATE TABLE #expected(
        [task_filter] varchar(100),
        [plan_name]          varchar(100)
    )

    INSERT INTO #actual (task_filter, plan_name)
    EXEC meta.usp_get_preprocessing_info @trigger_file_path = 'parent/20241212_121212/source_name1/', @trigger_file_name = 'filename'
    
    INSERT INTO #expected (task_filter, plan_name)
    values ('unittest_task_success', 'preprocessing')
    exec tSQLt.AssertEqualsTable @Expected = #expected, @Actual = #actual , @Message =  'Proc did not return expected task_filter'



END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_preprocessing_info\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_get_preprocessing_info].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN
    -- declare variables
	declare @expected  nvarchar(4000);
    declare @actual nvarchar(4000);
    declare @if_error int;

    -- Failure if trigger_file_path = null
	begin try
        set @if_error = 0
        EXEC meta.usp_get_preprocessing_info	
            @trigger_file_path = null,
            @trigger_file_name = 'filename'
	end try
    -- Validate that error matched pattern: trigger_file_path%mandatory%
	begin catch
        set @expected = '%trigger_file_path%mandatory%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @trigger_file_path = null'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @trigger_file_path = null'


END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_tasks\class_t_usp_get_tasks.sql">
CREATE SCHEMA [t_usp_get_tasks]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_get_tasks'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_tasks\setup_t_usp_get_tasks.sql">
CREATE PROCEDURE [t_usp_get_tasks].[setup]
AS
BEGIN
    
	exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1

	declare @plan_name varchar(100) = 'unit_test'

	--log_runs
	insert into meta.log_runs ( pipeline_id, new_plan, plan_name, registration_ts, current_status )
	values ( 'xxx', 1, @plan_name, GETDATE(), 'PENDING')

	declare @run_id int = scope_identity()

	--log_plans
	insert into meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status )
	values ( 'xxx', @plan_name, GETDATE(), 'PENDING')

	declare @plan_id int = scope_identity()

	--log_tasks
	insert into meta.log_tasks ( task_name, registration_ts, plan_id, current_status) 
	values 
		( 'Pipeline task 1 succeeded', GETDATE(), @plan_id, 'SUCCEEDED'),
		( 'Pipeline task 2 failed', GETDATE(), @plan_id, 'FAILED'),
		( 'Pipeline task 3 pending', GETDATE(), @plan_id, 'PENDING'),	

		( 'Spark task 1 succeeded', GETDATE(), @plan_id, 'SUCCEEDED'),
		( 'Spark task 2 failed', GETDATE(), @plan_id, 'FAILED'),
		( 'Spark task 3 pending', GETDATE(), @plan_id, 'PENDING'),

		( 'stored_proc task 1 succeeded', GETDATE(), @plan_id, 'SUCCEEDED'),
		( 'stored_proc task 2 failed', GETDATE(), @plan_id, 'FAILED'),
		( 'stored_proc task 3 pending', GETDATE(), @plan_id, 'PENDING')

	--log_run_plans
	insert into meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id)
	select 
		@run_id, @plan_id, @plan_name, @run_id
	from meta.log_plans


	--log_plan_tasks
	insert into meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name, task_group, task_type, worker_name, task_sequence, original_plan_id)
	select 
		plan_id, @plan_name, task_id, task_name, 'unit_test', LEFT(task_name,charindex(' ', task_name)), 'xxx', task_id, plan_id
	from meta.log_tasks
		

END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_tasks\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_get_tasks].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN
    -- declare variables
	declare @expected  nvarchar(4000);
    declare @actual nvarchar(4000);
    declare @if_error int;

    -- Failure if run_id = null
	begin try
        set @if_error = 0
        EXEC meta.usp_get_tasks	@run_id = null,
                            @plan_id = 1,
                            @task_group = 'TEST',
                            @task_type = 'DUMMY'
	end try
    -- Validate that error matched pattern: run_id%mandatory%
	begin catch
        set @expected = 'run_id%mandatory%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @run_id = null'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @run_id = null'


    -- Failure if plan_id = null
	begin try
        set @if_error = 0
        EXEC meta.usp_get_tasks	@run_id = 1,
                            @plan_id = null,
                            @task_group = 'TEST',
                            @task_type = 'DUMMY'
	end try
    -- Validate that error matched pattern: plan_id%mandatory%
	begin catch
        set @expected = 'plan_id%mandatory%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @plan_id = null'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @plan_id = null'


    -- Failure if task_group = null
	begin try
        set @if_error = 0
        EXEC meta.usp_get_tasks	@run_id = 1,
                            @plan_id = 1,
                            @task_group = null,
                            @task_type = 'DUMMY'
	end try
    -- Validate that error matched pattern: task_group%mandatory%
	begin catch
        set @expected = 'task_group%mandatory%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @task_group = null'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @task_group = null'

    -- Failure if task_type = null
	begin try
        set @if_error = 0
        EXEC meta.usp_get_tasks	@run_id = 1,
                            @plan_id = 1,
                            @task_group = 'TEST',
                            @task_type = null
	end try
    -- Validate that error matched pattern: task_group%mandatory%
	begin catch
        set @expected = 'task_type%mandatory%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @task_type = null'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @task_group = null'    

    
    -- Failure if run_id not in meta.log_runs
	begin try
        set @if_error = 0
        EXEC meta.usp_get_tasks	@run_id = 0,
                            @plan_id = 1,
                            @task_group = 'TEST',
                            @task_type = 'DUMMY'
	end try
    -- Validate that error matched pattern: %log_runs%
	begin catch
        set @expected = '%log_runs%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @run_id not in log_runs'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @run_id not in log_runs'


    -- Failure if plan_id not in meta.log_plans
	begin try
        set @if_error = 0
        EXEC meta.usp_get_tasks	@run_id = 1,
                            @plan_id = 0,
                            @task_group = 'TEST',
                            @task_type = 'DUMMY'
	end try
    -- Validate that error matched pattern: task_group%mandatory%
	begin catch
        set @expected = '%log_plans%'
		set @actual = error_message()
        set @if_error = 1
		EXEC tSQLt.AssertLike @ExpectedPattern = @expected, @Actual = @actual, @Message = 'Proc did not fail as expected: @plan_id not in log_plans'
	end catch
    -- Validate that error actually occured
    exec tSQLt.AssertEquals @Expected = 1, @Actual = @if_error, @Message = 'Proc did not fail as expected: @plan_id not in log_plans'


END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\class_t_usp_get_task_last_loaded_file.sql">
CREATE SCHEMA [t_usp_get_task_last_loaded_file]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_get_task_last_loaded_file'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\setup_t_usp_get_task_last_loaded_file_.sql">
CREATE PROCEDURE [t_usp_get_task_last_loaded_file].[setup]
AS
BEGIN
    
    exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1  				--used
	exec tSQLt.FakeTable	'meta.log_files', @Identity=1       		--used
	exec tSQLt.FakeTable	'meta.task_configuration', @Identity=1      --used
	exec tSQLt.FakeTable    'meta.log_run_plans', @Identity=1

	declare @plan_name varchar(100) = 'unit_test'

	--log_runs
	insert into meta.log_runs ( pipeline_id, new_plan, plan_name, registration_ts, current_status )
	values ( 'xxx', 1, @plan_name, GETDATE(), 'PENDING')

	declare @run_id int = scope_identity()

	--log_plans
	insert into meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status, run_id )
	values ( 'xxx', @plan_name, GETDATE(), 'PENDING', @run_id)

	declare @plan_id int = scope_identity()

	--log_tasks
	insert into meta.log_tasks ( task_name, registration_ts, plan_id, current_status) 
	values 
		( 'Pipeline task 1 no name'		, GETDATE(), @plan_id, 'SUCCEEDED'	),
		( 'Pipeline task 2 with_name'	, GETDATE(), @plan_id, 'FAILED'		)
	--log_run_plans

	insert into meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id)
	select 
		@run_id, @plan_id, @plan_name, @run_id
	from meta.log_plans

	--log_files
	insert into meta.log_files( task_id, plan_id, filename, extended_filename)
	select 
		 task_id, plan_id, filename='task_filename_20241217_123456.csv', extended_filename='task_filename_20241217_123456_EXTENDED_FILENAME'
	from meta.log_tasks


	-- task_configuration
	insert into meta.task_configuration (task_name, task_description, task_type, worker_name, container_name, enabled)
		select task_name, task_description= 'unit_test', task_type= 'SPARK_NOTEBOOK', worker_name='IngestionWorker', container_name='landing', enabled=1
	from meta.log_tasks
		

END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\test pos load filename.sql">
CREATE PROCEDURE [t_usp_get_task_last_loaded_file].[test pos load filename]
    @DEBUG int = 0
AS

BEGIN
    declare @result  TABLE( [filename] varchar(max) )

    declare @task_name varchar(max) = 'Pipeline task 1 no name'	

    insert into @result
        exec meta.usp_get_task_last_loaded_file @task_name

    declare @filename varchar(max) = null
    
    select @filename = [filename] from @result

    exec [tSQLt].[AssertEqualsString]  @Expected = 'task_filename_20241217_123456_EXTENDED_FILENAME', @Actual = @filename, @Message = 'Expected filename not found'
END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\test should fail missing mandatory params.sql">
CREATE PROCEDURE [t_usp_get_task_last_loaded_file].[test should fail missing mandatory params]
    @DEBUG int = 0
AS

BEGIN

    -- Fail: No run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_name%mandatory%' ;
    EXECUTE meta.usp_get_task_last_loaded_file  @task_name = null

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_last_loaded_file\test should fail task not exists.sql">
CREATE PROCEDURE [t_usp_get_task_last_loaded_file].[test should fail task not exists]
    @DEBUG int = 0
AS

BEGIN
    -- Fail: Task does not exist
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_name % not found in TASK_CONFIGURATION table%' ;
    EXECUTE meta.usp_get_task_last_loaded_file  @task_name = 'task_does_not_exists'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\class_t_usp_get_task_metadata.sql">
-- Creates a schema for OOP: Add as much Test Classes as you want
-- Create schema can also be used to create tables, views and set GRANT, DENY, REVOKE perms on it
CREATE SCHEMA [t_usp_get_task_metadata]
    -- Set the name of the database-level principal that will own the schema
    AUTHORIZATION dbo
GO

-- Create a new test class, which functions as a schema. If schema already exists, other one is dropped and new is created.
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_get_task_metadata'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\setup_t_usp_get_task_metadata.sql">
-- Create the stored procedure (DON'T USE ALTER: GIVES ERRORS)
CREATE PROCEDURE [t_usp_get_task_metadata].[setup]
AS
BEGIN
    -- /////////////////////
    -- TABLE CREATION
    -- /////////////////////
    ---- First, tables are created with the same name as the actual tables (!)
    ---- Set @Identity = 1 to preserve identity properties (auto increments), else leave it out

    exec tSQLt.FakeTable	'meta.log_tasks'
    exec tSQLt.FakeTable    'meta.task_configuration'
    exec tSQLt.FakeTable    'meta.source_configuration'
    exec tSQLt.FakeTable    'meta.source_column_configuration'
    exec tSQLt.FakeTable    'meta.plan_task_configuration'
    exec tSQLt.FakeTable    'meta.source_check_configuration'

	--exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    --exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    --exec tSQLt.FakeTable  'meta.plan_configuration'
    --exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    --exec tSQLt.FakeTable	'meta.log_plans', @Identity=1

    -- /////////////////////
    -- TABLE INSERTION
    -- /////////////////////

	---- log_tasks
	insert into meta.log_tasks ( task_id, task_name, registration_ts )
	values  ( 1,'unit_test', GETDATE() ),
            ( 2, 'unit_test_withworker_unsupported', GETDATE() ),
            ( 3, 'IngestionWorker', GETDATE() )

    -- Returns the last identity value inserted into an identity column in the same scope
    -- declare @Task_ID int = scope_identity()

    ---- task_configuration
    insert into meta.task_configuration( task_name, file_layout, worker_name )
    values  ( 'unit_test', 'unit_test_layout_normal', 'IngestionWorker' ),
            ( 'unit_test_withworker_unsupported', 'unit_test_layout_unsupported', 'worker_unsupported' ),
            ( 'IngestionWorker', 'unit_test_layout', 'IngestionWorker' )


    ---- source_configuration
    insert into meta.source_configuration ( file_layout, file_kind )
    values ( 'unit_test_layout', 'kind' )

    ---- source_column_configuration
    insert into meta.source_column_configuration ( file_layout )
    values ( 'unit_test_layout' )

    ---- source_check_configuration
    insert into meta.source_check_configuration (check_name, file_layout, [enabled])
    values ('data_type', 'unit_test_layout', 1)

END;
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\test json return statements.sql">
-- Create the stored procedure (DON'T USE ALTER: GIVES ERRORS)
CREATE PROCEDURE [t_usp_get_task_metadata].[test json return statements]
    @DEBUG int = 0
AS

BEGIN
    -- /////////////////////
    -- DECLARATIONS
    -- /////////////////////
    CREATE TABLE  [t_usp_get_task_metadata].[actual] (
        taskconfig			NVARCHAR(MAX),
        sourceconfig		NVARCHAR(MAX),
        sourcecolumnconfig	NVARCHAR(MAX),
        sourcecheckconfig	NVARCHAR(MAX)
    )

    -- DECLARE @ExpectedResults TABLE(
    --     taskconfig			NVARCHAR(MAX),
    --     sourceconfig		NVARCHAR(MAX),
    --     sourcecolumnconfig	NVARCHAR(MAX),
    --     sourcecheckconfig	NVARCHAR(MAX)
    -- )

    DECLARE @ActualJson     NVARCHAR(MAX)
    -- DECLARE @ExpectedJson   NVARCHAR(MAX)

    -- /////////////////////
    -- PREPARE OBJECTS
    -- /////////////////////
    -- INSERT INTO @ExpectedResults(taskconfig, sourceconfig, sourcecolumnconfig, sourcecheckconfig)
    -- VALUES('[{"task_name":"IngestionWorker","worker_name":"IngestionWorker","file_layout":"unit_test_layout"}]', '[{}]', '[{}]', '[{"check_name":"data_type"}]')

    INSERT INTO [t_usp_get_task_metadata].[actual]
    EXEC meta.usp_get_task_metadata @task_id = 3;

    SET @ActualJson = (
        SELECT * FROM [t_usp_get_task_metadata].[actual]
        FOR JSON PATH
    )

    -- SET @ExpectedJson = (
    --     SELECT * FROM @ExpectedResults
    --     FOR JSON PATH
    -- )

    -- /////////////////////
    -- CHECK
    -- /////////////////////

    -- Expected: [{"taskconfig":"[{\"task_name\":\"IngestionWorker\",\"worker_name\":\"IngestionWorker\",\"file_layout\":\"unit_test_layout\"}]","sourceconfig":"[{}]","sourcecolumnconfig":"[{}]","sourcecheckconfig":"[{\"check_name\":\"data_type\"}]"}]
    -- Check if actual results contains certain variables and certain values
    EXEC tSQLt.AssertLike @ExpectedPattern = '[[]%]', @Actual = @ActualJson, @Message='Job 1' -- Check for square brackets at the beginning &amp; end
    EXEC tSQLt.AssertLike @ExpectedPattern = '%taskconfig%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%task_name%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%IngestionWorker%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%worker_name%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%file_layout%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%file_kind%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%unit_test_layout%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%sourceconfig%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%sourcecolumnconfig%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%sourcecheckconfig%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%check_name%', @Actual = @ActualJson, @Message='Job 1'
    EXEC tSQLt.AssertLike @ExpectedPattern = '%data_type%', @Actual = @ActualJson, @Message='Job 1'


END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\test retrieve task and worker name.sql">
-- Create the stored procedure (DON'T USE ALTER: GIVES ERRORS)
CREATE PROCEDURE [t_usp_get_task_metadata].[test retrieve task and worker name]
    @DEBUG int = 0
AS

BEGIN

    -- /////////////////////
    -- PREP
    -- /////////////////////
    ---- Worker name
    -- NOT NULL &amp;&amp; FOUND &amp;&amp; NOT CONFIGURED | EXPECT ERROR
    -- Test to check what happens if worker is not configured in SQL
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%Unsupported%' ;
    EXECUTE meta.usp_get_task_metadata  @task_id = 2

    -- NOT NULL &amp;&amp; FOUND &amp;&amp; CONFIGURED | EXPECT NO ERROR
    -- Test to check what happens if worker is configured in SQL
    EXECUTE tSQLt.ExpectNoException ;
    EXECUTE meta.usp_get_task_metadata  @task_id = 3

    ---- Task ID
    -- NOT NULL &amp;&amp; NOT FOUND | EXPECT ERROR
    -- Test to check what happens if task_id is not present in meta.log_tasks
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%Could%not%locate%' ;
    EXECUTE meta.usp_get_task_metadata  @task_id = 4




END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_get_task_metadata\test should fail on bad params.sql">
-- Create the stored procedure (DON'T USE ALTER: GIVES ERRORS)
CREATE PROCEDURE [t_usp_get_task_metadata].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN

    -- /////////////////////
    -- PARAMETERS CHECK
    -- /////////////////////
    ---- task_id
    -- NULL | EXPECT ERROR
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_id%mandatory%' ;
    EXECUTE meta.usp_get_task_metadata  @task_id = null

    -- NOT NULL &amp;&amp; NOT EXISTS | EXPECT ERROR
    ---- The task_id isn't supposed to exist in log_tasks
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_tasks%' ;
    EXECUTE meta.usp_get_task_metadata  @task_id = 0


END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\class_t_usp_new_file.sql">
CREATE SCHEMA [t_usp_new_file]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_new_file'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\setup_t_usp_new_file.sql">

CREATE PROCEDURE [t_usp_new_file].[setup]
AS
BEGIN
    -- exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	-- exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    exec tSQLt.FakeTable    'meta.log_files', @Identity = 1
    -- exec tSQLt.FakeTable 'meta.plan_configuration'
    -- exec tSQLt.FakeTable 'meta.task_configuration'
    -- exec tSQLt.FakeTable 'meta.source_configuration'
    -- exec tSQLt.FakeTable 'meta.source_column_configuration'
    -- exec tSQLt.FakeTable 'meta.plan_task_configuration'

    INSERT INTO meta.log_tasks (task_name)
    VALUES ('unit_test')

    INSERT INTO meta.log_plans (plan_name)
    VALUES ('unit_test')

    INSERT INTO meta.log_plan_tasks (plan_id, plan_name, task_id, task_name)
    VALUES (1, 'unit_test', 1, 'unit_test')

    INSERT INTO meta.log_files( [filename], [extended_filename],[landing_status],[raw_status],[silver_status], [archive_status],task_id)
    VALUES  ( 'testfile.txt', 'testfile_TS.txt', 'SUCCEEDED', 'SUCCEEDED', 'SUCCEEDED', 'SUCCEEDED',1),
            ( 'testfilefailed.txt', 'testfilefailed_TS.txt', 'SUCCEEDED', 'SUCCEEDED', 'SUCCEEDED',null,1)

END;



</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_new_file].[test should fail on bad params]
    @DEBUG int = 0
AS

BEGIN

    --Fail: No task_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_id%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = null,
	                        @plan_id = 1,
	                        @filename = '',
	                        @extended_filename = '',
	                        @info_message = '',
                            @no_select = 1

    --Fail: No plan_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_id%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = 1,
	                        @plan_id = null,
	                        @filename = '',
	                        @extended_filename = '',
	                        @info_message = '',
                            @no_select = 1

    --Fail: No filename
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%filename%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = 1,
	                        @plan_id = 1,
	                        @filename = null,
	                        @extended_filename = '',
	                        @info_message = '',
                            @no_select = 1



    --Fail: No extended_filename
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%extended_filename%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = 1,
	                        @plan_id = 1,
	                        @filename = '',
	                        @extended_filename = null,
	                        @info_message = '',
                            @no_select = 1



    --Fail: No info_message
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%info_message%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = 1,
	                        @plan_id = 1,
	                        @filename = '',
	                        @extended_filename = '',
	                        @info_message = null,
                            @no_select = 1


    --Fail: Invalid task_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%info_message%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = 0,
	                        @plan_id = 1,
	                        @filename = '',
	                        @extended_filename = '',
	                        @info_message = '',
                            @no_select = 1


    --Fail: No plan_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%info_message%mandatory%' ;
    EXECUTE meta.usp_new_file   @task_id = 1,
	                        @plan_id = 0,
	                        @filename = '',
	                        @extended_filename = '',
	                        @info_message = '',
                            @no_select = 1





END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\test should fail on existing file.sql">
CREATE PROCEDURE [t_usp_new_file].[test should fail on existing file]
    @DEBUG int = 0
AS

BEGIN
    -- Fail: Extended filename already exists in log_files table and the archive_status is SUCCEEDED
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%exists%log_files%' ;
    EXECUTE meta.usp_new_file   @task_id = 1,
	                        @plan_id = 1,
	                        @filename = 'testfile.txt',
	                        @extended_filename = 'testfile_TS.txt',
	                        @info_message = '',
                            @no_select = 1
END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\test should rerun after a file has failed.sql">
CREATE PROCEDURE [t_usp_new_file].[test should rerun after a file has failed]
    @DEBUG int = 0
AS

BEGIN
    -- succeed: Extended filename already exists in log_files table and the archive_status is null
    CREATE TABLE #expected_log_runs(
	    [raw_status]            NVARCHAR(100),
        [silver_status]         NVARCHAR(100),
        [archive_status]        NVARCHAR(100),
        [task_id]               NVARCHAR(100),
        [extended_filename]     NVARCHAR(100)


    )

    CREATE TABLE #actual_log_runs(
	    [raw_status]            NVARCHAR(100),
        [silver_status]         NVARCHAR(100),
        [archive_status]        NVARCHAR(100),
        [task_id]               NVARCHAR(100),
        [extended_filename]     NVARCHAR(100)
    )
    INSERT INTO #expected_log_runs(raw_status,silver_status,archive_status,task_id,extended_filename)
    values (null, null, null,1,'testfilefailed_TS.txt')  ; 
    
    EXECUTE meta.usp_new_file   @task_id = 1,--that'll work
	                        @plan_id = 1,
	                        @filename = 'testfilefailed.txt',
	                        @extended_filename = 'testfilefailed_TS.txt',
	                        @info_message = '',
                            @no_select = 1


    INSERT INTO #actual_log_runs(raw_status,silver_status,archive_status,task_id,extended_filename)
    select raw_status,silver_status,archive_status,task_id,extended_filename
    from meta.log_files
    where extended_filename='testfilefailed_TS.txt';

    EXECUTE tSQLt.AssertEqualsTable @Expected = #expected_log_runs, @Actual = #actual_log_runs, @Message = 'It is expected that the row will have its status reset '
END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_file\t_usp_new_file.test should add a new row.sql">
CREATE PROCEDURE [t_usp_new_file].[test should add a new row] 
AS
BEGIN
	declare @count int; 
    EXECUTE meta.usp_new_file   @task_id = 1,--that'll work
	                        @plan_id = 1,
	                        @filename = 'testnewfile.txt',
	                        @extended_filename = 'testnewfile_TS.txt',
	                        @info_message = '',
                            @no_select = 1

	set @count= (select count(*) from meta.log_files where extended_filename = 'testnewfile_TS.txt');


	EXECUTE tSQLt.AssertEquals @Expected =1, @Actual = @count, @Message = 'Expected one row add int the meta.log_files'


END;
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\class_t_usp_new_run.sql">
CREATE SCHEMA [t_usp_new_run]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_new_run'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\setup_t_usp_new_run.sql">
CREATE PROCEDURE [t_usp_new_run].[setup]
    @DEBUG int = 0
AS
BEGIN
    -- Mock the instances of the logging tables
    -- Identity = 1 -&gt; Reset from identity = 1 
    exec tSQLt.FakeTable	'meta.log_runs',        @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans',       @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks',       @Identity=1
    exec tSQLt.FakeTable	'meta.log_run_plans',   @Identity=1
    exec tSQLt.FakeTable	'meta.log_plan_tasks',  @Identity=1

    -- Mock the instances of some of the configuration tables
    exec tSQLt.FakeTable        'meta.plan_configuration'
    exec tSQLt.FakeTable        'meta.task_configuration'
    exec tSQLt.FakeTable        'meta.plan_task_configuration'

    -- Insert fake data into the configuration tables
    -- Insert a plan that is enabled and one that is disabled
    INSERT INTO meta.plan_configuration( plan_name, [enabled] )
    VALUES  ( 'unit_test_enabled', 1 ),
            ( 'unit_test_notenabled', 0),
            ( 'unit_test_filtered', 1)

    -- Insert a task that is enabled and one that is disabled
    INSERT INTO meta.task_configuration( task_name, [enabled] )
    VALUES  ( 'unit_test_enabled', 1 ),
            ( 'unit_test_notenabled', 0),
            ( 'unit_test_filtered', 1)

    -- Insert plan-task pairs for all of the plans and tasks
    INSERT INTO meta.plan_task_configuration( plan_name, task_name,enabled)
    VALUES  ( 'unit_test_enabled',      'unit_test_enabled',1 ),
            ( 'unit_test_enabled',      'unit_test_notenabled',1),
            ( 'unit_test_notenabled',   'unit_test_enabled',1 ),
            ( 'unit_test_notenabled',   'unit_test_notenabled',1),
            ( 'unit_test_filtered',     'unit_test_filtered',1 ),
            ( 'unit_test_filtered',     'unit_test_enabled',1),
            ( 'unit_test_filtered',     'unit_test_notenabled',1)


   -- Insert fake data into the logging tables
   -- Insert 4 different rows for meta.log_runs
    INSERT INTO meta.log_runs(plan_name, current_status)
    VALUES  ('unit_test_enabled', 'FAILED'),
            ('unit_test_enabled', 'FAILED'),
            ('unit_test_enabled', 'CANCELLED'),
            ('unit_test_enabled', 'PENDING')

   -- Insert 4 different rows for meta.log_plans
    INSERT INTO meta.log_plans(plan_name, run_id, current_status)
    VALUES  ('unit_test_enabled',  1, 'FAILED'),
            ('unit_test_enabled',  2, 'FAILED'),
            ('unit_test_disabled', 3,'FAILED'),
            ('unit_test_enabled',  4, 'PENDING')

    -- Insert 4 different rows for meta.log_tasks
    INSERT INTO meta.log_tasks(task_name, plan_id, current_status)
    VALUES  ('unit_test_enabled',  1, 'FAILED'),
            ('unit_test_enabled',  2, 'FAILED'),
            ('unit_test_disabled', 3, 'FAILED'),
            ('unit_test_enabled',  4, 'PENDING')

    -- Insert 4 different rows for meta.log_plan_task
    INSERT INTO meta.log_plan_tasks(plan_id, plan_name, task_id, task_name)
    VALUES  (1, 'unit_test_enabled',  1, 'unit_test_enabled'),
            (2, 'unit_test_enabled',  2, 'unit_test_enabled'),
            (3, 'unit_test_disabled', 3, 'unit_test_disabled'),
            (4, 'unit_test_enabled',  4, 'unit_test_enabled')

    -- Insert 4 different rows for meta.log_run_plans
    INSERT INTO meta.log_run_plans(run_id, plan_id, plan_name, original_run_id)
    VALUES  (1, 1, 'unit_test_enabled',  1),
            (2, 2, 'unit_test_enabled',  2),
            (3, 3, 'unit_test_disabled', 3),
            (4, 4, 'unit_test_enabled',  4)

END

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_new_run].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no pipeline_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%pipeline_id%mandatory%' ;
    EXECUTE meta.usp_new_run    @pipeline_id = null,
                            @new_plan = 1,
                            @plan_name = 'unit_test',
                            @no_select = 1

    -- Fail: No plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_name%mandatory%' ;
    EXECUTE meta.usp_new_run    @pipeline_id = 'xxx',
                            @new_plan = 1,
                            @plan_name = null,
                            @no_select = 1


    -- Fail: Not enabled plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%exist%plan_configuration%' ;
    EXECUTE meta.usp_new_run    @pipeline_id = 'xxx',
                            @new_plan = 1,
                            @plan_name = 'random',
                            @no_select = 1

    -- Fail: Not enabled plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%not%enabled%plan_configuration%' ;
    EXECUTE meta.usp_new_run    @pipeline_id = 'xxx',
                            @new_plan = 1,
                            @plan_name = 'unit_test_notenabled',
                            @no_select = 1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test should fail on non-existing plan_name.sql">
CREATE PROCEDURE [t_usp_new_run].[test should fail on non-existing plan_name]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: Not enabled plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%exist%plan_configuration%' ;
    EXECUTE meta.usp_new_run    @pipeline_id = 'xxx',
                            @new_plan = 1,
                            @plan_name = 'random',
                            @no_select = 1

    -- Fail: Not enabled plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%not%enabled%plan_configuration%' ;
    EXECUTE meta.usp_new_run    @pipeline_id = 'xxx',
                            @new_plan = 1,
                            @plan_name = 'unit_test_notenabled',
                            @no_select = 1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test with filter should log specific tasks.sql">
/*

=======================================================
Author: Simon Plancke
Create date: 2024-12-13
Description: Test meta.usp_new_run for parameter new_plan = 1 and task_filter not null
=======================================================

When parameter task_filter not null:
* Expect only a specific tasks to be added to the log tasks as PENDING

=======================================================

*/


CREATE PROCEDURE [t_usp_new_run].[test with filter should log specific tasks]
    @DEBUG INT = 0
AS
BEGIN

--region DECLARE parameters of stored procedure
    DECLARE @plan_name  NVARCHAR(100) = 'unit_test_filtered'
    DECLARE @new_plan   BIT = 1
    DECLARE @task_filter NVARCHAR(100) = 'unit_test_filtered'
--endregion


--region CREATE temporary tables for assertions: EXPECTED and ACTUAL
    -- Create a set of temporary tables (based on the actual logging tables of the metadb)
    -- Only use the relevant column names (ids, names, status)

    -- log_runs
    CREATE TABLE #expected_log_runs(
	    [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
	    [current_status]    NVARCHAR(100)
    )

    CREATE TABLE #actual_log_runs(
	    [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
	    [current_status]    NVARCHAR(100)
    )

    -- log_plans
    CREATE TABLE #expected_log_plans(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [current_status] NVARCHAR(100),
        run_id          INT
    );

    CREATE TABLE #actual_log_plans(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [current_status] NVARCHAR(100),
        run_id          INT
    );

    -- log_tasks
    CREATE TABLE #expected_log_tasks(
        [task_id]       INT,
        [task_name]     NVARCHAR(100),
        [plan_id]       INT,
        [current_status] NVARCHAR(100)
    );

    CREATE TABLE #actual_log_tasks(
        [task_id]       INT,
        [task_name]     NVARCHAR(100),
        [plan_id]       INT,
        [current_status] NVARCHAR(100)
    );

    -- log_run_plans
    CREATE TABLE #expected_log_run_plans(
        [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
        [original_run_id]   INT
    );

    CREATE TABLE #actual_log_run_plans(
        [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
        [original_run_id]   INT
    );


    -- log_plan_tasks
    CREATE TABLE #expected_log_plan_tasks(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [task_id]       INT,
        [task_name]     NVARCHAR(100)
    );

    CREATE TABLE #actual_log_plan_tasks(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [task_id]       INT,
        [task_name]     NVARCHAR(100)
    );

--endregion


--region INSERT expected values into EXPECTED temporary tables
    -- Copy the existing tables from the setup into the expected tables
    -- For each table, add a new row that shows the expected values after executing the stored procedure meta.usp_new_run

    -- meta.log_runs
    INSERT INTO #expected_log_runs(run_id, plan_name, current_status)
    SELECT run_id, plan_name, current_status
    FROM meta.log_runs
    -- Expect a new row with a PENDING run
    INSERT INTO #expected_log_runs(run_id, plan_name, current_status)
    VALUES (5, @plan_name, 'PENDING')
    
    
    -- meta.log_plans
    INSERT INTO #expected_log_plans(plan_id, plan_name, current_status)
    SELECT plan_id, plan_name, current_status
    FROM meta.log_plans
    -- Expect a new row with a PENDING plan
    INSERT INTO #expected_log_plans(plan_id, plan_name, current_status)
    VALUES (5, @plan_name, 'PENDING')


    -- meta.log_tasks
    INSERT INTO #expected_log_tasks(task_id, task_name, plan_id, current_status)
    SELECT task_id, task_name, plan_id, current_status
    FROM meta.log_tasks
    -- Expect only 1 new PENDING task (so other tasks in the plan should not be logged (unit_test_enabled and unit_test_disabled))
    INSERT INTO #expected_log_tasks(task_id, task_name, plan_id, current_status)
    VALUES (5, @task_filter, 5, 'PENDING')


    -- meta.log_run_plans
    INSERT INTO #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    SELECT run_id, plan_id, plan_name, original_run_id
    FROM meta.log_run_plans
    -- Expect a new row in the log_run_plans
    INSERT INTO #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    VALUES (5, 5, @plan_name, 5)

   
    -- meta.log_plan_tasks
    INSERT INTO #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    SELECT plan_id, plan_name, task_id, task_name
    FROM meta.log_plan_tasks
    -- Expect a new row in the log_plan_tasks
    INSERT INTO #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    VALUES (5, @plan_name, 5, @task_filter)
--endregion


--region EXECUTE stored procedure and get results in ACTUAL temporary tables
    -- Execute the stored procedure with a task_filter
    -- Expect that only one task is added to the log_tasks table
    -- Tasks 'unit_test_enabled' and 'unit_test_disabled' should be skipped as they do not comply with the filter
    EXECUTE meta.usp_new_run @plan_name = @plan_name, @new_plan = @new_plan, @pipeline_id = "filtered_ingest", @task_filter = @task_filter

    -- Get the results from the FakeTables into the ACTUAL temporary tables
    INSERT INTO #actual_log_runs(run_id, plan_name, current_status)
    SELECT run_id, plan_name, current_status
    FROM meta.log_runs

    INSERT INTO #actual_log_plans(plan_id, plan_name, current_status)
    SELECT plan_id, plan_name, current_status
    FROM meta.log_plans

    INSERT INTO #actual_log_tasks(task_id, task_name, plan_id, current_status)
    SELECT task_id, task_name, plan_id, current_status
    FROM meta.log_tasks

    INSERT INTO #actual_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    SELECT run_id, plan_id, plan_name, original_run_id
    FROM meta.log_run_plans

    INSERT INTO #actual_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    SELECT plan_id, plan_name, task_id, task_name
    FROM meta.log_plan_tasks
--endregion


--region EVALUATE the expected tables against the actual tables
    -- Check that all expectations match with the actuals
    -- In other words, the actual tables should be the same as the expected tables
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_runs',       @Actual = '#actual_log_runs' ,      @Message = 'Expected log_runs table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_plans',      @Actual = '#actual_log_plans' ,     @Message = 'Expected log_plans table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_tasks',      @Actual = '#actual_log_tasks' ,     @Message = 'Expected log_tasks table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_run_plans',  @Actual = '#actual_log_run_plans',  @Message = 'Expected log_run_plans table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_plan_tasks', @Actual = '#actual_log_plan_tasks', @Message = 'Expected log_plan_tasks table check failed.'
--endregion

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test_new_plan_should_add_logging_items.sql">
/*

=======================================================
Author: Simon Plancke
Create date: 2024-09-03
Description: Test meta.usp_new_run for parameter new_plan = 1
=======================================================

When parameter new_plan == 1:
* Expect a new row to be added to all logging tables, with current_status = 'PENDING'

=======================================================

*/


CREATE PROCEDURE [t_usp_new_run].[test_new_plan_should_add_logging_items]
    @DEBUG INT = 0
AS
BEGIN

    -- DECLARE parameters of stored procedure
    DECLARE @plan_name  NVARCHAR(100) = 'unit_test_enabled'
    DECLARE @new_plan   BIT = 1


    -- CREATE temporary tables for assertions: EXPECTED and ACTUAL
    -- log_runs
    CREATE TABLE #expected_log_runs(
	    [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
	    [current_status]    NVARCHAR(100)
    )

    CREATE TABLE #actual_log_runs(
	    [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
	    [current_status]    NVARCHAR(100)
    )

    -- log_plans
    CREATE TABLE #expected_log_plans(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [current_status] NVARCHAR(100),
        run_id          INT
    );

    CREATE TABLE #actual_log_plans(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [current_status] NVARCHAR(100),
        run_id          INT
    );

    -- log_tasks
    CREATE TABLE #expected_log_tasks(
        [task_id]       INT,
        [task_name]     NVARCHAR(100),
        [plan_id]       INT,
        [current_status] NVARCHAR(100)
    );

    CREATE TABLE #actual_log_tasks(
        [task_id]       INT,
        [task_name]     NVARCHAR(100),
        [plan_id]       INT,
        [current_status] NVARCHAR(100)
    );

    -- log_run_plans
    CREATE TABLE #expected_log_run_plans(
        [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
        [original_run_id]   INT
    );

    CREATE TABLE #actual_log_run_plans(
        [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
        [original_run_id]   INT
    );


    -- log_plan_tasks
    CREATE TABLE #expected_log_plan_tasks(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [task_id]       INT,
        [task_name]     NVARCHAR(100)
    );

    CREATE TABLE #actual_log_plan_tasks(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [task_id]       INT,
        [task_name]     NVARCHAR(100)
    );


    -- INSERT expected values into expected tables
    -- meta.log_runs
    INSERT INTO #expected_log_runs(run_id, plan_name, current_status)
    SELECT run_id, plan_name, current_status
    FROM meta.log_runs

    INSERT INTO #expected_log_runs(run_id, plan_name, current_status)
    VALUES (5, @plan_name, 'PENDING')
    
    -- meta.log_plans
    INSERT INTO #expected_log_plans(plan_id, plan_name, current_status, run_id)
    SELECT plan_id, plan_name, current_status, run_id
    FROM meta.log_plans

    INSERT INTO #expected_log_plans(plan_id, plan_name, current_status, run_id)
    VALUES (5, @plan_name, 'PENDING', 5)    

    -- meta.log_tasks
    INSERT INTO #expected_log_tasks(task_id, task_name, plan_id, current_status)
    SELECT task_id, task_name, plan_id, current_status
    FROM meta.log_tasks
    
    INSERT INTO #expected_log_tasks(task_id, task_name, plan_id, current_status)
    VALUES (5, 'unit_test_enabled', 5, 'PENDING')

    -- meta.log_run_plans
    INSERT INTO #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    SELECT run_id, plan_id, plan_name, original_run_id
    FROM meta.log_run_plans

    INSERT INTO #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    VALUES (5, 5, @plan_name, 5)

    -- meta.log_plan_tasks
    INSERT INTO #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    SELECT plan_id, plan_name, task_id, task_name
    FROM meta.log_plan_tasks

    INSERT INTO #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    VALUES (5, @plan_name, 5, 'unit_test_enabled')


    EXECUTE meta.usp_new_run @plan_name = @plan_name, @new_plan = @new_plan, @pipeline_id = "successful_unittest"

    INSERT INTO #actual_log_runs(run_id, plan_name, current_status)
    SELECT run_id, plan_name, current_status
    FROM meta.log_runs

    INSERT INTO #actual_log_plans(plan_id, plan_name, current_status, run_id)
    SELECT plan_id, plan_name, current_status, run_id
    FROM meta.log_plans

    INSERT INTO #actual_log_tasks(task_id, task_name, plan_id, current_status)
    SELECT task_id, task_name, plan_id, current_status
    FROM meta.log_tasks

    INSERT INTO #actual_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    SELECT run_id, plan_id, plan_name, original_run_id
    FROM meta.log_run_plans

    INSERT INTO #actual_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    SELECT plan_id, plan_name, task_id, task_name
    FROM meta.log_plan_tasks

    
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_runs',       @Actual = '#actual_log_runs' ,      @Message = 'Expected log_runs table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_plans',      @Actual = '#actual_log_plans' ,     @Message = 'Expected log_plans table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_tasks',      @Actual = '#actual_log_tasks' ,     @Message = 'Expected log_tasks table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_run_plans',  @Actual = '#actual_log_run_plans',  @Message = 'Expected log_run_plans table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_plan_tasks', @Actual = '#actual_log_plan_tasks', @Message = 'Expected log_plan_tasks table check failed.'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_new_run\test_rerun_plan_should_reset_failure.sql">
/*

=======================================================
Author: Simon Plancke
Create date: 2024-09-03
Description: Test meta.usp_new_run for parameter new_plan = 1
=======================================================

When parameter new_plan == 0:
* Expect a new row to be added to meta.log_runs and meta.log_run_plans
* Expect the last failed plan with @plan_name to be reset to 'PENDING'
* Expect the failed tasks of this plan to be reset to 'PENDING'

=======================================================

*/


CREATE PROCEDURE [t_usp_new_run].[test_rerun_plan_should_reset_failure]
    @DEBUG INT = 0
AS
BEGIN

    -- DECLARE parameters of stored procedure
    DECLARE @plan_name  NVARCHAR(100) = 'unit_test_enabled'
    DECLARE @new_plan   BIT = 0


    -- CREATE temporary tables for assertions: EXPECTED and ACTUAL
    -- log_runs
    CREATE TABLE #expected_log_runs(
	    [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
	    [current_status]    NVARCHAR(100)
    )

    CREATE TABLE #actual_log_runs(
	    [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
	    [current_status]    NVARCHAR(100)
    )

    -- log_plans
    CREATE TABLE #expected_log_plans(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [current_status] NVARCHAR(100),
        run_id          INT
    );

    CREATE TABLE #actual_log_plans(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [current_status] NVARCHAR(100),
        run_id          INT
    );

    -- log_tasks
    CREATE TABLE #expected_log_tasks(
        [task_id]       INT,
        [task_name]     NVARCHAR(100),
        [plan_id]       INT,
        [current_status] NVARCHAR(100)
    );

    CREATE TABLE #actual_log_tasks(
        [task_id]       INT,
        [task_name]     NVARCHAR(100),
        [plan_id]       INT,
        [current_status] NVARCHAR(100)
    );

    -- log_run_plans
    CREATE TABLE #expected_log_run_plans(
        [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
        [original_run_id]   INT
    );

    CREATE TABLE #actual_log_run_plans(
        [run_id]            INT,
        [plan_id]           INT,
        [plan_name]         NVARCHAR(100),
        [original_run_id]   INT
    );


    -- log_plan_tasks
    CREATE TABLE #expected_log_plan_tasks(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [task_id]       INT,
        [task_name]     NVARCHAR(100)
    );

    CREATE TABLE #actual_log_plan_tasks(
        [plan_id]       INT,
        [plan_name]     NVARCHAR(100),
        [task_id]       INT,
        [task_name]     NVARCHAR(100)
    );


    -- INSERT expected values into expected tables
    -- meta.log_runs
    INSERT INTO #expected_log_runs(run_id, plan_name, current_status)
    SELECT run_id, plan_name, current_status
    FROM meta.log_runs

    INSERT INTO #expected_log_runs(run_id, plan_name, current_status)
    VALUES (5, @plan_name, 'PENDING')
    
    -- meta.log_plans
    INSERT INTO #expected_log_plans(plan_id, plan_name, current_status)
    SELECT plan_id, plan_name, current_status
    FROM meta.log_plans

    update #expected_log_plans
    set current_status = 'PENDING'
    where plan_id = 2

    -- meta.log_tasks
    INSERT INTO #expected_log_tasks(task_id, task_name, plan_id, current_status)
    SELECT task_id, task_name, plan_id, current_status
    FROM meta.log_tasks

    update #expected_log_tasks
    set current_status = 'PENDING'
    where plan_id = 2

    -- meta.log_run_plans
    INSERT INTO #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    SELECT run_id, plan_id, plan_name, original_run_id
    FROM meta.log_run_plans

    INSERT INTO #expected_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    VALUES (5, 2, @plan_name, 2)

    -- meta.log_plan_tasks
    INSERT INTO #expected_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    SELECT plan_id, plan_name, task_id, task_name
    FROM meta.log_plan_tasks


    EXECUTE meta.usp_new_run @plan_name = @plan_name, @new_plan = @new_plan, @pipeline_id = "successful_unittest"

    INSERT INTO #actual_log_runs(run_id, plan_name, current_status)
    SELECT run_id, plan_name, current_status
    FROM meta.log_runs

    INSERT INTO #actual_log_plans(plan_id, plan_name, current_status)
    SELECT plan_id, plan_name, current_status
    FROM meta.log_plans

    INSERT INTO #actual_log_tasks(task_id, task_name, plan_id, current_status)
    SELECT task_id, task_name, plan_id, current_status
    FROM meta.log_tasks

    INSERT INTO #actual_log_run_plans(run_id, plan_id, plan_name, original_run_id)
    SELECT run_id, plan_id, plan_name, original_run_id
    FROM meta.log_run_plans

    INSERT INTO #actual_log_plan_tasks(plan_id, plan_name, task_id, task_name)
    SELECT plan_id, plan_name, task_id, task_name
    FROM meta.log_plan_tasks

    
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_runs',       @Actual = '#actual_log_runs' ,      @Message = 'Expected log_runs table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_plans',      @Actual = '#actual_log_plans' ,     @Message = 'Expected log_plans table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_tasks',      @Actual = '#actual_log_tasks' ,     @Message = 'Expected log_tasks table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_run_plans',  @Actual = '#actual_log_run_plans',  @Message = 'Expected log_run_plans table check failed.'
    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_log_plan_tasks', @Actual = '#actual_log_plan_tasks', @Message = 'Expected log_plan_tasks table check failed.'


END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\class.sql">
CREATE SCHEMA [t_usp_proc_log]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_proc_log'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\setup.sql">
CREATE PROCEDURE [t_usp_proc_log].[setup]
AS
BEGIN

    exec tSQLt.FakeTable	'meta.proc_log', @Identity=1

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\test_normal_execution.sql">
/*
if object_id('[t_usp_proc_log].[test_normal_execution]') is not null 
    drop PROC [t_usp_proc_log].[test_normal_execution]
GO
*/
CREATE PROCEDURE [t_usp_proc_log].[test_normal_execution]
AS

BEGIN

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    
    declare @expectedValue varchar(1000);
    declare @actualValue varchar(1000);

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    exec meta.usp_proc_log
        @proc_name = 'unit_test',
        @log_type = 'warning',
        @statement_name = 'statement_name',
        @message = 'foo',
        @extra_parameters = '{"foo":"bar"}'

    ----------------------------------------------------------------------------------------------------------------
    -- assert
    ----------------------------------------------------------------------------------------------------------------

    -- expect 1 log record
    set @expectedValue = 1
    set @actualValue = (select count(*) from meta.proc_log)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue, @Message = 'rowcount not as expected.'

    -- check extra_parameters
    set @expectedValue = '{"foo":"bar"}'
    set @actualValue = (select extra_parameters from meta.proc_log)
    exec tSQLt.AssertEquals @Expected = @expectedValue, @Actual = @actualValue, @Message = 'extra_parameters not as expected.'

END

GO

--exec tSQLt.Run '[t_usp_proc_log].[test_normal_execution]'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_proc_log\test_should_fail_when_invalid_json.sql">
/*
if object_id('[t_usp_proc_log].[test_should_fail_when_invalid_json]') is not null 
    drop PROC [t_usp_proc_log].[test_should_fail_when_invalid_json]
GO
*/
CREATE PROCEDURE [t_usp_proc_log].[test_should_fail_when_invalid_json]
AS

BEGIN

    ----------------------------------------------------------------------------------------------------------------
    -- assemble
    ----------------------------------------------------------------------------------------------------------------

    declare @expectedValueString varchar(1000);
    declare @actualValueString varchar(1000);

    ----------------------------------------------------------------------------------------------------------------
    -- act &amp; assert
    ----------------------------------------------------------------------------------------------------------------
  
    -- add property to invalid start string
    set @expectedValueString = 'error%not valid json%';
    set @actualValueString = (select meta.fn_add_json_property('invalid', 'foo', 'bar'));
    exec tSQLt.AssertLike @ExpectedPattern = @expectedValueString, @Actual = @actualValueString , @Message = 'test: adding property to invalid json failed.'


END

GO

--exec tSQLt.Run '[t_usp_proc_log].[test_should_fail_when_invalid_json]'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_configuration\class_t_usp_plan_configuration.sql">
CREATE SCHEMA [t_usp_set_plan_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_set_plan_configuration'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_configuration\setup_t_usp_set_plan_configuration.sql">
CREATE PROCEDURE [t_usp_set_plan_configuration].[setup]
    @DEBUG int = 0
AS
BEGIN
    EXEC tSQLt.FakeTable 'meta.plan_configuration'

    INSERT INTO meta.plan_configuration ( plan_name, [enabled])
    VALUES  ( 'unit_test_deploy_enabled', 1 ),
            ( 'unit_test_deploy_disabled', 0 )

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_configuration\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_set_plan_configuration].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_name%cannot%null%empty%'
    EXECUTE deploy.usp_set_plan_configuration   @plan_name = null,
                                            @plan_description = 'unittest',
                                            @enabled = 1
    
    -- Fail: no plan_description
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_desc%cannot%null%empty%'
    EXECUTE deploy.usp_set_plan_configuration   @plan_name = 'unittest',
                                            @plan_description = null,
                                            @enabled = 1

    -- Fail: No enabled flag
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%enabled%cannot%null%'
    EXECUTE deploy.usp_set_plan_configuration   @plan_name = 'unittest',
                                            @plan_description = 'unittest',
                                            @enabled = null

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_task_configuration\class_t_usp_set_plan_task_configuration.sql">
CREATE SCHEMA [t_usp_set_plan_task_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_set_plan_task_configuration'

</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_task_configuration\setup_t_usp_set_plan_task_configuration.sql">
CREATE PROCEDURE [t_usp_set_plan_task_configuration].[setup]
    @DEBUG int = 0
AS
BEGIN
    EXEC tSQLt.FakeTable 'meta.plan_task_configuration'

    INSERT INTO meta.plan_task_configuration( plan_name, task_name, [enabled])
    VALUES ( 'unit_test_deploy_enabled', 'unit_test_deploy_enabled', 1),
            ( 'unit_test_deploy_disabled', 'unit_test_deploy_disabled', 0)

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_plan_task_configuration\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_set_plan_task_configuration].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no plan_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_name%cannot%null%empty%'
    EXECUTE deploy.usp_set_plan_task_configuration      @plan_name = null,
                                                    @plantask_data = 'unittest'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_check_configuration\class_t_usp_set_source_check_configuration.sql">
CREATE SCHEMA [t_usp_set_source_check_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_set_source_check_configuration'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_check_configuration\setup_t_usp_set_source_check_configuration.sql">
CREATE PROCEDURE [t_usp_set_source_check_configuration].[setup]
AS
BEGIN
    
	EXEC tSQLt.FakeTable	'meta.source_check_configuration'

	INSERT INTO meta.source_check_configuration (check_name, file_layout, [enabled])
	VALUES 	('unit_test_deploy_enabled', 'unit_test_deploy_enabled', 1),
			('unit_test_deploy_disabled', 'unit_test_deploy_disabled', 0)
		

END;

GO
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_check_configuration\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_set_source_check_configuration].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no file_layout
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%file_layout%cannot%NULL%empty%'
    EXECUTE deploy.usp_set_source_check_configuration   @file_layout = null,
                                                    @checks_data = 'unittest_mock'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\class_t_usp_set_source_column_configuration.sql">
CREATE SCHEMA [t_usp_set_source_column_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_set_source_column_configuration'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\setup_t_usp_set_source_column_configuration.sql">
CREATE PROCEDURE [t_usp_set_source_column_configuration].[setup]
    @DEBUG int = 0
AS
BEGIN
    EXEC tSQLt.FakeTable 'meta.source_column_configuration'

    -- INSERT INTO meta.source_column_configuration ( file_layout )
    -- VALUES  ( 'unit_test_deploy_enabled')

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_set_source_column_configuration].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no file_layout
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%file_layout%cannot%null%empty%'
    EXECUTE deploy.usp_set_source_column_configuration  @file_layout = null,
                                                        @column_data = 'unittest_mock'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_column_configuration\test_should_ingest_new_source_code.sql">
/*

=======================================================
Author: Simon Plancke
Create date: 2024-09-10
Description: Add a new row to meta.source_column_configuration
=======================================================

*/


CREATE PROCEDURE [t_usp_set_source_column_configuration].[test_should_ingest_new_source_code]
    @DEBUG int = 0
AS
BEGIN

    CREATE TABLE #expected_source_column_config(
        [file_layout]		varchar(100),
        [source_name]		varchar(100),
        [sink_name]			varchar(100),
        [column_order]		int,
        [dimension]			varchar(5),
        [data_type]			varchar(20),
	    [column_info]		varchar(max)
    )

    CREATE TABLE #actual_source_column_config(
        [file_layout]		varchar(100),
        [source_name]		varchar(100),
        [sink_name]			varchar(100),
        [column_order]		int,
        [dimension]			varchar(5),
        [data_type]			varchar(20),
	    [column_info]		varchar(max)
    )

    INSERT INTO #expected_source_column_config(file_layout, source_name, sink_name, column_order, dimension, data_type, column_info)
    VALUES  ('test_source_column', 'test_column_info', 'test_column_info', 1, 'PK', 'test_datatype_1', '{locale: fr-FR}'),
            ('test_source_column', 'test_no_column_info', 'test_no_column_info', 2, 'SCD2', 'test_datatype_2', NULL)

    exec [deploy].[usp_set_source_column_configuration]
        @file_layout = 'test_source_column',
        @column_data = '[
        {
            "column_sequence": 1,
            "source_column_name": "test_column_info",
            "sink_column_name": "test_column_info",
            "dimension": "PK",
            "data_type": "test_datatype_1",
            "column_info": "{locale: fr-FR}"
        },
        {
            "column_sequence": 2,
            "source_column_name": "test_no_column_info",
            "sink_column_name": "test_no_column_info",
            "dimension": "SCD2",
            "data_type": "test_datatype_2"
        }
        ]'

    INSERT INTO #actual_source_column_config(file_layout, source_name, sink_name, column_order, dimension, data_type, column_info)
    select * from meta.source_column_configuration

    EXECUTE tSQLt.AssertEqualsTable @Expected = '#expected_source_column_config',       @Actual = '#actual_source_column_config' ,      @Message = 'Expected source_column_config table check failed.'

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_configuration\class_t_usp_set_source_configuration.sql">
CREATE SCHEMA [t_usp_set_source_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_set_source_configuration'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_configuration\setup_t_usp_set_source_configuration.sql">
CREATE PROCEDURE [t_usp_set_source_configuration].[setup]
    @DEBUG int = 0
AS
BEGIN
    EXEC tSQLt.FakeTable 'meta.source_configuration'

    INSERT INTO meta.source_configuration ( file_layout, file_pattern, quote_character, escape_character)
    VALUES  ( 'unit_test_deploy_enabled', 'unit_test_deploy_enabled', 'm', 'm' )

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_source_configuration\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_set_source_configuration].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no file_layout
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%file_layout%cannot%null%empty%'
    EXECUTE deploy.usp_set_source_configuration @file_layout = null,
                                            @file_pattern = 'unittest',
                                            @file_extension = 'unittest',
                                            @file_kind = null,
                                            @column_delimiter = null,
                                            @row_delimiter = null,
                                            @escape_character = 'mock',
                                            @quote_character = 'mock',
                                            @header = null,
                                            @encoding = null,
                                            @skip_first_lines = null,
                                            @source_conditions = null

    -- Fail: no file_pattern
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%file_pattern%cannot%null%empty%'
    EXECUTE deploy.usp_set_source_configuration @file_layout = 'unittest',
                                            @file_pattern = null,
                                            @file_kind = 'unittest',
                                            @file_extension = null,
                                            @column_delimiter = null,
                                            @row_delimiter = null,
                                            @escape_character = 'mock',
                                            @quote_character = 'mock',
                                            @header = null,
                                            @encoding = null,
                                            @skip_first_lines = null,
                                            @source_conditions = null

    -- Fail: if a quote_character is specified, then the escape_character cannot be null or empty
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%quote_character%specified%escape_character%cannot%null%empty%'
    EXECUTE deploy.usp_set_source_configuration @file_layout = 'unittest',
                                            @file_pattern = 'unittest',
                                            @file_kind = 'unittest',
                                            @file_kind = null,
                                            @file_extension = null,
                                            @column_delimiter = null,
                                            @row_delimiter = null,
                                            @escape_character = null,
                                            @quote_character = 'mock',
                                            @header = null,
                                            @encoding = null,
                                            @skip_first_lines = null,
                                            @source_conditions = null

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_task_configuration\class_t_usp_set_task_configuration.sql">
CREATE SCHEMA [t_usp_set_task_configuration]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_set_task_configuration'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_task_configuration\setup_t_usp_set_task_configuration.sql">
CREATE PROCEDURE [t_usp_set_task_configuration].[setup]
    @DEBUG int = 0
AS
BEGIN
    EXEC tSQLt.FakeTable 'meta.task_configuration'

    INSERT INTO meta.task_configuration ( task_name, task_type, worker_name, task_description, file_layout, container_name, [enabled])
    VALUES  ( 'unit_test_deploy_enabled', 'mock_type', 'mock_worker', 'mock_description', 'mock_file', 'mock_container', 1 ),
            ( 'unit_test_deploy_disabled', 'mock_type2', 'mock_worker2', 'mock_description2', 'mock_file2', 'mock_container2', 0 )

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_set_task_configuration\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_set_task_configuration].[test should fail on bad params]
    @DEBUG int = 0
AS
BEGIN

    -- Fail: no task_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_name%cannot%null%empty%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = null,
                                            @task_type          = 'mock',
                                            @worker_name        = 'mock',
                                            @task_description   = 'mock',
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = 'mock',
                                            @source_folder      = null,
                                            @container_name     = 'mock',
                                            @enabled            = 1
    -- Fail: No task_type
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_type%cannot%null%empty%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = 'mock',
                                            @task_type          = null,
                                            @worker_name        = 'mock',
                                            @task_description   = 'mock',
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = 'mock',
                                            @source_folder      = null,
                                            @container_name     = 'mock',
                                            @enabled            = 1

    -- Fail: No worker_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%worker_name%cannot%null%empty%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = 'mock',
                                            @task_type          = 'mock',
                                            @worker_name        = null,
                                            @task_description   = 'mock',
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = 'mock',
                                            @source_folder      = null,
                                            @container_name     = 'mock',
                                            @enabled            = 1

    -- Fail: No task_description
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_description%cannot%null%empty%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = 'mock',
                                            @task_type          = 'mock',
                                            @worker_name        = 'mock',
                                            @task_description   = null,
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = 'mock',
                                            @source_folder      = null,
                                            @container_name     = 'mock',
                                            @enabled            = 1

    -- Fail: No file_layout
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%file_layout%cannot%null%empty%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = 'mock',
                                            @task_type          = 'mock',
                                            @worker_name        = 'mock',
                                            @task_description   = 'mock',
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = null,
                                            @source_folder      = null,
                                            @container_name     = 'mock',
                                            @enabled            = 1

    -- Fail: No container_name
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%container_name%cannot%null%empty%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = 'mock',
                                            @task_type          = 'mock',
                                            @worker_name        = 'mock',
                                            @task_description   = 'mock',
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = 'mock',
                                            @source_folder      = null,
                                            @container_name     = null,
                                            @enabled            = 1

    -- Fail: No enabled flag
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%enabled%cannot%null%'
    EXECUTE deploy.usp_set_task_configuration   @task_name          = 'mock',
                                            @task_type          = 'mock',
                                            @worker_name        = 'mock',
                                            @task_description   = 'mock',
                                            @table_name         = null,
                                            @target_options     = null,
                                            @file_layout        = 'mock',
                                            @source_folder      = null,
                                            @container_name     = 'mock',
                                            @enabled            = null

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_plan\class_t_usp_start_plan.sql">
CREATE SCHEMA [t_usp_start_plan]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_start_plan'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_plan\setup_t_usp_start_plan.sql">
CREATE PROCEDURE [t_usp_start_plan].[setup]
    @DEBUG int = 0
AS
BEGIN
    exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    -- exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    -- exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    -- exec tSQLt.FakeTable 'meta.log_files', @Identity = 1
    -- exec tSQLt.FakeTable 'meta.plan_configuration'
    -- exec tSQLt.FakeTable 'meta.task_configuration'
    -- exec tSQLt.FakeTable 'meta.source_configuration'
    -- exec tSQLt.FakeTable 'meta.source_column_configuration'
    -- exec tSQLt.FakeTable 'meta.plan_task_configuration'

    INSERT INTO meta.log_runs ( pipeline_id, new_plan, plan_name, current_status )
    VALUES ( 'xxx', 1, 'unit_test', 'PENDING' )

    DECLARE @run_id int = scope_identity()
    
    INSERT INTO meta.log_plans ( pipeline_id, plan_name, registration_ts, run_id, current_status )
    VALUES ( 'xxx', 'unit_test', GetDate(), @Run_ID, 'IN PROGRESS')

    DECLARE @plan_id int = scope_identity()

    INSERT INTO meta.log_run_plans ( run_id, plan_id, plan_name, original_run_id )
    VALUES ( @run_id, @plan_id, 'unit_test', @run_id )

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_plan\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_start_plan].[test should fail on bad params]
    @Debug int = 0
AS

BEGIN

    -- Fail: No plan_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%run_id%mandatory%'
    EXECUTE meta.usp_start_plan @plan_id = 1,
                            @run_id = null

    -- Fail: No run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%plan_id%mandatory%'
    EXECUTE meta.usp_start_plan @plan_id = null,
                            @run_id = 1

    -- Fail: Invalid run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_plans%'
    EXECUTE meta.usp_start_plan @plan_id = 0,
                            @run_id = 1


    -- Fail: Invalid plan_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_runs%'
    EXECUTE meta.usp_start_plan @plan_id = 1,
                            @run_id = 0

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_run\class_t_usp_start_run.sql">
CREATE SCHEMA [t_usp_start_run]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_start_run'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_run\setup_t_usp_start_run.sql">
CREATE PROCEDURE [t_usp_start_run].[setup]
    @DEBUG int = 0
AS
BEGIN
    exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    --exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    -- exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	-- exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    --exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    -- exec tSQLt.FakeTable 'meta.log_files', @Identity = 1
    -- exec tSQLt.FakeTable 'meta.plan_configuration'
    -- exec tSQLt.FakeTable 'meta.task_configuration'
    -- exec tSQLt.FakeTable 'meta.source_configuration'
    -- exec tSQLt.FakeTable 'meta.source_column_configuration'
    -- exec tSQLt.FakeTable 'meta.plan_task_configuration'

    INSERT INTO meta.log_runs ( pipeline_id, new_plan, plan_name, current_status )
    VALUES ( 'xxx', 1, 'unit_test', 'IN PROGRESS' )

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_run\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_start_run].[test should fail on bad params]
    @Debug int = 0
AS

BEGIN

    -- Fail: No run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%run_id%mandatory%'
    EXECUTE meta.usp_start_run @run_id = null

    -- Fail: Invalid run_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_runs%'
    EXECUTE meta.usp_start_run @run_id = 0

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_task\class_t_usp_start_task.sql">
CREATE SCHEMA [t_usp_start_task]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_start_task'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_task\setup_t_usp_start_task.sql">
CREATE PROCEDURE [t_usp_start_task].[setup]
    @DEBUG int = 0
AS
BEGIN
    --exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	-- exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    -- exec tSQLt.FakeTable 'meta.log_files', @Identity = 1
    -- exec tSQLt.FakeTable 'meta.plan_configuration'
    -- exec tSQLt.FakeTable 'meta.task_configuration'
    -- exec tSQLt.FakeTable 'meta.source_configuration'
    -- exec tSQLt.FakeTable 'meta.source_column_configuration'
    -- exec tSQLt.FakeTable 'meta.plan_task_configuration'
    
    INSERT INTO meta.log_plans ( pipeline_id, plan_name, registration_ts, current_status )
    VALUES ( 'xxx', 'unit_test', GetDate(), 'IN PROGRESS')

    DECLARE @plan_id int = scope_identity()

    INSERT INTO meta.log_tasks ( task_name, plan_id, current_status )
    VALUES ( 'unit_test', @plan_id, 'PENDING' )

    DECLARE @task_id int = scope_identity()

    
    INSERT INTO meta.log_plan_tasks ( plan_id, plan_name, task_id, task_name )
    VALUES ( @plan_id, 'unit_test', @task_id, 'unit_test' )

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_start_task\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_start_task].[test should fail on bad params]
    @Debug int = 0
AS

BEGIN

    -- Fail: No task_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = 'plan_id%mandatory%'
    EXECUTE meta.usp_start_task @task_id = 1,
                            @plan_id = null

    -- Fail: No plan_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%task_id%mandatory%'
    EXECUTE meta.usp_start_task @task_id = null,
                            @plan_id = 1

    -- Fail: Invalid plan_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_tasks%'
    EXECUTE meta.usp_start_task @task_id = 0,
                            @plan_id = 1


    -- Fail: Invalid task_id
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%log_plans%'
    EXECUTE meta.usp_start_task @task_id = 1,
                            @plan_id = 0

END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\class_t_usp_update_file_activity.sql">
CREATE SCHEMA [t_usp_update_file_activity]
    AUTHORIZATION dbo
GO
EXECUTE sp_addextendedproperty @name = 'tSQLt.TestClass', @value = 1, @level0type = 'SCHEMA', @level0name = 't_usp_update_file_activity'
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\setup_t_usp_update_file_activity.sql">

CREATE PROCEDURE [t_usp_update_file_activity].[setup]
    @DEBUG int = 0
AS
BEGIN
    -- exec tSQLt.FakeTable	'meta.log_runs', @Identity=1
    -- exec tSQLt.FakeTable	'meta.log_plans', @Identity=1
    -- exec tSQLt.FakeTable	'meta.log_tasks', @Identity=1
	-- exec tSQLt.FakeTable	'meta.log_run_plans', @Identity = 1
    -- exec tSQLt.FakeTable	'meta.log_plan_tasks', @Identity=1
    exec tSQLt.FakeTable 'meta.log_files'
    -- exec tSQLt.FakeTable 'meta.plan_configuration'
    -- exec tSQLt.FakeTable 'meta.task_configuration'
    -- exec tSQLt.FakeTable 'meta.source_configuration'
    -- exec tSQLt.FakeTable 'meta.source_column_configuration'
    -- exec tSQLt.FakeTable 'meta.plan_task_configuration'

    INSERT INTO meta.log_files ([file_id], [filename], [extended_filename], [archive_status] )
    VALUES (1, 'unit_test.txt',     'unit_test_TS.txt',     'SUCCEEDED'),
           (2, 'raw_test.txt',      'raw_test_TS.txt',      NULL),
           (3, 'silver_test.txt',   'silver_test_TS.txt',   NULL),
           (4, 'archive_test.txt',  'archive_test_TS.txt',  NULL)

END


</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\test should fail on bad params.sql">
CREATE PROCEDURE [t_usp_update_file_activity].[test should fail on bad params]
    @Debug int = 0
AS

BEGIN

    -- Fail: No extended_filename
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%extended_filename%mandatory%'
    EXECUTE meta.usp_update_file_activity   @extended_filename = null,
                                        @activity = 'RAW',
                                        @success = 1,
                                        @info_message = ''

   -- Fail: No activity
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%activity%mandatory%'
    EXECUTE meta.usp_update_file_activity   @extended_filename = 'unit_test_TS.txt',
                                        @activity = null,
                                        @success = 1,
                                        @info_message = ''

   -- Fail: No success_bit
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%success%mandatory%'
    EXECUTE meta.usp_update_file_activity   @extended_filename = 'unit_test_TS.txt',
                                        @activity = 'RAW',
                                        @success = null,
                                        @info_message = ''

   -- Fail: No info_message
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%info_message%mandatory%'
    EXECUTE meta.usp_update_file_activity   @extended_filename = 'unit_test_TS.txt',
                                        @activity = 'RAW',
                                        @success = 1,
                                        @info_message = null

    -- Fail: No info_message
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%info_message%mandatory%'
    EXECUTE meta.usp_update_file_activity   @extended_filename = 'invalid_unit_test_TS.txt',
                                        @activity = 'RAW',
                                        @success = 1,
                                        @info_message = ''

    -- Fail: Invalid activity
    EXECUTE tSQLt.ExpectException @ExpectedMessagePattern = '%info_message%mandatory%'
    EXECUTE meta.usp_update_file_activity   @extended_filename = 'invalid_unit_test_TS.txt',
                                        @activity = 'TEST',
                                        @success = 1,
                                        @info_message = ''


END
</file>
<file name="src\sql\metadb_test\t_unit_tests\t_usp_update_file_activity\update status for failed files.sql">
CREATE PROCEDURE [t_usp_update_file_activity].[update status for failed files]
    @Debug int = 0
AS

BEGIN

    -- Prepare expected and actual tables
    CREATE TABLE [t_usp_end_plan].expected_log_files (
        [file_id]                   INT,
        [filename]                  VARCHAR(20),
        [extended_filename]         VARCHAR(20),
        [raw_status]                VARCHAR(10),
        [raw_info]                  VARCHAR(50),
        [silver_status]             VARCHAR(10),
        [silver_info]               VARCHAR(50),
        [archive_status]            VARCHAR(10),
        [archive_info]              VARCHAR(50)
    )

    CREATE TABLE [t_usp_end_plan].actual_log_files (
        [file_id]                   INT,
        [filename]                  VARCHAR(20),
        [extended_filename]         VARCHAR(20),
        [raw_status]                VARCHAR(10),
        [raw_info]                  VARCHAR(50),
        [silver_status]             VARCHAR(10),
        [silver_info]               VARCHAR(50),
        [archive_status]            VARCHAR(10),
        [archive_info]              VARCHAR(50)
    )

    INSERT INTO [t_usp_end_plan].expected_log_files([file_id], [filename], [extended_filename], [raw_status], [raw_info], [silver_status], [silver_info], [archive_status], [archive_info])
    VALUES  (2, 'raw_test.txt',      'raw_test_TS.txt',      'FAILED', 'raw file ingestion failed', NULL, NULL, NULL, NULL),
            (3, 'silver_test.txt',   'silver_test_TS.txt',   NULL, NULL, 'FAILED', 'silver file ingestion failed', NULL, NULL),
            (4, 'archive_test.txt',  'archive_test_TS.txt',  NULL, NULL, NULL, NULL, 'FAILED', 'archive file ingestion failed')



    EXECUTE meta.usp_update_file_activity   
        @extended_filename = 'raw_test_TS.txt',
        @activity = 'RAW',
        @success = 0,
        @info_message = 'raw file ingestion failed'


    EXECUTE meta.usp_update_file_activity   
        @extended_filename = 'silver_test_TS.txt',
        @activity = 'SILVER',
        @success = 0,
        @info_message = 'silver file ingestion failed'


    EXECUTE meta.usp_update_file_activity   
        @extended_filename = 'archive_test_TS.txt',
        @activity = 'ARCHIVE',
        @success = 0,
        @info_message = 'archive file ingestion failed'

    INSERT INTO [t_usp_end_plan].actual_log_files([file_id], [filename], [extended_filename], [raw_status], [raw_info], [silver_status], [silver_info], [archive_status], [archive_info])
    SELECT [file_id], [filename], [extended_filename], [raw_status], [raw_info], [silver_status], [silver_info], [archive_status], [archive_info]
    FROM meta.log_files

    exec tSQLt.AssertEqualsTable @Expected = '[t_usp_end_plan].expected_log_files', @Actual = '[t_usp_end_plan].actual_log_files' , @Message = 'File-log test failed'

END
</file>
<file name="src\sql\_build\AzureV12_master.dacpac.readme.md">
The `AzureV12_master.dacpac` file was copied from `C:\Users\joachim.baert\.vscode\extensions\ms-mssql.sql-database-projects-vscode-0.19.0\BuildDirectory\SystemDacpacs\AzureV12\master.dacpac` on my local machine because it wasn't entirely clear where it could be located on the build agent.

The project references were updated from "system database" reference to "dacpac" reference to reflect that. Obviously, referencing the "system database" and let the build process figure out where to get the master.dacpac from, would be more generic but this works as well...


</file>
<file name="src\sql\_build\db_build.ps1">
[CmdletBinding()]

param(

    [string] $ProjectPath = "$env:REPOROOT/src/sql/metadb_test/metadb_test.sqlproj"

)


# see https://learn.microsoft.com/en-us/sql/azure-data-studio/extensions/sql-database-project-extension-sdk-style-projects?view=sql-server-2016
#$SystemDacpacsLocation = "/home/vscode/.vscode-server/extensions/ms-mssql.sql-database-projects-vscode-0.19.0/BuildDirectory"
#dotnet build $PSScriptRoot/../dwhdemo/dwhdemo.sqlproj /p:NetCoreBuild=true /p:SystemDacpacsLocation=$SystemDacpacsLocation


# Build error: https://github.com/microsoft/azuredatastudio/issues/20240
# dotnet nuget add source https://api.nuget.org/v3/index.json -n nuget.org
# dotnet nuget list source

dotnet build $ProjectPath /p:NetCoreBuild=true 

if ( $LASTEXITCODE -ne 0 ) {
    Write-Error "Dotnet build failed."
}

</file>
<file name="src\sql\_build\db_deploy.ps1">
[CmdletBinding()]
param (
    [Parameter()]
    [ValidateSet("Script","Publish")]
    [String] $Action = "Publish",

    [Parameter()]
    [String] $SqlPackageExePath = $env:AGENT_SQLPACKAGE_PATH,

    [Parameter()]
    [String] $DacpacPath = "$($env:REPOROOT)\src\sql\metadb_test\bin\Debug\metadb_test.dacpac",

    #defaults to name of folder &amp; name of dacpac with "_script.sql" suffix
    [Parameter()]
    [String] $OutputScriptPath = $null,

    [Parameter()]
    [String] $PublishProfilePath = $null,

    [Parameter()]
    [String] $TargetServer = $env:SQL_SERVER,

    [Parameter()]
    [String] $TargetDatabase = $env:SQL_DATABASE,

    [Parameter()]
    [String] $AccessToken = ((Get-AzAccessToken -ResourceUrl "https://database.windows.net" -WarningAction "Ignore" -AsSecureString -Debug:$false).Token | ConvertFrom-SecureString -AsPlainText)

)

# dotnet tool install -g microsoft.sqlpackage
# Download SQLPackage for Windows: https://learn.microsoft.com/en-us/sql/tools/sqlpackage/sqlpackage-download?view=sql-server-ver16#windows-net-6

#region local script settings
    $ErrorActionPreference = "Stop"
    Set-StrictMode -Version "latest"
#endregion

#region script info
    $inputParameters = $MyInvocation.MyCommand.Parameters.Keys | Where-Object { 
        ($_ -NotIn [System.Management.Automation.PSCmdlet]::CommonParameters ) -and
        ($_ -NotIn [System.Management.Automation.PSCmdlet]::OptionalCommonParameters )
    }
    $maxParameterLength = ( $inputParameters | Measure-Object -Maximum -Property Length).Maximum
    Write-Information ""
    Write-Information "----------------------------------------------------------------------"
    Write-Information "Starting script: $($MyInvocation.MyCommand.Name)"
    Write-Information "  Path: $(Split-Path $MyInvocation.MyCommand.Source)"
    Write-Information "  Parameters:"
    foreach ($param in $inputParameters) {
    Write-Information ("  | " + $param.PadRight( $maxParameterLength  ) + ": " + (Get-Variable -Name $param).Value )
    }
    Write-Information "----------------------------------------------------------------------"
#endregion script info


# create deploy script
if ( $Action -eq "Script") {

    # if no OutputScriptPath specified, create a default
    if ( -not $OutputScriptPath ) {
        $OutputScriptPath = Join-Path ( Split-Path $DacpacPath ) ((Split-Path $DacpacPath -LeafBase) + "_script.sql")
        Write-Output "Calculated OutputScriptPath as: $OutputScriptPath"
    }   
    # run sqlpackage in script mode
    . $SqlPackageExePath `
        /action:Script `
        /sourceFile:"$DacpacPath" /outputpath:"$OutputScriptPath" `
        /targetDatabaseName:"$TargetDatabase" /targetServerName:"$TargetServer" /Accesstoken:"$Accesstoken"
    $sqlpackageExitCode = $LastExitCode
    # Remove-Variable -Name "clearPwd"
    if ( $sqlpackageExitCode -ne 0) {
        Write-Error "sqlpackage returned with failure, exitcode: $sqlpackageExitCode"
    }
}


# deploy
elseif ( $Action -eq "Publish") {
    #$clearPwd = ConvertFrom-Securestring -SecureString $TargetPassword -AsPlainText
    . $SqlPackageExePath `
        /action:Publish `
        /sourceFile:"$DacpacPath" `
        /targetDatabaseName:"$TargetDatabase" /targetServerName:"$TargetServer" /Accesstoken:"$Accesstoken" 
    $sqlpackageExitCode = $LastExitCode
    # Remove-Variable -Name "clearPwd"
    if ( $sqlpackageExitCode -ne 0) {
    Write-Error "sqlpackage returned with failure, exitcode: $sqlpackageExitCode"
    }
}

# unsupported action
else {
    Throw "Unsupported action: '$Action'"
}

#region end of script
    Write-Information ""
    Write-Information "--- End of script: $($MyInvocation.MyCommand.Name) ---"
    Write-Information ""
#endregion

</file>
<file name="src\sql\_test\drop_tsqlt_testclasses.sql">
﻿/*

This script drops all the test classes (so not the tSQLt framework itself),
which is useful to remove obsolete test artifacts before redeploying.

*/
print 'Dropping all tSQLt test classes...'
declare @testclass sysname = '';
while @testclass is not null
begin
	select @testclass = MIN( [Name]) from tSQLt.TestClasses
	if @testclass is not null
	begin
		print '  Dropping testclass: ' + @testclass
		exec tSQLt.DropClass @ClassName = @testclass
	end
end

</file>
<file name="src\sql\_test\run_testclasses.ps1">
#Requires -Module SqlServer

[CmdletBinding()]
param (
    [Parameter(Mandatory)]
    [String] $TargetServer,

    [Parameter(Mandatory)]
    [String] $TargetDatabase,

    [Parameter(Mandatory=$false)]
    [String] $AccessToken = (Get-AzAccessToken -ResourceUrl https://database.windows.net -Debug:$false).Token,

    [Parameter(Mandatory)]
    [String[]] $TestSets = @( "design", "unit") 
)


foreach ($testSet in $TestSets) {
    # $query = "
    #     DECLARE @cmd NVARCHAR(MAX);
    #     SET @cmd='ALTER DATABASE ' + QUOTENAME(DB_NAME()) + ' SET TRUSTWORTHY ON;';
    #     EXEC(@cmd);
    # "
    # Invoke-Sqlcmd -ServerInstance $TargetServer -Database $TargetDatabase -AccessToken $AccessToken -Query $query -IncludeSqlUserErrors

    Write-Output ""
    Write-Output "-----------------------------------"
    Write-Output "Execute tSQLt testset: $testSet ..."
    Write-Output "-----------------------------------"    

    $query = "exec test.usp_run_tsqlt_tests @test_type = '$testSet', @debug = 1"
    #Add -Verbose so see the sql "print" output
    Write-Output $query
    Invoke-Sqlcmd -ServerInstance $TargetServer -Database $TargetDatabase -AccessToken $AccessToken -Query $query -IncludeSqlUserErrors

}




</file>
<file name="src\sql\_test\tSQLt_1.0.5873.27393.sql">
/*
   Copyright 2011 tSQLt

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
*/
DECLARE @Msg NVARCHAR(MAX);SELECT @Msg = 'Installed at '+CONVERT(NVARCHAR,GETDATE(),121);RAISERROR(@Msg,0,1);
GO

IF TYPE_ID('tSQLt.Private') IS NOT NULL DROP TYPE tSQLt.Private;
IF TYPE_ID('tSQLtPrivate') IS NOT NULL DROP TYPE tSQLtPrivate;
GO
IF OBJECT_ID('tSQLt.DropClass') IS NOT NULL
    EXEC tSQLt.DropClass tSQLt;
GO

IF EXISTS (SELECT 1 FROM sys.assemblies WHERE name = 'tSQLtCLR')
    DROP ASSEMBLY tSQLtCLR;
GO

CREATE SCHEMA tSQLt;
GO
SET QUOTED_IDENTIFIER ON;
GO


GO

CREATE PROCEDURE tSQLt.DropClass
    @ClassName NVARCHAR(MAX)
AS
BEGIN
    DECLARE @Cmd NVARCHAR(MAX);

    WITH ObjectInfo(name, type) AS
         (
           SELECT QUOTENAME(SCHEMA_NAME(O.schema_id))+'.'+QUOTENAME(O.name) , O.type
             FROM sys.objects AS O
            WHERE O.schema_id = SCHEMA_ID(@ClassName)
         ),
         TypeInfo(name) AS
         (
           SELECT QUOTENAME(SCHEMA_NAME(T.schema_id))+'.'+QUOTENAME(T.name)
             FROM sys.types AS T
            WHERE T.schema_id = SCHEMA_ID(@ClassName)
         ),
         XMLSchemaInfo(name) AS
         (
           SELECT QUOTENAME(SCHEMA_NAME(XSC.schema_id))+'.'+QUOTENAME(XSC.name)
             FROM sys.xml_schema_collections AS XSC
            WHERE XSC.schema_id = SCHEMA_ID(@ClassName)
         ),
         DropStatements(no,cmd) AS
         (
           SELECT 10,
                  'DROP ' +
                  CASE type WHEN 'P' THEN 'PROCEDURE'
                            WHEN 'PC' THEN 'PROCEDURE'
                            WHEN 'U' THEN 'TABLE'
                            WHEN 'IF' THEN 'FUNCTION'
                            WHEN 'TF' THEN 'FUNCTION'
                            WHEN 'FN' THEN 'FUNCTION'
                            WHEN 'V' THEN 'VIEW'
                   END +
                   ' ' + 
                   name + 
                   ';'
              FROM ObjectInfo
             UNION ALL
           SELECT 20,
                  'DROP TYPE ' +
                   name + 
                   ';'
              FROM TypeInfo
             UNION ALL
           SELECT 30,
                  'DROP XML SCHEMA COLLECTION ' +
                   name + 
                   ';'
              FROM XMLSchemaInfo
             UNION ALL
            SELECT 10000,'DROP SCHEMA ' + QUOTENAME(name) +';'
              FROM sys.schemas
             WHERE schema_id = SCHEMA_ID(PARSENAME(@ClassName,1))
         ),
         StatementBlob(xml)AS
         (
           SELECT cmd [text()]
             FROM DropStatements
            ORDER BY no
              FOR XML PATH(''), TYPE
         )
    SELECT @Cmd = xml.value('/', 'NVARCHAR(MAX)') 
      FROM StatementBlob;

    EXEC(@Cmd);
END;


GO

GO
CREATE FUNCTION tSQLt.Private_Bin2Hex(@vb VARBINARY(MAX))
RETURNS TABLE
AS
RETURN
  SELECT X.S AS bare, '0x'+X.S AS prefix
    FROM (SELECT LOWER(CAST('' AS XML).value('xs:hexBinary(sql:variable("@vb") )','VARCHAR(MAX)')))X(S);
GO


GO

CREATE TABLE tSQLt.Private_NewTestClassList (
  ClassName NVARCHAR(450) PRIMARY KEY CLUSTERED
);


GO

GO
CREATE PROCEDURE tSQLt.Private_ResetNewTestClassList
AS
BEGIN
  SET NOCOUNT ON;
  DELETE FROM tSQLt.Private_NewTestClassList;
END;
GO


GO

GO
CREATE VIEW tSQLt.Private_SysTypes AS SELECT * FROM sys.types AS T;
GO
IF(CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR(MAX)) LIKE '9.%')
BEGIN
  EXEC('ALTER VIEW tSQLt.Private_SysTypes AS SELECT *,0 is_table_type FROM sys.types AS T;');
END;
GO


GO

GO
CREATE FUNCTION tSQLt.Private_GetFullTypeName(@TypeId INT, @Length INT, @Precision INT, @Scale INT, @CollationName NVARCHAR(MAX))
RETURNS TABLE
AS
RETURN SELECT X.SchemaName + '.' + X.Name + X.Suffix + X.Collation AS TypeName, X.SchemaName, X.Name, X.Suffix, X.is_table_type AS IsTableType
FROM(
  SELECT QUOTENAME(SCHEMA_NAME(T.schema_id)) SchemaName, QUOTENAME(T.name) Name,
              CASE WHEN T.max_length = -1
                    THEN ''
                   WHEN @Length = -1
                    THEN '(MAX)'
                   WHEN T.name LIKE 'n%char'
                    THEN '(' + CAST(@Length / 2 AS NVARCHAR) + ')'
                   WHEN T.name LIKE '%char' OR T.name LIKE '%binary'
                    THEN '(' + CAST(@Length AS NVARCHAR) + ')'
                   WHEN T.name IN ('decimal', 'numeric')
                    THEN '(' + CAST(@Precision AS NVARCHAR) + ',' + CAST(@Scale AS NVARCHAR) + ')'
                   ELSE ''
               END Suffix,
              CASE WHEN @CollationName IS NULL OR T.is_user_defined = 1 THEN ''
                   ELSE ' COLLATE ' + @CollationName
               END Collation,
               T.is_table_type
          FROM tSQLt.Private_SysTypes AS T WHERE T.user_type_id = @TypeId
          )X;


GO

CREATE PROCEDURE tSQLt.Private_DisallowOverwritingNonTestSchema
  @ClassName NVARCHAR(MAX)
AS
BEGIN
  IF SCHEMA_ID(@ClassName) IS NOT NULL AND tSQLt.Private_IsTestClass(@ClassName) = 0
  BEGIN
    RAISERROR('Attempted to execute tSQLt.NewTestClass on ''%s'' which is an existing schema but not a test class', 16, 10, @ClassName);
  END
END;


GO

CREATE FUNCTION tSQLt.Private_QuoteClassNameForNewTestClass(@ClassName NVARCHAR(MAX))
  RETURNS NVARCHAR(MAX)
AS
BEGIN
  RETURN 
    CASE WHEN @ClassName LIKE '[[]%]' THEN @ClassName
         ELSE QUOTENAME(@ClassName)
     END;
END;


GO

CREATE PROCEDURE tSQLt.Private_MarkSchemaAsTestClass
  @QuotedClassName NVARCHAR(MAX)
AS
BEGIN
  SET NOCOUNT ON;

  DECLARE @UnquotedClassName NVARCHAR(MAX);

  SELECT @UnquotedClassName = name
    FROM sys.schemas
   WHERE QUOTENAME(name) = @QuotedClassName;

  EXEC sp_addextendedproperty @name = N'tSQLt.TestClass', 
                              @value = 1,
                              @level0type = 'SCHEMA',
                              @level0name = @UnquotedClassName;

  INSERT INTO tSQLt.Private_NewTestClassList(ClassName)
  SELECT @UnquotedClassName
   WHERE NOT EXISTS
             (
               SELECT * 
                 FROM tSQLt.Private_NewTestClassList AS NTC
                 WITH(UPDLOCK,ROWLOCK,HOLDLOCK)
                WHERE NTC.ClassName = @UnquotedClassName
             );
END;


GO

CREATE PROCEDURE tSQLt.NewTestClass
    @ClassName NVARCHAR(MAX)
AS
BEGIN
  BEGIN TRY
    EXEC tSQLt.Private_DisallowOverwritingNonTestSchema @ClassName;

    EXEC tSQLt.DropClass @ClassName = @ClassName;

    DECLARE @QuotedClassName NVARCHAR(MAX);
    SELECT @QuotedClassName = tSQLt.Private_QuoteClassNameForNewTestClass(@ClassName);

    EXEC ('CREATE SCHEMA ' + @QuotedClassName);  
    EXEC tSQLt.Private_MarkSchemaAsTestClass @QuotedClassName;
  END TRY
  BEGIN CATCH
    DECLARE @ErrMsg NVARCHAR(MAX);SET @ErrMsg = ERROR_MESSAGE() + ' (Error originated in ' + ERROR_PROCEDURE() + ')';
    DECLARE @ErrSvr INT;SET @ErrSvr = ERROR_SEVERITY();
    
    RAISERROR(@ErrMsg, @ErrSvr, 10);
  END CATCH;
END;


GO

CREATE PROCEDURE tSQLt.Fail
    @Message0 NVARCHAR(MAX) = '',
    @Message1 NVARCHAR(MAX) = '',
    @Message2 NVARCHAR(MAX) = '',
    @Message3 NVARCHAR(MAX) = '',
    @Message4 NVARCHAR(MAX) = '',
    @Message5 NVARCHAR(MAX) = '',
    @Message6 NVARCHAR(MAX) = '',
    @Message7 NVARCHAR(MAX) = '',
    @Message8 NVARCHAR(MAX) = '',
    @Message9 NVARCHAR(MAX) = ''
AS
BEGIN
   DECLARE @WarningMessage NVARCHAR(MAX);
   SET @WarningMessage = '';

   IF XACT_STATE() = -1
   BEGIN
     SET @WarningMessage = CHAR(13)+CHAR(10)+'Warning: Uncommitable transaction detected!';

     DECLARE @TranName NVARCHAR(MAX);
     SELECT @TranName = TranName
       FROM tSQLt.TestResult
      WHERE Id = (SELECT MAX(Id) FROM tSQLt.TestResult);

     DECLARE @TranCount INT;
     SET @TranCount = @@TRANCOUNT;
     ROLLBACK;
     WHILE(@TranCount&gt;0)
     BEGIN
       BEGIN TRAN;
       SET @TranCount = @TranCount -1;
     END;
     SAVE TRAN @TranName;
   END;

   INSERT INTO tSQLt.TestMessage(Msg)
   SELECT COALESCE(@Message0, '!NULL!')
        + COALESCE(@Message1, '!NULL!')
        + COALESCE(@Message2, '!NULL!')
        + COALESCE(@Message3, '!NULL!')
        + COALESCE(@Message4, '!NULL!')
        + COALESCE(@Message5, '!NULL!')
        + COALESCE(@Message6, '!NULL!')
        + COALESCE(@Message7, '!NULL!')
        + COALESCE(@Message8, '!NULL!')
        + COALESCE(@Message9, '!NULL!')
        + @WarningMessage;
        
   RAISERROR('tSQLt.Failure',16,10);
END;


GO

GO
CREATE TABLE tSQLt.TestResult(
    Id INT IDENTITY(1,1) PRIMARY KEY CLUSTERED,
    Class NVARCHAR(MAX) NOT NULL,
    TestCase NVARCHAR(MAX) NOT NULL,
    Name AS (QUOTENAME(Class) + '.' + QUOTENAME(TestCase)),
    TranName NVARCHAR(MAX) NOT NULL,
    Result NVARCHAR(MAX) NULL,
    Msg NVARCHAR(MAX) NULL,
    TestStartTime DATETIME NOT NULL CONSTRAINT [DF:TestResult(TestStartTime)] DEFAULT GETDATE(),
    TestEndTime DATETIME NULL
);
GO
CREATE TABLE tSQLt.TestMessage(
    Msg NVARCHAR(MAX)
);
GO
CREATE TABLE tSQLt.Run_LastExecution(
    TestName NVARCHAR(MAX),
    SessionId INT,
    LoginTime DATETIME
);
GO
CREATE TABLE tSQLt.Private_ExpectException(i INT);
GO
CREATE PROCEDURE tSQLt.Private_Print 
    @Message NVARCHAR(MAX),
    @Severity INT = 0
AS 
BEGIN
    DECLARE @SPos INT;SET @SPos = 1;
    DECLARE @EPos INT;
    DECLARE @Len INT; SET @Len = LEN(@Message);
    DECLARE @SubMsg NVARCHAR(MAX);
    DECLARE @Cmd NVARCHAR(MAX);
    
    DECLARE @CleanedMessage NVARCHAR(MAX);
    SET @CleanedMessage = REPLACE(@Message,'%','%%');
    
    WHILE (@SPos &lt;= @Len)
    BEGIN
      SET @EPos = CHARINDEX(CHAR(13)+CHAR(10),@CleanedMessage+CHAR(13)+CHAR(10),@SPos);
      SET @SubMsg = SUBSTRING(@CleanedMessage, @SPos, @EPos - @SPos);
      SET @Cmd = N'RAISERROR(@Msg,@Severity,10) WITH NOWAIT;';
      EXEC sp_executesql @Cmd, 
                         N'@Msg NVARCHAR(MAX),@Severity INT',
                         @SubMsg,
                         @Severity;
      SELECT @SPos = @EPos + 2,
             @Severity = 0; --Print only first line with high severity
    END

    RETURN 0;
END;
GO

CREATE PROCEDURE tSQLt.Private_PrintXML
    @Message XML
AS 
BEGIN
    SELECT @Message FOR XML PATH('');--Required together with ":XML ON" sqlcmd statement to allow more than 1mb to be returned
    RETURN 0;
END;
GO


CREATE PROCEDURE tSQLt.GetNewTranName
  @TranName CHAR(32) OUTPUT
AS
BEGIN
  SELECT @TranName = LEFT('tSQLtTran'+REPLACE(CAST(NEWID() AS NVARCHAR(60)),'-',''),32);
END;
GO



CREATE PROCEDURE tSQLt.SetTestResultFormatter
    @Formatter NVARCHAR(4000)
AS
BEGIN
    IF EXISTS (SELECT 1 FROM sys.extended_properties WHERE [name] = N'tSQLt.ResultsFormatter')
    BEGIN
        EXEC sp_dropextendedproperty @name = N'tSQLt.ResultsFormatter',
                                    @level0type = 'SCHEMA',
                                    @level0name = 'tSQLt',
                                    @level1type = 'PROCEDURE',
                                    @level1name = 'Private_OutputTestResults';
    END;

    EXEC sp_addextendedproperty @name = N'tSQLt.ResultsFormatter', 
                                @value = @Formatter,
                                @level0type = 'SCHEMA',
                                @level0name = 'tSQLt',
                                @level1type = 'PROCEDURE',
                                @level1name = 'Private_OutputTestResults';
END;
GO

CREATE FUNCTION tSQLt.GetTestResultFormatter()
RETURNS NVARCHAR(MAX)
AS
BEGIN
    DECLARE @FormatterName NVARCHAR(MAX);
    
    SELECT @FormatterName = CAST(value AS NVARCHAR(MAX))
    FROM sys.extended_properties
    WHERE name = N'tSQLt.ResultsFormatter'
      AND major_id = OBJECT_ID('tSQLt.Private_OutputTestResults');
      
    SELECT @FormatterName = COALESCE(@FormatterName, 'tSQLt.DefaultResultFormatter');
    
    RETURN @FormatterName;
END;
GO

CREATE PROCEDURE tSQLt.Private_OutputTestResults
  @TestResultFormatter NVARCHAR(MAX) = NULL
AS
BEGIN
    DECLARE @Formatter NVARCHAR(MAX);
    SELECT @Formatter = COALESCE(@TestResultFormatter, tSQLt.GetTestResultFormatter());
    EXEC (@Formatter);
END
GO

----------------------------------------------------------------------
CREATE FUNCTION tSQLt.Private_GetLastTestNameIfNotProvided(@TestName NVARCHAR(MAX))
RETURNS NVARCHAR(MAX)
AS
BEGIN
  IF(LTRIM(ISNULL(@TestName,'')) = '')
  BEGIN
    SELECT @TestName = TestName 
      FROM tSQLt.Run_LastExecution le
      JOIN sys.dm_exec_sessions es
        ON le.SessionId = es.session_id
       AND le.LoginTime = es.login_time
     WHERE es.session_id = @@SPID;
  END

  RETURN @TestName;
END
GO

CREATE PROCEDURE tSQLt.Private_SaveTestNameForSession 
  @TestName NVARCHAR(MAX)
AS
BEGIN
  DELETE FROM tSQLt.Run_LastExecution
   WHERE SessionId = @@SPID;  

  INSERT INTO tSQLt.Run_LastExecution(TestName, SessionId, LoginTime)
  SELECT TestName = @TestName,
         session_id,
         login_time
    FROM sys.dm_exec_sessions
   WHERE session_id = @@SPID;
END
GO

----------------------------------------------------------------------
CREATE VIEW tSQLt.TestClasses
AS
  SELECT s.name AS Name, s.schema_id AS SchemaId
    FROM sys.extended_properties ep
    JOIN sys.schemas s
      ON ep.major_id = s.schema_id
   WHERE ep.name = N'tSQLt.TestClass';
GO

CREATE VIEW tSQLt.Tests
AS
  SELECT classes.SchemaId, classes.Name AS TestClassName, 
         procs.object_id AS ObjectId, procs.name AS Name
    FROM tSQLt.TestClasses classes
    JOIN sys.procedures procs ON classes.SchemaId = procs.schema_id
   WHERE LOWER(procs.name) LIKE 'test%';
GO


CREATE FUNCTION tSQLt.TestCaseSummary()
RETURNS TABLE
AS
RETURN WITH A(Cnt, SuccessCnt, FailCnt, ErrorCnt) AS (
                SELECT COUNT(1),
                       ISNULL(SUM(CASE WHEN Result = 'Success' THEN 1 ELSE 0 END), 0),
                       ISNULL(SUM(CASE WHEN Result = 'Failure' THEN 1 ELSE 0 END), 0),
                       ISNULL(SUM(CASE WHEN Result = 'Error' THEN 1 ELSE 0 END), 0)
                  FROM tSQLt.TestResult
                  
                )
       SELECT 'Test Case Summary: ' + CAST(Cnt AS NVARCHAR) + ' test case(s) executed, '+
                  CAST(SuccessCnt AS NVARCHAR) + ' succeeded, '+
                  CAST(FailCnt AS NVARCHAR) + ' failed, '+
                  CAST(ErrorCnt AS NVARCHAR) + ' errored.' Msg,*
         FROM A;
GO

CREATE PROCEDURE tSQLt.Private_ValidateProcedureCanBeUsedWithSpyProcedure
    @ProcedureName NVARCHAR(MAX)
AS
BEGIN
    IF NOT EXISTS(SELECT 1 FROM sys.procedures WHERE object_id = OBJECT_ID(@ProcedureName))
    BEGIN
      RAISERROR('Cannot use SpyProcedure on %s because the procedure does not exist', 16, 10, @ProcedureName) WITH NOWAIT;
    END;
    
    IF (1020 &lt; (SELECT COUNT(*) FROM sys.parameters WHERE object_id = OBJECT_ID(@ProcedureName)))
    BEGIN
      RAISERROR('Cannot use SpyProcedure on procedure %s because it contains more than 1020 parameters', 16, 10, @ProcedureName) WITH NOWAIT;
    END;
END;
GO


CREATE PROCEDURE tSQLt.AssertEquals
    @Expected SQL_VARIANT,
    @Actual SQL_VARIANT,
    @Message NVARCHAR(MAX) = ''
AS
BEGIN
    IF ((@Expected = @Actual) OR (@Actual IS NULL AND @Expected IS NULL))
      RETURN 0;

    DECLARE @Msg NVARCHAR(MAX);
    SELECT @Msg = 'Expected: &lt;' + ISNULL(CAST(@Expected AS NVARCHAR(MAX)), 'NULL') + 
                  '&gt; but was: &lt;' + ISNULL(CAST(@Actual AS NVARCHAR(MAX)), 'NULL') + '&gt;';
    IF((COALESCE(@Message,'') &lt;&gt; '') AND (@Message NOT LIKE '% ')) SET @Message = @Message + ' ';
    EXEC tSQLt.Fail @Message, @Msg;
END;
GO

/*******************************************************************************************/
/*******************************************************************************************/
/*******************************************************************************************/
CREATE FUNCTION tSQLt.Private_GetCleanSchemaName(@SchemaName NVARCHAR(MAX), @ObjectName NVARCHAR(MAX))
RETURNS NVARCHAR(MAX)
AS
BEGIN
    RETURN (SELECT SCHEMA_NAME(schema_id) 
              FROM sys.objects 
             WHERE object_id = CASE WHEN ISNULL(@SchemaName,'') in ('','[]')
                                    THEN OBJECT_ID(@ObjectName)
                                    ELSE OBJECT_ID(@SchemaName + '.' + @ObjectName)
                                END);
END;
GO

CREATE FUNCTION [tSQLt].[Private_GetCleanObjectName](@ObjectName NVARCHAR(MAX))
RETURNS NVARCHAR(MAX)
AS
BEGIN
    RETURN (SELECT OBJECT_NAME(OBJECT_ID(@ObjectName)));
END;
GO

CREATE FUNCTION tSQLt.Private_ResolveFakeTableNamesForBackwardCompatibility 
 (@TableName NVARCHAR(MAX), @SchemaName NVARCHAR(MAX))
RETURNS TABLE AS 
RETURN
  SELECT QUOTENAME(OBJECT_SCHEMA_NAME(object_id)) AS CleanSchemaName,
         QUOTENAME(OBJECT_NAME(object_id)) AS CleanTableName
     FROM (SELECT CASE
                    WHEN @SchemaName IS NULL THEN OBJECT_ID(@TableName)
                    ELSE COALESCE(OBJECT_ID(@SchemaName + '.' + @TableName),OBJECT_ID(@TableName + '.' + @SchemaName)) 
                  END object_id
          ) ids;
GO


/*******************************************************************************************/
/*******************************************************************************************/
/*******************************************************************************************/
CREATE FUNCTION tSQLt.Private_GetOriginalTableName(@SchemaName NVARCHAR(MAX), @TableName NVARCHAR(MAX)) --DELETE!!!
RETURNS NVARCHAR(MAX)
AS
BEGIN
  RETURN (SELECT CAST(value AS NVARCHAR(4000))
    FROM sys.extended_properties
   WHERE class_desc = 'OBJECT_OR_COLUMN'
     AND major_id = OBJECT_ID(@SchemaName + '.' + @TableName)
     AND minor_id = 0
     AND name = 'tSQLt.FakeTable_OrgTableName');
END;
GO

CREATE FUNCTION tSQLt.Private_GetOriginalTableInfo(@TableObjectId INT)
RETURNS TABLE
AS
  RETURN SELECT CAST(value AS NVARCHAR(4000)) OrgTableName,
                OBJECT_ID(QUOTENAME(OBJECT_SCHEMA_NAME(@TableObjectId)) + '.' + QUOTENAME(CAST(value AS NVARCHAR(4000)))) OrgTableObjectId
    FROM sys.extended_properties
   WHERE class_desc = 'OBJECT_OR_COLUMN'
     AND major_id = @TableObjectId
     AND minor_id = 0
     AND name = 'tSQLt.FakeTable_OrgTableName';
GO



CREATE FUNCTION [tSQLt].[F_Num](
       @N INT
)
RETURNS TABLE 
AS 
RETURN WITH C0(c) AS (SELECT 1 UNION ALL SELECT 1),
            C1(c) AS (SELECT 1 FROM C0 AS A CROSS JOIN C0 AS B),
            C2(c) AS (SELECT 1 FROM C1 AS A CROSS JOIN C1 AS B),
            C3(c) AS (SELECT 1 FROM C2 AS A CROSS JOIN C2 AS B),
            C4(c) AS (SELECT 1 FROM C3 AS A CROSS JOIN C3 AS B),
            C5(c) AS (SELECT 1 FROM C4 AS A CROSS JOIN C4 AS B),
            C6(c) AS (SELECT 1 FROM C5 AS A CROSS JOIN C5 AS B)
       SELECT TOP(CASE WHEN @N&gt;0 THEN @N ELSE 0 END) ROW_NUMBER() OVER (ORDER BY c) no
         FROM C6;
GO

CREATE PROCEDURE [tSQLt].[Private_SetFakeViewOn_SingleView]
  @ViewName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @Cmd NVARCHAR(MAX),
          @SchemaName NVARCHAR(MAX),
          @TriggerName NVARCHAR(MAX);
          
  SELECT @SchemaName = OBJECT_SCHEMA_NAME(ObjId),
         @ViewName = OBJECT_NAME(ObjId),
         @TriggerName = OBJECT_NAME(ObjId) + '_SetFakeViewOn'
    FROM (SELECT OBJECT_ID(@ViewName) AS ObjId) X;

  SET @Cmd = 
     'CREATE TRIGGER $$SCHEMA_NAME$$.$$TRIGGER_NAME$$
      ON $$SCHEMA_NAME$$.$$VIEW_NAME$$ INSTEAD OF INSERT AS
      BEGIN
         RAISERROR(''Test system is in an invalid state. SetFakeViewOff must be called if SetFakeViewOn was called. Call SetFakeViewOff after creating all test case procedures.'', 16, 10) WITH NOWAIT;
         RETURN;
      END;
     ';
      
  SET @Cmd = REPLACE(@Cmd, '$$SCHEMA_NAME$$', QUOTENAME(@SchemaName));
  SET @Cmd = REPLACE(@Cmd, '$$VIEW_NAME$$', QUOTENAME(@ViewName));
  SET @Cmd = REPLACE(@Cmd, '$$TRIGGER_NAME$$', QUOTENAME(@TriggerName));
  EXEC(@Cmd);

  EXEC sp_addextendedproperty @name = N'SetFakeViewOnTrigger', 
                               @value = 1,
                               @level0type = 'SCHEMA',
                               @level0name = @SchemaName, 
                               @level1type = 'VIEW',
                               @level1name = @ViewName,
                               @level2type = 'TRIGGER',
                               @level2name = @TriggerName;

  RETURN 0;
END;
GO

CREATE PROCEDURE [tSQLt].[SetFakeViewOn]
  @SchemaName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @ViewName NVARCHAR(MAX);
    
  DECLARE viewNames CURSOR LOCAL FAST_FORWARD FOR
  SELECT QUOTENAME(OBJECT_SCHEMA_NAME(object_id)) + '.' + QUOTENAME([name]) AS viewName
    FROM sys.views
   WHERE schema_id = SCHEMA_ID(@SchemaName);
  
  OPEN viewNames;
  
  FETCH NEXT FROM viewNames INTO @ViewName;
  WHILE @@FETCH_STATUS = 0
  BEGIN
    EXEC tSQLt.Private_SetFakeViewOn_SingleView @ViewName;
    
    FETCH NEXT FROM viewNames INTO @ViewName;
  END;
  
  CLOSE viewNames;
  DEALLOCATE viewNames;
END;
GO

CREATE PROCEDURE [tSQLt].[Private_SetFakeViewOff_SingleView]
  @ViewName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @Cmd NVARCHAR(MAX),
          @SchemaName NVARCHAR(MAX),
          @TriggerName NVARCHAR(MAX);
          
  SELECT @SchemaName = QUOTENAME(OBJECT_SCHEMA_NAME(ObjId)),
         @TriggerName = QUOTENAME(OBJECT_NAME(ObjId) + '_SetFakeViewOn')
    FROM (SELECT OBJECT_ID(@ViewName) AS ObjId) X;
  
  SET @Cmd = 'DROP TRIGGER %SCHEMA_NAME%.%TRIGGER_NAME%;';
      
  SET @Cmd = REPLACE(@Cmd, '%SCHEMA_NAME%', @SchemaName);
  SET @Cmd = REPLACE(@Cmd, '%TRIGGER_NAME%', @TriggerName);
  
  EXEC(@Cmd);
END;
GO

CREATE PROCEDURE [tSQLt].[SetFakeViewOff]
  @SchemaName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @ViewName NVARCHAR(MAX);
    
  DECLARE viewNames CURSOR LOCAL FAST_FORWARD FOR
   SELECT QUOTENAME(OBJECT_SCHEMA_NAME(t.parent_id)) + '.' + QUOTENAME(OBJECT_NAME(t.parent_id)) AS viewName
     FROM sys.extended_properties ep
     JOIN sys.triggers t
       on ep.major_id = t.object_id
     WHERE ep.name = N'SetFakeViewOnTrigger'  
  OPEN viewNames;
  
  FETCH NEXT FROM viewNames INTO @ViewName;
  WHILE @@FETCH_STATUS = 0
  BEGIN
    EXEC tSQLt.Private_SetFakeViewOff_SingleView @ViewName;
    
    FETCH NEXT FROM viewNames INTO @ViewName;
  END;
  
  CLOSE viewNames;
  DEALLOCATE viewNames;
END;
GO

CREATE FUNCTION tSQLt.Private_GetQuotedFullName(@Objectid INT)
RETURNS NVARCHAR(517)
AS
BEGIN
    DECLARE @QuotedName NVARCHAR(517);
    SELECT @QuotedName = QUOTENAME(OBJECT_SCHEMA_NAME(@Objectid)) + '.' + QUOTENAME(OBJECT_NAME(@Objectid));
    RETURN @QuotedName;
END;
GO

CREATE FUNCTION tSQLt.Private_GetSchemaId(@SchemaName NVARCHAR(MAX))
RETURNS INT
AS
BEGIN
  RETURN (
    SELECT TOP(1) schema_id
      FROM sys.schemas
     WHERE @SchemaName IN (name, QUOTENAME(name), QUOTENAME(name, '"'))
     ORDER BY 
        CASE WHEN name = @SchemaName THEN 0 ELSE 1 END
  );
END;
GO

CREATE FUNCTION tSQLt.Private_IsTestClass(@TestClassName NVARCHAR(MAX))
RETURNS BIT
AS
BEGIN
  RETURN 
    CASE 
      WHEN EXISTS(
             SELECT 1 
               FROM tSQLt.TestClasses
              WHERE SchemaId = tSQLt.Private_GetSchemaId(@TestClassName)
            )
      THEN 1
      ELSE 0
    END;
END;
GO

CREATE FUNCTION tSQLt.Private_ResolveSchemaName(@Name NVARCHAR(MAX))
RETURNS TABLE 
AS
RETURN
  WITH ids(schemaId) AS
       (SELECT tSQLt.Private_GetSchemaId(@Name)
       ),
       idsWithNames(schemaId, quotedSchemaName) AS
        (SELECT schemaId,
         QUOTENAME(SCHEMA_NAME(schemaId))
         FROM ids
        )
  SELECT schemaId, 
         quotedSchemaName,
         CASE WHEN EXISTS(SELECT 1 FROM tSQLt.TestClasses WHERE TestClasses.SchemaId = idsWithNames.schemaId)
               THEN 1
              ELSE 0
         END AS isTestClass, 
         CASE WHEN schemaId IS NOT NULL THEN 1 ELSE 0 END AS isSchema
    FROM idsWithNames;
GO

CREATE FUNCTION tSQLt.Private_ResolveObjectName(@Name NVARCHAR(MAX))
RETURNS TABLE 
AS
RETURN
  WITH ids(schemaId, objectId) AS
       (SELECT SCHEMA_ID(OBJECT_SCHEMA_NAME(OBJECT_ID(@Name))),
               OBJECT_ID(@Name)
       ),
       idsWithNames(schemaId, objectId, quotedSchemaName, quotedObjectName) AS
        (SELECT schemaId, objectId,
         QUOTENAME(SCHEMA_NAME(schemaId)) AS quotedSchemaName, 
         QUOTENAME(OBJECT_NAME(objectId)) AS quotedObjectName
         FROM ids
        )
  SELECT schemaId, 
         objectId, 
         quotedSchemaName,
         quotedObjectName,
         quotedSchemaName + '.' + quotedObjectName AS quotedFullName, 
         CASE WHEN LOWER(quotedObjectName) LIKE '[[]test%]' 
               AND objectId = OBJECT_ID(quotedSchemaName + '.' + quotedObjectName,'P') 
              THEN 1 ELSE 0 END AS isTestCase
    FROM idsWithNames;
    
GO

CREATE FUNCTION tSQLt.Private_ResolveName(@Name NVARCHAR(MAX))
RETURNS TABLE 
AS
RETURN
  WITH resolvedNames(ord, schemaId, objectId, quotedSchemaName, quotedObjectName, quotedFullName, isTestClass, isTestCase, isSchema) AS
  (SELECT 1, schemaId, NULL, quotedSchemaName, NULL, quotedSchemaName, isTestClass, 0, 1
     FROM tSQLt.Private_ResolveSchemaName(@Name)
    UNION ALL
   SELECT 2, schemaId, objectId, quotedSchemaName, quotedObjectName, quotedFullName, 0, isTestCase, 0
     FROM tSQLt.Private_ResolveObjectName(@Name)
    UNION ALL
   SELECT 3, NULL, NULL, NULL, NULL, NULL, 0, 0, 0
   )
   SELECT TOP(1) schemaId, objectId, quotedSchemaName, quotedObjectName, quotedFullName, isTestClass, isTestCase, isSchema
     FROM resolvedNames
    WHERE schemaId IS NOT NULL 
       OR ord = 3
    ORDER BY ord
GO

CREATE PROCEDURE tSQLt.Uninstall
AS
BEGIN
  DROP TYPE tSQLt.Private;

  EXEC tSQLt.DropClass 'tSQLt';  
  
  DROP ASSEMBLY tSQLtCLR;
END;
GO


GO

GO
CREATE FUNCTION tSQLt.Private_GetExternalAccessKeyBytes()
RETURNS TABLE
AS
RETURN
  SELECT 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xternalAccessKeyBytes, 0x7722217D36028E4C AS ExternalAccessKeyThumbPrint;
GO



GO

GO
CREATE PROCEDURE tSQLt.RemoveExternalAccessKey
AS
BEGIN
  IF(NOT EXISTS(SELECT * FROM sys.fn_my_permissions(NULL,'server') AS FMP WHERE FMP.permission_name = 'CONTROL SERVER'))
  BEGIN
    RAISERROR('Only principals with CONTROL SERVER permission can execute this procedure.',16,10);
    RETURN -1;
  END;

  DECLARE @master_sys_sp_executesql NVARCHAR(MAX); SET @master_sys_sp_executesql = 'master.sys.sp_executesql';

  IF SUSER_ID('tSQLtExternalAccessKey') IS NOT NULL DROP LOGIN tSQLtExternalAccessKey;
  EXEC @master_sys_sp_executesql N'IF ASYMKEY_ID(''tSQLtExternalAccessKey'') IS NOT NULL DROP ASYMMETRIC KEY tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql N'IF EXISTS(SELECT * FROM sys.assemblies WHERE name = ''tSQLtExternalAccessKey'') DROP ASSEMBLY tSQLtExternalAccessKey;';
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.InstallExternalAccessKey
AS
BEGIN
  IF(NOT EXISTS(SELECT * FROM sys.fn_my_permissions(NULL,'server') AS FMP WHERE FMP.permission_name = 'CONTROL SERVER'))
  BEGIN
    RAISERROR('Only principals with CONTROL SERVER permission can execute this procedure.',16,10);
    RETURN -1;
  END;

  DECLARE @cmd NVARCHAR(MAX);
  DECLARE @cmd2 NVARCHAR(MAX);
  DECLARE @master_sys_sp_executesql NVARCHAR(MAX); SET @master_sys_sp_executesql = 'master.sys.sp_executesql';

  SET @cmd = 'IF EXISTS(SELECT * FROM sys.assemblies WHERE name = ''tSQLtExternalAccessKey'') DROP ASSEMBLY tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql @cmd;

  SET @cmd2 = 'SELECT @cmd = ''DROP ASSEMBLY ''+QUOTENAME(A.name)+'';'''+ 
              '  FROM master.sys.assemblies AS A'+
              ' WHERE A.clr_name LIKE ''tsqltexternalaccesskey, %'';';
  EXEC sys.sp_executesql @cmd2,N'@cmd NVARCHAR(MAX) OUTPUT',@cmd OUT;
  EXEC @master_sys_sp_executesql @cmd;

  SELECT @cmd = 
         'CREATE ASSEMBLY tSQLtExternalAccessKey AUTHORIZATION dbo FROM ' +
         BH.prefix +
         ' WITH PERMISSION_SET = SAFE;'       
    FROM tSQLt.Private_GetExternalAccessKeyBytes() AS PGEAKB
   CROSS APPLY tSQLt.Private_Bin2Hex(PGEAKB.ExternalAccessKeyBytes) BH;
  EXEC @master_sys_sp_executesql @cmd;

  IF SUSER_ID('tSQLtExternalAccessKey') IS NOT NULL DROP LOGIN tSQLtExternalAccessKey;

  SET @cmd = N'IF ASYMKEY_ID(''tSQLtExternalAccessKey'') IS NOT NULL DROP ASYMMETRIC KEY tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql @cmd;

  SET @cmd2 = 'SELECT @cmd = ISNULL(''DROP LOGIN ''+QUOTENAME(SP.name)+'';'','''')+''DROP ASYMMETRIC KEY '' + QUOTENAME(AK.name) + '';'''+
              '  FROM master.sys.asymmetric_keys AS AK'+
              '  JOIN tSQLt.Private_GetExternalAccessKeyBytes() AS PGEAKB'+
              '    ON AK.thumbprint = PGEAKB.ExternalAccessKeyThumbPrint'+
              '  LEFT JOIN master.sys.server_principals AS SP'+
              '    ON AK.sid = SP.sid;';
  EXEC sys.sp_executesql @cmd2,N'@cmd NVARCHAR(MAX) OUTPUT',@cmd OUT;
  EXEC @master_sys_sp_executesql @cmd;

  SET @cmd = 'CREATE ASYMMETRIC KEY tSQLtExternalAccessKey FROM ASSEMBLY tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql @cmd;
 
  SET @cmd = 'CREATE LOGIN tSQLtExternalAccessKey FROM ASYMMETRIC KEY tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql @cmd;

  SET @cmd = 'DROP ASSEMBLY tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql @cmd;

  SET @cmd = 'GRANT EXTERNAL ACCESS ASSEMBLY TO tSQLtExternalAccessKey;';
  EXEC @master_sys_sp_executesql @cmd;

END;
GO


GO

GO
CREATE PROCEDURE tSQLt.EnableExternalAccess
  @try BIT = 0,
  @enable BIT = 1
AS
BEGIN
  BEGIN TRY
    IF @enable = 1
    BEGIN
      EXEC('ALTER ASSEMBLY tSQLtCLR WITH PERMISSION_SET = EXTERNAL_ACCESS;');
    END
    ELSE
    BEGIN
      EXEC('ALTER ASSEMBLY tSQLtCLR WITH PERMISSION_SET = SAFE;');
    END
  END TRY
  BEGIN CATCH
    IF(@try = 0)
    BEGIN
      DECLARE @Message NVARCHAR(4000);
      SET @Message = 'The attempt to ' +
                      CASE WHEN @enable = 1 THEN 'enable' ELSE 'disable' END +
                      ' tSQLt features requiring EXTERNAL_ACCESS failed' +
                      ': '+ERROR_MESSAGE();
      RAISERROR(@Message,16,10);
    END;
    RETURN -1;
  END CATCH;
  RETURN 0;
END;
GO


GO

CREATE TABLE tSQLt.Private_Configurations (
  Name NVARCHAR(100) PRIMARY KEY CLUSTERED,
  Value SQL_VARIANT
);


GO

GO
CREATE PROCEDURE tSQLt.Private_SetConfiguration
  @Name NVARCHAR(100),
  @Value SQL_VARIANT
AS
BEGIN
  IF(EXISTS(SELECT 1 FROM tSQLt.Private_Configurations WITH(ROWLOCK,UPDLOCK) WHERE Name = @Name))
  BEGIN
    UPDATE tSQLt.Private_Configurations SET
           Value = @Value
     WHERE Name = @Name;
  END;
  ELSE
  BEGIN
     INSERT tSQLt.Private_Configurations(Name,Value)
     VALUES(@Name,@Value);
  END;
END;
GO


GO

GO
CREATE FUNCTION tSQLt.Private_GetConfiguration(
  @Name NVARCHAR(100)
)
RETURNS TABLE
AS
RETURN
  SELECT PC.Name,
         PC.Value 
    FROM tSQLt.Private_Configurations AS PC
   WHERE PC.Name = @Name;
GO


GO

GO
CREATE PROCEDURE tSQLt.SetVerbose
  @Verbose BIT = 1
AS
BEGIN
  EXEC tSQLt.Private_SetConfiguration @Name = 'Verbose', @Value = @Verbose;
END;
GO


GO

CREATE TABLE tSQLt.CaptureOutputLog (
  Id INT IDENTITY(1,1) PRIMARY KEY CLUSTERED,
  OutputText NVARCHAR(MAX)
);


GO

CREATE PROCEDURE tSQLt.LogCapturedOutput @text NVARCHAR(MAX)
AS
BEGIN
  INSERT INTO tSQLt.CaptureOutputLog (OutputText) VALUES (@text);
END;


GO

GO
CREATE ASSEMBLY [tSQLtCLR] AUTHORIZATION [dbo] FROM 0x4D5A90000300000004000000FFFF0000B800000000000000400000000000000000000000000000000000000000000000000000000000000000000000800000000E1FBA0E00B409CD21B8014CCD21546869732070726F6772616D2063616E6E6F742062652072756E20696E20444F53206D6F64652E0D0D0A2400000000000000504500004C0103005219AD560000000000000000E00002210B010B00004A000000060000000000001E68000000200000008000000000001000200000000200000400000000000000040000000000000000C00000000200001F550000030040850000100000100000000010000010000000000000100000000000000000000000C86700005300000000800000F80300000000000000000000000000000000000000A000000C000000906600001C0000000000000000000000000000000000000000000000000000000000000000000000000000000000000000200000080000000000000000000000082000004800000000000000000000002E746578740000002448000000200000004A000000020000000000000000000000000000200000602E72737263000000F80300000080000000040000004C0000000000000000000000000000400000402E72656C6F6300000C00000000A0000000020000005000000000000000000000000000004000004200000000000000000000000000000000006800000000000048000000020005003C3500005431000009000000000000000000000000000000502000008000000000000000000000000000000000000000000000000000000000000000000000009D63A517CED9D2CE805B163A00868E987936F818709CD02EE31F21244D8C9BA69ECF06878608961FBE7E9C5B009E4C7ACF794FE36C51E8C75909B8E31CFA047713754986E739D088652ADF79832925307B3F8F9953D425C7A3F29B7334245E5F0E98EA299D08D952730F071DD02A73DF0B645A801A953787753CA8109DF47E6A1B3002006D00000001000011140A18730F00000A0B28020000060C08731000000A0A066F1100000A731200000A0D09066F1300000A090F01FE16080000016F1400000A6F1500000A096F1600000A26DE0A072C06076F1700000ADCDE0F13047201000070110473060000067ADE0A062C06066F1800000ADC2A00000001280000020009003C45000A00000000000002004F51000F32000001020002006062000A00000000133003004E0000000200001173400000060A066F450000060B066F460000060C731900000A0D0972B6000070076F1A00000A0972CE000070178C350000016F1A00000A0972F6000070086F1A00000A096F1B00000A130411042A1E02281C00000A2A1E02281E00000A2A220203281F00000A2A26020304282000000A2A26020304282100000A2A3A02281C00000A02037D010000042A7A0203280B000006027B01000004027B010000046F420000066F4B0000062A220203280B0000062A0000133002001400000003000011027B01000004036F470000060A066F2200000A2A6A282500000A6F2600000A6F2700000A6F1400000A282800000A2A56282500000A6F2600000A6F2900000A282A00000A2A0000001330040032000000040000117216010070282C00000A0A1200FE16400000016F1400000A723A010070723E0100706F2D00000A282E00000A282800000A2A00001B3005002B020000050000110F00282F00000A2C0B7240010070731F00000A7A0F01282F00000A2C0C723E010070282800000A10010F02282F00000A2C0C723E010070282800000A100273400000060A0F000F0128150000060B0607282800000A6F470000060C080428160000060D16130409166F3000000A8E698D420000011305096F3100000A13102B3B1210283200000A13061613072B1F110511071105110794110611079A6F3300000A283400000A9E110717581307110711068E6932D91104175813041210283500000A2DBCDE0E1210FE160200001B6F1700000ADC1613082B1A110511081105110894209B000000283600000A9E110817581308110811058E6932DE161309110513111613122B161111111294130A110917110A58581309111217581312111211118E6932E21109175813091109110417585A130917130B1109733700000A130C096F3100000A131338B00000001213283200000A130D110B2D08110C6F3800000A2616130E2B2C110C72760100706F3900000A110D110E9A28140000061105110E9428130000066F3900000A26110E1758130E110E110D8E6932CC110C72760100706F3900000A26110B2C5116130B110C6F3800000A2616130F2B2C110C727A0100706F3900000A26110C110C6F3A00000A723A0100701105110F946F3B00000A26110F1758130F110F110D8E6932CC110C727A0100706F3900000A261213283500000A3A44FFFFFFDE0E1213FE160200001B6F1700000ADC110C6F1400000A282800000A733C00000A2A00011C000002007E0048C6000E0000000002004801C30B020E000000001330050039000000060000110228120000060A727E010070733D00000A0B070F00FE16080000016F1400000A72E201007017066F3E00000A0C08282800000A733C00000A2A327E04000004026F3F00000A2A000013300200FE000000070000110F00FE16080000016F1400000A6F4000000A0A150B160C160D16130438CD0000000813051105450600000005000000330000003F00000053000000760000008C00000038A00000000611049328110000063A92000000061104931F2D3307170C3883000000061104931F2F3304190C2B7711040B2B72061104931F2D336A180C2B66061104931F0D2E08061104931F0A3356160C2B52061104931F2A33081A0C0917580D2B42091631041A0C2B3A7200020070731F00000A7A061104931F2A33021B0C061104931F2F331D190C2B19061104931F2F330F0917590D092D04160C2B061A0C2B021A0C1104175813041104068E692F0707163F25FFFFFF072A9202734100000A16721A02007003026F3300000A59283B00000A6F1400000A282E00000A2AD2026F3300000A209B000000312502161F4B6F4200000A721E02007002026F3300000A1F4B591F4B6F4200000A284300000A2A022A133003004500000008000011722A02007002FE16080000016F1400000A282E00000A0A03FE16080000016F1400000A6F3300000A16311806724802007003FE16080000016F1400000A284300000A0A062A000000133004009202000009000011026F4400000A0A734500000A0B066F4600000A6F4700000A0C0F01FE16080000016F1400000A723E0100706F4800000A2C47088D410000010D1613042B2A066F4600000A11046F4900000A13050911041105725E0200706F4A00000A6F1400000AA211041758130411040832D107096F4B00000A380C0200000F0128170000061306734C00000A13071106130C16130D2B2E110C110D9A130811086F3300000A2C18110711087274020070727A0200706F2D00000A6F4D00000A110D1758130D110D110C8E6932CA0711076F4E00000A6F4B00000A38AB010000088D41000001130916130A388601000002110A6F4F00000A2C0F1109110A727E020070A23867010000066F4600000A110A6F4900000A728C0200706F4A00000AA549000001130B110B130E110E1F0F3024110E1A59450400000078000000BC000000DA00000000010000110E1F0F2E563809010000110E1F13594503000000DF000000F3000000DF000000110E1F1F59450400000005000000D9000000590000006D00000038D40000001109110A02110A6F5000000A285100000A2819000006A238CA0000001109110A02110A6F5000000A285100000A281B000006A238AE0000001109110A02110A6F5000000A285100000A281A000006A238920000001109110A02110A6F5000000A281C000006A22B7E1109110A02110A6F5200000A281D000006A22B6A1109110A02110A6F5300000A130F120FFE164A0000016F1400000AA22B4C1109110A02110A6F5400000A13101210285500000A1311121172A6020070285600000AA22B261109110A02110A6F5700000A281E000006A22B121109110A02110A6F5800000A6F1400000AA2110A1758130A110A026F5900000A3F6DFEFFFF0711096F4B00000A026F5A00000A3A4AFEFFFF072A000013300300280000000A00001172D002007002FE16080000016F1400000A72D6020070284300000A72DC020070285B00000A0A062A820272E8020070723E0100706F2D00000A727A020070723E0100706F2D00000A2A5E72EC0200700F00285C00000A8C11000001285D00000A2A5E720A0300700F00285C00000A8C11000001285D00000A2A5E72420300700F00285C00000A8C11000001285D00000A2A72726C0300700F00285E00000A735F00000A8C11000001285D00000A2A4672AC030070028C12000001285D00000A2A13300300440000000B000011734100000A72F4030070283900000A0A0F00286000000A0C160D2B1B0809910B06120172FA030070286100000A6F3900000A260917580D09088E6932DF066F1400000A2A2E7200040070731F00000A7A2E7200040070731F00000A7A2E7200040070731F00000A7A2E7200040070731F00000A7A1A736200000A7A1A736200000A7A000013300300430000000C000011736300000A0A061F20176F6400000A061F0A176F6400000A061F0D176F6400000A061F09176F6400000A061F0C176F6400000A061F0B176F6400000A0680040000042A1E02281E00000A2A220203281F00000A2A26020304282000000A2A26020304282100000A2A3A02281C00000A02037D0C0000042A001B300300340000000D0000110203282C0000060A0204282C0000060B027B0C00000406076F49000006DE140C027B0C000004086F6500000A6F4A000006DE002A01100000000000001F1F0014070000021B300200370000000E000011140A027B0C000004036F470000060A066F5A00000A260306282E0000060B0307282F0000060728300000060CDE0706282D000006DC082A0001100000020002002C2E0007000000002A022C06026F2200000A2A001B3003002F0000000F000011036F4400000A0BDE240A72740400700F00FE16080000016F1400000A7290040070284300000A0673280000067A072A00011000000000000009090024020000019A032D2272740400700F00FE16080000016F1400000A72D8040070284300000A73270000067A2A001B3004000F01000010000011723E0100700A026F4600000A6F6600000A0D38D5000000096F6700000A741B0000010B0772140500706F4A00000A6F1400000A7226050070286800000A39AA0000000672E8020070282E00000A0A026F6900000A6F6600000A13042B6311046F6700000A74160000010C0828310000062C4E0613051C8D0100000113061106161105A21106177230050070A2110618086F6A00000AA21106197234050070A211061A07086F6A00000A6F4A00000AA211061B7238050070A21106286B00000A0A11046F6C00000A2D94DE1511047507000001130711072C0711076F1700000ADC06727A020070282E00000A0A096F6C00000A3A20FFFFFFDE14097507000001130811082C0711086F1700000ADC062A00011C000002005B0070CB00150000000002001200E7F9001400000000AA026F6A00000A723C0500701B6F6D00000A2D15026F6A00000A72420500701B6F6D00000A16FE012A162A3A02281C00000A02037D0D0000042A000013300300A50000001100001102032834000006027B0D000004046F470000060A160B066F5900000A1631270717580B07286E00000A03286F00000A287000000A2C080628350000062B08066F7100000A2DD9066F2200000A07286E00000A03287200000A287000000A2C451B8D410000010C0816724C050070A208171201287300000AA20818727E050070A208190F01FE16170000016F1400000AA2081A72B0050070A208287400000A73270000067A2A000000033003004F000000000000000316286E00000A287200000A25287000000A2D110F01287500000A287600000A287700000A287000000A2C2272D40500700F01FE16170000016F1400000A7232060070284300000A73270000067A2A001330020029000000120000110228380000060A287800000A06737900000A6F7A00000A02062836000006287800000A6F7B00000A2A722B11287800000A020328370000066F7C00000A026F5A00000A2DE72A000013300200250000001300001103737900000A0A026F5900000A8D010000010B02076F7D00000A2606076F7E00000A26062A0000001B3003005800000014000011026F4400000A0A0628390000060B076F7F00000A8D180000010C160D076F8000000A13052B171205288100000A130408091104283A000006A20917580D1205288200000A2DE0DE0E1205FE160600001B6F1700000ADC082A01100000020024002448000E000000001B3002006600000015000011738300000A0A026F4600000A6F6600000A0C2B35086F6700000A741B0000010B0772140500706F4A00000A6F1400000A6F8400000A724E060070286800000A2C0806076F8500000A26086F6C00000A2DC3DE110875070000010D092C06096F1700000ADC062A000001100000020012004153001100000000133005006F0100001600001102728C0200706F4A00000AA5490000010A02725E0200706F4A00000A74410000010B0272580600706F4A00000A74570000010C06130411044523000000050000000D000000050000000D000000050000004B000000050000000500000005000000050000000D0000000500000026000000050000000500000005000000050000000500000005000000050000000500000026000000260000000500000086000000050000008600000086000000860000007D0000008600000005000000050000004B0000004B00000038810000000706738600000A2A070602726A0600706F4A00000AA5420000016A738700000A2A02726A0600706F4A00000AA5420000010D0920FF7F00003102150D0706096A738700000A2A07060272800600706F4A00000A288800000A288900000A0272A20600706F4A00000A288800000A288900000A738A00000A2A070608738B00000A2A72BC060070068C490000016F1400000A72D2060070284300000A738C00000A7A001330030014000000170000117340000006732A0000060A0602036F2B0000062A133003001400000018000011734000000673320000060A0602036F330000062A133002000E0000001900001173030000060A06026F010000062A000013300200130000001A000011734000000673080000060A06026F090000062A0013300200130000001A000011734000000673080000060A06026F0A0000062A3602281C00000A0228430000062A72027B100000042D0D02284400000602177D1000000402288D00000A2A1E027B0F0000042A9E02738E00000A7D0E000004027B0E000004721A0700706F8F00000A027B0E0000046F1100000A2A32027B0E0000046F9000000A2A0013300200280000001B00001102724C070070282800000A28470000060A066F5A00000A2606166F9100000A0B066F2200000A072A32027B0E0000046F9200000A2A00000013300300510000001C000011027E9300000A7D0F000004027B0E00000402FE0648000006739400000A6F9500000A731200000A0A06027B0E0000046F1300000A060F01FE16080000016F1400000A6F1500000A061A6F9600000A0B072A000000033004004400000000000000027C0F000004282F00000A2C1002723E010070282800000A7D0F00000402257B0F000004046F9700000A7296070070282E00000A282800000A289800000A7D0F0000042A13300300500000001D000011731200000A0A06027B0E0000046F1300000A06729C0700706F1500000A066F9900000A72CE070070036F9A00000A26066F9900000A72E0070070046F9A00000A26061A6F9B00000A066F1600000A262A133003003E0000001D000011731200000A0A06027B0E0000046F1300000A0672EE0700706F1500000A066F9900000A7204080070036F9A00000A26061A6F9B00000A066F1600000A262A000013300300430000001D000011731200000A0A06027B0E0000046F1300000A0672160800706F1500000A066F9900000A7246080070038C080000016F9A00000A26061A6F9B00000A066F1600000A262A0042534A4201000100000000000C00000076322E302E35303732370000000005006C000000D40E0000237E0000400F00002812000023537472696E677300000000682100005008000023555300B8290000100000002347554944000000C82900008C07000023426C6F620000000000000002000001579FA2090902000000FA25330016000001000000620000000B000000100000004B0000004C000000030000009B0000000800000010000000010000001D0000000200000005000000050000000600000001000000040000000100000000000A000100000000000600F100EA000600F800EA0006000201EA000A002D0118010A005201370106006301EA0006006801EA000A00740118010600D101B4010600E301B4010A005F0218010A008B0218010600E302C80206004703C8020A0064034E030A00A20318010600E803EA0006000604EA0006006E0464040600800464040A003D050C010A008F050C010A00C50518010A001D0637010A003E0637010E008506C8020A0092060C010A00E5064E030A006D074E0306006E095C09060085095C090600A2095C090600C1095C090600DA095C090600F3095C0906000E0A5C090600460A270A06005A0AEA000600700A5C090600890A5C090600B70AA40AA700CB0A00000600FA0ADA0A06001A0BDA0A12004C0B380B12005D0B380B0A00870B740B0A00990B4E030A00B30B740B0600F30BE30B0A00050C4E030A00200C740B0600430CEA000600600CEA000A00760C740B0A00830C37010A009F0C37010600A60C270A0600BC0C270A0600C70C5C090600E50C5C090600FA0CEA000A002C0D370106003F0DEA0006004C0DEA0006006B0DEA003B00710D00000600A10DEA000600C30DB70D0E00090EEA0D0A00400E0C010A005B0E0C010A009C0E0C010A00C40E18010A00DD0E18010600FE0EEA0006003A0FEA0006003F0FEA0006007D0F6A0F0A00970F0C010600C70FEA000A00E30F18010A00261037010A00311037016B00710D00000E009010C8020600A910EA000600C310AE100600E410EA000600EC10EA0006000311EA0006001511EA000E0054113E110A0075114E030A00A0110C010A00CA114E030A00F0114E030A000A120C01000000000100000000000100010000001000170027000500010001000120100030002700090001000400000010004900270005000100080009011000560027000D0002000C000301000063000000190005002600012010007A00270009000C002600000010009400270005000C002A0000001000AD00270005000D00320081011000BD00270005000E003B0000001000CE00270005000E0040000100F4012C0051803D0236005180490246003100F0026A0006069F0446005680A704E8005680AF04E8005680BE04E8005680CE04E8005680D904E8005680E804E8000100F4012C000100F4012C000100F3068C010100FE06900101000A079401D0200000000086007E010A0001007421000000009100860110000200CE21000000008618AE0114000200D621000000008618AE0114000200DE21000000008618AE0118000200E721000000008618AE011D000300F121000000008418AE0124000500FB21000000008618AE01300007000A2200000000830007020A00080029220000000083001F020A00090034220000000083002E020A000A0054220000000096005A024E000B006F22000000009600690253000B00882200000000960074024E000B00C822000000009600940258000B001C25000000009600A20263000E006125000000009100FB0272000F0070250000000091000C03770010007A260000000091001D037D0011009F26000000009100270383001300D4260000000091003703880014002827000000009100720392001600C82900000000910086039F001800FC290000000091009A03830019001D2A000000009100AE03A7001A00352A000000009100BE03A7001B004D2A000000009100D203A7001C00652A000000009100F103AD001D00822A0000000091001504B3001E00942A0000000091002F04B9001F00E42A0000000096084104BF002000F02A00000000E6094A04C4002000FC2A0000000096005504C8002000082B00000000C6005B04CF002100142B00000000E6017B04D30021001B2B00000000E6018D04D9002200242B000000009118570F15052300732B000000008618AE01140023007B2B000000008618AE0118002300842B000000008618AE011D0024008E2B000000008418AE0124002600982B000000008618AE0130002800A82B000000008600F2040A012900F82B000000008100130512012B004C2C000000009100310518012C00582C00000000910047051E012D00A42C0000000091005F0527012F00CC2C0000000091007D052F013100042E0000000091009A05350132002F2E000000008618AE0130003300402E000000008600CE053B013400F42E000000008100F00543013600502F000000009100080618013700852F000000009100290649013800A42F0000000091004C0652013A00D82F0000000091006A065C013C004C300000000091009A0664013D00D030000000009100AE066F013E004C32000000009600F20476013F006C32000000009600AD007E0141008C32000000009600C90686014300A832000000009600D70686014400C8320000000096001F0286014500E732000000008618AE0114004600F53200000000E60113071400460012330000000086081B07970146001A330000000081002B0714004600423300000000810033071400460050330000000086083E07CF00460084330000000086084D07CF00460094330000000086005E079C014600F4330000000084008507A301470044340000000086009307AA014900A034000000008600A00718004B00EC34000000008600BE070A004C0000000100F40700000100FC0700000100FC0700000200040800000100130800000200180800000100F40100000100F40700000100F40700000100F407000001002008000002002A08000003003608000001004108000001005108000001004108000001005308000002005908000001006008000001002008000002002A08000001006808000002006F08000001003608000001008C08000001009708000001009708000001009708000001009708000001009F0800000100A80800000100530800000100B20800000100B40800000100FC0700000100FC0700000200040800000100130800000200180800000100F40100000100B60800000200C60800000100F40700000100680800000100F40700000200680800000100F40700000200D40800000100D40800000100DB0800000100F40100000100E20800000200F40700000100E20800000100EE0800000100EE0800000200F90800000100EE0800000200F90800000100EE0800000100D40800000100FE0800000100B60800000200C608000001000C0900000200F40700000100F40700000100F40700000100F407000001001809000001002009000002002709000001002C09000002003B0900000100480900000100570905001100050015000B001D00F100AE011800F900AE0118000101AE0118000901AE0118001101AE0118001901AE0118002101AE0118002901AE01B9013101AE01B9013901AE0118004101AE0118004901AE01BE015901AE01C5016101AE0114006901AE016C02E100AE0118007901940B14008101AE0114008101A40B730209005B04CF008901BD0B18008901CD0B79023900130714007901DD0B14009901AE011400A1013A0C8C02A1014B0CCF000900AE011400B101AE0114001100AE0114001100AE0118001100AE011D001100AE012400B901DD0B1400C101AE01A202D101AE01D502E101D00CDC02E101F20CE202E901020DE80241000E0DEE02E9011A0DF40259000E0DF902F901AE0114000102440D91030902530D970309025B0D9D0341004A04C4000C00620DB0030C007C0DB60314008A0DC8030902960D79022102A60DCD031400AA0DC4002102B30DCD032902AE01C5012902D10DD3032902DC0DD9032902960D79022902E30DE0036100AE010A003102AE0118003102530D19041C000F0E300409021B0E36042902AE0114000902270E470409025B0D4D04B901310E58040C00AE011400A900520E5D044102760E79020902800E63043902620D6804D900620D6E040C00870E73042400AE0114002400870E730424008B0E7F04B901930E8504B901A60E8A0481000E0D90047900B20E97047900CF0E9D047900E70EA4045902F40EAB0461025B04AF047900050FB404B901120FBA04B9011B0F7902B9017B04C40031022A0FEB048100F40EF70409029F0CFC048900300F02058900AE0106055900F40EF40269025B04AF047102AE0114001C00AE0114001C00870E190511005E0FCF0041027C0D400579028A0D46050902890F4A05A900AC0F5005B100B80FCF0009025B0D56057902AA0DC4000902D80F7105B9000E0D7905B900EE0F7F059102FA0F8905B9010210C400B9000D107F0511025B04CF0009025B0D9005B9004A04C40091020E0D9E0591021910A50599023910B105C900AE01B705A1024210BE05A10253101400A1026210BE0579007110CA05C9007E10CA052C00760E79022C007C0DDE0534008A0DC8033400AA0DC4002C00AE01140009028810CF002C00A1100806C100AE012406C100AE012C06C102CF103506C902FC103B06C100AE014306C100AE014D06D902AE011800E10218117A06E100AE011400790129111800E90213071400B9015E117F0679016811CF00410093049001F102AE018A06E100901190068101B0119706E9005E0FCF004100BE11A7068101E111B0060103FD11B60689011612BE060E000800390008000C00490008001800EC0008001C00F10008002000F60008002400FB0008002800000108002C0005012E0023000E072E002B001E072E0073006B072E000B00CB062E001300D9062E001B0008072E004B0029072E006B0062072E00430008072E00330008072E005B002F072E0063005907A3001B01A902C0015B010003E0015B01000300025B01000300000100000005007D0292029D02A303E90321043B045404BF04F2040B0521052A05310539055C059605C405D005F0051406580666066B067006750684069F06C506050001000B00030000009304DF0000009804E4000000D007B0010000DC07B5010000E707B50102001F000300020020000500020042000700020045000900020046000B00A903C00329047904D705E8050480000001000000F116016B01000000CA01270000000200000000000000000000000100E1000000000002000000000000000000000001000C01000000000200000000000000000000000100EA00000000000200000000000000000000000100380B000000000600050000000000003C4D6F64756C653E007453514C74434C522E646C6C00436F6D6D616E644578656375746F72007453514C74434C5200436F6D6D616E644578656375746F72457863657074696F6E004F7574707574436170746F72007453514C7450726976617465004765745374617274506F736974696F6E53746174657300496E76616C6964526573756C74536574457863657074696F6E004D65746144617461457175616C697479417373657274657200526573756C7453657446696C7465720053746F72656450726F6365647572657300546573744461746162617365466163616465006D73636F726C69620053797374656D004F626A65637400457863657074696F6E0056616C7565547970650053797374656D2E446174610053797374656D2E446174612E53716C547970657300494E756C6C61626C65004D6963726F736F66742E53716C5365727665722E536572766572004942696E61727953657269616C697A6500456E756D0049446973706F7361626C650053716C537472696E67004578656375746500437265617465436F6E6E656374696F6E537472696E67546F436F6E746578744461746162617365002E63746F720053797374656D2E52756E74696D652E53657269616C697A6174696F6E0053657269616C697A6174696F6E496E666F0053747265616D696E67436F6E746578740074657374446174616261736546616361646500436170747572654F7574707574546F4C6F675461626C650053757070726573734F75747075740045786563757465436F6D6D616E64004E554C4C5F535452494E47004D41585F434F4C554D4E5F574944544800496E666F0053716C42696E617279005369676E696E674B657900437265617465556E697175654F626A6563744E616D650053716C4368617273005461626C65546F537472696E6700476574416C74657253746174656D656E74576974686F7574536368656D6142696E64696E670053797374656D2E436F6C6C656374696F6E732E47656E657269630044696374696F6E617279603200576869746573706163650049735768697465737061636543686172004765745374617274506F736974696F6E00506164436F6C756D6E005472696D546F4D61784C656E6774680067657453716C53746174656D656E74004C69737460310053797374656D2E446174612E53716C436C69656E740053716C44617461526561646572006765745461626C65537472696E6741727261790053706C6974436F6C756D6E4E616D654C69737400756E71756F74650053716C4461746554696D650053716C44617465546F537472696E670053716C4461746554696D65546F537472696E6700536D616C6C4461746554696D65546F537472696E67004461746554696D650053716C4461746554696D6532546F537472696E67004461746554696D654F66667365740053716C4461746554696D654F6666736574546F537472696E670053716C42696E617279546F537472696E67006765745F4E756C6C006765745F49734E756C6C00506172736500546F537472696E670053797374656D2E494F0042696E61727952656164657200526561640042696E617279577269746572005772697465004E756C6C0049734E756C6C0076616C75655F5F0044656661756C740041667465724669727374446173680041667465725365636F6E6444617368004166746572536C617368004166746572536C617368537461720041667465725374617200417373657274526573756C74536574734861766553616D654D6574614461746100637265617465536368656D61537472696E6746726F6D436F6D6D616E6400636C6F736552656164657200446174615461626C6500617474656D7074546F476574536368656D615461626C65007468726F77457863657074696F6E4966536368656D614973456D707479006275696C64536368656D61537472696E670044617461436F6C756D6E00636F6C756D6E50726F7065727479497356616C6964466F724D65746144617461436F6D70617269736F6E0053716C496E7433320073656E6453656C6563746564526573756C74536574546F53716C436F6E746578740076616C6964617465526573756C745365744E756D6265720073656E64526573756C747365745265636F7264730053716C4D657461446174610073656E64456163685265636F72644F66446174610053716C446174615265636F7264006372656174655265636F7264506F70756C617465645769746844617461006372656174654D65746144617461466F72526573756C74736574004C696E6B65644C69737460310044617461526F7700676574446973706C61796564436F6C756D6E730063726561746553716C4D65746144617461466F72436F6C756D6E004E6577436F6E6E656374696F6E00436170747572654F75747075740053716C436F6E6E656374696F6E00636F6E6E656374696F6E00696E666F4D65737361676500646973706F73656400446973706F7365006765745F496E666F4D65737361676500636F6E6E65637400646973636F6E6E656374006765745F5365727665724E616D65006765745F44617461626173654E616D650065786563757465436F6D6D616E640053716C496E666F4D6573736167654576656E7441726773004F6E496E666F4D65737361676500617373657274457175616C73006661696C5465737443617365416E645468726F77457863657074696F6E006C6F6743617074757265644F757470757400496E666F4D657373616765005365727665724E616D650044617461626173654E616D6500636F6D6D616E64006D65737361676500696E6E6572457863657074696F6E00696E666F00636F6E74657874005461626C654E616D65004F726465724F7074696F6E00436F6C756D6E4C6973740063726561746553746174656D656E74006300696E707574006C656E67746800726F774461746100726561646572005072696E744F6E6C79436F6C756D6E4E616D65416C6961734C69737400636F6C756D6E4E616D6500647456616C75650064746F56616C75650073716C42696E61727900720077006578706563746564436F6D6D616E640061637475616C436F6D6D616E6400736368656D6100636F6C756D6E00726573756C747365744E6F0064617461526561646572006D65746100636F6C756D6E44657461696C7300726573756C745365744E6F00436F6D6D616E640073656E6465720061726773006578706563746564537472696E670061637475616C537472696E67006661696C7572654D65737361676500746578740053797374656D2E5265666C656374696F6E00417373656D626C795469746C6541747472696275746500417373656D626C794465736372697074696F6E41747472696275746500417373656D626C79436F6E66696775726174696F6E41747472696275746500417373656D626C79436F6D70616E7941747472696275746500417373656D626C7950726F6475637441747472696275746500417373656D626C7954726164656D61726B41747472696275746500417373656D626C7943756C747572654174747269627574650053797374656D2E52756E74696D652E496E7465726F70536572766963657300436F6D56697369626C6541747472696275746500434C53436F6D706C69616E7441747472696275746500417373656D626C7956657273696F6E41747472696275746500417373656D626C79436F707972696768744174747269627574650053797374656D2E446961676E6F73746963730044656275676761626C6541747472696275746500446562756767696E674D6F6465730053797374656D2E52756E74696D652E436F6D70696C6572536572766963657300436F6D70696C6174696F6E52656C61786174696F6E734174747269627574650052756E74696D65436F6D7061746962696C6974794174747269627574650053797374656D2E5472616E73616374696F6E73005472616E73616374696F6E53636F7065005472616E73616374696F6E53636F70654F7074696F6E0053797374656D2E446174612E436F6D6D6F6E004462436F6E6E656374696F6E004F70656E0053716C436F6D6D616E64007365745F436F6E6E656374696F6E004462436F6D6D616E64007365745F436F6D6D616E645465787400457865637574654E6F6E517565727900436C6F73650053797374656D2E5365637572697479005365637572697479457863657074696F6E0053716C436F6E6E656374696F6E537472696E674275696C646572004462436F6E6E656374696F6E537472696E674275696C646572007365745F4974656D00426F6F6C65616E006765745F436F6E6E656374696F6E537472696E670053657269616C697A61626C65417474726962757465004462446174615265616465720053716C55736572446566696E65645479706541747472696275746500466F726D6174005374727563744C61796F7574417474726962757465004C61796F75744B696E6400417373656D626C7900476574457865637574696E67417373656D626C7900417373656D626C794E616D65004765744E616D650056657273696F6E006765745F56657273696F6E006F705F496D706C69636974004765745075626C69634B6579546F6B656E0053716C4D6574686F644174747269627574650047756964004E65774775696400537472696E67005265706C61636500436F6E636174006765745F4974656D00496E74333200456E756D657261746F7200476574456E756D657261746F72006765745F43757272656E74006765745F4C656E677468004D617468004D6178004D6F76654E657874004D696E0053797374656D2E5465787400537472696E674275696C64657200417070656E644C696E6500417070656E6400496E736572740053797374656D2E546578742E526567756C617245787072657373696F6E7300526567657800436F6E7461696E734B657900546F43686172417272617900537562737472696E6700476574536368656D615461626C650044617461526F77436F6C6C656374696F6E006765745F526F777300496E7465726E616C44617461436F6C6C656374696F6E42617365006765745F436F756E7400457175616C730041646400546F417272617900497344424E756C6C0053716C446254797065004765744461746554696D65004765744461746554696D654F66667365740053716C446563696D616C0047657453716C446563696D616C0053716C446F75626C650047657453716C446F75626C65006765745F56616C756500446F75626C650047657453716C42696E6172790047657456616C7565006765745F4669656C64436F756E740053706C6974006765745F5469636B730042797465004E6F74496D706C656D656E746564457863657074696F6E002E6363746F72006765745F4D6573736167650053797374656D2E436F6C6C656374696F6E730049456E756D657261746F72006F705F496E657175616C6974790044617461436F6C756D6E436F6C6C656374696F6E006765745F436F6C756D6E73006765745F436F6C756D6E4E616D6500537472696E67436F6D70617269736F6E00537461727473576974680053716C426F6F6C65616E006F705F457175616C697479006F705F54727565004E657874526573756C74006F705F4C6573735468616E006F705F426974776973654F720053716C436F6E746578740053716C50697065006765745F506970650053656E64526573756C747353746172740053656E64526573756C7473456E640053656E64526573756C7473526F770047657453716C56616C7565730053657456616C75657300546F4C6F776572004C696E6B65644C6973744E6F64656031004164644C61737400547970650053797374656D2E476C6F62616C697A6174696F6E0043756C74757265496E666F006765745F496E76617269616E7443756C7475726500436F6E766572740049466F726D617450726F766964657200546F4279746500417267756D656E74457863657074696F6E00474300537570707265737346696E616C697A65007365745F436F6E6E656374696F6E537472696E670053797374656D2E436F6D706F6E656E744D6F64656C00436F6D706F6E656E7400476574537472696E67006765745F44617461626173650053716C496E666F4D6573736167654576656E7448616E646C6572006164645F496E666F4D65737361676500436F6D6D616E644265686176696F720045786563757465526561646572006F705F4164646974696F6E0053716C506172616D65746572436F6C6C656374696F6E006765745F506172616D65746572730053716C506172616D65746572004164645769746856616C756500436F6D6D616E6454797065007365745F436F6D6D616E64547970650000000080B34500720072006F007200200063006F006E006E0065006300740069006E006700200074006F002000640061007400610062006100730065002E00200059006F00750020006D006100790020006E00650065006400200074006F00200063007200650061007400650020007400530051004C007400200061007300730065006D0062006C007900200077006900740068002000450058005400450052004E0041004C005F004100430043004500530053002E0000174400610074006100200053006F007500720063006500002749006E0074006500670072006100740065006400200053006500630075007200690074007900001F49006E0069007400690061006C00200043006100740061006C006F00670000237400530051004C0074005F00740065006D0070006F0062006A006500630074005F0000032D00010100354F0062006A0065006300740020006E0061006D0065002000630061006E006E006F00740020006200650020004E0055004C004C0000037C0000032B0000634300520045004100540045005C0073002B00560049004500570028005C0073002A002E002A003F005C0073002A00290057004900540048005C0073002B0053004300480045004D004100420049004E00440049004E0047005C0073002B0041005300001D41004C00540045005200200056004900450057002400310041005300001975006E006500780070006500630074006500640020002F0000032000000B3C002E002E002E003E00001D530045004C0045004300540020002A002000460052004F004D002000001520004F0052004400450052002000420059002000001543006F006C0075006D006E004E0061006D00650000055D005D0000035D00000D21004E0055004C004C0021000019500072006F00760069006400650072005400790070006500002930002E0030003000300030003000300030003000300030003000300030003000300045002B00300000055D002C0000052C005B00000B5C005D002C005C005B0000035B00001D7B0030003A0079007900790079002D004D004D002D00640064007D0001377B0030003A0079007900790079002D004D004D002D00640064002000480048003A006D006D003A00730073002E006600660066007D0001297B0030003A0079007900790079002D004D004D002D00640064002000480048003A006D006D007D00013F7B0030003A0079007900790079002D004D004D002D00640064002000480048003A006D006D003A00730073002E0066006600660066006600660066007D0001477B0030003A0079007900790079002D004D004D002D00640064002000480048003A006D006D003A00730073002E00660066006600660066006600660020007A007A007A007D0001053000780000055800320000737400530051004C007400500072006900760061007400650020006900730020006E006F007400200069006E00740065006E00640065006400200074006F002000620065002000750073006500640020006F0075007400730069006400650020006F00660020007400530051004C0074002100001B540068006500200063006F006D006D0061006E00640020005B0000475D00200064006900640020006E006F0074002000720065007400750072006E00200061002000760061006C0069006400200072006500730075006C0074002000730065007400003B5D00200064006900640020006E006F0074002000720065007400750072006E0020006100200072006500730075006C0074002000730065007400001149007300480069006400640065006E000009540072007500650000037B0000033A0000037D0000054900730000094200610073006500003145007800650063007500740069006F006E002000720065007400750072006E006500640020006F006E006C00790020000031200052006500730075006C00740053006500740073002E00200052006500730075006C00740053006500740020005B0000235D00200064006F006500730020006E006F0074002000650078006900730074002E00005D52006500730075006C007400530065007400200069006E00640065007800200062006500670069006E007300200061007400200031002E00200052006500730075006C007400530065007400200069006E0064006500780020005B00001B5D00200069007300200069006E00760061006C00690064002E0000097400720075006500001144006100740061005400790070006500001543006F006C0075006D006E00530069007A00650000214E0075006D00650072006900630050007200650063006900730069006F006E0000194E0075006D0065007200690063005300630061006C006500001541007200670075006D0065006E00740020005B0000475D0020006900730020006E006F0074002000760061006C0069006400200066006F007200200052006500730075006C007400530065007400460069006C007400650072002E00003143006F006E007400650078007400200043006F006E006E0065006300740069006F006E003D0074007200750065003B000049530045004C004500430054002000530045005200560045005200500052004F0050004500520054005900280027005300650072007600650072004E0061006D006500270029003B0001050D000A0000317400530051004C0074002E0041007300730065007200740045007100750061006C00730053007400720069006E006700001145007800700065006300740065006400000D410063007400750061006C0000157400530051004C0074002E004600610069006C0000114D006500730073006100670065003000002F7400530051004C0074002E004C006F006700430061007000740075007200650064004F0075007400700075007400000974006500780074000005DE9AFB029CE74BA9DC99DAF9484EAD0008B77A5C561934E0890520010111210300000E03200001042001010E062002010E120907200201122511290306122C05200101122C02060E0C21004E0055004C004C002100020608049B0000000400001121040000112D0A0003123111211121112106000112311121070615123502030204000102030500010811210500020E0E080400010E0E0900020E1011211011210C0002151239011D0E123D11210700011D0E1011210500010E11410500010E11450500010E11490500010E112D040000111403200002060001111411210320000E05200101124D0520010112510408001114032800020306111804000000000401000000040200000004030000000404000000040500000007200201112111210520010E112105000101123D08000212551121123D07000201112112550500010E125505000102125907200201115D112105200101115D08000201123D1D12610900021265123D1D12610700011D1261123D0A000115126901126D12550600011261126D070002011121112107000201115D112105000101112103061271030611210206020420001121062001123D1121062002011C1275052002010E0E04280011210328000E0420010102062001011180A9042001010880A00024000004800000940000000602000000240000525341310004000001000100F7D9A45F2B508C2887A8794B053CE5DEB28743B7C748FF545F1F51218B684454B785054629C1417D1D3542B095D80BA171294948FCF978A502AA03240C024746B563BC29B4D8DCD6956593C0C425446021D699EF6FB4DC2155DE7E393150AD6617EDC01216EA93FCE5F8F7BE9FF605AD2B8344E8CC01BEDB924ED06FD368D1D0062001011180B9052001011271032000080E070512711280B50E1280C11280C9052002010E1C0A0705122C0E0E1280CD0E040701123D062001011180E52B010002000000020054080B4D61784279746553697A650100000054020D497346697865644C656E67746801062001011180ED0500001280F10520001280F50520001280F905000111210E0420001D05060001112D1D05808F010001005455794D6963726F736F66742E53716C5365727665722E5365727665722E446174614163636573734B696E642C2053797374656D2E446174612C2056657273696F6E3D322E302E302E302C2043756C747572653D6E65757472616C2C205075626C69634B6579546F6B656E3D623737613563353631393334653038390A44617461416363657373010000000500001181010520020E0E0E0500020E0E0E05070111810106151239011D0E0520011300080920001511810D011300071511810D011D0E04200013000500020808080520001281150620011281150E082003128115080E082F0714122C0E123D151239011D0E081D081D0E08080808021281151D0E08081511810D011D0E1D08081511810D011D0E0720040E0E0E0808070703081281190E061512350203020520010213000420001D030B07061D03081118080811180520020E08080600030E0E0E0E0307010E042000125505200012811D042001020E052001126D080420011C0E05200101130005151239010E0520001D13000420010208052001114508060001114111450520011149080620011181290806200111812D080320000D0420010E0E052001112D080420011C082B07121255151239011D0E081D0E08126D1D0E151239010E0E1D0E081181251D0E0811812511812911812D0D0600021D0E0E0E0407011D0E04200011450500020E0E1C0320000A042001010A090704128115051D05080300000107200201130013010807011512350203020607030E0E121C070703123D12550E0607021209125505200012813D0320001C050002020E0E0520001281410500010E1D1C1407090E126D125912813D12813D1C1D1C121D121D072002020E118145050001115D08090002118149115D115D060001021181490500010E1D0E070703123D081D0E060001118149020B0002118149118149118149050000128151062001011D12610520010112650507011D1261052001081D1C06070212651D1C0615126901126D09200015118155011300071511815501126D170706125515126901126D1D126108126D1511815501126D0B20011512815901130013000F070415126901126D126D12813D121D072002010E118125082003010E1181250A050000128161070002051C128169092004010E11812505050A2003010E11812512815D0D07051181250E12815D081181250407011220040701122404070112080407011210040001011C0420010E08050702123D0E052002011C1806200101128179072001123D11817D0707021280C1123D0800021121112111210520001281810720021281850E1C062001011181890507011280C10D0100087453514C74434C5200002E010029434C527320666F7220746865207453514C7420756E69742074657374696E67206672616D65776F726B00000501000000000F01000A73716C6974792E6E657400000A0100057453514C74000005010001000029010024436F7079726967687420C2A9202073716C6974792E6E65742032303130202D203230313500000801000200000000000801000800000000001E01000100540216577261704E6F6E457863657074696F6E5468726F7773010000000000005219AD5600000000020000001C010000AC660000AC48000052534453683BAB7F3D6D534A9E23408F5CA7F1A701000000633A5C5465616D436974795C6275696C644167656E745C776F726B5C666264353737636331386432383966385C7453514C74434C525C7453514C74434C525C6F626A5C437275697365436F6E74726F6C5C7453514C74434C522E7064620000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000F067000000000000000000000E6800000020000000000000000000000000000000000000000000000068000000000000000000000000000000005F436F72446C6C4D61696E006D73636F7265652E646C6C0000000000FF2500200010000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000100100000001800008000000000000000000000000000000100010000003000008000000000000000000000000000000100000000004800000058800000A00300000000000000000000A00334000000560053005F00560045005200530049004F004E005F0049004E0046004F0000000000BD04EFFE0000010000000100016BF11600000100016BF1163F000000000000000400000002000000000000000000000000000000440000000100560061007200460069006C00650049006E0066006F00000000002400040000005400720061006E0073006C006100740069006F006E00000000000000B00400030000010053007400720069006E006700460069006C00650049006E0066006F000000DC02000001003000300030003000300034006200300000006C002A00010043006F006D006D0065006E0074007300000043004C0052007300200066006F007200200074006800650020007400530051004C007400200075006E00690074002000740065007300740069006E00670020006600720061006D00650077006F0072006B00000038000B00010043006F006D00700061006E0079004E0061006D00650000000000730071006C006900740079002E006E0065007400000000003C0009000100460069006C0065004400650073006300720069007000740069006F006E00000000007400530051004C00740043004C0052000000000040000F000100460069006C006500560065007200730069006F006E000000000031002E0030002E0035003800370033002E0032003700330039003300000000003C000D00010049006E007400650072006E0061006C004E0061006D00650000007400530051004C00740043004C0052002E0064006C006C00000000006C00240001004C006500670061006C0043006F007000790072006900670068007400000043006F0070007900720069006700680074002000A90020002000730071006C006900740079002E006E00650074002000320030003100300020002D0020003200300031003500000044000D0001004F0072006900670069006E0061006C00460069006C0065006E0061006D00650000007400530051004C00740043004C0052002E0064006C006C00000000002C0006000100500072006F0064007500630074004E0061006D006500000000007400530051004C007400000044000F000100500072006F006400750063007400560065007200730069006F006E00000031002E0030002E0035003800370033002E00320037003300390033000000000048000F00010041007300730065006D0062006C0079002000560065007200730069006F006E00000031002E0030002E0035003800370033002E0032003700330039003300000000000000000000000000006000000C000000203800000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000 WITH PERMISSION_SET = SAFE;
GO



GO

GO

CREATE PROCEDURE tSQLt.ResultSetFilter @ResultsetNo INT, @Command NVARCHAR(MAX)
AS
EXTERNAL NAME tSQLtCLR.[tSQLtCLR.StoredProcedures].ResultSetFilter;
GO

CREATE PROCEDURE tSQLt.AssertResultSetsHaveSameMetaData @expectedCommand NVARCHAR(MAX), @actualCommand NVARCHAR(MAX)
AS
EXTERNAL NAME tSQLtCLR.[tSQLtCLR.StoredProcedures].AssertResultSetsHaveSameMetaData;
GO

CREATE TYPE tSQLt.[Private] EXTERNAL NAME tSQLtCLR.[tSQLtCLR.tSQLtPrivate];
GO

CREATE PROCEDURE tSQLt.NewConnection @command NVARCHAR(MAX)
AS
EXTERNAL NAME tSQLtCLR.[tSQLtCLR.StoredProcedures].NewConnection;
GO

CREATE PROCEDURE tSQLt.CaptureOutput @command NVARCHAR(MAX)
AS
EXTERNAL NAME tSQLtCLR.[tSQLtCLR.StoredProcedures].CaptureOutput;
GO

CREATE PROCEDURE tSQLt.SuppressOutput @command NVARCHAR(MAX)
AS
EXTERNAL NAME tSQLtCLR.[tSQLtCLR.StoredProcedures].SuppressOutput;
GO



GO

GO
CREATE PROCEDURE tSQLt.TableToText
    @txt NVARCHAR(MAX) OUTPUT,
    @TableName NVARCHAR(MAX),
    @OrderBy NVARCHAR(MAX) = NULL,
    @PrintOnlyColumnNameAliasList NVARCHAR(MAX) = NULL
AS
BEGIN
    SET @txt = tSQLt.Private::TableToString(@TableName, @OrderBy, @PrintOnlyColumnNameAliasList);
END;
GO


GO

CREATE TABLE tSQLt.Private_RenamedObjectLog (
  Id INT IDENTITY(1,1) CONSTRAINT PK__Private_RenamedObjectLog__Id PRIMARY KEY CLUSTERED,
  ObjectId INT NOT NULL,
  OriginalName NVARCHAR(MAX) NOT NULL
);


GO

CREATE PROCEDURE tSQLt.Private_MarkObjectBeforeRename
    @SchemaName NVARCHAR(MAX), 
    @OriginalName NVARCHAR(MAX)
AS
BEGIN
  INSERT INTO tSQLt.Private_RenamedObjectLog (ObjectId, OriginalName) 
  VALUES (OBJECT_ID(@SchemaName + '.' + @OriginalName), @OriginalName);
END;


GO

CREATE PROCEDURE tSQLt.Private_RenameObjectToUniqueName
    @SchemaName NVARCHAR(MAX),
    @ObjectName NVARCHAR(MAX),
    @NewName NVARCHAR(MAX) = NULL OUTPUT
AS
BEGIN
   SET @NewName=tSQLt.Private::CreateUniqueObjectName();

   DECLARE @RenameCmd NVARCHAR(MAX);
   SET @RenameCmd = 'EXEC sp_rename ''' + 
                          @SchemaName + '.' + @ObjectName + ''', ''' + 
                          @NewName + ''';';
   
   EXEC tSQLt.Private_MarkObjectBeforeRename @SchemaName, @ObjectName;


   EXEC tSQLt.SuppressOutput @RenameCmd;

END;


GO

CREATE PROCEDURE tSQLt.Private_RenameObjectToUniqueNameUsingObjectId
    @ObjectId INT,
    @NewName NVARCHAR(MAX) = NULL OUTPUT
AS
BEGIN
   DECLARE @SchemaName NVARCHAR(MAX);
   DECLARE @ObjectName NVARCHAR(MAX);
   
   SELECT @SchemaName = QUOTENAME(OBJECT_SCHEMA_NAME(@ObjectId)), @ObjectName = QUOTENAME(OBJECT_NAME(@ObjectId));
   
   EXEC tSQLt.Private_RenameObjectToUniqueName @SchemaName,@ObjectName, @NewName OUTPUT;
END;


GO

GO
CREATE PROCEDURE tSQLt.RemoveObject 
    @ObjectName NVARCHAR(MAX),
    @NewName NVARCHAR(MAX) = NULL OUTPUT,
    @IfExists INT = 0
AS
BEGIN
  DECLARE @ObjectId INT;
  SELECT @ObjectId = OBJECT_ID(@ObjectName);
  
  IF(@ObjectId IS NULL)
  BEGIN
    IF(@IfExists = 1) RETURN;
    RAISERROR('%s does not exist!',16,10,@ObjectName);
  END;

  EXEC tSQLt.Private_RenameObjectToUniqueNameUsingObjectId @ObjectId, @NewName = @NewName OUTPUT;
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.RemoveObjectIfExists 
    @ObjectName NVARCHAR(MAX),
    @NewName NVARCHAR(MAX) = NULL OUTPUT
AS
BEGIN
  EXEC tSQLt.RemoveObject @ObjectName = @ObjectName, @NewName = @NewName OUT, @IfExists = 1;
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_CleanTestResult
AS
BEGIN
   DELETE FROM tSQLt.TestResult;
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_Init
AS
BEGIN
  EXEC tSQLt.Private_CleanTestResult;

  DECLARE @enable BIT; SET @enable = 1;
  DECLARE @version_match BIT;SET @version_match = 0;
  BEGIN TRY
    EXEC sys.sp_executesql N'SELECT @r = CASE WHEN I.Version = I.ClrVersion THEN 1 ELSE 0 END FROM tSQLt.Info() AS I;',N'@r BIT OUTPUT',@version_match OUT;
  END TRY
  BEGIN CATCH
    RAISERROR('Cannot access CLR. Assembly might be in an invalid state. Try running EXEC tSQLt.EnableExternalAccess @enable = 0; or reinstalling tSQLt.',16,10);
    RETURN;
  END CATCH;
  IF(@version_match = 0)
  BEGIN
    RAISERROR('tSQLt is in an invalid state. Please reinstall tSQLt.',16,10);
    RETURN;
  END;

  IF((SELECT SqlEdition FROM tSQLt.Info()) &lt;&gt; 'SQL Azure')
  BEGIN
    EXEC tSQLt.EnableExternalAccess @enable = @enable, @try = 1;
  END;
END;
GO


GO


CREATE PROCEDURE tSQLt.Private_GetSetupProcedureName
  @TestClassId INT = NULL,
  @SetupProcName NVARCHAR(MAX) OUTPUT
AS
BEGIN
    SELECT @SetupProcName = tSQLt.Private_GetQuotedFullName(object_id)
      FROM sys.procedures
     WHERE schema_id = @TestClassId
       AND LOWER(name) = 'setup';
END;
GO

CREATE PROCEDURE tSQLt.Private_RunTest
   @TestName NVARCHAR(MAX),
   @SetUp NVARCHAR(MAX) = NULL
AS
BEGIN
    DECLARE @Msg NVARCHAR(MAX); SET @Msg = '';
    DECLARE @Msg2 NVARCHAR(MAX); SET @Msg2 = '';
    DECLARE @Cmd NVARCHAR(MAX); SET @Cmd = '';
    DECLARE @TestClassName NVARCHAR(MAX); SET @TestClassName = '';
    DECLARE @TestProcName NVARCHAR(MAX); SET @TestProcName = '';
    DECLARE @Result NVARCHAR(MAX); SET @Result = 'Success';
    DECLARE @TranName CHAR(32); EXEC tSQLt.GetNewTranName @TranName OUT;
    DECLARE @TestResultId INT;
    DECLARE @PreExecTrancount INT;

    DECLARE @VerboseMsg NVARCHAR(MAX);
    DECLARE @Verbose BIT;
    SET @Verbose = ISNULL((SELECT CAST(Value AS BIT) FROM tSQLt.Private_GetConfiguration('Verbose')),0);
    
    TRUNCATE TABLE tSQLt.CaptureOutputLog;
    CREATE TABLE #ExpectException(ExpectException INT,ExpectedMessage NVARCHAR(MAX), ExpectedSeverity INT, ExpectedState INT, ExpectedMessagePattern NVARCHAR(MAX), ExpectedErrorNumber INT, FailMessage NVARCHAR(MAX));

    IF EXISTS (SELECT 1 FROM sys.extended_properties WHERE name = N'SetFakeViewOnTrigger')
    BEGIN
      RAISERROR('Test system is in an invalid state. SetFakeViewOff must be called if SetFakeViewOn was called. Call SetFakeViewOff after creating all test case procedures.', 16, 10) WITH NOWAIT;
      RETURN -1;
    END;

    SELECT @Cmd = 'EXEC ' + @TestName;
    
    SELECT @TestClassName = OBJECT_SCHEMA_NAME(OBJECT_ID(@TestName)), --tSQLt.Private_GetCleanSchemaName('', @TestName),
           @TestProcName = tSQLt.Private_GetCleanObjectName(@TestName);
           
    INSERT INTO tSQLt.TestResult(Class, TestCase, TranName, Result) 
        SELECT @TestClassName, @TestProcName, @TranName, 'A severe error happened during test execution. Test did not finish.'
        OPTION(MAXDOP 1);
    SELECT @TestResultId = SCOPE_IDENTITY();

    IF(@Verbose = 1)
    BEGIN
      SET @VerboseMsg = 'tSQLt.Run '''+@TestName+'''; --Starting';
      EXEC tSQLt.Private_Print @Message =@VerboseMsg, @Severity = 0;
    END;

    BEGIN TRAN;
    SAVE TRAN @TranName;

    SET @PreExecTrancount = @@TRANCOUNT;
    
    TRUNCATE TABLE tSQLt.TestMessage;

    DECLARE @TmpMsg NVARCHAR(MAX);
    DECLARE @TestEndTime DATETIME; SET @TestEndTime = NULL;
    BEGIN TRY
        IF (@SetUp IS NOT NULL) EXEC @SetUp;
        EXEC (@Cmd);
        SET @TestEndTime = GETDATE();
        IF(EXISTS(SELECT 1 FROM #ExpectException WHERE ExpectException = 1))
        BEGIN
          SET @TmpMsg = COALESCE((SELECT FailMessage FROM #ExpectException)+' ','')+'Expected an error to be raised.';
          EXEC tSQLt.Fail @TmpMsg;
        END
    END TRY
    BEGIN CATCH
        SET @TestEndTime = ISNULL(@TestEndTime,GETDATE());
        IF ERROR_MESSAGE() LIKE '%tSQLt.Failure%'
        BEGIN
            SELECT @Msg = Msg FROM tSQLt.TestMessage;
            SET @Result = 'Failure';
        END
        ELSE
        BEGIN
          DECLARE @ErrorInfo NVARCHAR(MAX);
          SELECT @ErrorInfo = 
            COALESCE(ERROR_MESSAGE(), '&lt;ERROR_MESSAGE() is NULL&gt;') + 
            '[' +COALESCE(LTRIM(STR(ERROR_SEVERITY())), '&lt;ERROR_SEVERITY() is NULL&gt;') + ','+COALESCE(LTRIM(STR(ERROR_STATE())), '&lt;ERROR_STATE() is NULL&gt;') + ']' +
            '{' + COALESCE(ERROR_PROCEDURE(), '&lt;ERROR_PROCEDURE() is NULL&gt;') + ',' + COALESCE(CAST(ERROR_LINE() AS NVARCHAR), '&lt;ERROR_LINE() is NULL&gt;') + '}';

          IF(EXISTS(SELECT 1 FROM #ExpectException))
          BEGIN
            DECLARE @ExpectException INT;
            DECLARE @ExpectedMessage NVARCHAR(MAX);
            DECLARE @ExpectedMessagePattern NVARCHAR(MAX);
            DECLARE @ExpectedSeverity INT;
            DECLARE @ExpectedState INT;
            DECLARE @ExpectedErrorNumber INT;
            DECLARE @FailMessage NVARCHAR(MAX);
            SELECT @ExpectException = ExpectException,
                   @ExpectedMessage = ExpectedMessage, 
                   @ExpectedSeverity = ExpectedSeverity,
                   @ExpectedState = ExpectedState,
                   @ExpectedMessagePattern = ExpectedMessagePattern,
                   @ExpectedErrorNumber = ExpectedErrorNumber,
                   @FailMessage = FailMessage
              FROM #ExpectException;

            IF(@ExpectException = 1)
            BEGIN
              SET @Result = 'Success';
              SET @TmpMsg = COALESCE(@FailMessage+' ','')+'Exception did not match expectation!';
              IF(ERROR_MESSAGE() &lt;&gt; @ExpectedMessage)
              BEGIN
                SET @TmpMsg = @TmpMsg +CHAR(13)+CHAR(10)+
                           'Expected Message: &lt;'+@ExpectedMessage+'&gt;'+CHAR(13)+CHAR(10)+
                           'Actual Message  : &lt;'+ERROR_MESSAGE()+'&gt;';
                SET @Result = 'Failure';
              END
              IF(ERROR_MESSAGE() NOT LIKE @ExpectedMessagePattern)
              BEGIN
                SET @TmpMsg = @TmpMsg +CHAR(13)+CHAR(10)+
                           'Expected Message to be like &lt;'+@ExpectedMessagePattern+'&gt;'+CHAR(13)+CHAR(10)+
                           'Actual Message            : &lt;'+ERROR_MESSAGE()+'&gt;';
                SET @Result = 'Failure';
              END
              IF(ERROR_NUMBER() &lt;&gt; @ExpectedErrorNumber)
              BEGIN
                SET @TmpMsg = @TmpMsg +CHAR(13)+CHAR(10)+
                           'Expected Error Number: '+CAST(@ExpectedErrorNumber AS NVARCHAR(MAX))+CHAR(13)+CHAR(10)+
                           'Actual Error Number  : '+CAST(ERROR_NUMBER() AS NVARCHAR(MAX));
                SET @Result = 'Failure';
              END
              IF(ERROR_SEVERITY() &lt;&gt; @ExpectedSeverity)
              BEGIN
                SET @TmpMsg = @TmpMsg +CHAR(13)+CHAR(10)+
                           'Expected Severity: '+CAST(@ExpectedSeverity AS NVARCHAR(MAX))+CHAR(13)+CHAR(10)+
                           'Actual Severity  : '+CAST(ERROR_SEVERITY() AS NVARCHAR(MAX));
                SET @Result = 'Failure';
              END
              IF(ERROR_STATE() &lt;&gt; @ExpectedState)
              BEGIN
                SET @TmpMsg = @TmpMsg +CHAR(13)+CHAR(10)+
                           'Expected State: '+CAST(@ExpectedState AS NVARCHAR(MAX))+CHAR(13)+CHAR(10)+
                           'Actual State  : '+CAST(ERROR_STATE() AS NVARCHAR(MAX));
                SET @Result = 'Failure';
              END
              IF(@Result = 'Failure')
              BEGIN
                SET @Msg = @TmpMsg;
              END
            END 
            ELSE
            BEGIN
                SET @Result = 'Failure';
                SET @Msg = 
                  COALESCE(@FailMessage+' ','')+
                  'Expected no error to be raised. Instead this error was encountered:'+
                  CHAR(13)+CHAR(10)+
                  @ErrorInfo;
            END
          END
          ELSE
          BEGIN
            SET @Result = 'Error';
            SET @Msg = @ErrorInfo;
          END  
        END;
    END CATCH

    BEGIN TRY
        ROLLBACK TRAN @TranName;
    END TRY
    BEGIN CATCH
        DECLARE @PostExecTrancount INT;
        SET @PostExecTrancount = @PreExecTrancount - @@TRANCOUNT;
        IF (@@TRANCOUNT &gt; 0) ROLLBACK;
        BEGIN TRAN;
        IF(   @Result &lt;&gt; 'Success'
           OR @PostExecTrancount &lt;&gt; 0
          )
        BEGIN
          SELECT @Msg = COALESCE(@Msg, '&lt;NULL&gt;') + ' (There was also a ROLLBACK ERROR --&gt; ' + COALESCE(ERROR_MESSAGE(), '&lt;ERROR_MESSAGE() is NULL&gt;') + '{' + COALESCE(ERROR_PROCEDURE(), '&lt;ERROR_PROCEDURE() is NULL&gt;') + ',' + COALESCE(CAST(ERROR_LINE() AS NVARCHAR), '&lt;ERROR_LINE() is NULL&gt;') + '})';
          SET @Result = 'Error';
        END
    END CATCH    

    If(@Result &lt;&gt; 'Success') 
    BEGIN
      SET @Msg2 = @TestName + ' failed: (' + @Result + ') ' + @Msg;
      EXEC tSQLt.Private_Print @Message = @Msg2, @Severity = 0;
    END

    IF EXISTS(SELECT 1 FROM tSQLt.TestResult WHERE Id = @TestResultId)
    BEGIN
        UPDATE tSQLt.TestResult SET
            Result = @Result,
            Msg = @Msg,
            TestEndTime = @TestEndTime
         WHERE Id = @TestResultId;
    END
    ELSE
    BEGIN
        INSERT tSQLt.TestResult(Class, TestCase, TranName, Result, Msg)
        SELECT @TestClassName, 
               @TestProcName,  
               '?', 
               'Error', 
               'TestResult entry is missing; Original outcome: ' + @Result + ', ' + @Msg;
    END    
      

    COMMIT;

    IF(@Verbose = 1)
    BEGIN
    SET @VerboseMsg = 'tSQLt.Run '''+@TestName+'''; --Finished';
      EXEC tSQLt.Private_Print @Message =@VerboseMsg, @Severity = 0;
    END;

END;
GO

CREATE PROCEDURE tSQLt.Private_RunTestClass
  @TestClassName NVARCHAR(MAX)
AS
BEGIN
    DECLARE @TestCaseName NVARCHAR(MAX);
    DECLARE @TestClassId INT; SET @TestClassId = tSQLt.Private_GetSchemaId(@TestClassName);
    DECLARE @SetupProcName NVARCHAR(MAX);
    EXEC tSQLt.Private_GetSetupProcedureName @TestClassId, @SetupProcName OUTPUT;
    
    DECLARE testCases CURSOR LOCAL FAST_FORWARD 
        FOR
     SELECT tSQLt.Private_GetQuotedFullName(object_id)
       FROM sys.procedures
      WHERE schema_id = @TestClassId
        AND LOWER(name) LIKE 'test%';

    OPEN testCases;
    
    FETCH NEXT FROM testCases INTO @TestCaseName;

    WHILE @@FETCH_STATUS = 0
    BEGIN
        EXEC tSQLt.Private_RunTest @TestCaseName, @SetupProcName;

        FETCH NEXT FROM testCases INTO @TestCaseName;
    END;

    CLOSE testCases;
    DEALLOCATE testCases;
END;
GO

CREATE PROCEDURE tSQLt.Private_Run
   @TestName NVARCHAR(MAX),
   @TestResultFormatter NVARCHAR(MAX)
AS
BEGIN
SET NOCOUNT ON;
    DECLARE @FullName NVARCHAR(MAX);
    DECLARE @TestClassId INT;
    DECLARE @IsTestClass BIT;
    DECLARE @IsTestCase BIT;
    DECLARE @IsSchema BIT;
    DECLARE @SetUp NVARCHAR(MAX);SET @SetUp = NULL;
    
    SELECT @TestName = tSQLt.Private_GetLastTestNameIfNotProvided(@TestName);
    EXEC tSQLt.Private_SaveTestNameForSession @TestName;
    
    SELECT @TestClassId = schemaId,
           @FullName = quotedFullName,
           @IsTestClass = isTestClass,
           @IsSchema = isSchema,
           @IsTestCase = isTestCase
      FROM tSQLt.Private_ResolveName(@TestName);

    IF @IsSchema = 1
    BEGIN
        EXEC tSQLt.Private_RunTestClass @FullName;
    END
    
    IF @IsTestCase = 1
    BEGIN
      DECLARE @SetupProcName NVARCHAR(MAX);
      EXEC tSQLt.Private_GetSetupProcedureName @TestClassId, @SetupProcName OUTPUT;

      EXEC tSQLt.Private_RunTest @FullName, @SetupProcName;
    END;

    EXEC tSQLt.Private_OutputTestResults @TestResultFormatter;
END;
GO


CREATE PROCEDURE tSQLt.Private_RunCursor
  @TestResultFormatter NVARCHAR(MAX),
  @GetCursorCallback NVARCHAR(MAX)
AS
BEGIN
  SET NOCOUNT ON;
  DECLARE @TestClassName NVARCHAR(MAX);
  DECLARE @TestProcName NVARCHAR(MAX);

  DECLARE @TestClassCursor CURSOR;
  EXEC @GetCursorCallback @TestClassCursor = @TestClassCursor OUT;
----  
  WHILE(1=1)
  BEGIN
    FETCH NEXT FROM @TestClassCursor INTO @TestClassName;
    IF(@@FETCH_STATUS&lt;&gt;0)BREAK;

    EXEC tSQLt.Private_RunTestClass @TestClassName;
    
  END;
  
  CLOSE @TestClassCursor;
  DEALLOCATE @TestClassCursor;
  
  EXEC tSQLt.Private_OutputTestResults @TestResultFormatter;
END;
GO

CREATE PROCEDURE tSQLt.Private_GetCursorForRunAll
  @TestClassCursor CURSOR VARYING OUTPUT
AS
BEGIN
  SET @TestClassCursor = CURSOR LOCAL FAST_FORWARD FOR
   SELECT Name
     FROM tSQLt.TestClasses;

  OPEN @TestClassCursor;
END;
GO

CREATE PROCEDURE tSQLt.Private_RunAll
  @TestResultFormatter NVARCHAR(MAX)
AS
BEGIN
  EXEC tSQLt.Private_RunCursor @TestResultFormatter = @TestResultFormatter, @GetCursorCallback = 'tSQLt.Private_GetCursorForRunAll';
END;
GO

CREATE PROCEDURE tSQLt.Private_GetCursorForRunNew
  @TestClassCursor CURSOR VARYING OUTPUT
AS
BEGIN
  SET @TestClassCursor = CURSOR LOCAL FAST_FORWARD FOR
   SELECT TC.Name
     FROM tSQLt.TestClasses AS TC
     JOIN tSQLt.Private_NewTestClassList AS PNTCL
       ON PNTCL.ClassName = TC.Name;

  OPEN @TestClassCursor;
END;
GO

CREATE PROCEDURE tSQLt.Private_RunNew
  @TestResultFormatter NVARCHAR(MAX)
AS
BEGIN
  EXEC tSQLt.Private_RunCursor @TestResultFormatter = @TestResultFormatter, @GetCursorCallback = 'tSQLt.Private_GetCursorForRunNew';
END;
GO

CREATE PROCEDURE tSQLt.Private_RunMethodHandler
  @RunMethod NVARCHAR(MAX),
  @TestResultFormatter NVARCHAR(MAX) = NULL,
  @TestName NVARCHAR(MAX) = NULL
AS
BEGIN
  SELECT @TestResultFormatter = ISNULL(@TestResultFormatter,tSQLt.GetTestResultFormatter());

  EXEC tSQLt.Private_Init;
  IF(@@ERROR = 0)
  BEGIN  
    IF(EXISTS(SELECT * FROM sys.parameters AS P WHERE P.object_id = OBJECT_ID(@RunMethod) AND name = '@TestName'))
    BEGIN
      EXEC @RunMethod @TestName = @TestName, @TestResultFormatter = @TestResultFormatter;
    END;
    ELSE
    BEGIN  
      EXEC @RunMethod @TestResultFormatter = @TestResultFormatter;
    END;
  END;
END;
GO

--------------------------------------------------------------------------------

GO
CREATE PROCEDURE tSQLt.RunAll
AS
BEGIN
  EXEC tSQLt.Private_RunMethodHandler @RunMethod = 'tSQLt.Private_RunAll';
END;
GO

CREATE PROCEDURE tSQLt.RunNew
AS
BEGIN
  EXEC tSQLt.Private_RunMethodHandler @RunMethod = 'tSQLt.Private_RunNew';
END;
GO

CREATE PROCEDURE tSQLt.RunTest
   @TestName NVARCHAR(MAX)
AS
BEGIN
  RAISERROR('tSQLt.RunTest has been retired. Please use tSQLt.Run instead.', 16, 10);
END;
GO

CREATE PROCEDURE tSQLt.Run
   @TestName NVARCHAR(MAX) = NULL,
   @TestResultFormatter NVARCHAR(MAX) = NULL
AS
BEGIN
  EXEC tSQLt.Private_RunMethodHandler @RunMethod = 'tSQLt.Private_Run', @TestResultFormatter = @TestResultFormatter, @TestName = @TestName; 
END;
GO
CREATE PROCEDURE tSQLt.Private_InputBuffer
  @InputBuffer NVARCHAR(MAX) OUTPUT
AS
BEGIN
  CREATE TABLE #inputbuffer(EventType SYSNAME, Parameters SMALLINT, EventInfo NVARCHAR(MAX));
  INSERT INTO #inputbuffer
  EXEC('DBCC INPUTBUFFER(@@SPID) WITH NO_INFOMSGS;');
  SELECT @InputBuffer = I.EventInfo FROM #inputbuffer AS I;
END;
GO
CREATE PROCEDURE tSQLt.RunC
AS
BEGIN
  DECLARE @TestName NVARCHAR(MAX);SET @TestName = NULL;
  DECLARE @InputBuffer NVARCHAR(MAX);
  EXEC tSQLt.Private_InputBuffer @InputBuffer = @InputBuffer OUT;
  IF(@InputBuffer LIKE 'EXEC tSQLt.RunC;--%')
  BEGIN
    SET @TestName = LTRIM(RTRIM(STUFF(@InputBuffer,1,18,'')));
  END;
  EXEC tSQLt.Run @TestName = @TestName;
END;
GO

CREATE PROCEDURE tSQLt.RunWithXmlResults
   @TestName NVARCHAR(MAX) = NULL
AS
BEGIN
  EXEC tSQLt.Run @TestName = @TestName, @TestResultFormatter = 'tSQLt.XmlResultFormatter';
END;
GO

CREATE PROCEDURE tSQLt.RunWithNullResults
    @TestName NVARCHAR(MAX) = NULL
AS
BEGIN
  EXEC tSQLt.Run @TestName = @TestName, @TestResultFormatter = 'tSQLt.NullTestResultFormatter';
END;
GO

CREATE PROCEDURE tSQLt.DefaultResultFormatter
AS
BEGIN
    DECLARE @Msg1 NVARCHAR(MAX);
    DECLARE @Msg2 NVARCHAR(MAX);
    DECLARE @Msg3 NVARCHAR(MAX);
    DECLARE @Msg4 NVARCHAR(MAX);
    DECLARE @IsSuccess INT;
    DECLARE @SuccessCnt INT;
    DECLARE @Severity INT;
    
    SELECT ROW_NUMBER() OVER(ORDER BY Result DESC, Name ASC) No,Name [Test Case Name],
           RIGHT(SPACE(7)+CAST(DATEDIFF(MILLISECOND,TestStartTime,TestEndTime) AS VARCHAR(7)),7) AS [Dur(ms)], Result
      INTO #TestResultOutput
      FROM tSQLt.TestResult;
    
    EXEC tSQLt.TableToText @Msg1 OUTPUT, '#TestResultOutput', 'No';

    SELECT @Msg3 = Msg, 
           @IsSuccess = 1 - SIGN(FailCnt + ErrorCnt),
           @SuccessCnt = SuccessCnt
      FROM tSQLt.TestCaseSummary();
      
    SELECT @Severity = 16*(1-@IsSuccess);
    
    SELECT @Msg2 = REPLICATE('-',LEN(@Msg3)),
           @Msg4 = CHAR(13)+CHAR(10);
    
    
    EXEC tSQLt.Private_Print @Msg4,0;
    EXEC tSQLt.Private_Print '+----------------------+',0;
    EXEC tSQLt.Private_Print '|Test Execution Summary|',0;
    EXEC tSQLt.Private_Print '+----------------------+',0;
    EXEC tSQLt.Private_Print @Msg4,0;
    EXEC tSQLt.Private_Print @Msg1,0;
    EXEC tSQLt.Private_Print @Msg2,0;
    EXEC tSQLt.Private_Print @Msg3, @Severity;
    EXEC tSQLt.Private_Print @Msg2,0;
END;
GO

CREATE PROCEDURE tSQLt.XmlResultFormatter
AS
BEGIN
    DECLARE @XmlOutput XML;

    SELECT @XmlOutput = (
      SELECT *--Tag, Parent, [testsuites!1!hide!hide], [testsuite!2!name], [testsuite!2!tests], [testsuite!2!errors], [testsuite!2!failures], [testsuite!2!timestamp], [testsuite!2!time], [testcase!3!classname], [testcase!3!name], [testcase!3!time], [failure!4!message]  
        FROM (
          SELECT 1 AS Tag,
                 NULL AS Parent,
                 'root' AS [testsuites!1!hide!hide],
                 NULL AS [testsuite!2!id],
                 NULL AS [testsuite!2!name],
                 NULL AS [testsuite!2!tests],
                 NULL AS [testsuite!2!errors],
                 NULL AS [testsuite!2!failures],
                 NULL AS [testsuite!2!timestamp],
                 NULL AS [testsuite!2!time],
                 NULL AS [testsuite!2!hostname],
                 NULL AS [testsuite!2!package],
                 NULL AS [properties!3!hide!hide],
                 NULL AS [testcase!4!classname],
                 NULL AS [testcase!4!name],
                 NULL AS [testcase!4!time],
                 NULL AS [failure!5!message],
                 NULL AS [failure!5!type],
                 NULL AS [error!6!message],
                 NULL AS [error!6!type],
                 NULL AS [system-out!7!hide],
                 NULL AS [system-err!8!hide]
          UNION ALL
          SELECT 2 AS Tag, 
                 1 AS Parent,
                 'root',
                 ROW_NUMBER()OVER(ORDER BY Class),
                 Class,
                 COUNT(1),
                 SUM(CASE Result WHEN 'Error' THEN 1 ELSE 0 END),
                 SUM(CASE Result WHEN 'Failure' THEN 1 ELSE 0 END),
                 CONVERT(VARCHAR(19),MIN(TestResult.TestStartTime),126),
                 CAST(CAST(DATEDIFF(MILLISECOND,MIN(TestResult.TestStartTime),MAX(TestResult.TestEndTime))/1000.0 AS NUMERIC(20,3))AS VARCHAR(MAX)),
                 CAST(SERVERPROPERTY('ServerName') AS NVARCHAR(MAX)),
                 'tSQLt',
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL
            FROM tSQLt.TestResult
          GROUP BY Class
          UNION ALL
          SELECT 3 AS Tag,
                 2 AS Parent,
                 'root',
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL
            FROM tSQLt.TestResult
           GROUP BY Class
          UNION ALL
          SELECT 4 AS Tag,
                 2 AS Parent,
                 'root',
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 Class,
                 TestCase,
                 CAST(CAST(DATEDIFF(MILLISECOND,TestResult.TestStartTime,TestResult.TestEndTime)/1000.0 AS NUMERIC(20,3))AS VARCHAR(MAX)),
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL
            FROM tSQLt.TestResult
          UNION ALL
          SELECT 5 AS Tag,
                 4 AS Parent,
                 'root',
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 Class,
                 TestCase,
                 CAST(CAST(DATEDIFF(MILLISECOND,TestResult.TestStartTime,TestResult.TestEndTime)/1000.0 AS NUMERIC(20,3))AS VARCHAR(MAX)),
                 Msg,
                 'tSQLt.Fail',
                 NULL,
                 NULL,
                 NULL,
                 NULL
            FROM tSQLt.TestResult
           WHERE Result IN ('Failure')
          UNION ALL
          SELECT 6 AS Tag,
                 4 AS Parent,
                 'root',
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 Class,
                 TestCase,
                 CAST(CAST(DATEDIFF(MILLISECOND,TestResult.TestStartTime,TestResult.TestEndTime)/1000.0 AS NUMERIC(20,3))AS VARCHAR(MAX)),
                 NULL,
                 NULL,
                 Msg,
                 'SQL Error',
                 NULL,
                 NULL
            FROM tSQLt.TestResult
           WHERE Result IN ( 'Error')
          UNION ALL
          SELECT 7 AS Tag,
                 2 AS Parent,
                 'root',
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL
            FROM tSQLt.TestResult
           GROUP BY Class
          UNION ALL
          SELECT 8 AS Tag,
                 2 AS Parent,
                 'root',
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 Class,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL,
                 NULL
            FROM tSQLt.TestResult
           GROUP BY Class
        ) AS X
       ORDER BY [testsuite!2!name],CASE WHEN Tag IN (7,8) THEN 1 ELSE 0 END, [testcase!4!name], Tag
       FOR XML EXPLICIT
       );

    EXEC tSQLt.Private_PrintXML @XmlOutput;
END;
GO

CREATE PROCEDURE tSQLt.NullTestResultFormatter
AS
BEGIN
  RETURN 0;
END;
GO

CREATE PROCEDURE tSQLt.RunTestClass
   @TestClassName NVARCHAR(MAX)
AS
BEGIN
    EXEC tSQLt.Run @TestClassName;
END
GO    
--Build-


GO

CREATE PROCEDURE tSQLt.ExpectException
@ExpectedMessage NVARCHAR(MAX) = NULL,
@ExpectedSeverity INT = NULL,
@ExpectedState INT = NULL,
@Message NVARCHAR(MAX) = NULL,
@ExpectedMessagePattern NVARCHAR(MAX) = NULL,
@ExpectedErrorNumber INT = NULL
AS
BEGIN
 IF(EXISTS(SELECT 1 FROM #ExpectException WHERE ExpectException = 1))
 BEGIN
   DELETE #ExpectException;
   RAISERROR('Each test can only contain one call to tSQLt.ExpectException.',16,10);
 END;
 
 INSERT INTO #ExpectException(ExpectException, ExpectedMessage, ExpectedSeverity, ExpectedState, ExpectedMessagePattern, ExpectedErrorNumber, FailMessage)
 VALUES(1, @ExpectedMessage, @ExpectedSeverity, @ExpectedState, @ExpectedMessagePattern, @ExpectedErrorNumber, @Message);
END;


GO

CREATE PROCEDURE tSQLt.ExpectNoException
  @Message NVARCHAR(MAX) = NULL
AS
BEGIN
 IF(EXISTS(SELECT 1 FROM #ExpectException WHERE ExpectException = 0))
 BEGIN
   DELETE #ExpectException;
   RAISERROR('Each test can only contain one call to tSQLt.ExpectNoException.',16,10);
 END;
 IF(EXISTS(SELECT 1 FROM #ExpectException WHERE ExpectException = 1))
 BEGIN
   DELETE #ExpectException;
   RAISERROR('tSQLt.ExpectNoException cannot follow tSQLt.ExpectException inside a single test.',16,10);
 END;
 
 INSERT INTO #ExpectException(ExpectException, FailMessage)
 VALUES(0, @Message);
END;


GO

GO
CREATE FUNCTION tSQLt.Private_SqlVersion()
RETURNS TABLE
AS
RETURN
  SELECT CAST(SERVERPROPERTY('ProductVersion')AS NVARCHAR(128)) ProductVersion,
         CAST(SERVERPROPERTY('Edition')AS NVARCHAR(128)) Edition;
GO


GO

CREATE FUNCTION tSQLt.Info()
RETURNS TABLE
AS
RETURN
SELECT Version = '1.0.5873.27393',
       ClrVersion = (SELECT tSQLt.Private::Info()),
       ClrSigningKey = (SELECT tSQLt.Private::SigningKey()),
       V.SqlVersion,
       V.SqlBuild,
       V.SqlEdition
  FROM
  (
    SELECT CAST(VI.major+'.'+VI.minor AS NUMERIC(10,2)) AS SqlVersion,
           CAST(VI.build+'.'+VI.revision AS NUMERIC(10,2)) AS SqlBuild,
           SqlEdition
      FROM
      (
        SELECT PARSENAME(PSV.ProductVersion,4) major,
               PARSENAME(PSV.ProductVersion,3) minor, 
               PARSENAME(PSV.ProductVersion,2) build,
               PARSENAME(PSV.ProductVersion,1) revision,
               Edition AS SqlEdition
          FROM tSQLt.Private_SqlVersion() AS PSV
      )VI
  )V;


GO

IF((SELECT SqlVersion FROM tSQLt.Info())&gt;9)
BEGIN
  EXEC('CREATE VIEW tSQLt.Private_SysIndexes AS SELECT * FROM sys.indexes;');
END
ELSE
BEGIN
  EXEC('CREATE VIEW tSQLt.Private_SysIndexes AS SELECT *,0 AS has_filter,'''' AS filter_definition FROM sys.indexes;');
END;


GO

GO
CREATE FUNCTION tSQLt.Private_ScriptIndex
(
  @object_id INT,
  @index_id INT
)
RETURNS TABLE
AS
RETURN
  SELECT I.index_id,
         I.name AS index_name,
         I.is_primary_key,
         I.is_unique,
         I.is_disabled,
         'CREATE ' +
         CASE WHEN I.is_unique = 1 THEN 'UNIQUE ' ELSE '' END +
         CASE I.type
           WHEN 1 THEN 'CLUSTERED'
           WHEN 2 THEN 'NONCLUSTERED'
           WHEN 5 THEN 'CLUSTERED COLUMNSTORE'
           WHEN 6 THEN 'NONCLUSTERED COLUMNSTORE'
           ELSE '{Index Type Not Supported!}' 
         END +
         ' INDEX ' +
         QUOTENAME(I.name)+
         ' ON ' + QUOTENAME(OBJECT_SCHEMA_NAME(@object_id)) + '.' + QUOTENAME(OBJECT_NAME(@object_id)) +
         CASE WHEN I.type NOT IN (5)
           THEN
             '('+ 
             CL.column_list +
             ')'
           ELSE ''
         END +
         CASE WHEN I.has_filter = 1
           THEN 'WHERE' + I.filter_definition
           ELSE ''
         END +
         CASE WHEN I.is_hypothetical = 1
           THEN 'WITH(STATISTICS_ONLY = -1)'
           ELSE ''
         END +
         ';' AS create_cmd
    FROM tSQLt.Private_SysIndexes AS I
   CROSS APPLY
   (
     SELECT
      (
        SELECT 
          CASE WHEN OIC.rn &gt; 1 THEN ',' ELSE '' END +
          CASE WHEN OIC.rn = 1 AND OIC.is_included_column = 1 AND I.type NOT IN (6) THEN ')INCLUDE(' ELSE '' END +
          QUOTENAME(OIC.name) +
          CASE WHEN OIC.is_included_column = 0
            THEN CASE WHEN OIC.is_descending_key = 1 THEN 'DESC' ELSE 'ASC' END
            ELSE ''
          END
          FROM
          (
            SELECT C.name,
                   IC.is_descending_key, 
                   IC.key_ordinal,
                   IC.is_included_column,
                   ROW_NUMBER()OVER(PARTITION BY IC.is_included_column ORDER BY IC.key_ordinal, IC.index_column_id) AS rn
              FROM sys.index_columns AS IC
              JOIN sys.columns AS C
                ON IC.column_id = C.column_id
               AND IC.object_id = C.object_id
             WHERE IC.object_id = I.object_id
               AND IC.index_id = I.index_id
          )OIC
         ORDER BY OIC.is_included_column, OIC.rn
           FOR XML PATH(''),TYPE
      ).value('.','NVARCHAR(MAX)') AS column_list
   )CL
   WHERE I.object_id = @object_id
     AND I.index_id = ISNULL(@index_id,I.index_id);
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_RemoveSchemaBinding
  @object_id INT
AS
BEGIN
  DECLARE @cmd NVARCHAR(MAX);
  SELECT @cmd = tSQLt.[Private]::GetAlterStatementWithoutSchemaBinding(SM.definition)
    FROM sys.sql_modules AS SM
   WHERE SM.object_id = @object_id;
   EXEC(@cmd);
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_RemoveSchemaBoundReferences
  @object_id INT
AS
BEGIN
  DECLARE @cmd NVARCHAR(MAX);
  SELECT @cmd = 
  (
    SELECT 
      'EXEC tSQLt.Private_RemoveSchemaBoundReferences @object_id = '+STR(SED.referencing_id)+';'+
      'EXEC tSQLt.Private_RemoveSchemaBinding @object_id = '+STR(SED.referencing_id)+';'
      FROM
      (
        SELECT DISTINCT SEDI.referencing_id,SEDI.referenced_id 
          FROM sys.sql_expression_dependencies AS SEDI
         WHERE SEDI.is_schema_bound_reference = 1
      ) AS SED 
     WHERE SED.referenced_id = @object_id
       FOR XML PATH(''),TYPE
  ).value('.','NVARCHAR(MAX)');
  EXEC(@cmd);
END;
GO


GO

GO
CREATE FUNCTION tSQLt.Private_GetForeignKeyParColumns(
    @ConstraintObjectId INT
)
RETURNS TABLE
AS
RETURN SELECT STUFF((
                 SELECT ','+QUOTENAME(pci.name) FROM sys.foreign_key_columns c
                   JOIN sys.columns pci
                   ON pci.object_id = c.parent_object_id
                  AND pci.column_id = c.parent_column_id
                   WHERE @ConstraintObjectId = c.constraint_object_id
                   FOR XML PATH(''),TYPE
                   ).value('.','NVARCHAR(MAX)'),1,1,'')  AS ColNames
GO

CREATE FUNCTION tSQLt.Private_GetForeignKeyRefColumns(
    @ConstraintObjectId INT
)
RETURNS TABLE
AS
RETURN SELECT STUFF((
                 SELECT ','+QUOTENAME(rci.name) FROM sys.foreign_key_columns c
                   JOIN sys.columns rci
                  ON rci.object_id = c.referenced_object_id
                  AND rci.column_id = c.referenced_column_id
                   WHERE @ConstraintObjectId = c.constraint_object_id
                   FOR XML PATH(''),TYPE
                   ).value('.','NVARCHAR(MAX)'),1,1,'')  AS ColNames;
GO

CREATE FUNCTION tSQLt.Private_GetForeignKeyDefinition(
    @SchemaName NVARCHAR(MAX),
    @ParentTableName NVARCHAR(MAX),
    @ForeignKeyName NVARCHAR(MAX),
    @NoCascade BIT
)
RETURNS TABLE
AS
RETURN SELECT 'CONSTRAINT ' + name + ' FOREIGN KEY (' +
              parCols + ') REFERENCES ' + refName + '(' + refCols + ')'+
              CASE WHEN @NoCascade = 1 THEN ''
                ELSE delete_referential_action_cmd + ' ' + update_referential_action_cmd 
              END AS cmd,
              CASE 
                WHEN RefTableIsFakedInd = 1
                  THEN 'CREATE UNIQUE INDEX ' + tSQLt.Private::CreateUniqueObjectName() + ' ON ' + refName + '(' + refCols + ');' 
                ELSE '' 
              END CreIdxCmd
         FROM (SELECT QUOTENAME(SCHEMA_NAME(k.schema_id)) AS SchemaName,
                      QUOTENAME(k.name) AS name,
                      QUOTENAME(OBJECT_NAME(k.parent_object_id)) AS parName,
                      QUOTENAME(SCHEMA_NAME(refTab.schema_id)) + '.' + QUOTENAME(refTab.name) AS refName,
                      parCol.ColNames AS parCols,
                      refCol.ColNames AS refCols,
                      'ON UPDATE '+
                      CASE k.update_referential_action
                        WHEN 0 THEN 'NO ACTION'
                        WHEN 1 THEN 'CASCADE'
                        WHEN 2 THEN 'SET NULL'
                        WHEN 3 THEN 'SET DEFAULT'
                      END AS update_referential_action_cmd,
                      'ON DELETE '+
                      CASE k.delete_referential_action
                        WHEN 0 THEN 'NO ACTION'
                        WHEN 1 THEN 'CASCADE'
                        WHEN 2 THEN 'SET NULL'
                        WHEN 3 THEN 'SET DEFAULT'
                      END AS delete_referential_action_cmd,
                      CASE WHEN e.name IS NULL THEN 0
                           ELSE 1 
                       END AS RefTableIsFakedInd
                 FROM sys.foreign_keys k
                 CROSS APPLY tSQLt.Private_GetForeignKeyParColumns(k.object_id) AS parCol
                 CROSS APPLY tSQLt.Private_GetForeignKeyRefColumns(k.object_id) AS refCol
                 LEFT JOIN sys.extended_properties e
                   ON e.name = 'tSQLt.FakeTable_OrgTableName'
                  AND e.value = OBJECT_NAME(k.referenced_object_id)
                 JOIN sys.tables refTab
                   ON COALESCE(e.major_id,k.referenced_object_id) = refTab.object_id
                WHERE k.parent_object_id = OBJECT_ID(@SchemaName + '.' + @ParentTableName)
                  AND k.object_id = OBJECT_ID(@SchemaName + '.' + @ForeignKeyName)
               )x;
GO


GO

GO
CREATE FUNCTION tSQLt.Private_GetQuotedTableNameForConstraint(@ConstraintObjectId INT)
RETURNS TABLE
AS
RETURN
  SELECT QUOTENAME(SCHEMA_NAME(newtbl.schema_id)) + '.' + QUOTENAME(OBJECT_NAME(newtbl.object_id)) QuotedTableName,
         SCHEMA_NAME(newtbl.schema_id) SchemaName,
         OBJECT_NAME(newtbl.object_id) TableName,
         OBJECT_NAME(constraints.parent_object_id) OrgTableName
      FROM sys.objects AS constraints
      JOIN sys.extended_properties AS p
      JOIN sys.objects AS newtbl
        ON newtbl.object_id = p.major_id
       AND p.minor_id = 0
       AND p.class_desc = 'OBJECT_OR_COLUMN'
       AND p.name = 'tSQLt.FakeTable_OrgTableName'
        ON OBJECT_NAME(constraints.parent_object_id) = CAST(p.value AS NVARCHAR(4000))
       AND constraints.schema_id = newtbl.schema_id
       AND constraints.object_id = @ConstraintObjectId;
GO

CREATE FUNCTION tSQLt.Private_FindConstraint
(
  @TableObjectId INT,
  @ConstraintName NVARCHAR(MAX)
)
RETURNS TABLE
AS
RETURN
  SELECT TOP(1) constraints.object_id AS ConstraintObjectId, type_desc AS ConstraintType
    FROM sys.objects constraints
    CROSS JOIN tSQLt.Private_GetOriginalTableInfo(@TableObjectId) orgTbl
   WHERE @ConstraintName IN (constraints.name, QUOTENAME(constraints.name))
     AND constraints.parent_object_id = orgTbl.OrgTableObjectId
   ORDER BY LEN(constraints.name) ASC;
GO

CREATE FUNCTION tSQLt.Private_ResolveApplyConstraintParameters
(
  @A NVARCHAR(MAX),
  @B NVARCHAR(MAX),
  @C NVARCHAR(MAX)
)
RETURNS TABLE
AS 
RETURN
  SELECT ConstraintObjectId, ConstraintType
    FROM tSQLt.Private_FindConstraint(OBJECT_ID(@A), @B)
   WHERE @C IS NULL
   UNION ALL
  SELECT *
    FROM tSQLt.Private_FindConstraint(OBJECT_ID(@A + '.' + @B), @C)
   UNION ALL
  SELECT *
    FROM tSQLt.Private_FindConstraint(OBJECT_ID(@C + '.' + @A), @B);
GO

CREATE PROCEDURE tSQLt.Private_ApplyCheckConstraint
  @ConstraintObjectId INT
AS
BEGIN
  DECLARE @Cmd NVARCHAR(MAX);
  SELECT @Cmd = 'CONSTRAINT ' + QUOTENAME(name) + ' CHECK' + definition 
    FROM sys.check_constraints
   WHERE object_id = @ConstraintObjectId;
  
  DECLARE @QuotedTableName NVARCHAR(MAX);
  
  SELECT @QuotedTableName = QuotedTableName FROM tSQLt.Private_GetQuotedTableNameForConstraint(@ConstraintObjectId);

  EXEC tSQLt.Private_RenameObjectToUniqueNameUsingObjectId @ConstraintObjectId;
  SELECT @Cmd = 'ALTER TABLE ' + @QuotedTableName + ' ADD ' + @Cmd
    FROM sys.objects 
   WHERE object_id = @ConstraintObjectId;

  EXEC (@Cmd);

END; 
GO

CREATE PROCEDURE tSQLt.Private_ApplyForeignKeyConstraint 
  @ConstraintObjectId INT,
  @NoCascade BIT
AS
BEGIN
  DECLARE @SchemaName NVARCHAR(MAX);
  DECLARE @OrgTableName NVARCHAR(MAX);
  DECLARE @TableName NVARCHAR(MAX);
  DECLARE @ConstraintName NVARCHAR(MAX);
  DECLARE @CreateFkCmd NVARCHAR(MAX);
  DECLARE @AlterTableCmd NVARCHAR(MAX);
  DECLARE @CreateIndexCmd NVARCHAR(MAX);
  DECLARE @FinalCmd NVARCHAR(MAX);
  
  SELECT @SchemaName = SchemaName,
         @OrgTableName = OrgTableName,
         @TableName = TableName,
         @ConstraintName = OBJECT_NAME(@ConstraintObjectId)
    FROM tSQLt.Private_GetQuotedTableNameForConstraint(@ConstraintObjectId);
      
  SELECT @CreateFkCmd = cmd, @CreateIndexCmd = CreIdxCmd
    FROM tSQLt.Private_GetForeignKeyDefinition(@SchemaName, @OrgTableName, @ConstraintName, @NoCascade);
  SELECT @AlterTableCmd = 'ALTER TABLE ' + QUOTENAME(@SchemaName) + '.' + QUOTENAME(@TableName) + 
                          ' ADD ' + @CreateFkCmd;
  SELECT @FinalCmd = @CreateIndexCmd + @AlterTableCmd;

  EXEC tSQLt.Private_RenameObjectToUniqueName @SchemaName, @ConstraintName;
  EXEC (@FinalCmd);
END;
GO

CREATE PROCEDURE tSQLt.Private_ApplyUniqueConstraint 
  @ConstraintObjectId INT
AS
BEGIN
  DECLARE @SchemaName NVARCHAR(MAX);
  DECLARE @OrgTableName NVARCHAR(MAX);
  DECLARE @TableName NVARCHAR(MAX);
  DECLARE @ConstraintName NVARCHAR(MAX);
  DECLARE @CreateConstraintCmd NVARCHAR(MAX);
  DECLARE @AlterColumnsCmd NVARCHAR(MAX);
  
  SELECT @SchemaName = SchemaName,
         @OrgTableName = OrgTableName,
         @TableName = TableName,
         @ConstraintName = OBJECT_NAME(@ConstraintObjectId)
    FROM tSQLt.Private_GetQuotedTableNameForConstraint(@ConstraintObjectId);
      
  SELECT @AlterColumnsCmd = NotNullColumnCmd,
         @CreateConstraintCmd = CreateConstraintCmd
    FROM tSQLt.Private_GetUniqueConstraintDefinition(@ConstraintObjectId, QUOTENAME(@SchemaName) + '.' + QUOTENAME(@TableName));

  EXEC tSQLt.Private_RenameObjectToUniqueName @SchemaName, @ConstraintName;
  EXEC (@AlterColumnsCmd);
  EXEC (@CreateConstraintCmd);
END;
GO

CREATE FUNCTION tSQLt.Private_GetConstraintType(@TableObjectId INT, @ConstraintName NVARCHAR(MAX))
RETURNS TABLE
AS
RETURN
  SELECT object_id,type,type_desc
    FROM sys.objects 
   WHERE object_id = OBJECT_ID(SCHEMA_NAME(schema_id)+'.'+@ConstraintName)
     AND parent_object_id = @TableObjectId;
GO

CREATE PROCEDURE tSQLt.ApplyConstraint
       @TableName NVARCHAR(MAX),
       @ConstraintName NVARCHAR(MAX),
       @SchemaName NVARCHAR(MAX) = NULL, --parameter preserved for backward compatibility. Do not use. Will be removed soon.
       @NoCascade BIT = 0
AS
BEGIN
  DECLARE @ConstraintType NVARCHAR(MAX);
  DECLARE @ConstraintObjectId INT;
  
  SELECT @ConstraintType = ConstraintType, @ConstraintObjectId = ConstraintObjectId
    FROM tSQLt.Private_ResolveApplyConstraintParameters (@TableName, @ConstraintName, @SchemaName);

  IF @ConstraintType = 'CHECK_CONSTRAINT'
  BEGIN
    EXEC tSQLt.Private_ApplyCheckConstraint @ConstraintObjectId;
    RETURN 0;
  END

  IF @ConstraintType = 'FOREIGN_KEY_CONSTRAINT'
  BEGIN
    EXEC tSQLt.Private_ApplyForeignKeyConstraint @ConstraintObjectId, @NoCascade;
    RETURN 0;
  END;  
   
  IF @ConstraintType IN('UNIQUE_CONSTRAINT', 'PRIMARY_KEY_CONSTRAINT')
  BEGIN
    EXEC tSQLt.Private_ApplyUniqueConstraint @ConstraintObjectId;
    RETURN 0;
  END;  
   
  RAISERROR ('ApplyConstraint could not resolve the object names, ''%s'', ''%s''. Be sure to call ApplyConstraint and pass in two parameters, such as: EXEC tSQLt.ApplyConstraint ''MySchema.MyTable'', ''MyConstraint''', 
             16, 10, @TableName, @ConstraintName);
  RETURN 0;
END;
GO


GO

CREATE PROCEDURE tSQLt.Private_ValidateFakeTableParameters
  @SchemaName NVARCHAR(MAX),
  @OrigTableName NVARCHAR(MAX),
  @OrigSchemaName NVARCHAR(MAX)
AS
BEGIN
   IF @SchemaName IS NULL
   BEGIN
        DECLARE @FullName NVARCHAR(MAX); SET @FullName = @OrigTableName + COALESCE('.' + @OrigSchemaName, '');
        
        RAISERROR ('FakeTable could not resolve the object name, ''%s''. (When calling tSQLt.FakeTable, avoid the use of the @SchemaName parameter, as it is deprecated.)', 
                   16, 10, @FullName);
   END;
END;


GO

GO
CREATE FUNCTION tSQLt.Private_GetDataTypeOrComputedColumnDefinition(@UserTypeId INT, @MaxLength INT, @Precision INT, @Scale INT, @CollationName NVARCHAR(MAX), @ObjectId INT, @ColumnId INT, @ReturnDetails BIT)
RETURNS TABLE
AS
RETURN SELECT 
              COALESCE(cc.IsComputedColumn, 0) AS IsComputedColumn,
              COALESCE(cc.ComputedColumnDefinition, GFTN.TypeName) AS ColumnDefinition
        FROM (SELECT @UserTypeId, @MaxLength, @Precision, @Scale, @CollationName, @ObjectId, @ColumnId, @ReturnDetails) 
             AS V(UserTypeId, MaxLength, Precision, Scale, CollationName, ObjectId, ColumnId, ReturnDetails)
       CROSS APPLY tSQLt.Private_GetFullTypeName(V.UserTypeId, V.MaxLength, V.Precision, V.Scale, V.CollationName) AS GFTN
        LEFT JOIN (SELECT 1 AS IsComputedColumn,
                          ' AS '+ cci.definition + CASE WHEN cci.is_persisted = 1 THEN ' PERSISTED' ELSE '' END AS ComputedColumnDefinition,
                          cci.object_id,
                          cci.column_id
                     FROM sys.computed_columns cci
                  )cc
               ON cc.object_id = V.ObjectId
              AND cc.column_id = V.ColumnId
              AND V.ReturnDetails = 1;               


GO

CREATE FUNCTION tSQLt.Private_GetIdentityDefinition(@ObjectId INT, @ColumnId INT, @ReturnDetails BIT)
RETURNS TABLE
AS
RETURN SELECT 
              COALESCE(IsIdentity, 0) AS IsIdentityColumn,
              COALESCE(IdentityDefinition, '') AS IdentityDefinition
        FROM (SELECT 1) X(X)
        LEFT JOIN (SELECT 1 AS IsIdentity,
                          ' IDENTITY(' + CAST(seed_value AS NVARCHAR(MAX)) + ',' + CAST(increment_value AS NVARCHAR(MAX)) + ')' AS IdentityDefinition, 
                          object_id, 
                          column_id
                     FROM sys.identity_columns
                  ) AS id
               ON id.object_id = @ObjectId
              AND id.column_id = @ColumnId
              AND @ReturnDetails = 1;               


GO

GO
CREATE FUNCTION tSQLt.Private_GetDefaultConstraintDefinition(@ObjectId INT, @ColumnId INT, @ReturnDetails BIT)
RETURNS TABLE
AS
RETURN SELECT 
              COALESCE(IsDefault, 0) AS IsDefault,
              COALESCE(DefaultDefinition, '') AS DefaultDefinition
        FROM (SELECT 1) X(X)
        LEFT JOIN (SELECT 1 AS IsDefault,' DEFAULT '+ definition AS DefaultDefinition,parent_object_id,parent_column_id
                     FROM sys.default_constraints
                  )dc
               ON dc.parent_object_id = @ObjectId
              AND dc.parent_column_id = @ColumnId
              AND @ReturnDetails = 1;               


GO

GO
CREATE FUNCTION tSQLt.Private_GetUniqueConstraintDefinition
(
    @ConstraintObjectId INT,
    @QuotedTableName NVARCHAR(MAX)
)
RETURNS TABLE
AS
RETURN
  SELECT 'ALTER TABLE '+
         @QuotedTableName +
         ' ADD CONSTRAINT ' +
         QUOTENAME(OBJECT_NAME(@ConstraintObjectId)) +
         ' ' +
         CASE WHEN KC.type_desc = 'UNIQUE_CONSTRAINT' 
              THEN 'UNIQUE'
              ELSE 'PRIMARY KEY'
           END +
         '(' +
         STUFF((
                 SELECT ','+QUOTENAME(C.name)
                   FROM sys.index_columns AS IC
                   JOIN sys.columns AS C
                     ON IC.object_id = C.object_id
                    AND IC.column_id = C.column_id
                  WHERE KC.unique_index_id = IC.index_id
                    AND KC.parent_object_id = IC.object_id
                    FOR XML PATH(''),TYPE
               ).value('.','NVARCHAR(MAX)'),
               1,
               1,
               ''
              ) +
         ');' AS CreateConstraintCmd,
         CASE WHEN KC.type_desc = 'UNIQUE_CONSTRAINT' 
              THEN ''
              ELSE (
                     SELECT 'ALTER TABLE ' +
                            @QuotedTableName +
                            ' ALTER COLUMN ' +
                            QUOTENAME(C.name)+
                            cc.ColumnDefinition +
                            ' NOT NULL;'
                       FROM sys.index_columns AS IC
                       JOIN sys.columns AS C
                         ON IC.object_id = C.object_id
                        AND IC.column_id = C.column_id
                      CROSS APPLY tSQLt.Private_GetDataTypeOrComputedColumnDefinition(C.user_type_id, C.max_length, C.precision, C.scale, C.collation_name, C.object_id, C.column_id, 0) cc
                      WHERE KC.unique_index_id = IC.index_id
                        AND KC.parent_object_id = IC.object_id
                        FOR XML PATH(''),TYPE
                   ).value('.','NVARCHAR(MAX)')
           END AS NotNullColumnCmd
    FROM sys.key_constraints AS KC
   WHERE KC.object_id = @ConstraintObjectId;
GO


GO

CREATE PROCEDURE tSQLt.Private_CreateFakeOfTable
  @SchemaName NVARCHAR(MAX),
  @TableName NVARCHAR(MAX),
  @OrigTableFullName NVARCHAR(MAX),
  @Identity BIT,
  @ComputedColumns BIT,
  @Defaults BIT
AS
BEGIN
   DECLARE @Cmd NVARCHAR(MAX);
   DECLARE @Cols NVARCHAR(MAX);
   
   SELECT @Cols = 
   (
    SELECT
       ',' +
       QUOTENAME(name) + 
       cc.ColumnDefinition +
       dc.DefaultDefinition + 
       id.IdentityDefinition +
       CASE WHEN cc.IsComputedColumn = 1 OR id.IsIdentityColumn = 1 
            THEN ''
            ELSE ' NULL'
       END
      FROM sys.columns c
     CROSS APPLY tSQLt.Private_GetDataTypeOrComputedColumnDefinition(c.user_type_id, c.max_length, c.precision, c.scale, c.collation_name, c.object_id, c.column_id, @ComputedColumns) cc
     CROSS APPLY tSQLt.Private_GetDefaultConstraintDefinition(c.object_id, c.column_id, @Defaults) AS dc
     CROSS APPLY tSQLt.Private_GetIdentityDefinition(c.object_id, c.column_id, @Identity) AS id
     WHERE object_id = OBJECT_ID(@OrigTableFullName)
     ORDER BY column_id
     FOR XML PATH(''), TYPE
    ).value('.', 'NVARCHAR(MAX)');
    
   SELECT @Cmd = 'CREATE TABLE ' + @SchemaName + '.' + @TableName + '(' + STUFF(@Cols,1,1,'') + ')';
   
   EXEC (@Cmd);
END;


GO

CREATE PROCEDURE tSQLt.Private_MarkFakeTable
  @SchemaName NVARCHAR(MAX),
  @TableName NVARCHAR(MAX),
  @NewNameOfOriginalTable NVARCHAR(4000)
AS
BEGIN
   DECLARE @UnquotedSchemaName NVARCHAR(MAX);SET @UnquotedSchemaName = OBJECT_SCHEMA_NAME(OBJECT_ID(@SchemaName+'.'+@TableName));
   DECLARE @UnquotedTableName NVARCHAR(MAX);SET @UnquotedTableName = OBJECT_NAME(OBJECT_ID(@SchemaName+'.'+@TableName));

   EXEC sys.sp_addextendedproperty 
      @name = N'tSQLt.FakeTable_OrgTableName', 
      @value = @NewNameOfOriginalTable, 
      @level0type = N'SCHEMA', @level0name = @UnquotedSchemaName, 
      @level1type = N'TABLE',  @level1name = @UnquotedTableName;
END;


GO

CREATE PROCEDURE tSQLt.FakeTable
    @TableName NVARCHAR(MAX),
    @SchemaName NVARCHAR(MAX) = NULL, --parameter preserved for backward compatibility. Do not use. Will be removed soon.
    @Identity BIT = NULL,
    @ComputedColumns BIT = NULL,
    @Defaults BIT = NULL
AS
BEGIN
   DECLARE @OrigSchemaName NVARCHAR(MAX);
   DECLARE @OrigTableName NVARCHAR(MAX);
   DECLARE @NewNameOfOriginalTable NVARCHAR(4000);
   DECLARE @OrigTableFullName NVARCHAR(MAX); SET @OrigTableFullName = NULL;
   
   SELECT @OrigSchemaName = @SchemaName,
          @OrigTableName = @TableName
   
   IF(@OrigTableName NOT IN (PARSENAME(@OrigTableName,1),QUOTENAME(PARSENAME(@OrigTableName,1)))
      AND @OrigSchemaName IS NOT NULL)
   BEGIN
     RAISERROR('When @TableName is a multi-part identifier, @SchemaName must be NULL!',16,10);
   END

   SELECT @SchemaName = CleanSchemaName,
          @TableName = CleanTableName
     FROM tSQLt.Private_ResolveFakeTableNamesForBackwardCompatibility(@TableName, @SchemaName);
   
   EXEC tSQLt.Private_ValidateFakeTableParameters @SchemaName,@OrigTableName,@OrigSchemaName;

   EXEC tSQLt.Private_RenameObjectToUniqueName @SchemaName, @TableName, @NewNameOfOriginalTable OUTPUT;

   SELECT @OrigTableFullName = S.base_object_name
     FROM sys.synonyms AS S 
    WHERE S.object_id = OBJECT_ID(@SchemaName + '.' + @NewNameOfOriginalTable);

   IF(@OrigTableFullName IS NOT NULL)
   BEGIN
     IF(COALESCE(OBJECT_ID(@OrigTableFullName,'U'),OBJECT_ID(@OrigTableFullName,'V')) IS NULL)
     BEGIN
       RAISERROR('Cannot fake synonym %s.%s as it is pointing to %s, which is not a table or view!',16,10,@SchemaName,@TableName,@OrigTableFullName);
     END;
   END;
   ELSE
   BEGIN
     SET @OrigTableFullName = @SchemaName + '.' + @NewNameOfOriginalTable;
   END;

   EXEC tSQLt.Private_CreateFakeOfTable @SchemaName, @TableName, @OrigTableFullName, @Identity, @ComputedColumns, @Defaults;

   EXEC tSQLt.Private_MarkFakeTable @SchemaName, @TableName, @NewNameOfOriginalTable;
END


GO

CREATE PROCEDURE tSQLt.Private_CreateProcedureSpy
    @ProcedureObjectId INT,
    @OriginalProcedureName NVARCHAR(MAX),
    @LogTableName NVARCHAR(MAX),
    @CommandToExecute NVARCHAR(MAX) = NULL
AS
BEGIN
    DECLARE @Cmd NVARCHAR(MAX);
    DECLARE @ProcParmList NVARCHAR(MAX),
            @TableColList NVARCHAR(MAX),
            @ProcParmTypeList NVARCHAR(MAX),
            @TableColTypeList NVARCHAR(MAX);
            
    DECLARE @Seperator CHAR(1),
            @ProcParmTypeListSeparater CHAR(1),
            @ParamName sysname,
            @TypeName sysname,
            @IsOutput BIT,
            @IsCursorRef BIT,
            @IsTableType BIT;
            

      
    SELECT @Seperator = '', @ProcParmTypeListSeparater = '', 
           @ProcParmList = '', @TableColList = '', @ProcParmTypeList = '', @TableColTypeList = '';
      
    DECLARE Parameters CURSOR FOR
     SELECT p.name, t.TypeName, p.is_output, p.is_cursor_ref, t.IsTableType
       FROM sys.parameters p
       CROSS APPLY tSQLt.Private_GetFullTypeName(p.user_type_id,p.max_length,p.precision,p.scale,NULL) t
      WHERE object_id = @ProcedureObjectId;
    
    OPEN Parameters;
    
    FETCH NEXT FROM Parameters INTO @ParamName, @TypeName, @IsOutput, @IsCursorRef, @IsTableType;
    WHILE (@@FETCH_STATUS = 0)
    BEGIN
        IF @IsCursorRef = 0
        BEGIN
            SELECT @ProcParmList = @ProcParmList + @Seperator + 
                                   CASE WHEN @IsTableType = 1 
                                     THEN '(SELECT * FROM '+@ParamName+' FOR XML PATH(''row''),TYPE,ROOT('''+STUFF(@ParamName,1,1,'')+'''))' 
                                     ELSE @ParamName 
                                   END, 
                   @TableColList = @TableColList + @Seperator + '[' + STUFF(@ParamName,1,1,'') + ']', 
                   @ProcParmTypeList = @ProcParmTypeList + @ProcParmTypeListSeparater + @ParamName + ' ' + @TypeName + 
                                       CASE WHEN @IsTableType = 1 THEN ' READONLY' ELSE ' = NULL ' END+ 
                                       CASE WHEN @IsOutput = 1 THEN ' OUT' ELSE '' END, 
                   @TableColTypeList = @TableColTypeList + ',[' + STUFF(@ParamName,1,1,'') + '] ' + 
                          CASE 
                               WHEN @IsTableType = 1
                               THEN 'XML'
                               WHEN @TypeName LIKE '%nchar%'
                                 OR @TypeName LIKE '%nvarchar%'
                               THEN 'NVARCHAR(MAX)'
                               WHEN @TypeName LIKE '%char%'
                               THEN 'VARCHAR(MAX)'
                               ELSE @TypeName
                          END + ' NULL';

            SELECT @Seperator = ',';        
            SELECT @ProcParmTypeListSeparater = ',';
        END
        ELSE
        BEGIN
            SELECT @ProcParmTypeList = @ProcParmTypeListSeparater + @ParamName + ' CURSOR VARYING OUTPUT';
            SELECT @ProcParmTypeListSeparater = ',';
        END;
        
        FETCH NEXT FROM Parameters INTO @ParamName, @TypeName, @IsOutput, @IsCursorRef, @IsTableType;
    END;
    
    CLOSE Parameters;
    DEALLOCATE Parameters;
    
    DECLARE @InsertStmt NVARCHAR(MAX);
    SELECT @InsertStmt = 'INSERT INTO ' + @LogTableName + 
                         CASE WHEN @TableColList = '' THEN ' DEFAULT VALUES'
                              ELSE ' (' + @TableColList + ') SELECT ' + @ProcParmList
                         END + ';';
                         
    SELECT @Cmd = 'CREATE TABLE ' + @LogTableName + ' (_id_ int IDENTITY(1,1) PRIMARY KEY CLUSTERED ' + @TableColTypeList + ');';
    EXEC(@Cmd);

    SELECT @Cmd = 'CREATE PROCEDURE ' + @OriginalProcedureName + ' ' + @ProcParmTypeList + 
                  ' AS BEGIN ' + 
                     @InsertStmt + 
                     ISNULL(@CommandToExecute, '') + ';' +
                  ' END;';
    EXEC(@Cmd);

    RETURN 0;
END;


GO

CREATE PROCEDURE tSQLt.SpyProcedure
    @ProcedureName NVARCHAR(MAX),
    @CommandToExecute NVARCHAR(MAX) = NULL
AS
BEGIN
    DECLARE @ProcedureObjectId INT;
    SELECT @ProcedureObjectId = OBJECT_ID(@ProcedureName);

    EXEC tSQLt.Private_ValidateProcedureCanBeUsedWithSpyProcedure @ProcedureName;

    DECLARE @LogTableName NVARCHAR(MAX);
    SELECT @LogTableName = QUOTENAME(OBJECT_SCHEMA_NAME(@ProcedureObjectId)) + '.' + QUOTENAME(OBJECT_NAME(@ProcedureObjectId)+'_SpyProcedureLog');

    EXEC tSQLt.Private_RenameObjectToUniqueNameUsingObjectId @ProcedureObjectId;

    EXEC tSQLt.Private_CreateProcedureSpy @ProcedureObjectId, @ProcedureName, @LogTableName, @CommandToExecute;

    RETURN 0;
END;


GO

GO
CREATE FUNCTION tSQLt.Private_GetCommaSeparatedColumnList (@Table NVARCHAR(MAX), @ExcludeColumn NVARCHAR(MAX))
RETURNS NVARCHAR(MAX)
AS 
BEGIN
  RETURN STUFF((
     SELECT ',' + CASE WHEN system_type_id = TYPE_ID('timestamp') THEN ';TIMESTAMP columns are unsupported!;' ELSE QUOTENAME(name) END 
       FROM sys.columns 
      WHERE object_id = OBJECT_ID(@Table) 
        AND name &lt;&gt; @ExcludeColumn 
      ORDER BY column_id
     FOR XML PATH(''), TYPE).value('.','NVARCHAR(MAX)')
    ,1, 1, '');
        
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_CreateResultTableForCompareTables
 @ResultTable NVARCHAR(MAX),
 @ResultColumn NVARCHAR(MAX),
 @BaseTable NVARCHAR(MAX)
AS
BEGIN
  DECLARE @Cmd NVARCHAR(MAX);
  SET @Cmd = '
     SELECT ''='' AS ' + @ResultColumn + ', Expected.* INTO ' + @ResultTable + ' 
       FROM tSQLt.Private_NullCellTable N 
       LEFT JOIN ' + @BaseTable + ' AS Expected ON N.I &lt;&gt; N.I 
     TRUNCATE TABLE ' + @ResultTable + ';' --Need to insert an actual row to prevent IDENTITY property from transfering (IDENTITY_COL can't be NULLable);
  EXEC(@Cmd);
END
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_ValidateThatAllDataTypesInTableAreSupported
 @ResultTable NVARCHAR(MAX),
 @ColumnList NVARCHAR(MAX)
AS
BEGIN
    BEGIN TRY
      EXEC('DECLARE @EatResult INT; SELECT @EatResult = COUNT(1) FROM ' + @ResultTable + ' GROUP BY ' + @ColumnList + ';');
    END TRY
    BEGIN CATCH
      RAISERROR('The table contains a datatype that is not supported for tSQLt.AssertEqualsTable. Please refer to http://tsqlt.org/user-guide/assertions/assertequalstable/ for a list of unsupported datatypes.',16,10);
    END CATCH
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_CompareTablesFailIfUnequalRowsExists
 @UnequalRowsExist INT,
 @ResultTable NVARCHAR(MAX),
 @ResultColumn NVARCHAR(MAX),
 @ColumnList NVARCHAR(MAX),
 @FailMsg NVARCHAR(MAX)
AS
BEGIN
  IF @UnequalRowsExist &gt; 0
  BEGIN
   DECLARE @TableToTextResult NVARCHAR(MAX);
   DECLARE @OutputColumnList NVARCHAR(MAX);
   SELECT @OutputColumnList = '[_m_],' + @ColumnList;
   EXEC tSQLt.TableToText @TableName = @ResultTable, @OrderBy = @ResultColumn, @PrintOnlyColumnNameAliasList = @OutputColumnList, @txt = @TableToTextResult OUTPUT;
   
   DECLARE @Message NVARCHAR(MAX);
   SELECT @Message = @FailMsg + CHAR(13) + CHAR(10);

    EXEC tSQLt.Fail @Message, @TableToTextResult;
  END;
END
GO


GO

GO
CREATE PROCEDURE tSQLt.Private_CompareTables
    @Expected NVARCHAR(MAX),
    @Actual NVARCHAR(MAX),
    @ResultTable NVARCHAR(MAX),
    @ColumnList NVARCHAR(MAX),
    @MatchIndicatorColumnName NVARCHAR(MAX)
AS
BEGIN
    DECLARE @cmd NVARCHAR(MAX);
    DECLARE @RestoredRowIndexCounterColName NVARCHAR(MAX);
    SET @RestoredRowIndexCounterColName = @MatchIndicatorColumnName + '_RR';
    
    SELECT @cmd = 
    '
    INSERT INTO ' + @ResultTable + ' (' + @MatchIndicatorColumnName + ', ' + @ColumnList + ') 
    SELECT 
      CASE 
        WHEN RestoredRowIndex.'+@RestoredRowIndexCounterColName+' &lt;= CASE WHEN [_{Left}_]&lt;[_{Right}_] THEN [_{Left}_] ELSE [_{Right}_] END
         THEN ''='' 
        WHEN RestoredRowIndex.'+@RestoredRowIndexCounterColName+' &lt;= [_{Left}_] 
         THEN ''&lt;'' 
        ELSE ''&gt;'' 
      END AS ' + @MatchIndicatorColumnName + ', ' + @ColumnList + '
    FROM(
      SELECT SUM([_{Left}_]) AS [_{Left}_], 
             SUM([_{Right}_]) AS [_{Right}_], 
             ' + @ColumnList + ' 
      FROM (
        SELECT 1 AS [_{Left}_], 0[_{Right}_], ' + @ColumnList + '
          FROM ' + @Expected + '
        UNION ALL 
        SELECT 0[_{Left}_], 1 AS [_{Right}_], ' + @ColumnList + ' 
          FROM ' + @Actual + '
      ) AS X 
      GROUP BY ' + @ColumnList + ' 
    ) AS CollapsedRows
    CROSS APPLY (
       SELECT TOP(CASE WHEN [_{Left}_]&gt;[_{Right}_] THEN [_{Left}_] 
                       ELSE [_{Right}_] END) 
              ROW_NUMBER() OVER(ORDER BY(SELECT 1)) 
         FROM (SELECT 1 
                 FROM ' + @Actual + ' UNION ALL SELECT 1 FROM ' + @Expected + ') X(X)
              ) AS RestoredRowIndex(' + @RestoredRowIndexCounterColName + ');';
    
    EXEC (@cmd); --MainGroupQuery
    
    SET @cmd = 'SET @r = 
         CASE WHEN EXISTS(
                  SELECT 1 
                    FROM ' + @ResultTable + 
                 ' WHERE ' + @MatchIndicatorColumnName + ' IN (''&lt;'', ''&gt;'')) 
              THEN 1 ELSE 0 
         END';
    DECLARE @UnequalRowsExist INT;
    EXEC sp_executesql @cmd, N'@r INT OUTPUT',@UnequalRowsExist OUTPUT;
    
    RETURN @UnequalRowsExist;
END;


GO

GO
CREATE TABLE tSQLt.Private_NullCellTable(
  I INT 
);
GO

INSERT INTO tSQLt.Private_NullCellTable (I) VALUES (NULL);
GO

CREATE TRIGGER tSQLt.Private_NullCellTable_StopDeletes ON tSQLt.Private_NullCellTable INSTEAD OF DELETE, INSERT, UPDATE
AS
BEGIN
  RETURN;
END;
GO


GO

CREATE PROCEDURE tSQLt.AssertObjectExists
    @ObjectName NVARCHAR(MAX),
    @Message NVARCHAR(MAX) = ''
AS
BEGIN
    DECLARE @Msg NVARCHAR(MAX);
    IF(@ObjectName LIKE '#%')
    BEGIN
     IF OBJECT_ID('tempdb..'+@ObjectName) IS NULL
     BEGIN
         SELECT @Msg = '''' + COALESCE(@ObjectName, 'NULL') + ''' does not exist';
         EXEC tSQLt.Fail @Message, @Msg;
         RETURN 1;
     END;
    END
    ELSE
    BEGIN
     IF OBJECT_ID(@ObjectName) IS NULL
     BEGIN
         SELECT @Msg = '''' + COALESCE(@ObjectName, 'NULL') + ''' does not exist';
         EXEC tSQLt.Fail @Message, @Msg;
         RETURN 1;
     END;
    END;
    RETURN 0;
END;


GO

CREATE PROCEDURE tSQLt.AssertObjectDoesNotExist
    @ObjectName NVARCHAR(MAX),
    @Message NVARCHAR(MAX) = ''
AS
BEGIN
     DECLARE @Msg NVARCHAR(MAX);
     IF OBJECT_ID(@ObjectName) IS NOT NULL
     OR(@ObjectName LIKE '#%' AND OBJECT_ID('tempdb..'+@ObjectName) IS NOT NULL)
     BEGIN
         SELECT @Msg = '''' + @ObjectName + ''' does exist!';
         EXEC tSQLt.Fail @Message,@Msg;
     END;
END;


GO

GO
CREATE PROCEDURE tSQLt.AssertEqualsString
    @Expected NVARCHAR(MAX),
    @Actual NVARCHAR(MAX),
    @Message NVARCHAR(MAX) = ''
AS
BEGIN
    IF ((@Expected = @Actual) OR (@Actual IS NULL AND @Expected IS NULL))
      RETURN 0;

    DECLARE @Msg NVARCHAR(MAX);
    SELECT @Msg = CHAR(13)+CHAR(10)+
                  'Expected: ' + ISNULL('&lt;'+@Expected+'&gt;', 'NULL') +
                  CHAR(13)+CHAR(10)+
                  'but was : ' + ISNULL('&lt;'+@Actual+'&gt;', 'NULL');
    EXEC tSQLt.Fail @Message, @Msg;
END;
GO


GO

CREATE PROCEDURE tSQLt.AssertEqualsTable
    @Expected NVARCHAR(MAX),
    @Actual NVARCHAR(MAX),
    @Message NVARCHAR(MAX) = NULL,
    @FailMsg NVARCHAR(MAX) = 'Unexpected/missing resultset rows!'
AS
BEGIN

    EXEC tSQLt.AssertObjectExists @Expected;
    EXEC tSQLt.AssertObjectExists @Actual;

    DECLARE @ResultTable NVARCHAR(MAX);    
    DECLARE @ResultColumn NVARCHAR(MAX);    
    DECLARE @ColumnList NVARCHAR(MAX);    
    DECLARE @UnequalRowsExist INT;
    DECLARE @CombinedMessage NVARCHAR(MAX);

    SELECT @ResultTable = tSQLt.Private::CreateUniqueObjectName();
    SELECT @ResultColumn = 'RC_' + @ResultTable;

    EXEC tSQLt.Private_CreateResultTableForCompareTables 
      @ResultTable = @ResultTable,
      @ResultColumn = @ResultColumn,
      @BaseTable = @Expected;
        
    SELECT @ColumnList = tSQLt.Private_GetCommaSeparatedColumnList(@ResultTable, @ResultColumn);

    EXEC tSQLt.Private_ValidateThatAllDataTypesInTableAreSupported @ResultTable, @ColumnList;    
    
    EXEC @UnequalRowsExist = tSQLt.Private_CompareTables 
      @Expected = @Expected,
      @Actual = @Actual,
      @ResultTable = @ResultTable,
      @ColumnList = @ColumnList,
      @MatchIndicatorColumnName = @ResultColumn;
        
    SET @CombinedMessage = ISNULL(@Message + CHAR(13) + CHAR(10),'') + @FailMsg;
    EXEC tSQLt.Private_CompareTablesFailIfUnequalRowsExists 
      @UnequalRowsExist = @UnequalRowsExist,
      @ResultTable = @ResultTable,
      @ResultColumn = @ResultColumn,
      @ColumnList = @ColumnList,
      @FailMsg = @CombinedMessage;   
END;


GO

GO
CREATE PROCEDURE tSQLt.StubRecord(@SnTableName AS NVARCHAR(MAX), @BintObjId AS BIGINT)  
AS   
BEGIN  

    RAISERROR('Warning, tSQLt.StubRecord is not currently supported. Use at your own risk!', 0, 1) WITH NOWAIT;

    DECLARE @VcInsertStmt NVARCHAR(MAX),  
            @VcInsertValues NVARCHAR(MAX);  
    DECLARE @SnColumnName NVARCHAR(MAX); 
    DECLARE @SintDataType SMALLINT; 
    DECLARE @NvcFKCmd NVARCHAR(MAX);  
    DECLARE @VcFKVal NVARCHAR(MAX); 
  
    SET @VcInsertStmt = 'INSERT INTO ' + @SnTableName + ' ('  
      
    DECLARE curColumns CURSOR  
        LOCAL FAST_FORWARD  
    FOR  
    SELECT syscolumns.name,  
           syscolumns.xtype,  
           cmd.cmd  
    FROM syscolumns  
        LEFT OUTER JOIN dbo.sysconstraints ON syscolumns.id = sysconstraints.id  
                                      AND syscolumns.colid = sysconstraints.colid  
                                      AND sysconstraints.status = 1    -- Primary key constraints only  
        LEFT OUTER JOIN (select fkeyid id,fkey colid,N'select @V=cast(min('+syscolumns.name+') as NVARCHAR) from '+sysobjects.name cmd  
                        from sysforeignkeys   
                        join sysobjects on sysobjects.id=sysforeignkeys.rkeyid  
                        join syscolumns on sysobjects.id=syscolumns.id and syscolumns.colid=rkey) cmd  
            on cmd.id=syscolumns.id and cmd.colid=syscolumns.colid  
    WHERE syscolumns.id = OBJECT_ID(@SnTableName)  
      AND (syscolumns.isnullable = 0 )  
    ORDER BY ISNULL(sysconstraints.status, 9999), -- Order Primary Key constraints first  
             syscolumns.colorder  
  
    OPEN curColumns  
  
    FETCH NEXT FROM curColumns  
    INTO @SnColumnName, @SintDataType, @NvcFKCmd  
  
    -- Treat the first column retrieved differently, no commas need to be added  
    -- and it is the ObjId column  
    IF @@FETCH_STATUS = 0  
    BEGIN  
        SET @VcInsertStmt = @VcInsertStmt + @SnColumnName  
        SELECT @VcInsertValues = ')VALUES(' + ISNULL(CAST(@BintObjId AS nvarchar), 'NULL')  
  
        FETCH NEXT FROM curColumns  
        INTO @SnColumnName, @SintDataType, @NvcFKCmd  
    END  
    ELSE  
    BEGIN  
        -- No columns retrieved, we need to insert into any first column  
        SELECT @VcInsertStmt = @VcInsertStmt + syscolumns.name  
        FROM syscolumns  
        WHERE syscolumns.id = OBJECT_ID(@SnTableName)  
          AND syscolumns.colorder = 1  
  
        SELECT @VcInsertValues = ')VALUES(' + ISNULL(CAST(@BintObjId AS nvarchar), 'NULL')  
  
    END  
  
    WHILE @@FETCH_STATUS = 0  
    BEGIN  
        SET @VcInsertStmt = @VcInsertStmt + ',' + @SnColumnName  
        SET @VcFKVal=',0'  
        if @NvcFKCmd is not null  
        BEGIN  
            set @VcFKVal=null  
            exec sp_executesql @NvcFKCmd,N'@V NVARCHAR(MAX) output',@VcFKVal output  
            set @VcFKVal=isnull(','''+@VcFKVal+'''',',NULL')  
        END  
        SET @VcInsertValues = @VcInsertValues + @VcFKVal  
  
        FETCH NEXT FROM curColumns  
        INTO @SnColumnName, @SintDataType, @NvcFKCmd  
    END  
      
    CLOSE curColumns  
    DEALLOCATE curColumns  
  
    SET @VcInsertStmt = @VcInsertStmt + @VcInsertValues + ')'  
  
    IF EXISTS (SELECT 1   
               FROM syscolumns  
               WHERE status = 128   
                 AND id = OBJECT_ID(@SnTableName))  
    BEGIN  
        SET @VcInsertStmt = 'SET IDENTITY_INSERT ' + @SnTableName + ' ON ' + CHAR(10) +   
                             @VcInsertStmt + CHAR(10) +   
                             'SET IDENTITY_INSERT ' + @SnTableName + ' OFF '  
    END  
  
    EXEC (@VcInsertStmt)    -- Execute the actual INSERT statement  
  
END  

GO


GO

GO
CREATE PROCEDURE [tSQLt].[AssertLike] 
  @ExpectedPattern NVARCHAR(MAX),
  @Actual NVARCHAR(MAX),
  @Message NVARCHAR(MAX) = ''
AS
BEGIN
  IF (LEN(@ExpectedPattern) &gt; 4000)
  BEGIN
    RAISERROR ('@ExpectedPattern may not exceed 4000 characters.', 16, 10);
  END;

  IF ((@Actual LIKE @ExpectedPattern) OR (@Actual IS NULL AND @ExpectedPattern IS NULL))
  BEGIN
    RETURN 0;
  END

  DECLARE @Msg NVARCHAR(MAX);
  SELECT @Msg = CHAR(13) + CHAR(10) + 'Expected: &lt;' + ISNULL(@ExpectedPattern, 'NULL') + '&gt;' +
                CHAR(13) + CHAR(10) + ' but was: &lt;' + ISNULL(@Actual, 'NULL') + '&gt;';
  EXEC tSQLt.Fail @Message, @Msg;
END;
GO


GO

CREATE PROCEDURE tSQLt.AssertNotEquals
    @Expected SQL_VARIANT,
    @Actual SQL_VARIANT,
    @Message NVARCHAR(MAX) = ''
AS
BEGIN
  IF (@Expected = @Actual)
  OR (@Expected IS NULL AND @Actual IS NULL)
  BEGIN
    DECLARE @Msg NVARCHAR(MAX);
    SET @Msg = 'Expected actual value to not ' + 
               COALESCE('equal &lt;' + tSQLt.Private_SqlVariantFormatter(@Expected)+'&gt;', 'be NULL') + 
               '.';
    EXEC tSQLt.Fail @Message,@Msg;
  END;
  RETURN 0;
END;


GO

CREATE FUNCTION tSQLt.Private_SqlVariantFormatter(@Value SQL_VARIANT)
RETURNS NVARCHAR(MAX)
AS
BEGIN
  RETURN CASE UPPER(CAST(SQL_VARIANT_PROPERTY(@Value,'BaseType')AS sysname))
           WHEN 'FLOAT' THEN CONVERT(NVARCHAR(MAX),@Value,2)
           WHEN 'REAL' THEN CONVERT(NVARCHAR(MAX),@Value,1)
           WHEN 'MONEY' THEN CONVERT(NVARCHAR(MAX),@Value,2)
           WHEN 'SMALLMONEY' THEN CONVERT(NVARCHAR(MAX),@Value,2)
           WHEN 'DATE' THEN CONVERT(NVARCHAR(MAX),@Value,126)
           WHEN 'DATETIME' THEN CONVERT(NVARCHAR(MAX),@Value,126)
           WHEN 'DATETIME2' THEN CONVERT(NVARCHAR(MAX),@Value,126)
           WHEN 'DATETIMEOFFSET' THEN CONVERT(NVARCHAR(MAX),@Value,126)
           WHEN 'SMALLDATETIME' THEN CONVERT(NVARCHAR(MAX),@Value,126)
           WHEN 'TIME' THEN CONVERT(NVARCHAR(MAX),@Value,126)
           WHEN 'BINARY' THEN CONVERT(NVARCHAR(MAX),@Value,1)
           WHEN 'VARBINARY' THEN CONVERT(NVARCHAR(MAX),@Value,1)
           ELSE CAST(@Value AS NVARCHAR(MAX))
         END;
END


GO

CREATE PROCEDURE tSQLt.AssertEmptyTable
  @TableName NVARCHAR(MAX),
  @Message NVARCHAR(MAX) = ''
AS
BEGIN
  EXEC tSQLt.AssertObjectExists @TableName;

  DECLARE @FullName NVARCHAR(MAX);
  IF(OBJECT_ID(@TableName) IS NULL AND OBJECT_ID('tempdb..'+@TableName) IS NOT NULL)
  BEGIN
    SET @FullName = CASE WHEN LEFT(@TableName,1) = '[' THEN @TableName ELSE QUOTENAME(@TableName)END;
  END;
  ELSE
  BEGIN
    SET @FullName = tSQLt.Private_GetQuotedFullName(OBJECT_ID(@TableName));
  END;

  DECLARE @cmd NVARCHAR(MAX);
  DECLARE @exists INT;
  SET @cmd = 'SELECT @exists = CASE WHEN EXISTS(SELECT 1 FROM '+@FullName+') THEN 1 ELSE 0 END;'
  EXEC sp_executesql @cmd,N'@exists INT OUTPUT', @exists OUTPUT;
  
  IF(@exists = 1)
  BEGIN
    DECLARE @TableToText NVARCHAR(MAX);
    EXEC tSQLt.TableToText @TableName = @FullName,@txt = @TableToText OUTPUT;
    DECLARE @Msg NVARCHAR(MAX);
    SET @Msg = @FullName + ' was not empty:' + CHAR(13) + CHAR(10)+ @TableToText;
    EXEC tSQLt.Fail @Message,@Msg;
  END
END


GO

CREATE PROCEDURE tSQLt.ApplyTrigger
  @TableName NVARCHAR(MAX),
  @TriggerName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @OrgTableObjectId INT;
  SELECT @OrgTableObjectId = OrgTableObjectId FROM tSQLt.Private_GetOriginalTableInfo(OBJECT_ID(@TableName)) orgTbl
  IF(@OrgTableObjectId IS NULL)
  BEGIN
    RAISERROR('%s does not exist or was not faked by tSQLt.FakeTable.', 16, 10, @TableName);
  END;
  
  DECLARE @FullTriggerName NVARCHAR(MAX);
  DECLARE @TriggerObjectId INT;
  SELECT @FullTriggerName = QUOTENAME(SCHEMA_NAME(schema_id))+'.'+QUOTENAME(name), @TriggerObjectId = object_id
  FROM sys.objects WHERE PARSENAME(@TriggerName,1) = name AND parent_object_id = @OrgTableObjectId;
  
  DECLARE @TriggerCode NVARCHAR(MAX);
  SELECT @TriggerCode = m.definition
    FROM sys.sql_modules m
   WHERE m.object_id = @TriggerObjectId;
  
  IF (@TriggerCode IS NULL)
  BEGIN
    RAISERROR('%s is not a trigger on %s', 16, 10, @TriggerName, @TableName);
  END;
 
  EXEC tSQLt.RemoveObject @FullTriggerName;
  
  EXEC(@TriggerCode);
END;


GO

GO
CREATE PROCEDURE tSQLt.Private_ValidateObjectsCompatibleWithFakeFunction
  @FunctionName NVARCHAR(MAX),
  @FakeFunctionName NVARCHAR(MAX),
  @FunctionObjectId INT OUTPUT,
  @FakeFunctionObjectId INT OUTPUT,
  @IsScalarFunction BIT OUTPUT
AS
BEGIN
  SET @FunctionObjectId = OBJECT_ID(@FunctionName);
  SET @FakeFunctionObjectId = OBJECT_ID(@FakeFunctionName);

  IF(@FunctionObjectId IS NULL)
  BEGIN
    RAISERROR('%s does not exist!',16,10,@FunctionName);
  END;
  IF(@FakeFunctionObjectId IS NULL)
  BEGIN
    RAISERROR('%s does not exist!',16,10,@FakeFunctionName);
  END;
  
  DECLARE @FunctionType CHAR(2);
  DECLARE @FakeFunctionType CHAR(2);
  SELECT @FunctionType = type FROM sys.objects WHERE object_id = @FunctionObjectId;
  SELECT @FakeFunctionType = type FROM sys.objects WHERE object_id = @FakeFunctionObjectId;

  IF((@FunctionType IN('FN','FS') AND @FakeFunctionType NOT IN('FN','FS'))
     OR
     (@FunctionType IN('TF','IF','FT') AND @FakeFunctionType NOT IN('TF','IF','FT'))
     OR
     (@FunctionType NOT IN('FN','FS','TF','IF','FT'))
     )    
  BEGIN
    RAISERROR('Both parameters must contain the name of either scalar or table valued functions!',16,10);
  END;
  
  SET @IsScalarFunction = CASE WHEN @FunctionType IN('FN','FS') THEN 1 ELSE 0 END;
  
  IF(EXISTS(SELECT 1 
              FROM sys.parameters AS P
             WHERE P.object_id IN(@FunctionObjectId,@FakeFunctionObjectId)
             GROUP BY P.name, P.max_length, P.precision, P.scale, P.parameter_id
            HAVING COUNT(1) &lt;&gt; 2
           ))
  BEGIN
    RAISERROR('Parameters of both functions must match! (This includes the return type for scalar functions.)',16,10);
  END; 
END;
GO
  


GO

GO
CREATE PROCEDURE tSQLt.Private_CreateFakeFunction
  @FunctionName NVARCHAR(MAX),
  @FakeFunctionName NVARCHAR(MAX),
  @FunctionObjectId INT,
  @FakeFunctionObjectId INT,
  @IsScalarFunction BIT
AS
BEGIN
  DECLARE @ReturnType NVARCHAR(MAX);
  SELECT @ReturnType = T.TypeName
    FROM sys.parameters AS P
   CROSS APPLY tSQLt.Private_GetFullTypeName(P.user_type_id,P.max_length,P.precision,P.scale,NULL) AS T
   WHERE P.object_id = @FunctionObjectId
     AND P.parameter_id = 0;
     
  DECLARE @ParameterList NVARCHAR(MAX);
  SELECT @ParameterList = COALESCE(
     STUFF((SELECT ','+P.name+' '+T.TypeName+CASE WHEN T.IsTableType = 1 THEN ' READONLY' ELSE '' END
              FROM sys.parameters AS P
             CROSS APPLY tSQLt.Private_GetFullTypeName(P.user_type_id,P.max_length,P.precision,P.scale,NULL) AS T
             WHERE P.object_id = @FunctionObjectId
               AND P.parameter_id &gt; 0
             ORDER BY P.parameter_id
               FOR XML PATH(''),TYPE
           ).value('.','NVARCHAR(MAX)'),1,1,''),'');
           
  DECLARE @ParameterCallList NVARCHAR(MAX);
  SELECT @ParameterCallList = COALESCE(
     STUFF((SELECT ','+P.name
              FROM sys.parameters AS P
             CROSS APPLY tSQLt.Private_GetFullTypeName(P.user_type_id,P.max_length,P.precision,P.scale,NULL) AS T
             WHERE P.object_id = @FunctionObjectId
               AND P.parameter_id &gt; 0
             ORDER BY P.parameter_id
               FOR XML PATH(''),TYPE
           ).value('.','NVARCHAR(MAX)'),1,1,''),'');


  IF(@IsScalarFunction = 1)
  BEGIN
    EXEC('CREATE FUNCTION '+@FunctionName+'('+@ParameterList+') RETURNS '+@ReturnType+' AS BEGIN RETURN '+@FakeFunctionName+'('+@ParameterCallList+');END;'); 
  END
  ELSE
  BEGIN
    EXEC('CREATE FUNCTION '+@FunctionName+'('+@ParameterList+') RETURNS TABLE AS RETURN SELECT * FROM '+@FakeFunctionName+'('+@ParameterCallList+');'); 
  END;
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.FakeFunction
  @FunctionName NVARCHAR(MAX),
  @FakeFunctionName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @FunctionObjectId INT;
  DECLARE @FakeFunctionObjectId INT;
  DECLARE @IsScalarFunction BIT;

  EXEC tSQLt.Private_ValidateObjectsCompatibleWithFakeFunction 
               @FunctionName = @FunctionName,
               @FakeFunctionName = @FakeFunctionName,
               @FunctionObjectId = @FunctionObjectId OUT,
               @FakeFunctionObjectId = @FakeFunctionObjectId OUT,
               @IsScalarFunction = @IsScalarFunction OUT;

  EXEC tSQLt.RemoveObject @ObjectName = @FunctionName;

  EXEC tSQLt.Private_CreateFakeFunction 
               @FunctionName = @FunctionName,
               @FakeFunctionName = @FakeFunctionName,
               @FunctionObjectId = @FunctionObjectId,
               @FakeFunctionObjectId = @FakeFunctionObjectId,
               @IsScalarFunction = @IsScalarFunction;

END;
GO


GO

CREATE PROCEDURE tSQLt.RenameClass
   @SchemaName NVARCHAR(MAX),
   @NewSchemaName NVARCHAR(MAX)
AS
BEGIN
  DECLARE @MigrateObjectsCommand NVARCHAR(MAX);

  SELECT @NewSchemaName = PARSENAME(@NewSchemaName, 1),
         @SchemaName = PARSENAME(@SchemaName, 1);

  EXEC tSQLt.NewTestClass @NewSchemaName;

  SELECT @MigrateObjectsCommand = (
    SELECT Cmd AS [text()] FROM (
    SELECT 'ALTER SCHEMA ' + QUOTENAME(@NewSchemaName) + ' TRANSFER ' + QUOTENAME(@SchemaName) + '.' + QUOTENAME(name) + ';' AS Cmd
      FROM sys.objects
     WHERE schema_id = SCHEMA_ID(@SchemaName)
       AND type NOT IN ('PK', 'F')
    UNION ALL 
    SELECT 'ALTER SCHEMA ' + QUOTENAME(@NewSchemaName) + ' TRANSFER XML SCHEMA COLLECTION::' + QUOTENAME(@SchemaName) + '.' + QUOTENAME(name) + ';' AS Cmd
      FROM sys.xml_schema_collections
     WHERE schema_id = SCHEMA_ID(@SchemaName)
    UNION ALL 
    SELECT 'ALTER SCHEMA ' + QUOTENAME(@NewSchemaName) + ' TRANSFER TYPE::' + QUOTENAME(@SchemaName) + '.' + QUOTENAME(name) + ';' AS Cmd
      FROM sys.types
     WHERE schema_id = SCHEMA_ID(@SchemaName)
    ) AS Cmds
       FOR XML PATH(''), TYPE).value('.', 'NVARCHAR(MAX)');

  EXEC (@MigrateObjectsCommand);

  EXEC tSQLt.DropClass @SchemaName;
END;


GO

GO
CREATE TABLE [tSQLt].[Private_AssertEqualsTableSchema_Actual]
(
  name NVARCHAR(256) NULL,
  [RANK(column_id)] INT NULL,
  system_type_id NVARCHAR(MAX) NULL,
  user_type_id NVARCHAR(MAX) NULL,
  max_length SMALLINT NULL,
  precision TINYINT NULL,
  scale TINYINT NULL,
  collation_name NVARCHAR(256) NULL,
  is_nullable BIT NULL,
  is_identity BIT NULL
);
GO
EXEC('
  SET NOCOUNT ON;
  SELECT TOP(0) * 
    INTO tSQLt.Private_AssertEqualsTableSchema_Expected
    FROM tSQLt.Private_AssertEqualsTableSchema_Actual AS AETSA;
');
GO


GO

GO
CREATE PROCEDURE tSQLt.AssertEqualsTableSchema
    @Expected NVARCHAR(MAX),
    @Actual NVARCHAR(MAX),
    @Message NVARCHAR(MAX) = NULL
AS
BEGIN
  INSERT INTO tSQLt.Private_AssertEqualsTableSchema_Expected([RANK(column_id)],name,system_type_id,user_type_id,max_length,precision,scale,collation_name,is_nullable)
  SELECT 
      RANK()OVER(ORDER BY C.column_id),
      C.name,
      CAST(C.system_type_id AS NVARCHAR(MAX))+QUOTENAME(TS.name) system_type_id,
      CAST(C.user_type_id AS NVARCHAR(MAX))+CASE WHEN TU.system_type_id&lt;&gt; TU.user_type_id THEN QUOTENAME(SCHEMA_NAME(TU.schema_id))+'.' ELSE '' END + QUOTENAME(TU.name) user_type_id,
      C.max_length,
      C.precision,
      C.scale,
      C.collation_name,
      C.is_nullable
    FROM sys.columns AS C
    JOIN sys.types AS TS
      ON C.system_type_id = TS.user_type_id
    JOIN sys.types AS TU
      ON C.user_type_id = TU.user_type_id
   WHERE C.object_id = OBJECT_ID(@Expected);
  INSERT INTO tSQLt.Private_AssertEqualsTableSchema_Actual([RANK(column_id)],name,system_type_id,user_type_id,max_length,precision,scale,collation_name,is_nullable)
  SELECT 
      RANK()OVER(ORDER BY C.column_id),
      C.name,
      CAST(C.system_type_id AS NVARCHAR(MAX))+QUOTENAME(TS.name) system_type_id,
      CAST(C.user_type_id AS NVARCHAR(MAX))+CASE WHEN TU.system_type_id&lt;&gt; TU.user_type_id THEN QUOTENAME(SCHEMA_NAME(TU.schema_id))+'.' ELSE '' END + QUOTENAME(TU.name) user_type_id,
      C.max_length,
      C.precision,
      C.scale,
      C.collation_name,
      C.is_nullable
    FROM sys.columns AS C
    JOIN sys.types AS TS
      ON C.system_type_id = TS.user_type_id
    JOIN sys.types AS TU
      ON C.user_type_id = TU.user_type_id
   WHERE C.object_id = OBJECT_ID(@Actual);
  
  EXEC tSQLt.AssertEqualsTable 'tSQLt.Private_AssertEqualsTableSchema_Expected','tSQLt.Private_AssertEqualsTableSchema_Actual',@Message=@Message,@FailMsg='Unexpected/missing column(s)';  
END;
GO


GO

GO
IF NOT(CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR(MAX)) LIKE '9.%')
BEGIN
  EXEC('CREATE TYPE tSQLt.AssertStringTable AS TABLE(value NVARCHAR(MAX));');
END;
GO


GO

GO
IF NOT(CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR(MAX)) LIKE '9.%')
BEGIN
EXEC('
CREATE PROCEDURE tSQLt.AssertStringIn
  @Expected tSQLt.AssertStringTable READONLY,
  @Actual NVARCHAR(MAX),
  @Message NVARCHAR(MAX) = ''''
AS
BEGIN
  IF(NOT EXISTS(SELECT 1 FROM @Expected WHERE value = @Actual))
  BEGIN
    DECLARE @ExpectedMessage NVARCHAR(MAX);
    SELECT value INTO #ExpectedSet FROM @Expected;
    EXEC tSQLt.TableToText @TableName = ''#ExpectedSet'', @OrderBy = ''value'',@txt = @ExpectedMessage OUTPUT;
    SET @ExpectedMessage = ISNULL(''&lt;''+@Actual+''&gt;'',''NULL'')+CHAR(13)+CHAR(10)+''is not in''+CHAR(13)+CHAR(10)+@ExpectedMessage;
    EXEC tSQLt.Fail @Message, @ExpectedMessage;
  END;
END;
');
END;
GO


GO

GO
CREATE PROCEDURE tSQLt.Reset
AS
BEGIN
  EXEC tSQLt.Private_ResetNewTestClassList;
END;
GO


GO

GO
SET NOCOUNT ON;
DECLARE @ver NVARCHAR(MAX); 
DECLARE @match INT; 
SELECT @ver = '| tSQLt Version: ' + I.Version,
       @match = CASE WHEN I.Version = I.ClrVersion THEN 1 ELSE 0 END
  FROM tSQLt.Info() AS I;
SET @ver = @ver+SPACE(42-LEN(@ver))+'|';
 
RAISERROR('',0,1)WITH NOWAIT;
RAISERROR('+-----------------------------------------+',0,1)WITH NOWAIT;
RAISERROR('|                                         |',0,1)WITH NOWAIT;
RAISERROR('| Thank you for using tSQLt.              |',0,1)WITH NOWAIT;
RAISERROR('|                                         |',0,1)WITH NOWAIT;
RAISERROR(@ver,0,1)WITH NOWAIT;
IF(@match = 0)
BEGIN
  RAISERROR('|                                         |',0,1)WITH NOWAIT;
  RAISERROR('| ERROR: mismatching CLR Version.         |',0,1)WITH NOWAIT;
  RAISERROR('| Please download a new version of tSQLt. |',0,1)WITH NOWAIT;
END
RAISERROR('|                                         |',0,1)WITH NOWAIT;
RAISERROR('+-----------------------------------------+',0,1)WITH NOWAIT;


GO


</file>
<file name="src\sql\_test\tsqlt_config_ci.json">
{
    "testclassses": [
        {
            "name" : "Design rules",
            "classfilter": "t_design_rules",
            "description": "Verify design rules",
            "max_failures": "0"
        },
        {
            "name": "procedure unit tests",
            "classfilter": "t_usp_%",
            "description" : "All stored procedure unit tests",
            "max_failures": "0"
        },
        {
            "name": "Function unit tests",
            "classfilter": "t_fn_%",
            "description" : "All stored procedure unit tests",
            "max_failures": "0"
        }
    ]
}
</file>
<file name="src\sql\_test\tsqlt_with_xml_output.ps1">
﻿&lt;#

.SYNOPSIS
This script will run all the tSQLt tests deployed in the TargetServer and export the results as an xml file

.DESCRIPTION


#&gt;

[CmdletBinding()]
param (
    [Parameter( Mandatory = $false)] [string] $ConfigFile = "${env:REPOROOT}/src/sql/_test/tsqlt_config_ci.json",                        # ConfigFile - path to json definition of the tests to run
    [Parameter( Mandatory = $false)] [string] $OutputPath = "${env:REPOROOT}/src/sql/_test/output",                                  # OutputPath - path to the folder where to store the xml results
    [Parameter( Mandatory = $true)]  [string] $TargetServer,                                                                           # Name of the SQL server to access
    [Parameter( Mandatory = $true)]  [string] $DatabaseName,                                                                          # Name of the SQL database to access
    [Parameter( Mandatory = $false)] [string] $AccessToken = (Get-AzAccessToken -ResourceUrl https://database.windows.net).Token     # Accesstoken for the user trying to accesss the DB

 )


# Create OutputPath if it does not exist already
If(!(test-path $OutputPath)) {
    New-Item -ItemType Directory -Force -Path $OutputPath | Out-Null
}


# SQL query template to execute 
$sql_run_test_class = "
    BEGIN TRY 
	    EXEC tSQLt.Run '&lt;CLASSNAME&gt;'; 
    END TRY 
    BEGIN CATCH END 
    CATCH; 
    EXEC tSQLt.XmlResultFormatter;

    select 
	    COUNT(Id) as NumTests,
	    SUM(case when Result = 'Success' then 1 else 0 end) as NumSuccess,
	    SUM(case when Result = 'Success' then 0 else 1 end) as NumFailed
    from tSQLt.TestResult
    "

# Read the JSON ConfigFile and extract all the testclasses to execute
$configdata = Get-Content -Raw -Path $ConfigFile | ConvertFrom-Json
$testclasses = $configdata.testclassses

if ( (-not $testclasses) -or ( $testclasses.count -eq 0) ) {
    Write-Error "No testclasses found in configuration"
    return
}
else{
Write-Information "Number of testclasses to run: $($testclasses.count)"
}


## Initialize parameters before running the test classes

# Boolean: Tests have executed successfully
$TestResult = $true
# List of error messages received
$ErrorMessages = [System.Text.StringBuilder]::new()
# Total number of test classes that have been executed
$NumClasses = 0


Try {

    # loop all testclass definition in the json configuration
    ForEach ( $testclass in $testclasses ) {

        $testname = $testclass.name
        $classfilter = $testclass.classfilter
        $maxErrors= $testclass.max_failures

        Write-Information ""
        Write-Information "---------------------------------------"    
        Write-Information "Class     : $testname"   
        Write-Information "Filter    : $classfilter"    
        Write-Information "Max errors: $maxErrors"    
        Write-Information "---------------------------------------"    
        Write-Information ""

        $params = @{
            ServerInstance = $TargetServer
            Database = $DatabaseName
            AccessToken = $AccessToken
        }

        #get class names matching the filter
        $test_cases = Invoke-SQLCmd -Query "select Name from tsqlt.testClasses where Name like '$classfilter'" @params

        # List of test cases to execute
        Write-Information "List of test cases: "
        $test_cases | ForEach-Object { Write-Information "  $($_.Name)"}

        # Total number of failed tests within a class
        $numFailed = 0

        #loop all matched cases
        ForEach( $test_case in $test_cases ) {

            $classname = $test_case.Name

            $query = ( $sql_run_test_class -replace "&lt;CLASSNAME&gt;", $classname )


            $params = @{
                ServerInstance = $TargetServer
                Database = $DatabaseName
                AccessToken = $AccessToken
            }

            $sqlResult = Invoke-SQLCmd -Query $query @params -OutputAs DataSet
            $numResultTables = $sqlResult.Tables.Count

            # get XMl output (expected in last-but-one result table)
            $xmlColumn = ($sqlResult.tables[$numResultTables-2] | Get-Member -MemberType Property).Name
            $xmlData = $sqlResult.tables[$numResultTables-2].$xmlColumn
            # sanity check: xml output in this table?
            if ( $xmlData -notlike "&lt;testsuites&gt;*" ) {
                Write-Error "XML test output could not be located"
            } else {
                # Write the XML data to a file
                $xmlData | Out-File "${OutputPath}\tsqlt_result.$Databasename.$classname.xml"
            }

            # keep track of classes tested
            $NumClasses++

            # keep track of failed tests count (in last resultset table)
            $testsFailed = $sqlResult.Tables[$numResultTables-1].Rows[0].NumFailed
            if( $testsFailed -gt 0) {
                Write-Warning "Test [$classname]: [$testsFailed] test failure(s) detected"
                $numFailed += $testsFailed
            }        
        }

        # evaluate outcome
        If ( $numFailed -gt $maxErrors ) {
            $TestResult = $false
            $ErrorMessages.AppendLine( "[${testname}] tests: Detected more failed tests ($NumFailed) than the maximum ($maxErrors)" )
        } 

    }

} 

catch {

    $TestResult = $false
    $ErrorMessages.AppendLine( $_ )

} 

if ($TestResult -eq $false ) {
    Write-Error $ErrorMessages.ToString()
} else {
    Write-Information "Tests completed successfully."
    Write-Information "Wrote $NumClasses test result files to $OutputPath"
}


</file>
<file name="src\synapse\studio\publish_config.json">
{"publishBranch":"synapse_publish","enableGitComment":true}
</file>
<file name="src\synapse\studio\template-parameters-definition.json">
{
    "Microsoft.Synapse/workspaces/notebooks": {
        "properties": {
            "bigDataPool": {
                "referenceName": "-:referenceName:string"
            },
            "sessionProperties": {
                "runAsWorkspaceSystemIdentity": "-:runAsWorkspaceSystemIdentity:bool"
            }
        }
    },
    "Microsoft.Synapse/workspaces/sqlscripts": {
        "properties": {
            "content": {
                "currentConnection": {
                    "*": "-"
                }
            }
        }
    },
    "Microsoft.Synapse/workspaces/triggers": {
        "properties": {
            "typeProperties": {
                "recurrence": {
                    "*": "=",
                    "interval": "=:triggerSuffix:int",
                    "frequency": "=:-freq"
                },
                "maxConcurrency": "="
            }
        }
    },
    "Microsoft.Synapse/workspaces/linkedServices": {
        "*": {
            "properties": {
                "typeProperties": {
                    "accountName": "=",
                    "username": "=",
                    "connectionString": "|:-connectionString:secureString",
                    "secretAccessKey": "|"
                }
            }
        },
        "AzureDataLakeStore": {
            "properties": {
                "typeProperties": {
                    "dataLakeStoreUri": "="
                }
            }
        },
        "AzureKeyVault": {
            "properties": {
                "typeProperties": {
                    "baseUrl": "|:baseUrl:secureString"
                },
                "parameters": {
                    "KeyVaultURL": {
                        "type": "=",
                        "defaultValue": "|:defaultValue:secureString"
                    }
                }
            }
        },
        "AzureBlobFS": {
            "properties": {
                "typeProperties": {
                    "url": "=:url:string"
                }
            }
        }
    },
    "Microsoft.Synapse/workspaces/datasets": {
        "*": {
            "properties": {
                "typeProperties": {
                    "folderPath": "=",
                    "fileName": "="
                }
            }
        }
    },
    "Microsoft.Synapse/workspaces/credentials" : {
        "properties": {
            "typeProperties": {
                "resourceId": "="
            }
        }
    },
    "Microsoft.Synapse/workspaces/pipelines":{
        "properties": {
            "variables": {
                "ENV_CODE": {
                    "defaultValue":"=:defaultValue:string"
                },
                "spark_pool": {
                    "defaultValue":"=:defaultValue:string"
                }
            }
        }
    },
    "Microsoft.Synapse/workspaces/sparkConfigurations":{
        "properties": {
            "configs": {
                "spark.environment_code": "-:environment_code:string"
            }
        }
    }
}
</file>
<file name="src\synapse\studio\credential\WorkspaceSystemIdentity.json">
{
	"name": "WorkspaceSystemIdentity",
	"properties": {
		"type": "ManagedIdentity"
	}
}
</file>
<file name="src\synapse\studio\dataset\ds_binary_file.json">
{
	"name": "ds_binary_file",
	"properties": {
		"linkedServiceName": {
			"referenceName": "ls_dap_adls_01",
			"type": "LinkedServiceReference"
		},
		"parameters": {
			"container_name": {
				"type": "string"
			},
			"folder_directory": {
				"type": "string"
			},
			"file_name": {
				"type": "string"
			}
		},
		"annotations": [],
		"type": "Binary",
		"typeProperties": {
			"location": {
				"type": "AzureBlobFSLocation",
				"fileName": {
					"value": "@dataset().file_name",
					"type": "Expression"
				},
				"folderPath": {
					"value": "@dataset().folder_directory",
					"type": "Expression"
				},
				"fileSystem": {
					"value": "@dataset().container_name",
					"type": "Expression"
				}
			}
		}
	}
}
</file>
<file name="src\synapse\studio\dataset\ds_binary_zip_folder.json">
{
	"name": "ds_binary_zip_folder",
	"properties": {
		"linkedServiceName": {
			"referenceName": "ls_dap_adls_01",
			"type": "LinkedServiceReference"
		},
		"parameters": {
			"container_name": {
				"type": "string"
			},
			"folder_directory": {
				"type": "string"
			},
			"file_name": {
				"type": "string"
			}
		},
		"annotations": [],
		"type": "Binary",
		"typeProperties": {
			"location": {
				"type": "AzureBlobFSLocation",
				"fileName": {
					"value": "@dataset().file_name",
					"type": "Expression"
				},
				"folderPath": {
					"value": "@dataset().folder_directory",
					"type": "Expression"
				},
				"fileSystem": {
					"value": "@dataset().container_name",
					"type": "Expression"
				}
			},
			"compression": {
				"type": "ZipDeflate",
				"level": "Optimal"
			}
		}
	}
}
</file>
<file name="src\synapse\studio\dataset\ds_dap_sql_meta.json">
{
	"name": "ds_dap_sql_meta",
	"properties": {
		"description": "This dataset is used to execute stored procedures and look at tables inside the dev-dap-sql-core (hosted on Azure)",
		"linkedServiceName": {
			"referenceName": "ls_dap_sql_meta",
			"type": "LinkedServiceReference"
		},
		"parameters": {
			"schema": {
				"type": "string",
				"defaultValue": "meta"
			},
			"target": {
				"type": "string"
			}
		},
		"annotations": [],
		"type": "AzureSqlTable",
		"schema": [],
		"typeProperties": {
			"schema": {
				"value": "@dataset().schema",
				"type": "Expression"
			},
			"table": {
				"value": "@dataset().target",
				"type": "Expression"
			}
		}
	}
}
</file>
<file name="src\synapse\studio\integrationRuntime\AutoResolveIntegrationRuntime.json">
{
	"name": "AutoResolveIntegrationRuntime",
	"properties": {
		"type": "Managed",
		"typeProperties": {
			"computeProperties": {
				"location": "AutoResolve",
				"dataFlowProperties": {
					"computeType": "General",
					"coreCount": 8,
					"timeToLive": 0
				}
			}
		},
		"managedVirtualNetwork": {
			"type": "ManagedVirtualNetworkReference",
			"referenceName": "default"
		}
	}
}
</file>
<file name="src\synapse\studio\linkedService\dev-dap-syn-core-WorkspaceDefaultSqlServer.json">
{
	"name": "dev-dap-syn-core-WorkspaceDefaultSqlServer",
	"type": "Microsoft.Synapse/workspaces/linkedservices",
	"properties": {
		"typeProperties": {
			"connectionString": "Data Source=tcp:dev-dap-syn-core.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"parameters": {
			"DBName": {
				"type": "String"
			}
		},
		"type": "AzureSqlDW",
		"connectVia": {
			"referenceName": "AutoResolveIntegrationRuntime",
			"type": "IntegrationRuntimeReference"
		},
		"annotations": []
	}
}
</file>
<file name="src\synapse\studio\linkedService\dev-dap-syn-core-WorkspaceDefaultStorage.json">
{
	"name": "dev-dap-syn-core-WorkspaceDefaultStorage",
	"type": "Microsoft.Synapse/workspaces/linkedservices",
	"properties": {
		"typeProperties": {
			"url": "https://devdapstcoresyn.dfs.core.windows.net/"
		},
		"type": "AzureBlobFS",
		"connectVia": {
			"referenceName": "AutoResolveIntegrationRuntime",
			"type": "IntegrationRuntimeReference"
		},
		"annotations": []
	}
}
</file>
<file name="src\synapse\studio\linkedService\ls_dap_adls_01.json">
{
	"name": "ls_dap_adls_01",
	"properties": {
		"annotations": [],
		"type": "AzureBlobFS",
		"typeProperties": {
			"url": "https://devdapstdala1.dfs.core.windows.net/"
		},
		"connectVia": {
			"referenceName": "AutoResolveIntegrationRuntime",
			"type": "IntegrationRuntimeReference"
		}
	},
	"type": "Microsoft.Synapse/workspaces/linkedservices"
}
</file>
<file name="src\synapse\studio\linkedService\ls_dap_kv_core.json">
{
	"name": "ls_dap_kv_core",
	"type": "Microsoft.Synapse/workspaces/linkedservices",
	"properties": {
		"annotations": [],
		"type": "AzureKeyVault",
		"typeProperties": {
			"baseUrl": "https://dev-dap-key-core-syn.vault.azure.net/"
		}
	}
}
</file>
<file name="src\synapse\studio\linkedService\ls_dap_sql_meta.json">
{
	"name": "ls_dap_sql_meta",
	"properties": {
		"description": "SQL Database with meta data",
		"annotations": [],
		"type": "AzureSqlDatabase",
		"typeProperties": {
			"connectionString": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=dev-dap-sql-core.database.windows.net;Initial Catalog=dev-dap-sqldb-core-meta"
		},
		"connectVia": {
			"referenceName": "AutoResolveIntegrationRuntime",
			"type": "IntegrationRuntimeReference"
		}
	}
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default.json">
{
	"name": "default",
	"type": "Microsoft.Synapse/workspaces/managedVirtualNetworks"
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-custstgacct--sbx-dap-syn-01-sbxdapadls01.json">
{
	"name": "synapse-ws-custstgacct--sbx-dap-syn-01-sbxdapadls01",
	"properties": {
		"privateLinkResourceId": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-network/providers/Microsoft.Storage/storageAccounts/sbxdapadls01",
		"groupId": "dfs",
		"fqdns": [
			"sbxdapadls01.dfs.core.windows.net"
		]
	}
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-dap-key-core-syn.json">
{
	"name": "synapse-ws-dap-key-core-syn",
	"properties": {
		"privateLinkResourceId": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.KeyVault/vaults/dev-dap-key-core-syn",
		"groupId": "vault",
		"fqdns": [
			"dev-dap-key-core-syn.vault.azure.net"
		]
	}
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-dap-sql-core.json">
{
	"name": "synapse-ws-dap-sql-core",
	"properties": {
		"privateLinkResourceId": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Sql/servers/dev-dap-sql-core",
		"groupId": "sqlServer"
	}
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-dapstdala1.json">
{
	"name": "synapse-ws-dapstdala1",
	"properties": {
		"privateLinkResourceId": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-dala/providers/Microsoft.Storage/storageAccounts/devdapstdala1",
		"groupId": "dfs"
	}
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-sql--sbx-dap-syn-01.json">
{
	"name": "synapse-ws-sql--sbx-dap-syn-01",
	"properties": {
		"privateLinkResourceId": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-network/providers/Microsoft.Synapse/workspaces/sbx-dap-syn-01",
		"groupId": "sql",
		"fqdns": [
			"sbx-dap-syn-01.c87ced63-969a-42fd-8878-843403cf71eb.sql.azuresynapse.net"
		]
	}
}
</file>
<file name="src\synapse\studio\managedVirtualNetwork\default\managedPrivateEndpoint\synapse-ws-sqlOnDemand--sbx-dap-syn-01.json">
{
	"name": "synapse-ws-sqlOnDemand--sbx-dap-syn-01",
	"properties": {
		"privateLinkResourceId": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-network/providers/Microsoft.Synapse/workspaces/sbx-dap-syn-01",
		"groupId": "sqlOnDemand",
		"fqdns": [
			"sbx-dap-syn-01-ondemand.c87ced63-969a-42fd-8878-843403cf71eb.sql.azuresynapse.net"
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Classes.json">
{
	"name": "Classes",
	"properties": {
		"folder": {
			"name": "Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a30fc411-79c3-491d-919d-fae0c710276f"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# CLASSES\n",
					"\n",
					"The classes notebook contains all the classes relevant for communication to the SQL Meta Database and logging purposes. It contains 4 classes, related to the logging tables in the SQL Meta Databases:\n",
					"- SQLServerConnection: Class that will establish a connection to a database and execute queries on this database\n",
					"- Plan: Class that contains all the necessary functionalities that need to occur against a plan (logging, etc.)\n",
					"- Task: Class that contains all the necessary functionalities that need to occur against a task (logging, file-collection, etc.)\n",
					"- File: Class that contains all the necessary functionalities that need to occur against a source file (logging, moving, etc.)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Script contents:\n",
					"1. Import the relevant packages\n",
					"2. SQLServerConnection Class\n",
					"3. Plan Class\n",
					"4. Task Class\n",
					"5. File Class"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Import packages"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Import specific methods from python-native libraries\n",
					"from notebookutils import mssparkutils\n",
					"from json import loads\n",
					"\n",
					"# Import the regex and datetime package\n",
					"import re\n",
					"import datetime\n",
					"import time\n",
					"\n",
					"# import delta.tables as dt\n",
					"# import numpy as np\n",
					"# import os\n",
					"# from pyspark.sql.functions import col, when, to_timestamp, to_date\n",
					"# from pyspark.sql import Row, DataFrame\n",
					"# import pyspark.sql as sql\n",
					"# from pyspark.sql.types import StructType"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class SQLServerConnection(object):\n",
					"\n",
					"    def __init__(self:object, env_code:str, port:int, debug:bool=False) -&gt; object:\n",
					"        \"\"\"\n",
					"        Initialize a set of class-arguments related to SQL Server connections: server_url and linked_service\n",
					"        Validate the input parameters before creating the arguments\n",
					"\n",
					"        :param env_code: The environment code for the SQL Server (dev, int, tst, acc, prd)\n",
					"        :param port: The port used to connect to the SQL Server (1433)\n",
					"        :param debug: Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"        \"\"\"\n",
					"\n",
					"        if debug: print(f\"[Classes:SQLServerConnection] : Initialize SQL Server Connection arguments...\")\n",
					"        self.debug:bool = debug\n",
					"\n",
					"        # Before variable initialisation: Validate that the arguments contain expected values\n",
					"        if self.debug: print(f\"[Classes:SQLServerConnection]: Validate environment code: {env_code}\")\n",
					"        self.validate_env_argument(environment_code=env_code)\n",
					"        if self.debug: print(f\"[Classes:SQLServerConnection]: Validate port: {port}\")\n",
					"        self.validate_port_argument(port_argument=port)\n",
					"\n",
					"        # Set the relevant member-values\n",
					"        # Dev-Info: The class is using semi-static values for the members. There has been no need to connect to servers other than the SQL Meta Database.\n",
					"        #   If there would be a need for this, the class can be made more generic by replacing these semi-static values by arguments that are passed to the class\n",
					"        self.server:str = f\"{env_code}-dap-sql-core.database.windows.net\"\n",
					"        self.database:str = f\"{env_code}-dap-sqldb-core-meta\"\n",
					"        \n",
					"        # Initiliaze the member-values for the server_url and related linked_service\n",
					"        # Dev-Info: The linked_service variable will be used to retrieve access tokens to the server\n",
					"        self.server_url:str = f\"jdbc:sqlserver://{self.server}:{port};database={self.database}\"\n",
					"        self.linked_service:str = \"ls_dap_sql_meta\"\n",
					"        \n",
					"        # Validate that the linked_service is an allowed value\n",
					"        if self.debug: print(f\"[Classes:SQLServerConnection]: Validate linked service: {self.linked_service}\")\n",
					"        self.validate_sql_linked_service()\n",
					"\n",
					"        # Initialize an empty access_token\n",
					"        # The first query executed using the SQLServerConnection will initialize an access_token using self.linked_service\n",
					"        self.access_token = None\n",
					"\n",
					"        if self.debug: print(f\"[Classes:SQLServerConnection]: Set initialisation arguments: server_url={self.server_url}; linked_service={self.linked_service}\")\n",
					"    \n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------- Methods to validate argument values  --------------------------------------------------------------\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    @staticmethod\n",
					"    def validate_env_argument(environment_code:str):\n",
					"        \"\"\"\n",
					"        Validate that the given value for environment_code is one of the expected/allowed values\n",
					"        \n",
					"        :param environment_code: Code of the environmet where the SQL Database is located (dev, int, tst, acc, prd)\n",
					"        \"\"\"\n",
					"\n",
					"        # Set the list of allowed values\n",
					"        allowed_envs = ['dev', 'int', 'tst', 'acc', 'prd']\n",
					"        # [Functions/GenericFunctions] Validate that the argument for environment_code is an item in the allowed_envs list\n",
					"        validate_argument('env_code', environment_code, allowed_envs)\n",
					"\n",
					"    @staticmethod\n",
					"    def validate_port_argument(port_argument:int):\n",
					"        \"\"\"\n",
					"        Validate that the given value for port_argument is one of the expected/allowed values\n",
					"        \n",
					"        :param port_argument: TPC port for communication with Microsoft SQL Server\n",
					"        \"\"\"\n",
					"\n",
					"        # Set the list of allowed values\n",
					"        allowed_ports = [1433]\n",
					"        # [Functions/GenericFunctions] Validate that the argument for port_argument is an item in the allowed_ports list\n",
					"        validate_argument('port', port_argument, allowed_ports)\n",
					"\n",
					"    def validate_sql_linked_service(self:object):\n",
					"        \"\"\"\n",
					"        Validate that the linked service has actually been configured\n",
					"        \"\"\"\n",
					"        # [Functions/GenericFunctions] Validate that the argument for self.linked_service has actually been configured\n",
					"        validate_linked_service(self.linked_service)\n",
					"\n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ----------------------------------------------------------- Methods to establish a connection to the SQL Server  ------------------------------------------------------\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    def set_access_token(self: object) -&gt; str:\n",
					"        \"\"\"\n",
					"        Return a new access token using the linked_service\n",
					"        \"\"\"\n",
					"\n",
					"        access_token = mssparkutils.credentials.getConnectionStringOrCreds(self.linked_service)\n",
					"        if self.debug: print(f\"[Classes:SQLServerConnection]: {self.linked_service}: New Access Token: {access_token}\")\n",
					"        return access_token\n",
					"\n",
					"    def set_sql_connection(self: object) -&gt; list:\n",
					"        \"\"\"\n",
					"        Return a new access token and a connection-object to the SQL Server\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(\"[Classes:SQLServerConnection]: Establish SQL Connection\")\n",
					"\n",
					"        # [Classes:SQLServerConnection]: Set a new access token before establishing a connection\n",
					"        access_token:str = self.set_access_token()\n",
					"        # Set a DriverManager object instance to manage the JDBC driver (-&gt; see: self.server_url)\n",
					"        # Dev-Info: Spark is run on a Java Virtual Machine (JVM), where the DriverManager-class is defined.\n",
					"        #   When there is a need to access specific underlying spark classes, this gateway structure can be used\n",
					"        driver_manager:object = spark._sc._gateway.jvm.java.sql.DriverManager\n",
					"\n",
					"        # Set a Properties object instance and pass the access_token as part of a dictionary of connection-objects\n",
					"        connection_properties:object = spark._sc._gateway.jvm.java.util.Properties()\n",
					"        connection_dictionary:dict = {\n",
					"            'AccessToken': access_token\n",
					"        }\n",
					"        connection_properties.putAll(connection_dictionary)\n",
					"\n",
					"        # Try to establish a connection to the SQL Server using the driver_manager and connection_properties instances\n",
					"        try:\n",
					"            connection:object = driver_manager.getConnection(self.server_url, connection_properties)\n",
					"        except Exception as e:\n",
					"            if re.search(r\"No suitable driver found for\", str(e)):\n",
					"                raise ValueError(f\"No suitable driver found for SQL Server {self.server_url}\")\n",
					"            elif re.search(r'The TCP/IP connection to the host.' , str(e)):\n",
					"                raise ValueError(f\"Connection to SQL Server {self.server_url} cannot be made. Validate url-value and check firewall/access settings\")\n",
					"            else:\n",
					"                raise Exception(e)\n",
					"\n",
					"        # Return the access_token string and connection object instance\n",
					"        return access_token, connection\n",
					"\n",
					"    def check_access_token(self: object) -&gt; list:\n",
					"        \"\"\"\n",
					"        Validate whether the access_token has expired yet\n",
					"        Set a new access token (and connection) if expired\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(\"[Classes:SQLServerConnection]: Check access token\")\n",
					"\n",
					"        # If a token has already been set\n",
					"        if (self.access_token != None):\n",
					"            # Validate that the access_token has not yet expired\n",
					"            tokenIsValid:bool = mssparkutils.credentials.isValidToken(self.access_token)\n",
					"\n",
					"            if not tokenIsValid:\n",
					"                if self.debug: print(\"[Classes:SQLServerConnection]: Access token expired. Setting new one\")\n",
					"                # [Classes:SQLServerConnection]: Create a new access token and connection\n",
					"                token, connection = self.set_sql_connection()\n",
					"                # Return the re-established values\n",
					"                return token, connection\n",
					"            else:\n",
					"                if self.debug: print(\"[Classes:SQLServerConnection]: Access token not expired. Return existing arguments\")\n",
					"                # Return the valid (not expired) values\n",
					"                return self.access_token, self.connection\n",
					"\n",
					"        # First time setting access token\n",
					"        else:\n",
					"            if self.debug: print(\"[Classes:SQLServerConnection]: Establishing first SQL connection\")\n",
					"            # [Classes:SQLServerConnection]: Create a new access token and connection\n",
					"            token, connection = self.set_sql_connection()\n",
					"            # Return the established values\n",
					"            return token, connection\n",
					"\n",
					"\n",
					"    def check_lifetime_db(self: object):\n",
					"        \"\"\"\n",
					"        The function will try and execute a query against the SQL Database. If the result throws an error, the database is most likely paused and will start resuming because of the query\n",
					"        The start-up time takes on average 1 minute. A query will be executed every 20 seconds, for 5 times. If the response is still invalid after this, there is most likely a different issue\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(\"[Classes:SQLServerConnection]: Resume SQL Database...\")\n",
					"\n",
					"        # Set variables for query execution\n",
					"        serverIsSleeping    = True   # Assume that the SQL database is sleeping\n",
					"        sleep_time          = 20     # Time in between sending a new calls to the SQL database\n",
					"        max_calls           = 5      # Total number of calls to send to the SQL database before failure\n",
					"        total_calls         = 0      # Current total number of calls made to the SQL database\n",
					"\n",
					"        while (serverIsSleeping):\n",
					"            total_calls += 1\n",
					"            try:\n",
					"                # [Classes:SQLServerConnection]: Execute a query to the SQL Database\n",
					"                self.execute_query('SELECT TOP 1 * FROM meta.source_configuration', primary_checks=False, expect_return=True)\n",
					"\n",
					"            except:\n",
					"                # Throw an error when total number of calls exceeds the maximum allowed calls\n",
					"                if total_calls &gt;= max_calls:\n",
					"                    if self.debug: print(f\"[Classes:SQLServerConnection]: Could not resume SQL Database {self.database} for server {self.server} (URL: {self.server_url})...\")\n",
					"                    raise ConnectionError(\"Connecting to SQL MetaDB takes too long. Validate the connection...\")\n",
					"                \n",
					"                # Sleep for {sleep_time} and retry execute_query()\n",
					"                if self.debug: print(f\"[Classes:SQLServerConnection]: No response yet after {total_calls} calls, sleeping for {sleep_time} seconds...\")\n",
					"                time.sleep(sleep_time)\n",
					"\n",
					"            else:\n",
					"                # If try was successful, the database is awake and the while-loop can be exited\n",
					"                if self.debug: print(f\"[Classes:SQLServerConnection]: Database resumed!\")\n",
					"                serverIsSleeping = False\n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods to execute SQL queries against the SQL Server\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    @staticmethod\n",
					"    def execute_query_without_return(sql_statement:object, statement:str):\n",
					"        \"\"\"\n",
					"        Use the sql_statement object instance to execute a query without expecting a return value\n",
					"        \n",
					"        :param sql_statement:   Object of createStatement() that is used to execute SQL statements and retrieve the results.\n",
					"        :param statement:       The SQL query that will be executed\n",
					"        \"\"\"\n",
					"        sql_statement.execute(statement)\n",
					"        \n",
					"\n",
					"    @staticmethod\n",
					"    def execute_query_with_return(sql_statement, statement)-&gt; list:\n",
					"        \"\"\"\n",
					"        Use the sql_statement object instance to execute a query and expect a return value\n",
					"        \n",
					"        :param sql_statement:       Object of createStatement() that is used to execute SQL statements and retrieve the results.\n",
					"        :param statement:           The SQL query that will be executed\n",
					"\n",
					"        :return row_objects:list:    A list of json objects, where an object represents a row with the name of the column and the value for that column \n",
					"\n",
					"        Example:\n",
					"            [\n",
					"                {column_name_1:row_value_11, column_name_2:row_value_12, etc.},\n",
					"                {column_name_1:row_value_21, column_name_2:row_value_22, etc.},\n",
					"                etc.\n",
					"            ]\n",
					"        \"\"\"\n",
					"\n",
					"        # Use the sql_statement object instance to execute a query\n",
					"        sql_statement.executeQuery(statement)\n",
					"        # Get the resultSet from the query\n",
					"        # Dev-Note: Check out the different between getResultSet() and fetchall()\n",
					"        query_result = sql_statement.getResultSet()\n",
					"\n",
					"\n",
					"        # Get the column names from the resulting query\n",
					"        # Dev-Note: There is only 1 resultSet expected to be returned in the current set-up\n",
					"        query_result_metadata = query_result.getMetaData()\n",
					"        column_count = query_result_metadata.getColumnCount()\n",
					"        column_names = [query_result_metadata.getColumnName(i) for i in range (1, column_count+1)]\n",
					"\n",
					"\n",
					"        row_objects = []\n",
					"        while (query_result.next()):\n",
					"            result = {column:query_result.getString(column) for column in column_names}\n",
					"            row_objects.append(result)\n",
					"        return row_objects\n",
					"\n",
					"\n",
					"\n",
					"    def execute_query(self: object, statement: str, primary_checks: bool = True, expect_return: bool = False) -&gt; list:\n",
					"        \"\"\"\n",
					"        Execute a query against the SQL Database\n",
					"\n",
					"        :param statement: The query to execute against the SQL Database\n",
					"        :param primary_checks: Boolean indicating whether to execute connection checks before trying to execute the query\n",
					"        :param expect_return: Boolean indicating whether the statement expects something to be returned from the SQL Database\n",
					"\n",
					"\n",
					"        :note expected_return:\n",
					"            Depending on the value of expected_return, one of the following methods will be executed:\n",
					"            expected_return = False:    .execute() -&gt; Executes the statement\n",
					"            expected_return = True:     .executeQuery() -&gt; Executes the statement and expects a ResultSet to be returned\n",
					"          \n",
					"            While .execute() can also be used when expecting ResultSets to be returned, it only considers a non-empty return value when the first command executed by the statement returns a ResultSet. \n",
					"                That is, if the statement calls a stored procedure where the first statement does not return a ResultSet (say, a declare-statement), the .execute()-method will say that no ResultSets\n",
					"                were returned, even though the stored procedure might contain several select-statements after the declare-statement. Therefore, the boolean value 'expected_return' will make sure the\n",
					"                correct method is called to avoid such errors.\n",
					"\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Classes:SQLServerConnection]: Execute query: {statement}\")\n",
					"\n",
					"        # If self.connection does not exist or it is None -&gt; Primary checks need to be executed\n",
					"        if not hasattr(self, 'connection') or getattr(self, 'connection', None) is None:\n",
					"            primary_checks = True\n",
					"\n",
					"        # First check if the access_token is valid and whether the database is paused before trying to execute a query\n",
					"        if primary_checks:\n",
					"            if self.debug: print(f\"[Classes:SQLServerConnection]: Execute primary checks\")\n",
					"\n",
					"            # [Classes:SQLServerConnection]: Check if access_token has expired\n",
					"            self.access_token, self.connection = self.check_access_token()\n",
					"\n",
					"            # [Classes:SQLServerConnection]: Check if database is paused\n",
					"            self.check_lifetime_db()\n",
					"            \n",
					"        \n",
					"        # Create a createStatement object instance\n",
					"        # Dev-Info: The Statement object is used to send SQL commands to the database and retrieve the results.\n",
					"        sql_statement:object = self.connection.createStatement()\n",
					"        if expect_return:\n",
					"            if self.debug: print(f\"[Classes:SQLServerConnection]: Expect return from statement: {statement}\")\n",
					"            # [Classes:SQLServerConnection]: Execute athe statement and expect a resultSet to be returned\n",
					"            result:list = self.execute_query_with_return(sql_statement, statement)\n",
					"            sql_statement.close()\n",
					"            if self.debug: print(f\"[Classes:SQLServerConnection]: Return from statement: {result}\")\n",
					"            return result\n",
					"\n",
					"        else:\n",
					"            if self.debug: print(f\"[Classes:SQLServerConnection]: Expect no return from statement: {statement}\")\n",
					"            # [Classes:SQLServerConnection]: Execute the statement\n",
					"            self.execute_query_without_return(sql_statement, statement)\n",
					"            sql_statement.close()\n",
					""
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"source": [
					"class Plan():\n",
					"\n",
					"    def __init__(self: object, plan_id: int, plan_name: str, SQLServerConnection: object, debug:bool=False) -&gt; object:\n",
					"        \"\"\"\n",
					"        Initialize an object of class 'Plan'\n",
					"        \n",
					"        :param plan_id:             ID of the plan in meta.log_plans\n",
					"        :param plan_name:           Name of the plan in meta.plan_configuration\n",
					"        :param SQLServerCOnnection: object-instance of class SQLServerConnection\n",
					"        :param debug:               Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"\n",
					"        :note:\n",
					"            This class has no real purpose as of now, but could be relevant towards the future.\n",
					"            It is already included in the framework to avoid issues in the future\n",
					"        \"\"\"\n",
					"\n",
					"        if debug: print(f\"[Classes:Plan] : Initialize Plan arguments...\")\n",
					"        self.plan_id:int = plan_id\n",
					"        self.SQLServerConnection:object = SQLServerConnection\n",
					"        self.plan_name:str = plan_name\n",
					"        self.debug:bool = debug"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Task(object):\n",
					"    def __init__(self: object, task_name: str, task_id: int, plan_id: int, SQLServerConnection: object, worker_name:str, debug:bool=False) -&gt; object:\n",
					"        \"\"\"\n",
					"        Initialize an object of class 'Task'\n",
					"\n",
					"        :param task_name:           Nmae of the task in meta.task_configuration\n",
					"        :param task_id:             ID of the task in meta.log_tasks\n",
					"        :param plan_id:             ID of the plan in meta.log_plans\n",
					"        :param SQLServerCOnnection: object-instance of class SQLServerConnection\n",
					"        :param worker_name:         Name of the worker that will execute the task\n",
					"        :param debug:               Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"        \"\"\"\n",
					"\n",
					"        if debug: print(f\"[Classes:Task] : Initialize Task arguments...\")\n",
					"        self.plan_id:int = plan_id\n",
					"        self.task_id :int= task_id\n",
					"        self.task_name:str = task_name\n",
					"        self.SQLServerConnection:object = SQLServerConnection\n",
					"        self.worker_name = worker_name\n",
					"        self.debug:bool = debug\n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods to execute SQL queries against the SQL Server: Specifically related to the task\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def start_task(self: object):\n",
					"        \"\"\"\n",
					"        Execute SQL query to start the task and set its status to 'IN PROGRESS'\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:Task] : Start Task...\")\n",
					"\n",
					"        # Set the query to be executed on the SQL Database\n",
					"        statement:str = f'EXEC meta.usp_start_task @task_id = {self.task_id}, @plan_id = {self.plan_id}'\n",
					"        \n",
					"        # [Classes:SQLServerConnection]: Execute the statement\n",
					"        self.SQLServerConnection.execute_query(statement)\n",
					"\n",
					"\n",
					"    def end_task(self: object, Success_Flag: bool, Comment: str = 'SUCCES'):\n",
					"        \"\"\"\n",
					"        Execute SQL query to start the task and set its status to 'SUCCEEDED' or 'FAILED'\n",
					"\n",
					"        :param Success_Flag: Boolean indicating whether the task has succeeded or failed\n",
					"        :param Comment:      String value giving additional information. If the status is failed, the error-message is passed through here\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:Task] : End Task...\")\n",
					"\n",
					"        # Check if the Comment-argument is a dictionary value\n",
					"        # If yes -&gt; This is an object coming from notebook [ErrorHandling] which contains \n",
					"        if isinstance(Comment, dict):\n",
					"            # If the object contains key 'custom_message':\n",
					"            # Replace all token-values (e.g. %{&lt;token&gt;}% ) by actual values\n",
					"            if Comment.get('custom_message'):\n",
					"                Comment['custom_message'] = Comment['custom_message'].replace(\"%{task_id}%\",    str(self.task_id))\n",
					"                Comment['custom_message'] = Comment['custom_message'].replace(\"%{task_name}%\",  self.task_name)\n",
					"                Comment['custom_message'] = Comment['custom_message'].replace(\"%{plan_id}%\",    str(self.plan_id))\n",
					"            # Convert the dictionary to a json-object so that SQL can deal with it\n",
					"            Comment = json.dumps(Comment)\n",
					"        else:\n",
					"            # Replace all the ' values from the Comment as this will mess up the query-statement\n",
					"            Comment:str = Comment.replace(\"\\'\", \"\")\n",
					"        # Set the query to be executed on the SQL Database\n",
					"        statement:str = f'EXEC meta.usp_end_task @task_id = {self.task_id}, @plan_id = {self.plan_id}, @success_flag = {Success_Flag}, @comment = \\'{Comment}\\''\n",
					"        \n",
					"        # [Classes:SQLServerConnection]: Execute the statement\n",
					"        self.SQLServerConnection.execute_query(statement)\n",
					"\n",
					"    def run_usp_getTaskMetadata(self: object) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Execute SQL query to get the metadata related to the task\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:Task] : Invoke meta.usp_get_task_metadata...\")\n",
					"\n",
					"        # Set the query to be executed on the SQL Database\n",
					"        statement:str = f\"exec meta.usp_get_task_metadata @task_id={str(self.task_id)}\"\n",
					"        # [Classes:SQLServerConnection]: Execute the statement\n",
					"        task_metadata:dict = self.SQLServerConnection.execute_query(statement, expect_return=True)\n",
					"\n",
					"        return task_metadata\n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods to set members of the task instance\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"\n",
					"    def get_task_metadata(self, variables:list):\n",
					"        \"\"\"\n",
					"        Set the keys and values for dictionary self.variables\n",
					"\n",
					"        :param variables: List that contains all the columns names that are expected to be returned\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Classes:Task] : Get task metadata\")\n",
					"        \n",
					"        # [Classes:Task]: Execute stored procedure meta.usp_get_task_metadata\n",
					"        task_metadata:list = self.run_usp_getTaskMetadata()\n",
					"\n",
					"        # Convert the variables-list to a dictionary where the keys are the column names and the values are still empty\n",
					"        self.variables:dict = dict.fromkeys(variables)\n",
					"\n",
					"        # For the below keys, multiple values will be returned. They will be added to a list of items\n",
					"        # For this, it is preventively specified that the value for these keys will be a list\n",
					"        list_columns:list = ['source_name', 'sink_name', 'dimension', 'data_type', 'check_name', 'config_params', 'column_info']\n",
					"        for column in list_columns:\n",
					"            self.variables[column] = []\n",
					"        \n",
					"        \n",
					"        if not task_metadata:\n",
					"            raise UnboundLocalError(f\"No task_metadata found for for task_id {str(self.task_id)}\")\n",
					"        else:\n",
					"\n",
					"            # meta.usp_get_task_metadata only returns one row of values\n",
					"            # Preventively, only the first object of the task_metadata-list will be analyzed.\n",
					"            for column_name in task_metadata[0]:\n",
					"                # meta.usp_get_task_metadata returns an object for each row (per column).  The object contains the relevant metadata for each configuration-table that has been queried\n",
					"                # Try to convert the object to a json-object \n",
					"                try: \n",
					"                    # The returned value for the column is expected to be an array of json-objects, but formatted as a string\n",
					"                    # Convert the string to the list before looping over the objects\n",
					"                    column_contents:object = loads(task_metadata[0][column_name])\n",
					"\n",
					"                    # Loop over the columns of the json-object: The value of the parent keys consists of an array of JSON-objects\n",
					"                    for columns in column_contents:\n",
					"                        # Loop over each item of the json-object \n",
					"                        for column in columns:\n",
					"                            # Make sure to only set the variables that were asked for\n",
					"                            if column in self.variables:\n",
					"                                # If the column is one of the list_columns: Use append to append the value to the list\n",
					"                                if (column in list_columns):\n",
					"                                    self.variables[column].append(columns[column])\n",
					"                                # Else: Set the value for the column-name\n",
					"                                else:\n",
					"                                    self.variables[column] = columns[column]\n",
					"                except:\n",
					"                    print(f\"Empty JSON for {column_name}\")\n",
					"    \n",
					"    def set_ingestion_variables(self: object, storage_account: str):\n",
					"        \"\"\"\n",
					"        Set all the arguments needed for file ingestion\n",
					"\n",
					"        :param storage_account:     Name of the storage account where the files are located\n",
					"\n",
					"        :return silver_path:        The location of where the delta table is located (storage_account, container_name, and table_name)\n",
					"        :return raw_format:         The data-format of the file in raw (default: parquet)\n",
					"        :return target_options:     Options related to the target-location (=delta table) that need to be taken into account: partitioning, etc.\n",
					"        :return skip_first_lines:   During ingestion, there might be a need to skip the first lines of a file, which is indicated by this member\n",
					"        \"\"\"\n",
					"\n",
					"\n",
					"        if self.debug: print(f\"[Classes:Task] : Set ingestion variables\")\n",
					"\n",
					"        self.silver_path:str        = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', storage_account, self.table_name)\n",
					"        # self.raw_format:str         = \"parquet\"\n",
					"        self.skip_first_lines:int   = self.variables['skip_first_lines']\n",
					"\n",
					"\n",
					"        # If there is a value for the target_options key, try converting it to a json. If not possible, throw a configuration error\n",
					"        # If no value: return an empty dictionary\n",
					"        if self.variables['target_options']:\n",
					"            try:\n",
					"                self.target_options:dict =   loads(self.variables['target_options'])\n",
					"            except:\n",
					"                raise ValueError(f\"Given target_options cannot be converted to JSON. Validate configuration for {self.task_name}: {self.variables['target_options']}\")\n",
					"        else:\n",
					"            self.target_options:dict = dict()\n",
					"\n",
					"\n",
					"    def set_dataset_variables(self: object, storage_account: str):\n",
					"        \"\"\"\n",
					"        Set all the arguments needed for dataset manipulations\n",
					"\n",
					"        :param storage_account:     Name of the storage account where the files are located\n",
					"\n",
					"        :return table_name:             The name of the delta table where the source file needs to be ingested into\n",
					"        :return container_name:         The name of the container where the sources files are expected\n",
					"        :return file_path:              The naming-pattern that a source file is expected to follow\n",
					"        :return sink_column_names:      The list of column names of the silver table\n",
					"        :return source_column_names:    The list of column names in the source file\n",
					"        :return column_datatypes:       The list of datatypes of the different columns\n",
					"        :return dimensions:             The list of dimensions of the different columns (PK, SCD2,...)\n",
					"        :return separator:              The column-delimiter used in the file\n",
					"        :return file_extension:         The extension of the file (csv, json, fil, txt,...)\n",
					"        :return landing_path:           The overarching path where the datasets are landed (not in detail: &lt;env_code&gt;/&lt;timestamp&gt;/&lt;datasource&gt;)\n",
					"        :return file_kind:              The type of file that is being dealth with (csv, json, zip...)\n",
					"        :return header:                 Boolean value indicating whether the dataframe is expected to have a headerline\n",
					"        :return file_pattern:       The (regex) filename-template that the task is meant to ingest\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Classes:Task] : Set dataset variables\")\n",
					"\n",
					"        self.table_name:str             = self.variables['table_name']\n",
					"        self.container_name:str         = self.variables['container_name']\n",
					"        self.file_path:str              = self.variables['source_folder']\n",
					"        self.sink_column_names:list     = self.variables['sink_name']\n",
					"        self.source_column_names:list   = self.variables['source_name']\n",
					"        self.column_datatypes:list      = self.variables['data_type']\n",
					"        self.dimensions:list            = self.variables['dimension']\n",
					"        self.column_information:list    = [json.loads(object_) for object_ in self.variables['column_info']] # Convert strings to objects\n",
					"        self.separator:str              = self.variables['column_delimiter']\n",
					"        self.file_extension:str         = self.variables['file_extension']\n",
					"        self.landing_path:str           = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.container_name, storage_account, self.file_path)\n",
					"        self.file_kind:str              = self.variables['file_kind']\n",
					"        self.file_pattern:str           = self.variables['file_pattern']\n",
					"        self.escape_character:str       = self.variables['escape_character']\n",
					"        self.quote_character:str        = self.variables['quote_character']\n",
					"        # Dev-Note: Header is a string value in the SQL DB, and will therefore be returned as 'True' or 'False'. The combat this issue, the below solution has been implemented\n",
					"        # This solution is not ideal, and the header value should be stored as a binary value in SQL instead\n",
					"        string_header:str =  str(self.variables['header'])\n",
					"        if (string_header.lower() == 'true'): # True or true\n",
					"            self.header = True\n",
					"        elif (string_header.lower() == 'false'): # False or false\n",
					"            self.header = False\n",
					"        else:\n",
					"            raise ValueError(\"Invalid boolean value for the header: {}\".format(string_header))\n",
					"\n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods to get files that need to be handled by the task instance\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    def get_files_objects(self: object, storage_account: str) -&gt; list:\n",
					"        \"\"\"\n",
					"        Recursively get a list of files from task.landing_path and create instance of class 'File'\n",
					"\n",
					"        :param storage_account: The Azure data lake storage account where the files are located\n",
					"\n",
					"        :return file_objects:   A list of object-instances of the class 'File'\n",
					"\n",
					"        :Dev-Note:\n",
					"            parameter 'storage account' could be extracted from the landing_path instead of having to pass it as a parameter\n",
					"\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:Task] : Get file objects\")\n",
					"        # [Functions/GenericFunctions] Get a list of files and folders from a specific abfss-path\n",
					"        files, folders = list_directory_content(self.landing_path, list(), list())\n",
					"        if self.debug: print(f\"[Classes:Task] : Found files: {files}\")        \n",
					"        if self.debug: print(f\"[Classes:Task] : Found folders: {folders}\")  \n",
					"        # [Functions/GenericFunctions] Filter through the file-list and return the items that (regex) match with the file_pattern\n",
					"        patterned_files:list = filter_list(full_list=files, pattern=self.file_pattern, extension=self.file_extension)\n",
					"        if self.debug: print(f\"[Classes:Task] : Filtered files: {patterned_files}\")\n",
					"        # Set a dictionary with {file-path: file-name}\n",
					"        found_files:dict = {file.path:file.name.split('.')[0]  for file in patterned_files}\n",
					"\n",
					"        # [Classes/File] Create a set of File-class objects based on the dictionary\n",
					"        file_objects:list = [\n",
					"            File(\n",
					"                task=self, \n",
					"                SQLServerConnection=self.SQLServerConnection, \n",
					"                landing_path=file, \n",
					"                filename=found_files[file], \n",
					"                timestamp_folder=file.split('/')[-2], \n",
					"                storage_account=storage_account, \n",
					"                debug=self.debug\n",
					"            ) \n",
					"            for file in found_files\n",
					"        ]\n",
					"\n",
					"        return file_objects"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class File(object):\n",
					"    # Dev-note: SQLServerConnection is already part of the 'task' object so passing it as an extra argument could be redundant\n",
					"    def __init__(self:object, task:object, SQLServerConnection:object, landing_path:str, filename:str, timestamp_folder:str, storage_account:str, debug:bool=False):\n",
					"        \"\"\"\n",
					"        Initialise a set of arguments for a File-instance and log the file in meta.log_files\n",
					"\n",
					"        :param task:                        Instance of class 'Task'\n",
					"        :param SQLServerConnection:         Instance of class 'SQLServerConnection'\n",
					"        :param landing_path:                Abfss-Path to the source file in the landing zone\n",
					"        :param filename:                    Name of the source file in the landing zone\n",
					"        :param timestamp_folder:            Name of the subfolder in which the source file is located (expected to be a timestamp)\n",
					"        :param storage_account:             Name of the storage account where the source file is located\n",
					"        :param debug:                       Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"\n",
					"        :return raw_path:                   Abfss-Path where the source file will be copied to on the raw-container\n",
					"        :return silver_path:                Abfss-Path to the Delta Table on the silver-container where the source file will be ingested into\n",
					"        :return archive_path:               Abfss-Path where the source file will be moved to after ingestion\n",
					"        :return target_source_columnnames:  Dictionary matching the column names of the Delta Table and the source file ({target_column: source_column})\n",
					"        :return checks_parameters:          List of parameters that are needed for certain check-methods \n",
					"        \n",
					"        \"\"\"\n",
					"        if debug: print(f\"[Classes:File] : Initialize File arguments: {filename}\")\n",
					"\n",
					"        # ------------------------------------------    Use arguments to create class-members   ------------------------------------------\n",
					"\n",
					"        self.task:object                = task\n",
					"        self.SQLServerConnection:object = SQLServerConnection\n",
					"        self.file_name:str              = filename\n",
					"        self.landing_path:str           = landing_path\n",
					"        self.extended_filename:str      = filename + '_' + timestamp_folder\n",
					"        self.storage_account            = storage_account\n",
					"        self.debug                      = debug\n",
					"\n",
					"\n",
					"        # ------------------------------------------    Set path-variables for file movements   ------------------------------------------\n",
					"\n",
					"        # [Functions/IngestionFunctions] Remove the extension from the filename to create the raw_path\n",
					"        # Dev-Info: The files on raw are parquet files, which will be 'part-&lt;xxx&gt;.parquet'. There is no control over the filename, and the raw_path will be to folder-directory of where the file will be landed\n",
					"        #   Therefore: In raw, the filename will be the name of the folder where the parquet file will be landed and not the actual name of the file\n",
					"        #   Removing the extension makes sure that the name of the folder is just a little bit cleaner\n",
					"        self.raw_path:str                   = remove_extension(string_with_extension=landing_path.replace(\"landing\", \"raw\"), extension_value=self.task.file_extension)\n",
					"        self.silver_path:str                = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', self.storage_account, self.task.table_name)\n",
					"        self.archive_path:str               = self.landing_path.replace(\"landing\", \"archive\")\n",
					"        self.target_source_columnnames:dict = self.get_sinksource_columnname_dictionary()\n",
					"        # For the checks, convert the list of parameters needed to execute the checks to a list of json-objects\n",
					"        # Dev-Note: Make it so that this is passed as an argument during the creation of the self.spark_dataframe_instance\n",
					"        self.checks_parameters:dict = jsons_to_dict(self.task.variables['config_params'])\n",
					"\n",
					"\n",
					"        # Execute preliminary methods \n",
					"        # [Classes/File] Log the file in the meta.log_files tables of the SQL Meta Database\n",
					"        self.log_file()\n",
					"\n",
					"\n",
					"    def create_dataframe_instance(self, source_path:str, file_kind:str, skip_lines:int=None, source_kind:str=None) -&gt; object:\n",
					"        \"\"\"\n",
					"        Return an object of class \"SparkDataFrameChecks\", which will load a dataframe into memory (during the creation of the object) \n",
					"\n",
					"        :param source_path: The exact path to the file that needs to be loaded\n",
					"        :param file_kind:   The kind of the file that will be loaded (csv, json, parquet)\n",
					"        :param skip_lines:  Optional argument, declaring the number of rows that need to be skipped at the beginning of the file\n",
					"        :param source_kind: Optional argument, specifying the kind of the original source file. Relevant parameter when executing header_checks from raw to silver\n",
					"\n",
					"        :return checks_object: Instance of class 'SparkDataFrameChecks'\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:File] : Create Checks-class object\")\n",
					"\n",
					"        # Set a list-argument specifying all the checks that need to be executed for the task on each source file/dataframe\n",
					"        check_names:list = self.task.variables['check_name']\n",
					"\n",
					"        # [Classes/File] Get a dictionary {source_name: datatype} to match the column names of the raw file with their expected datatype\n",
					"        columnname_datatypes:dict = self.get_columndatatype_dictionary()\n",
					"        column_information:list   = self.task.column_information + [None]*9 # Add a None-type for each of the technical columns\n",
					"    \n",
					"        # [Classes/Checks] Create a checks-object instance for the file to execute quality checks with \n",
					"        checks_object:object = SparkDataFrameChecks(\n",
					"            source_path         = source_path, \n",
					"            header              = self.task.header, \n",
					"            separator           = self.task.separator, \n",
					"            file_kind           = file_kind, \n",
					"            column_names        = list(columnname_datatypes.keys()), \n",
					"            data_types          = list(columnname_datatypes.values()), \n",
					"            checks              = check_names, \n",
					"            column_information  = column_information,\n",
					"            skip_lines          = skip_lines,\n",
					"            debug               = self.debug,\n",
					"            source_kind         = source_kind,\n",
					"            quote_character     = self.task.quote_character,\n",
					"            escape_character    = self.task.escape_character\n",
					"        )\n",
					"        return checks_object\n",
					"\n",
					"    \n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods for column handling: Create dictionaries with column_names and their matching data types, as well as the match between raw and silver column names\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def get_primary_key_dictionary(self) -&gt; dict:\n",
					"\n",
					"        \"\"\"\n",
					"        Generate a dictionary {sink_name: source_name}, matching the column names of the Delta Table and those of the raw data source for the Primary Keys\n",
					"        \"\"\"\n",
					"        pk_column_names:dict = {self.task.sink_column_names[index]:self.task.source_column_names[index] for index, dimension in enumerate(self.task.dimensions) if dimension == 'PK'}\n",
					"        return pk_column_names\n",
					"\n",
					"    @staticmethod\n",
					"    def get_technical_column_names() -&gt; dict:\n",
					"        \"\"\"\n",
					"        Return a dictionary {sink_column:source_column} for the technical fields of the delta table and raw data file\n",
					"        \"\"\"\n",
					"        technical_column_names:dict = {'t_load_date_raw': 't_load_date_raw', 't_load_date_silver': 't_load_date_silver', 't_extract_date': 't_extract_date', 't_update_date': 't_update_date','t_insert_date': 't_insert_date', 't_plan_id': 't_plan_id', 't_task_id': 't_task_id', 't_file_id': 't_file_id', 't_file_name': 't_file_name'}\n",
					"        return technical_column_names\n",
					"\n",
					"    @staticmethod\n",
					"    def get_technical_column_datatypes() -&gt; dict:\n",
					"        \"\"\"\n",
					"        Return a dictionary {source_column:datatype} for the technical fields of the raw data file\n",
					"        \"\"\"\n",
					"        technical_column_datatypes = {'t_load_date_raw': 'timestamp', 't_load_date_silver': 'timestamp', 't_extract_date':'timestamp','t_update_date':'timestamp','t_insert_date':'timestamp','t_plan_id': 'integer', 't_task_id': 'integer', 't_file_id': 'integer', 't_file_name': 'string'}\n",
					"        return technical_column_datatypes\n",
					"\n",
					"    def get_sinksource_columnname_dictionary(self) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Create a dictionary for the non-techincal column names: {sink_name: source_name}\n",
					"        \"\"\"\n",
					"        configured_column_names:dict = dict(zip(self.task.sink_column_names, self.task.source_column_names))\n",
					"        # [Classes/File] Get the dictionary for the techincal column names: {sink_name: source_name}\n",
					"        technical_column_names:dict = self.get_technical_column_names()\n",
					"        target_source_columnnames:dict = {**configured_column_names, **technical_column_names}\n",
					"\n",
					"        return target_source_columnnames\n",
					"\n",
					"    def get_columndatatype_dictionary(self) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Create a dictionary for the non-techincal column names: {source_name: datatype}\n",
					"        \"\"\"\n",
					"        configured_column_datatypes:dict = dict(zip(self.task.source_column_names, self.task.column_datatypes))\n",
					"        # [Classes/File] Get the dictionary for the techincal column names: {source_name: datatype}\n",
					"        technical_column_datatypes:dict = self.get_technical_column_datatypes()\n",
					"        # Merge the dictionaries\n",
					"        columnname_datatypes:dict = {**configured_column_datatypes, **technical_column_datatypes} \n",
					"\n",
					"        return columnname_datatypes\n",
					"    \n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods to log file movements in the SQL Meta Database\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def log_file(self: object):\n",
					"        \"\"\"\n",
					"        Execute stored procedure meta.usp_new_file and set the file_id argument \n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:File] : Log file in SQL Database: {self.file_name}\")\n",
					"\n",
					"        # Set the query to be executed on the SQL Database\n",
					"        statement:str = f\"\"\"Exec meta.usp_new_file @filename='{self.file_name}', @task_id={self.task.task_id}, @plan_id ={self.task.plan_id}, @extended_filename = '{self.extended_filename}', @info_message = '{self.landing_path}', @no_select = 0\"\"\"\n",
					"        # [Classes:SQLServerConnection]: Execute the statement\n",
					"        result:list = self.SQLServerConnection.execute_query(statement, expect_return=True)\n",
					"        if self.debug: print(f\"[Classes:File]: Logging result: {result}\")\n",
					"        # If the result is empty -&gt; Throw an error\n",
					"        if not result:\n",
					"            raise UnboundLocalError(f\"No file_id returned when logging file {self.extended_filename} for plan_id ({str(self.task.plan_id)}) and task_id ({str(self.task.task_id)})\")\n",
					"        \n",
					"        # Get the value for key 'file_id'\n",
					"        for key in result[0]:\n",
					"            if key == 'file_id':\n",
					"                self.file_id = result[0][key]\n",
					"\n",
					"        # Validate that the file_id member has been set\n",
					"        try:\n",
					"            self.file_id\n",
					"            if self.debug: print(f\"[Classes:File]: File ID is set: {self.file_name}:{self.file_id}\")\n",
					"        except AttributeError as e:\n",
					"            raise AttributeError(f\"There has been no file_id set for file {self.extended_filename} for plan_id ({str(self.task.plan_id)}) and task_id ({str(self.task.task_id)})\")\n",
					"\n",
					"\n",
					"\n",
					"    def update_file_activity(self: object, activity: str, success: bool, info_message: str):\n",
					"        \"\"\"\n",
					"        Execute stored procedure meta.usp_update_file_activity with the new status of the file\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Classes:File] : Update file in SQL Database: {self.file_name}:{activity}\")\n",
					"\n",
					"        # Set the query to be executed on the SQL Database\n",
					"        statement = f\"\"\"Exec meta.usp_update_file_activity @extended_filename='{self.extended_filename}', @activity={activity}, @success={success}, @info_message = '{info_message}'\"\"\"\n",
					"        \n",
					"        # [Classes:SQLServerConnection]: Execute the statement\n",
					"        self.SQLServerConnection.execute_query(statement)\n",
					"\n",
					"\n",
					"\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # Methods to move files in between the different stages: raw, silver, archive\n",
					"    # -----------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def landing_to_raw(self):\n",
					"        print(\"=========================\")\n",
					"        print(f\"[Classes:File]: Execute landing_to_raw: File={self.file_name} ({datetime.datetime.now().strftime('%H:%M:%S')})\")\n",
					"        print(\"=========================\")\n",
					"\n",
					"        # [Classes:Checks] Before executing ingestion logic: Execute quality checks for the landing-phase\n",
					"        landing_dataframe_class_instance:object = self.create_dataframe_instance(source_path=self.landing_path, file_kind=self.task.file_kind, skip_lines=self.task.skip_first_lines)\n",
					"        landing_dataframe_class_instance.start_checks(phase=\"landing\", parameters=self.checks_parameters)\n",
					"\n",
					"        # [Dataframes_v2:DataframeClass] Add the technical columns to the dataframe\n",
					"        landing_dataframe_class_instance.add_logging_columns_to_dataframe(task_id=self.task.task_id, plan_id=self.task.plan_id, file_id=self.file_id, file_name=self.file_name)\n",
					"\n",
					"        # [Dataframes_v2:DataframeClass]Write the dataframe to the raw_path and overwrite already existing files on the path\n",
					"        landing_dataframe_class_instance.write_dataframe(write_format=\"parquet\", write_mode=\"overwrite\", destination_path=self.raw_path)\n",
					"        raw_amount=landing_dataframe_class_instance.dataframe.count()\n",
					"        # [Dataframes_v2] Uncache dataframe\n",
					"        landing_dataframe_class_instance.unpersist_dataframe()\n",
					"        raw_message=create_json_object(path=self.raw_path,raw_amount=raw_amount)\n",
					"        # [Classes:File] Update the logging tables: \n",
					"        self.update_file_activity(activity=\"RAW\", success=True, info_message=raw_message)\n",
					"\n",
					"\n",
					"    def raw_to_silver(self):\n",
					"        print(\"=========================\")\n",
					"        print(f\"[Classes:File]: Execute raw_to_silver: File={self.file_name} ({datetime.datetime.now().strftime('%H:%M:%S')})\")\n",
					"        print(\"=========================\")\n",
					"\n",
					"        # [Classes:File] Get dictionary that matches the primary key columns of the delta table (sink) to those of the raw data file (source) \n",
					"        pk_column_dict:dict = self.get_primary_key_dictionary()\n",
					"        self.checks_parameters[\"primary_key_columns\"] = list(pk_column_dict.values())\n",
					"\n",
					"        # [Classes:Checks] Load the raw data file and execute the raw-phase checks using the checks_object instance\n",
					"        raw_dataframe_class_instance:object = self.create_dataframe_instance(source_path=self.raw_path, file_kind=\"parquet\", source_kind=self.task.file_kind)\n",
					"        raw_dataframe_class_instance.start_checks(phase=\"raw\", parameters=self.checks_parameters)\n",
					"        rawData = raw_dataframe_class_instance.dataframe\n",
					"        \n",
					"        # Load the delta table where the raw data file needs to be merged into\n",
					"        delta_table_class = DeltaTableClass_v2(debug=self.debug)\n",
					"        deltaTable = delta_table_class.load_delta_table(source_path=self.silver_path)\n",
					"\n",
					"        # [IngestionFunctions] Ingest the raw data file into the delta table\n",
					"        MergeFile(deltaTable=deltaTable, rawData=rawData, pk_columns_dict=pk_column_dict, column_names_dict=self.target_source_columnnames, target_options=self.task.target_options)\n",
					"        # Dev-Note: Temporary solution while still inserting empty files\n",
					"        history = deltaTable.history().filter(\"operation = 'MERGE'\").select(\"operationMetrics.numTargetRowsInserted\")\n",
					"        if history.isEmpty(): \n",
					"            num_inserted_rows=0\n",
					"            num_updated_rows=0\n",
					"            num_sourcefile_rows=0\n",
					"        else :\n",
					"            num_inserted_rows,num_updated_rows,num_sourcefile_rows=delta_table_class.count_delta_table()\n",
					"        if self.debug : print (f'the amount of inserted row is {num_inserted_rows}, the amount of updated row is {num_updated_rows}, the amount of row is {num_sourcefile_rows}')\n",
					"        # [Dataframes_v2] Uncache dataframe\n",
					"        raw_dataframe_class_instance.unpersist_dataframe()\n",
					"        silver_message=create_json_object(path=self.silver_path,row_inserted=num_inserted_rows,row_updated=num_updated_rows,source_row=num_sourcefile_rows)\n",
					"        \n",
					"        # [Classes:File] Update the logging tables:\n",
					"        self.update_file_activity(activity=\"SILVER\", success=True, info_message=silver_message)\n",
					"     \n",
					"    def landing_to_archive(self):\n",
					"        # Move the file from landing to archive\n",
					"        if self.debug: print(\"moving file\", self.landing_path, self.archive_path)\n",
					"        mssparkutils.fs.mv(src=self.landing_path, dest=self.archive_path, create_path=True, overwrite=False)\n",
					"        #remove empty folders and file called finished\n",
					"        if self.debug: print(\"remove dropfolder\", self.landing_path)\n",
					"        remove_dropfolder(self.landing_path,True)\n",
					"        # [Classes:File] Update the logging tables: \n",
					"        self.update_file_activity(activity=\"ARCHIVE\", success=True, info_message=self.archive_path)"
				],
				"execution_count": 5
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\CleanWorkspace.json">
{
	"name": "CleanWorkspace",
	"properties": {
		"folder": {
			"name": "Functions"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "599a7d0d-33c3-4eb9-99a4-fba188d958f0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Clean workspace"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Code"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_containers(env_code: str, containers: list = ['landing', 'silver', 'raw', 'archive', 'logs']):\r\n",
					"    \"\"\"\r\n",
					"    Recursively clean a set of containers from a specific storage account\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    env_code (str): The environment of where the storage account is located. This will be used to form the name of the storage account: {env_code}dapstdala1. Only the following are allowed: ['dev', 'int']\r\n",
					"    containers (list): List of the containers that need to be cleaned. Only the following are allowed: ['landing', 'silver', 'raw', 'archive', 'logs']\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Variables: Define a list of the expected/allowed containers and environments\r\n",
					"    expected_containers = ['landing', 'silver', 'raw', 'archive', 'logs']\r\n",
					"    expected_environments = ['dev', 'int']\r\n",
					"\r\n",
					"    # Validate that the parameters env_code and containers are contained in the allowed lists\r\n",
					"    if (env_code not in expected_environments):\r\n",
					"        raise ValueError(f'Not allowed to clean containers for environment {env_code}. Only dev and int cleanups are allowed')\r\n",
					"    if not set(containers).issubset(expected_containers):\r\n",
					"        unexpected_containers = set(containers) - set(expected_containers)\r\n",
					"        raise ValueError(f'List of containers ({containers}) contains one or more unexpected containers: {list(unexpected_containers)}')\r\n",
					"\r\n",
					"    # If the tests have passed: Loop over all containers and clean them recursively\r\n",
					"    for container in containers:\r\n",
					"        mssparkutils.fs.rm(dir=f'abfss://{container}@{env_code}dapstdala1.dfs.core.windows.net/', recurse=True)"
				],
				"execution_count": 50
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_delta_table(env_code: str, delta_lake: str, table_name: str):\r\n",
					"    \"\"\"\r\n",
					"    Remove a delta table from the delta lake \r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    env_code (str): The environment of where the delta lake is located. This will be used to form the name of the delta lake: {env_code}dapstdala1. Only the following are allowed: ['dev', 'int']\r\n",
					"    delta_lake (list): Name of the delta lake \"database\". Only the following are allowed: silver\r\n",
					"    table_name (str): Name of the table to delete \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Variables: Define a list of the expected/allowed containers and environments\r\n",
					"    expected_deltalake = ['silver', 'unittest']\r\n",
					"    expected_environments = ['dev', 'int']\r\n",
					"\r\n",
					"    # Validate that the parameters env_code and delta_lake are contained in the allowed lists\r\n",
					"    if (env_code not in expected_environments):\r\n",
					"        raise ValueError(f'Not allowed to clean containers for environment {env_code}. Only dev and int cleanups are allowed')\r\n",
					"\r\n",
					"    if delta_lake not in expected_deltalake:\r\n",
					"        raise ValueError(f\"Only allowed to clean delta lake 'silver', not {delta_lake}\")\r\n",
					"\r\n",
					"    # Try to remove the delta table by recursively removing all the files\r\n",
					"    try:\r\n",
					"        mssparkutils.fs.rm(dir=f'abfss://{delta_lake}@{env_code}dapstdala1.dfs.core.windows.net/{table_name}', recurse=True)\r\n",
					"    \r\n",
					"    # Catch error: If error contains 'PathNotFoundException' -&gt; Delta table does not exist, no need to throw an error\r\n",
					"    except Exception as exception:\r\n",
					"        if 'PathNotFoundException' in str(exception):\r\n",
					"                print(\"Delta table does not exist\")\r\n",
					"        else:\r\n",
					"            raise f\"clean_delta_table got an unexpected exectption: {exception}\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_table(table_name:str):\r\n",
					"    \"\"\"\r\n",
					"    Remove an external table from the lake database\r\n",
					"\r\n",
					"    Parameters:\r\n",
					"    table_name (str): Name of the table to delete \r\n",
					"\r\n",
					"    Note:\r\n",
					"    This will only remove the reference to the underlying delta table, but not the contents of the delta table itself\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"    spark.sql(\"DROP TABLE IF EXISTS silver.\"+table_name)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def clean_folder(path):\r\n",
					"    try:\r\n",
					"        mssparkutils.fs.rm(path, recurse=False)\r\n",
					"    except Exception as exception:\r\n",
					"         raise Exception(f\"The path could not be deleted {path}\") "
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Dataframes.json">
{
	"name": "Dataframes",
	"properties": {
		"description": "A collection of classes that contain logic which will alter the structure of a dataframe",
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "d2777023-c64a-4d38-804c-2aa830aea1d9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.functions import lit, col\r\n",
					"from pyspark.sql import DataFrame\r\n",
					"import datetime\r\n",
					"import delta.tables as dt\r\n",
					"import delta\r\n",
					"import pyspark\r\n",
					"from delta.tables import DeltaOptimizeBuilder, DeltaTable"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"class CalculatedFieldHeaders:\r\n",
					"    def __init__(self: object, path: str, source_folder: str, destination_folder:str, hasHeader: bool, column_delimiter: str, file_extension: str, columns: list, start_index: int, end_index: int = None):\r\n",
					"        self.path = path\r\n",
					"        self.preprocess_path = path.replace(source_folder, destination_folder)\r\n",
					"        \r\n",
					"        self.header = hasHeader\r\n",
					"        self.column_delimiter = column_delimiter\r\n",
					"        self.file_extension = file_extension\r\n",
					"\r\n",
					"        self.columns = columns\r\n",
					"        \r\n",
					"        self.start_index = start_index\r\n",
					"        self.end_index = end_index\r\n",
					"        \r\n",
					"        # Execute some checks after creating your object\r\n",
					"        self.object_checks()\r\n",
					"\r\n",
					"    def object_checks(self):\r\n",
					"        # Do some checks before allowing full init of the CalculatedFieldHeaders class-object\r\n",
					"        # Dev-note: Currently, we only expect \"False\", but in the future it's also possible that there will be a \"True\"-case\r\n",
					"        # Dev-note: So, this needs to be updated in the future\r\n",
					"        if self.header != \"False\":\r\n",
					"            # The header can't be anything except for 'False'. Else the whole logic will fail\r\n",
					"            raise Exception(f\"CalculatedFieldHeaders: Dataframe header option was set to '{self.header}' for '{self.path}'. Expected: 'False'\")\r\n",
					"\r\n",
					"        if not self.end_index:\r\n",
					"            # If the end index is not provided (end_index == None), then set the end_index == last index in the column list\r\n",
					"            self.end_index = len(self.columns) - 1\r\n",
					"        elif self.end_index &gt; (len(self.columns) - 1):\r\n",
					"            # The end_index cannot be bigger than the last index of the configured columns\r\n",
					"            raise IndexError(f\"CalculatedFieldHeaders: The end index '{self.end_index}' to select the secondary columns cannot be higher than the last index of the configured columns '{len(self.columns) - 1}'\")\r\n",
					"        elif self.end_index &lt; self.start_index:\r\n",
					"            # The end_index cannot be smaller than the starting index, to avoid confusion (-1 would return the second to last column, but it's too tricky for people who don't understand how Python works)\r\n",
					"            # Also: Working with negative indexes will break some logic, so just keep 'em positive (OutOfBounds errors, potentially)\r\n",
					"            raise IndexError(f\"CalculatedFieldHeaders: The end index '{self.end_index}' cannot be smaller than the starting index '{self.start_index}'\")\r\n",
					"\r\n",
					"        if self.start_index &lt; 0:\r\n",
					"            # The start index cannot be smaller than 0 (don't start looking from the back of the columns)\r\n",
					"            raise IndexError(f\"CalculatedFieldHeaders: The start index '{self.start_index}' to select the secondary columns cannot be lower than 0\")\r\n",
					"\r\n",
					"    def convertFirstRowToCols(self: object):\r\n",
					"        ## This method extract the first row data of each dataframe, and adds it as an extra column to the same dataframe\r\n",
					"        ## The columns have the same value for each row, which can be found on the first row of the original dataframe\r\n",
					"\r\n",
					"        # Create the dataframe you're going to work with\r\n",
					"        df = spark.read.load(self.path, header=self.header, sep=self.column_delimiter, format=self.file_extension)\r\n",
					"\r\n",
					"        # See how many primary columns you have / need\r\n",
					"        ## First get the count of the secondary columns, so you know how much you have left (= primary columns)\r\n",
					"        count_secondary_columns = len(self.columns[self.start_index:self.end_index+1]) # Get the count of the primary columns (add +1, since it's EXCL that number)\r\n",
					"        secondary_columns = self.columns[-count_secondary_columns:] # Get the secondary columns (= extra columns you're going to add)\r\n",
					"\r\n",
					"        ## Then, if you know how many secondary columns you have, you can extract the primary columns\r\n",
					"        ### Primary columns count will always be the amount of columns + 1, (e.g. 'count_primary_columns is 5, it will take the first 4 columns)\r\n",
					"        count_primary_columns = len(self.columns) - count_secondary_columns # Based on the count of the secondary colums, you know how many 'primary columns' you expect\r\n",
					"        df = df.select(df.columns[0:count_primary_columns]) # Get the primary columns out of your dataset (makes sure that trailing column delimiters are filtered out)\r\n",
					"        \r\n",
					"        primary_columns = self.columns[0:count_primary_columns] # Then, take the primary columns out of the full list &amp; add it to the df (right sequence in config files needed)\r\n",
					"        # count_primary_columns = len(df.columns) # See how many 'primary columns' the dataframe has (= amount of columns before adding extra ones via this function)\r\n",
					"        df = df.toDF(*primary_columns) # The header == 'False', so we need to paste the headers on the dataframe\r\n",
					"\r\n",
					"        # print(df.columns)\r\n",
					"        # print(count_secondary_columns)\r\n",
					"        # print(len(self.columns))\r\n",
					"\r\n",
					"        # Map the extra headers now to the first row data\r\n",
					"        ## Extract the first row data (end_index +1, since the number is EXCL that index)\r\n",
					"        first_row_data = list(df.collect()[0][self.start_index:self.end_index+1])\r\n",
					"        \r\n",
					"        ## Then, save them in a dictionary\r\n",
					"        mapped_headers = dict(zip(secondary_columns, first_row_data))\r\n",
					"\r\n",
					"        ## Now, add them to the dataframe as a new column with the same value for all available rows\r\n",
					"        for key in list(mapped_headers.keys()):\r\n",
					"            df = df.withColumn(key, lit(mapped_headers[key]))\r\n",
					"            \r\n",
					"        ## Dynamically remove the first row from the dataframe (so you can re-use this function)\r\n",
					"        ## 0 -&gt; First row only in this case\r\n",
					"        ## Found in: IngestionFunctions\r\n",
					"        df = removeRowsDF(df, [0])\r\n",
					"\r\n",
					"        # Save the dataframe (after row conversion)\r\n",
					"        self.saveDataframe(df)\r\n",
					"            \r\n",
					"        df.show()\r\n",
					"\r\n",
					"    def saveDataframe(self, dataframe: DataFrame):\r\n",
					"        # Dev-note: This seems really great to have as an ingestion function --&gt; OOPS\r\n",
					"        # Dev-note: Pull it away from this class and put it as an IngestionFunction\r\n",
					"        # Write the whole dataset again to the storage account\r\n",
					"        ## coalesce(1) -&gt; Make sure it's written to one file, use same format &amp; delimiter as original dataset, and save it inside the folder with the name of the file\r\n",
					"        # print(\"Dataframe inside function:\")\r\n",
					"        dataframe.coalesce(1).write.format(self.file_extension).option(\"delimiter\", f\"{self.column_delimiter}\").save(f'{remove_extension(self.preprocess_path)}')\r\n",
					"        \r\n",
					"        # Now, the file is saved in a folder with the with the name of the original file (without the extension)\r\n",
					"        # Inside that folder, is a part-... file with the self.file_extension, so we have to rename that one\r\n",
					"        # You can't immediately give the file the proper name, so you have to loop over it\r\n",
					"        ## Get all the contents from the folder you've just written to\r\n",
					"        ## Will contain '__SUCCESS' and then the 'part-...' file -&gt; the last one you're trying to rename\r\n",
					"        saved_files, saved_folders = list_directory_content(remove_extension(self.preprocess_path), list(), list())\r\n",
					"\r\n",
					"        ## Go over the contents\r\n",
					"        for sub_file in saved_files:\r\n",
					"            # If it is a file with the right extension, go and rename it\r\n",
					"            if sub_file.isFile and sub_file.name.endswith(f'.{self.file_extension}'):\r\n",
					"                # First, get the name of the parent folder the file is in, this will be the original folder name\r\n",
					"                original_file_name = get_parent_folder(f'{remove_extension(self.preprocess_path)}/{sub_file.name}')\r\n",
					"\r\n",
					"                # Then, just overwrite the old path (1) with the new path (2)\r\n",
					"                mssparkutils.fs.mv(sub_file.path, f'{remove_extension(self.preprocess_path)}/{original_file_name}.{self.file_extension}', False)\r\n",
					"            else:\r\n",
					"                # Remove the '__SUCCESS' file\r\n",
					"                mssparkutils.fs.rm(sub_file.path, False)\r\n",
					"\r\n",
					"# df_object = CalculatedFieldHeaders(path=\"abfss://unittest@devdapstdala1.dfs.core.windows.net/finwin/prd/timestamp/mock_firstrow_to_headers_trailing_sep.fil\", source_folder='finwin', destination_folder='finwin/preprocess', hasHeader=\"False\", column_delimiter=\"\\t\", file_extension=\"csv\", columns=[\"FK_CONNECTION\", \"FK_GAME\", \"CAT_CLASS\", \"NB_PAYMENT\", \"AMT_PAYMENT\", \"X_DT_EXTRACT\", \"X_DT_TRANSACTION\", \"DRAWNUMBER\"], start_index=1, end_index=3)\r\n",
					"# df_object.convertFirstRowToCols()"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (\"unittest\", \"devdapstdala1\", \"Modules_Dataframes\")\r\n",
					"# test = filter_directory_content(path, \"*\")\r\n",
					"# # print(test)\r\n",
					"\r\n",
					"# df = spark.read.load(\"abfss://unittest@devdapstdala1.dfs.core.windows.net/Modules_Dataframes/mock_firstrow_to_headers_trailing_sep.fil\", header=\"False\", sep='\\t', format=\"csv\")\r\n",
					"# columns = [\"FK_CONNECTION\", \"FK_GAME\", \"CAT_CLASS\", \"NB_PAYMENT\", \"AMT_PAYMENT\", \"X_DT_EXTRACT\", \"X_DT_TRANSACTION\", \"DRAWNUMBER\"]\r\n",
					"# secondary_columns_len = len(columns[1:3+1])\r\n",
					"# secondary_columns = columns[-secondary_columns_len:]\r\n",
					"# # print(secondary_columns)\r\n",
					"# # print(columns[1:3+1])\r\n",
					"\r\n",
					"# primary_cols_count = len(columns) - secondary_columns_len\r\n",
					"# # print(primary_cols_count)\r\n",
					"\r\n",
					"# df = df.select(df.columns[0:primary_cols_count])\r\n",
					"# df = df.toDF(*columns[0:primary_cols_count])\r\n",
					"\r\n",
					"# # print(primary_cols_count)\r\n",
					"# # print(len(df.columns))\r\n",
					"# # print(secondary_columns_len)\r\n",
					"# df.show()\r\n",
					"# # df.select(df.columns[:5]).show()"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class DataframeClass:\r\n",
					"    # Define the set of arguments for each class-object\r\n",
					"    def __init__(self, dataframe):\r\n",
					"        self.dataframe = dataframe\r\n",
					"        \r\n",
					"    # Add a set of literal columns to the dataframe\r\n",
					"    # The columns are expected to be part of all delta tables and should improve tracking and logging\r\n",
					"    def add_logging_columns(self, task_id, plan_id, file_id, file_name):\r\n",
					"\r\n",
					"        # Get the current datetime -&gt; This will be the datetime given to the load_date-column\r\n",
					"        current_time = datetime.datetime.now()\r\n",
					"\r\n",
					"        # Add the literal columns to the dataframe: load_date, plan_id, task_id, file_id\r\n",
					"        self.dataframe = (self.dataframe\r\n",
					"            .withColumn('t_load_date_raw',    lit(current_time).cast(sql.types.TimestampType()))\r\n",
					"            .withColumn('t_load_date_silver', lit(None).cast(sql.types.TimestampType()))\r\n",
					"            .withColumn('t_plan_id',          lit(plan_id).cast(sql.types.IntegerType()))\r\n",
					"            .withColumn('t_task_id',          lit(task_id).cast(sql.types.IntegerType()))\r\n",
					"            .withColumn('t_file_id',          lit(file_id).cast(sql.types.IntegerType()))\r\n",
					"            .withColumn('t_file_name',        lit(file_name).cast(sql.types.StringType()))\r\n",
					"        )\r\n",
					"\r\n",
					"        # Return the enhanced dataframe\r\n",
					"        return self.dataframe\r\n",
					""
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class DeltaTableOptimizeClass:\r\n",
					"    # Class defined for methods related to DeltaTable-modifications\r\n",
					"    # Example: Optimizations\r\n",
					"    def __init__(self:object, table_name:str, env_code:str, debug:bool=False) -&gt; pyspark.sql.dataframe.DataFrame :\r\n",
					"        if debug: print(f\"[Dataframes:DeltaTableOptimizeClass] Initialize object for {table_name} for environment {env_code}\")\r\n",
					"        self.table_name = table_name\r\n",
					"        self.delta_table_path = f'abfss://silver@{env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\r\n",
					"        \r\n",
					"        # Load the delta table from the silver-container\r\n",
					"        self.delta_table = dt.DeltaTable.forPath(spark, self.delta_table_path)\r\n",
					"        \r\n",
					"        self.debug=debug\r\n",
					"\r\n",
					"    def optimize_deltatable(self):\r\n",
					"        # Execute the .optimize() method on the delta table\r\n",
					"        # This will create a DeltaOptimizeBuilder-object instance, which can be used for optimization methods\r\n",
					"        if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass]: Optimization object for {self.table_name}...\")\r\n",
					"        self.delta_table_optimized = self.delta_table.optimize()\r\n",
					"        if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass]: Optimized Delta table {self.delta_table_optimized}...\")\r\n",
					"\r\n",
					"\r\n",
					"    def compact_deltatable(self):\r\n",
					"        # Execute the .executeCompaction() method on the DeltaOptimizeBuilder-object\r\n",
					"        # Dev-Info: This method will group together multiple smaller files into one bigger file based on the limits set in the configuration\r\n",
					"        #   The default limit of a spark session configuration is 1GB.\r\n",
					"        #    executeCompaction() will use a bin-packing method to sort the data in files of an equal size, respecting the 1GB limit\r\n",
					"\r\n",
					"        # First: Validate that the DeltaOptimizeBuilder-object exists as a class-member\r\n",
					"        if hasattr(self, \"delta_table_optimized\"):\r\n",
					"            if isinstance(self.delta_table_optimized, DeltaOptimizeBuilder):\r\n",
					"                if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass]: Compact delta table {self.table_name}...\")\r\n",
					"                # Execute compaction on the delta table\r\n",
					"                compaction_metrics:object = self.delta_table_optimized.executeCompaction()\r\n",
					"            else:\r\n",
					"                raise TypeError(\"[Dataframes:DeltaTableOptimizeClass] delta_table_optimized member is not of class DeltaOptimizeBuilder for compaction\")\r\n",
					"        \r\n",
					"        else:\r\n",
					"            raise ValueError(\"[Dataframes:DeltaTableOptimizeClass] Cannot execute compaction when optimize object does not exist\")\r\n",
					"\r\n",
					"        # Print a select set of metrics that give an initial insight in the compaction\r\n",
					"        if self.debug: compaction_metrics.select(\"path\", \"metrics.numFilesAdded\", \"metrics.numFilesRemoved\").show(truncate=False)\r\n",
					"\r\n",
					"    def z_order_deltatable(self, z_order_columns:list):\r\n",
					"        # Execute the .executeZOrderBy() method on the DeltaOptimizeBuilder-object\r\n",
					"        # Dev-Info: This method will cluster a set of rows that belong together within a file\r\n",
					"        #   ZOrder provides clustering of related data inside the files (for a specific partition, if applicable) that may contain multiple possible values for given column.\r\n",
					"        #   This will also allow for data skipping when querying the data, improving overall performance\r\n",
					"\r\n",
					"        # First: Validate that the DeltaOptimizeBuilder-object exists as a class-member\r\n",
					"        if hasattr(self, \"delta_table_optimized\"):\r\n",
					"            if isinstance(self.delta_table_optimized, DeltaOptimizeBuilder):\r\n",
					"                if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass]: Z-order delta table {self.table_name} on column(s): {z_order_columns}...\")\r\n",
					"                # Execute Z-order on the delta table\r\n",
					"                self.delta_table_optimized.executeZOrderBy(z_order_columns=z_order_columns)\r\n",
					"            else:\r\n",
					"                raise TypeError(\"[Dataframes:DeltaTableOptimizeClass] delta_table_optimized member is not of class DeltaOptimizeBuilder for Z-order\")\r\n",
					"        \r\n",
					"        else:\r\n",
					"            raise ValueError(\"[Dataframes:DeltaTableOptimizeClass] Cannot execute Z-order when optimize object does not exist\")\r\n",
					"\r\n",
					"\r\n",
					"    def optimize_table_storage(self, z_order_columns:list=None):\r\n",
					"        # Orchestrator function that will create a DeltaOptimizeBuilder-object and execute optimization methods (compaction and z-order) on the object\r\n",
					"        if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass] Optimize table storage...\")\r\n",
					"\r\n",
					"        # Create the DeltaOptimizeBuilder-object\r\n",
					"        self.optimize_deltatable()\r\n",
					"        # Execute compaction on the object\r\n",
					"        self.compact_deltatable()\r\n",
					"\r\n",
					"        # If a list of columns is given, make sure to also execute the executeZOrderBy-method\r\n",
					"        if z_order_columns:\r\n",
					"            self.z_order_deltatable(z_order_columns)\r\n",
					"\r\n",
					"    def vacuum_deltatable(self, retention_period:int=168):\r\n",
					"        # Remove files that are not being used anymore\r\n",
					"        # This will often be related to files that have been compacted into larger files\r\n",
					"        if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass] Vacuum table storage...\")\r\n",
					"\r\n",
					"        # Dev-Note: Set \"spark.databricks.delta.vacuum.parallelDelete.enabled\" to \"true\" if vacuum takes to much time\r\n",
					"        self.delta_table.vacuum(retention_period)\r\n",
					"\r\n",
					"        # Print a select set of metrics that give an initial insight in the vacuum\r\n",
					"        # if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass]: Vacuum  metrics:\")\r\n",
					"        # if self.debug: vacuum_metrics.select(\"numDeletedFiles\", \"numVacuumedDirectories\", \"numFilesToDelete\").show(truncate=False)"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set the configuration property\r\n",
					"# spark.conf.set(\"spark.databricks.delta.optimize.maxFileSize\", \"50m\") # Default = 1GB\r\n",
					"# spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True) # -&gt; Before writing \r\n",
					"# spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)   # -&gt; After writing\r\n",
					"# spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\r\n",
					"# delta_table.optimize().executeCompaction()"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Optimized Write combines many small writes to the same partition into one larger write operation. It is an optimization performed before the data is written to your Delta table.\r\n",
					"\r\n",
					"Auto Compaction combines many small files into larger, more efficient files. It is an optimization performed after the data is written to your Delta table. You may need to perform a VACUUM operation afterwards to clean up the remaining small files."
				]
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Dataframes_v2.json">
{
	"name": "Dataframes_v2",
	"properties": {
		"folder": {
			"name": "Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "618bbc52-3f56-450a-8d63-00e2e7729465"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Dataframes_v2\n",
					"This is the second version of the Dataframes Notebook.\n",
					"The notebook contains 2 classes: SparkDataFrameClass and SparkDataFrameChecks.\n",
					"\n",
					"- SparkDataFrameClass: This class contains a set of methods that can be used to create, load, manipulate, write, etc. objects of the class pyspark.sql.DataFrames\n",
					"- SparkDataFrameChecks: This class contains a set of methods that can be used to apply quality checks on objects of the class pyspark.sql.DataFrames\n",
					"\n",
					"**Note**: When creating an object of class SparkDataFrameChecks, an object of class SparkDataFrameClass is initialized as well, which inherits all methods of the latter."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import python libraries"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"from pyspark.sql.types import StructType, StructField, ArrayType, BinaryType, BooleanType, ByteType, DateType, DecimalType, FloatType, IntegerType, LongType, StringType, TimestampType\n",
					"import pyspark.sql\n",
					"from pyspark.sql.functions import lit, col, when, to_timestamp, to_date, monotonically_increasing_id, regexp_replace, from_unixtime\n",
					"from pyspark.sql import DataFrame\n",
					"\n",
					"import datetime\n",
					"import re"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" notebooks that are referenced by the class-methods"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/GenericFunctions"
				],
				"execution_count": 50
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Class definitions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class SparkDataFrameClass():\n",
					"    \"\"\"\n",
					"    This class will contain a set of methods that can be used to load, write, create, manipulate, etc. dataframes of the class pyspark.sql.DataFrame\n",
					"    Intended use: \n",
					"        * Every time a new functionality needs to be added to the ingestion framework that is related to DataFrame manipulation, it should be added as a method to this class\n",
					"        * Every reference to a Spark dataframe should be initialized using this class, so that the dataframe-object will be able to call the methods defined in this class\n",
					"    \"\"\"\n",
					"\n",
					"    # When defining the class object, only the debug-attribute is defined for the class\n",
					"    # Depending on the situation, load_dataframe or create_dataframe can be used to initialise the self.dataframe attribute of the class-instance\n",
					"    def __init__(self, debug:bool=False):\n",
					"        self.debug:bool = debug\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------ DATAFRAME DEFINITION --------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    # Given a source_path: Load a file as a DataFrame-object\n",
					"    def load_dataframe(self, source_path:str, header:bool, separator:str, file_kind:str, skip_lines:int=None, escape_character:str='\\\\', quote_character:str='\"') -&gt; pyspark.sql.DataFrame:\n",
					"        \"\"\"\n",
					"        Load a source file into the spark session as a dataframe\n",
					"\n",
					"        :param source_path: The full abfss-path to the source file\n",
					"        :param header:      Boolean value indicating whether the source file has a headerline \n",
					"        :param separator:   The column delimiter used in the source file\n",
					"        :param file_kind:   The kind of file that is being loaded (csv, json, parquet, etc)\n",
					"        :param skip_lines:  Integer indicating the number of lines that need to be skipped at the beginning of the file\n",
					"\n",
					"        :return dataframe:  Instance of class pyspark.sql.DataFrame\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(\"[Dataframes_v2:SparkDataFrameClass] Loading dataframe from file...\")\n",
					"\n",
					"        if skip_lines:\n",
					"            if self.debug: print(\"[Dataframes_v2:SparkDataFrameClass] Executing skip_line dataframe load...\")\n",
					"            # [IngestionFunctions]: Return a dataframe with the first x lines are skipped\n",
					"            self.dataframe:object = skip_first_lines(landing_path=source_path, delimiter=separator, header=header, skiplines=skip_lines)\n",
					"\n",
					"        elif file_kind == 'json':\n",
					"            if self.debug: print(\"[Dataframes_v2:SparkDataFrameClass] Executing json dataframe load...\")\n",
					"            # self.dataframe:object = spark.read.option(\"multiline\", \"true\").json(path=source_path, lineSep=separator)\n",
					"            # By default: If file_kind == json -&gt; Use multiLine option to load the file\n",
					"            self.dataframe:object = spark.read.json(path=source_path, lineSep=separator, multiLine=True)\n",
					"\n",
					"        else:\n",
					"            if self.debug: print(\"[Dataframes_v2:SparkDataFrameClass] Executing standard dataframe load...\")\n",
					"            # Load the entire dataframe without any preliminary changes\n",
					"            # Dev-Info: Load using cache()\n",
					"            #   Spark uses the concept of delayed execution. So when executing a \"read\", this is not actually executed up until an action needs to be taken (ingest, write, etc)\n",
					"            #   Cache() materialises the dataframe during the load instead of using the delayed execution.\n",
					"            #   This is done because some column-specific actions cause issues on execution.\n",
					"            #   Example: When a string-column starts with the comma-separator and the preceeding column is empty, the comma is interpreted as a new column: \"\", \", text\" -&gt; \"\", \"\", \"text\"\n",
					"            self.dataframe:object = spark.read.load(path=source_path, header=header, sep=separator, format=file_kind, quote=quote_character, escape=escape_character).cache()\n",
					"\n",
					"        if self.debug: self.dataframe.show(10)\n",
					"        return self.dataframe\n",
					"\n",
					"    @staticmethod\n",
					"    def load_dataframe_with_schema( \n",
					"        source_path:str,\n",
					"        header:bool,\n",
					"        schema:StructType,\n",
					"        column_selection:list,\n",
					"        separator:str=',', \n",
					"        escape_character:str='\\\\', \n",
					"        quote_character:str='\"',\n",
					"        locale:str='en-US'\n",
					"        ):\n",
					"\n",
					"\n",
					"        # Dev-Note: A locale can only be used when importing csv files. Since this is not transferable to other file formats,\n",
					"        # The decision has been made not to use this function in its current format.\n",
					"        # The code is kept in the code-base, as there might be a use for the function in the future\n",
					"        # The function is unit tested (Test_Dataframes_v2), but in the current set-up\n",
					"        # To integrate this function into the framework, additional try-except blocks should be introduced to make sure the function\n",
					"        # is only executed on csv files (and potential other fitting file formats (to be discovered)). \n",
					"        file_kind = source_path.rsplit('.', 1)[1]\n",
					"        if file_kind == 'csv':\n",
					"\n",
					"            corrupt_records_column =\"_corrupt_record\"\n",
					"            column_selection += [corrupt_records_column]\n",
					"            structfield = StructField(name=corrupt_records_column, dataType=StringType(), nullable=True, metadata={corrupt_records_column: f\"Gathering of all the records where a data-casting went wrong\"})\n",
					"            schema = schema.add(structfield)\n",
					"            formatted_dataframe = spark.read.load(path=source_path, \n",
					"                schema=schema, \n",
					"                header=header, \n",
					"                sep=separator, \n",
					"                format=file_kind, \n",
					"                quote=quote_character, \n",
					"                escape=escape_character,\n",
					"                columnNameOfCorruptRecord=corrupt_records_column,\n",
					"                locale=locale\n",
					"            ).select(column_selection)\n",
					"\n",
					"            return formatted_dataframe\n",
					"\n",
					"        else:\n",
					"            raise AttributeError(\"[Dataframes_v2:SparkDataFrameClass:LoadDataframeWithSchema] Parquet files cannot be reloaded with schema as they do no support all locale versions.\")\n",
					"\n",
					"\n",
					"    # Given a list of column-objects: Create a dataframe using the attributes defined for each object\n",
					"    def create_dataframe(self:object, column_information:list, table_description:str=None) -&gt; pyspark.sql.DataFrame:\n",
					"        if self.debug: print(\"[Dataframes_v2:SparkDataFrameClass] Creating new dataframe...\")\n",
					"\n",
					"        \"\"\"\n",
					"        Parse a list of column_objects and convert it into an empty dataframe-object with a defined schema\n",
					"        \n",
					"        :param column_information:  A list of objects containing information on the metadata of each column.\n",
					"        :param table_description:   Description of the data that is in the table \n",
					"                \n",
					"        :return dataframe:          An object of class pyspark.sql.DataFrame \n",
					"\n",
					"        :example column_information\n",
					"            Structure: { \"column_sequence\": x, \"column_name\": &lt;name of the column&gt;, \"dimension\": &lt;PK,SCD2, etc.&gt;, \"data_type\": &lt;string, integer, varchar(x), etc.&gt;}\n",
					"            Example:  { \"column_sequence\": 1, \"column_name\": \"firstname\", \"dimension\": \"PK\", \"data_type\": \"varchar(100)\"}\n",
					"        \"\"\"\n",
					"\n",
					"\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Create a list of StructField-objects, which will be used to define the schema of the dataframe\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Create StructField objects with column information\")\n",
					"        structfield_objects:list = self.create_structfield_objects(column_information=column_information)\n",
					"\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Add technical fields of type StructField to the dataframe \n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Add technical fields as StructField-objects to dataframe-schema\")\n",
					"        structfield_objects:list = self.add_logging_columns_to_schema(structfields=structfield_objects)\n",
					"\n",
					"        # Create an empty dataframe as class pyspark.sql.DataFrame\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] List of StructField objects defining the dataframe schema: {structfield_objects}...\")\n",
					"        self.dataframe = spark.createDataFrame(data=[], schema=StructType(structfield_objects))  \n",
					"        if self.debug: self.dataframe.show()\n",
					"\n",
					"\n",
					"    def unpersist_dataframe(self):\n",
					"        \"\"\"\n",
					"        Remove dataframe from cache\n",
					"        \"\"\"\n",
					"        if hasattr(self, 'dataframe'):\n",
					"            self.dataframe.unpersist()\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------- Write dataframe manipulations ----------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"\n",
					"    def write_dataframe(self, write_format:str, write_mode:str, destination_path:str, partitioning_columns:list=None, column_delimiter:str=',', merge_schema:bool=False):\n",
					"        \"\"\"\n",
					"        Given an in-memory dataframe: Write the dataframe to a specific location\n",
					"\n",
					"        :param write_format:         csv, json, delta, parquet, etc.\n",
					"        :param write_mode:           overwrite or append\n",
					"        :param destination_path:     abfss-path to the location where the dataframe needs to be written to\n",
					"        :param partitioning_columns: list of column names on which to partition on\n",
					"        :param merge_schema:         Boolean, indicating whether or not the schema of the dataframe needs to be merged with the schema on the destination_path\n",
					"\n",
					"        :note Partitioning and merge_schema cannot both be true in this function. Reasoning is to prevent as much as possible schema conflicts. \n",
					"              When merge_schema is True, the dataframe is also not allowed to have any data. This is a very strict assumption to prevent possible data overrides\n",
					"        \"\"\"\n",
					"\n",
					"        if ((merge_schema) and  (write_mode!='append')):\n",
					"            raise ValueError(\"[Dataframes_v2:SparkDataFrameClass] Not allowed to mergeSchema when write_mode is not 'append'. This will overwrite the entire dataframe\")\n",
					"        if ((merge_schema) and (self.dataframe.count() != 0)):\n",
					"\n",
					"            raise ValueError(\"[Dataframes_v2:SparkDataFrameClass] Not allowed to mergeSchema when dataframe is not empty. This is a strict data override prevention mode\")\n",
					"\n",
					"        # Check if the self.dataframe attribute exists\n",
					"        if hasattr(self, 'dataframe'):\n",
					"            if partitioning_columns:\n",
					"                if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Writing partitioned dataframe to {destination_path}\")\n",
					"                # Use the partitionBy-option to write the dataframe\n",
					"                self.dataframe.write.format(write_format).mode(write_mode).partitionBy(*partitioning_columns).save(destination_path)\n",
					"            else:\n",
					"                if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Writing non-partitioned dataframe to {destination_path}\")\n",
					"                self.dataframe.write.format(write_format).mode(write_mode).option(\"delimiter\", column_delimiter).option(\"mergeSchema\", merge_schema).save(destination_path)\n",
					"        else:\n",
					"            raise AttributeError(f\"[SparkDataFrameClass] There is no dataframe attribute defined. Cannot write the dataframe to destination {destination_path}. Failing task...\")\n",
					"\n",
					"\n",
					"    @staticmethod\n",
					"    def rename_dataframe_file(source_path:str, file_extension:str):\n",
					"        \"\"\"\n",
					"        Given a source_path: Rename a file in the source_path to the name of the subfolder\n",
					"\n",
					"        :param source_path:     Abfss-path to the folder that contains the files to be renamed\n",
					"        :param file_extension:  Expected extension of the source file\n",
					"\n",
					"        :note   The method only works if there is a maximum of 2 files in the folder, one of which is a \"_SUCCESS\" file\n",
					"                If there would be multiple files, all of them would get the same name which is not possible of course\n",
					"        \"\"\"\n",
					"        saved_files, saved_folders = list_directory_content(path=source_path, files=list(), folders=list())\n",
					"        saved_file_names = [file.name for file in saved_files]\n",
					"\n",
					"        if len(saved_files) &gt; 2:\n",
					"            raise ValueError(\"Too many files in destination path. Cannot rename all files in path.\")\n",
					"\n",
					"        if '_SUCCESS' not in saved_file_names:\n",
					"            raise ValueError(\"No successfile present in path. The files have not been writen by Spark and will therefore not be renamed.\")\n",
					"\n",
					"        # Rename the file with the expected file_extension\n",
					"        for sub_file in saved_files:\n",
					"            if sub_file.isFile and sub_file.name.endswith(f'.{file_extension}'):\n",
					"                # Get the name of the folder and re-use it as the name of the file\n",
					"                subfolder_name = get_parent_folder(path=f'{source_path}/{sub_file.name}')\n",
					"                # Dev-Note: Overwrite = False -&gt; Do not overwrite any other files in the dictionary (strictly not necessary but just in case)\n",
					"                mssparkutils.fs.mv(src=sub_file.path, dest=f'{source_path}/{subfolder_name}.{file_extension}', create_path=False, overwrite=False)\n",
					"            else:\n",
					"                # Remove the '_SUCCESS' file\n",
					"                mssparkutils.fs.rm(dir=sub_file.path, recurse=False)\n",
					"\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------- Technical field manipulations ----------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def add_logging_columns_to_dataframe(self:object, task_id:int, plan_id:int, file_id:int, file_name:str):\n",
					"        \"\"\"\n",
					"        Given a dataframe: Add technical fields to the dataframe with a set of literal values\n",
					"\n",
					"        :params task_id:    ID of the task in meta.log_tasks\n",
					"        :params plan_id:    ID of the plan in meta.log_plans\n",
					"        :params file_id:    ID of the file in meta.log_files\n",
					"        :params file_name:  Name of the file where the dataframe is coming from\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Add technical fields to existing dataframe\")\n",
					"\n",
					"        # Check if the self.dataframe attribute exists\n",
					"        if hasattr(self, 'dataframe'):\n",
					"            # Get the current datetime -&gt; This will be the datetime given to the t_load_date_raw-field\n",
					"            current_time = datetime.datetime.now()\n",
					"\n",
					"            # Add technical fields with literal values to dataframe\n",
					"            self.dataframe = (self.dataframe\n",
					"                .withColumn(colName='t_load_date_raw',    col=lit(current_time).cast(TimestampType()))\n",
					"                .withColumn(colName='t_load_date_silver', col=lit(None).cast(TimestampType()))\n",
					"                .withColumn(colName='t_extract_date',     col=lit(None).cast(TimestampType()))\n",
					"                .withColumn(colName='t_update_date',      col=lit(None).cast(TimestampType()))\n",
					"                .withColumn(colName='t_insert_date',      col=lit(None).cast(TimestampType()))\n",
					"                .withColumn(colName='t_plan_id',          col=lit(plan_id).cast(IntegerType()))\n",
					"                .withColumn(colName='t_task_id',          col=lit(task_id).cast(IntegerType()))\n",
					"                .withColumn(colName='t_file_id',          col=lit(file_id).cast(IntegerType()))\n",
					"                .withColumn(colName='t_file_name',        col=lit(file_name).cast(StringType()))\n",
					"            )\n",
					"        else:\n",
					"            raise AttributeError(\"[SparkDataFrameClass] There is no dataframe attribute defined. Cannot add logging columns. Failing task...\")\n",
					"\n",
					"\n",
					"    @staticmethod\n",
					"    def add_logging_columns_to_schema(structfields:list) -&gt; list:\n",
					"        \"\"\"\n",
					"        Given a list of StructField-objects for a schema, add StructField-objects for technical fields\n",
					"\n",
					"        :param structfields: A list of instances of class 'StructField', which is used to define a column in a spark dataframe\n",
					"        \"\"\"\n",
					"\n",
					"        structfields.append(StructField(name='t_load_date_raw',     dataType=TimestampType(),   nullable=True,  metadata={'t_load_date_raw':    f\"Date of when the row was loaded into the raw layer table\"}))\n",
					"        structfields.append(StructField(name='t_load_date_silver',  dataType=TimestampType(),   nullable=True,  metadata={'t_load_date_silver': f\"Date of when the row was loaded into the delta table\"}))\n",
					"        structfields.append(StructField(name='t_extract_date',      dataType=TimestampType(),   nullable=True,  metadata={'t_extract_date':     f\"Date of when the row was extracted from the source system\"}))\n",
					"        structfields.append(StructField(name='t_update_date',       dataType=TimestampType(),   nullable=True,  metadata={'t_update_date':      f\"Date of when the row was extracted from the source system\"}))\n",
					"        structfields.append(StructField(name='t_insert_date',       dataType=TimestampType(),   nullable=True,  metadata={'t_insert_date':      f\"Date of when the row was extracted from the source system\"}))\n",
					"        structfields.append(StructField(name='t_file_name',         dataType=StringType(),      nullable=True,  metadata={'t_load_date':        f\"Name of the file where the data is coming from\"}))\n",
					"        structfields.append(StructField(name='t_plan_id',           dataType=IntegerType(),     nullable=False, metadata={'t_plan_id':          f\"The id of the plan in the meta.log_plans table\"}))\n",
					"        structfields.append(StructField(name='t_task_id',           dataType=IntegerType(),     nullable=False, metadata={'t_task_id':          f\"The id of the task in the meta.log_tasks table\"}))\n",
					"        structfields.append(StructField(name='t_file_id',           dataType=IntegerType(),     nullable=False, metadata={'t_file_id':          f\"The id of the file in the meta.log_files table\"}))\n",
					"\n",
					"        return structfields\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------- Schema manipulations -------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def create_structfield_objects(self:object, column_information:list) -&gt; list:\n",
					"        \"\"\"\n",
					"        Create a list of StructField-objects that will be used to initialize a DataFrame schema\n",
					"\n",
					"        :param column_information: A list of objects containing information on the metadata of each column.\n",
					"        :example\n",
					"            Structure: { \"column_sequence\": x, \"column_name\": &lt;name of the column&gt;, \"dimension\": &lt;PK,SCD2, etc.&gt;, \"data_type\": &lt;string, integer, varchar(x), etc.&gt;}\n",
					"            Example:  { \"column_sequence\": 1, \"column_name\": \"firstname\", \"dimension\": \"PK\", \"data_type\": \"varchar(100)\"}\n",
					"\n",
					"        :result structfield_objects: A list of StructField objects\n",
					"        :example\n",
					"            Structure: StructField(name, datatype, nullable, metadata)\n",
					"            Example: StructField(\"firstname\", StringType(), True, {'__CHAR_VARCHAR_TYPE_STRING': \"varchar(100)})\n",
					"\n",
					"        :note\n",
					"            A dictionary (datatypes) is initialized in the configure_datatypes() method. This dictionary maps a set of datatypes to a spark datatype\n",
					"            Each column_object has a reference to a datatype. If the reference is varchar, additional metadata is added to the StructField instance.\n",
					"            This is because PySpark does not, by default, set a \"length\" limitation on StringType-datatypes. The additional metadata forces these constraints.\n",
					"        \"\"\"\n",
					"\n",
					"        # Derive a dictionary of pyspark.sql.types functions to match the given datatypes\n",
					"        # Structure: { &lt;datatype_reference&gt;: &lt;pyspark_datatype_reference&gt; }\n",
					"        # Example: { 'string': pyspark.sql.type.StringType(), 'integer': pyspark.sql.type.IntegerType()}\n",
					"        table_data_types:list = [column_object['data_type'] for column_object in column_information]\n",
					"        configured_datatypes:dict = self.configure_datatypes(table_data_types=table_data_types)\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Datatypes configured for the dataframe: {configured_datatypes}...\")\n",
					"        \n",
					"        \n",
					"        # Dev-Note: Before or during for-loop: Check if the objects are valid -&gt; Contain keys column_name, data_type, and dimension\n",
					"        structfield_objects = list()\n",
					"        for column_object in column_information:\n",
					"            \n",
					"            # If the data_type of the column is a varchar, additional metadata needs to be added to the column-definition\n",
					"            # Dev-Info: Delta does not force \"limits\" on varchar-datatypes and converts everything to a string (which is similar to varchar(max)). \n",
					"            #   To have more control of the data that goes into the delta table, a constraint is added using the '__CHAR_VARCHAR_TYPE_STRING' option which will enforce constraints on the length of the values added to a column\n",
					"            varchar_pattern = r'^varchar\\((\\d{1,2})\\)$'\n",
					"            if re.match(varchar_pattern, column_object['data_type']):\n",
					"                structfield_objects.append(StructField(name=column_object['column_name'],dataType=configured_datatypes[column_object['data_type']],nullable=True, metadata={column_object['column_name']: f\"metadata about the column {column_object['column_name']}\", '__CHAR_VARCHAR_TYPE_STRING': column_object['data_type']}))\n",
					"            \n",
					"            # If no special case: Add the column without additional metadata-cases\n",
					"            else:\n",
					"                structfield_objects.append(StructField(name=column_object['column_name'],dataType=configured_datatypes[column_object['data_type']],nullable=True, metadata={column_object['column_name']: f\"metadata about the column {column_object['column_name']}\"}))\n",
					"\n",
					"        return structfield_objects\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------- Datatype manipulations -----------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    @staticmethod\n",
					"    def add_varchar_datatype(varchar_value:str, datatypes:dict) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Add a varchar-datatype to the datatypes dictionary and match it with datatype StringType()\n",
					"\n",
					"        :param varchar_value:   String-value looking like 'varchar(&lt;xx&gt;)' that needs to be added to the datatypes dictionary\n",
					"        :param datatypes:       Dictionary of datatypes and their matching spark-datatype function\n",
					"        :example\n",
					"            {'string': StringType(), 'int': IntegerType(), etc.}\n",
					"\n",
					"        :note\n",
					"            For varchar(x), there is no direct mapping available to a spark datatype\n",
					"            To combat this, the varchar-column will be added as a StringType but an additional constraint will be added to these columns\n",
					"            This function will add the varchar-datatype to the datatypes-dictionary as {varchar(x): StringType}\n",
					"        \"\"\"\n",
					"\n",
					"        # Regex pattern: The varchar is expected to look like 'varchar(&lt;xx&gt;)'\n",
					"        pattern = r\"varchar\\(\\d+\\)\"\n",
					"\n",
					"        # Varchar(max) is already part of the datatypes dictionary so no special case needs to be added -&gt; spark-datatype: StringType()\n",
					"        if varchar_value == 'varchar(max)':\n",
					"            return datatypes\n",
					"\n",
					"        # Check if there is a match with the regex\n",
					"        elif re.match(pattern=pattern, string=varchar_value):\n",
					"            # If there is a match, add the value to the datatypes-dictionary with a StringType() datatype\n",
					"            # During the assignment, the value for 'varchar_value' will be added as metadata about the column\n",
					"            datatypes[varchar_value] = StringType()\n",
					"            return datatypes\n",
					"\n",
					"        # If there is also no match: The varchar-value has been configured incorrectly -&gt; Error\n",
					"        else:\n",
					"            raise ValueError(f'The varchar value {varchar_value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...')\n",
					"\n",
					"        \n",
					"    @staticmethod\n",
					"    def add_decimal_datatype(decimal_value:str, datatypes:dict) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Add a decimal-datatype to the datatypes dictionary and match it with datatype DecimalType(&lt;xx&gt;, &lt;yy&gt;)\n",
					"\n",
					"        :param varchar_value:   String-value looking like 'decimal(&lt;xx&gt;, &lt;yy&gt;)' that needs to be added to the datatypes dictionary\n",
					"        :param datatypes:       Dictionary of datatypes and their matching spark-datatype function\n",
					"        :example\n",
					"            {'string': StringType(), 'int': IntegerType(), etc.}\n",
					"\n",
					"        :note\n",
					"            Instead of having to foresee every possible instance of DecimalType(&lt;xx&gt;, &lt;yy&gt;) in datatypes, \n",
					"            this method will only add the instances that are needed for a specific delta table or source file\n",
					"        \"\"\"\n",
					"\n",
					"        # Regex pattern: The decimal is expected to look like 'decimal(&lt;xx&gt;,&lt;yy&gt;)'\n",
					"        pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\n",
					"\n",
					"        # First check if the decimal contains additional parameters\n",
					"        # decimal is already part of the datatypes dictionary so no special case needs to be added -&gt; spark-datatype: DecimalType(38,4)\n",
					"        if decimal_value == 'decimal':\n",
					"            return datatypes\n",
					"\n",
					"        # Check if there is a match with the regex\n",
					"        elif re.match(pattern=pattern, string=decimal_value):\n",
					"            # If there is a match: Extract the numbers from the string\n",
					"            # Assign the numerical values as the total number of characters and the numbers past the comma\n",
					"            match = re.match(pattern=pattern, string=decimal_value)\n",
					"            num_characters = int(match.group(1))\n",
					"            num_post_comma = int(match.group(2))\n",
					"\n",
					"            # Dev-Note: Add check to make sure that num_characters does not exceed 38 as pyspark cannot handle more, and that num_characters &gt; num_post_comma\n",
					"            datatypes[decimal_value] = DecimalType(precision=num_characters, scale=num_post_comma)\n",
					"            return datatypes\n",
					"\n",
					"        # If there is also no match: The decimal-value has been configured incorrectly -&gt; Error\n",
					"        else:\n",
					"            raise ValueError(f'The decimal value {decimal_value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...')\n",
					"\n",
					"\n",
					"    def configure_datatypes(self:object, table_data_types:list) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Parse dictionary table_data_types and add all relevant datatypes to a dictionary called datatypes\n",
					"        \n",
					"        :param table_data_types:    List with all the possible datatypes that could occur in the delta table\n",
					"\n",
					"        :result datatypes:          Dictionary matching string values with a spark datatype\n",
					"        :example\n",
					"            {'string': StringType(), 'int': IntegerType(), etc.}\n",
					"\n",
					"\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Configure datatypes dictionary...\")\n",
					"\n",
					"        # Create a dictionary with all of the data types that can be read by spark\n",
					"        # The keys of the dictionary are the values used in the metadata configuration files (SQL Database)\n",
					"        # The values of the dictionary are pyspark.sql.types datatypes \n",
					"        # Dev-Info: The default definition used for decimals is: 38 characters total, with 4 after the comma. \n",
					"        #           If specifics are given in the column_info (eg. decimal(12,2)), a new input will be added to the types-dictionary to accomodate this datatype\n",
					"        # https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.DecimalType.html\n",
					"\n",
					"        datatypes = {\n",
					"            \"array\": ArrayType(StringType()), \"binary\": BinaryType(), \"boolean\": BooleanType(), \"date\": DateType(), \"string\": StringType(), \n",
					"            \"varchar(max)\": StringType(), \"timestamp\": TimestampType(), \"decimal\": DecimalType(38, 4), \"float\": FloatType(), \"byte\": ByteType(), \n",
					"            \"integer\": IntegerType(), \"int\": IntegerType(), \"long_integer\":  LongType()\n",
					"        }\n",
					"\n",
					"\n",
					"        # Some datatypes come with an additional (set of) parameters. This function will add those datatypes to the initial types-dictionary defined above.\n",
					"        for data_type in table_data_types:\n",
					"            # If the data_type contains a value with substring 'decimal' -&gt; Invoke the add_decimal_datatype function\n",
					"            if 'decimal' in data_type:\n",
					"\n",
					"                if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Adding datatype {data_type} to the datatypes-dictionary\")\n",
					"                datatypes = self.add_decimal_datatype(decimal_value=data_type, datatypes=datatypes)\n",
					"\n",
					"            # If the data_type contains a value with substring 'varchar' -&gt; Invoke the add_varchar_datatype function\n",
					"            elif 'varchar' in data_type:\n",
					"                if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Adding datatype {data_type} to the datatypes-dictionary\")\n",
					"                datatypes = self.add_varchar_datatype(varchar_value=data_type, datatypes=datatypes)\n",
					"\n",
					"            # Throw an error if the value is not part of the datatypes dictionary and there is no if-statement configured to add it to the dictionary\n",
					"            elif data_type not in datatypes.keys():\n",
					"                raise ValueError(f\"The datatype '{data_type}' has not been configured as a datatype in the current set-up.\")\n",
					"\n",
					"        return datatypes\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------- Dataframe manipulations ----------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    @staticmethod\n",
					"    def validate_column_indexes(start_index:int, end_index:int, column_names:list, header:bool):\n",
					"        \"\"\"\n",
					"        Validate that the given argument values are within a set of expected boundaries\n",
					"\n",
					"        :param start_index:     A column index where operations need to start at\n",
					"        :param end_index:       A column index where operations need to end at \n",
					"        :param column_names:    The list of column names\n",
					"        :param header:          Boolean indicating whether there is a header present\n",
					"\n",
					"        :note\n",
					"            This method is specifically written for validation for method convert_firstrow_to_columns()\n",
					"            More generic changes can be added whenever there would be a need for this case in the future\n",
					"        \"\"\"\n",
					"\n",
					"        # Dev-note: Currently, we only expect \"False\", but in the future it's also possible that there will be a \"True\"-case\n",
					"        if header != False:\n",
					"            # The header can't be anything except for 'False'. Else the whole logic will fail\n",
					"            raise Exception(f\"[SparkDataFrameClass]: Dataframe header option was set to '{header}'. Expected: 'False'\")\n",
					"\n",
					"        if end_index &gt; (len(column_names) - 1):\n",
					"            # The end_index cannot be bigger than the last index of the configured columns\n",
					"            raise IndexError(f\"[SparkDataFrameClass]: The end index '{end_index}' to select the secondary columns cannot be higher than the last index of the configured columns '{len(column_names) - 1}'\")\n",
					"        \n",
					"        if end_index &lt; start_index:\n",
					"            # The end_index cannot be smaller than the starting index, to avoid confusion (-1 would return the second to last column, but it's too tricky for people who don't understand how Python works)\n",
					"            raise IndexError(f\"[SparkDataFrameClass]: The end index '{end_index}' cannot be smaller than the starting index '{start_index}'\")\n",
					"\n",
					"        if start_index &lt; 0:\n",
					"            # The start index cannot be smaller than 0 (don't start looking from the back of the columns)\n",
					"            raise IndexError(f\"[SparkDataFrameClass]: The start index '{start_index}' to select the secondary columns cannot be lower than 0\")\n",
					"\n",
					"        if end_index &lt; 0:\n",
					"            # The start index cannot be smaller than 0 (don't start looking from the back of the columns)\n",
					"            raise IndexError(f\"[SparkDataFrameClass]: The end index '{end_index}' to select the secondary columns cannot be lower than 0\")\n",
					"\n",
					"    # Given a dataframe: Convert (some of) the values of the first row to columns with literal values\n",
					"    def convert_firstrow_to_columns(self:object, start_index:int, end_index:int, column_names:list, header:bool, destination_path:str, file_extension:str, column_delimiter:str):\n",
					"        \"\"\"\n",
					"        Convert a set of values of the first row of a dataframe to separate, literal columns \n",
					"            Use case: The first column contains relevant information such as a date of ID. To keep this information, these values will be added to each row as a separate column (timestamp or ID in this case)\n",
					"\n",
					"        :param start_index:         Column index of where to start the operation from\n",
					"        :param end_index:           Column index of where to end the operation\n",
					"        :param column_names:        List of column names of the delta table (so already including the column names to be added as well)\n",
					"        :param header:              Boolean indicating whether a header is already present\n",
					"        :param destionation_path:   Abfss-path of where to write the dataframe to after modifications\n",
					"        :param file_extension:      Extension of the newly written file (Should ideally match with that of the original source file to avoid confusion)\n",
					"        :param column_delimiter:    Delimiter of the newly written file (Should ideally match with that of the original source file to avoid confusion)\n",
					"\n",
					"        :example:\n",
					"            Arguments: start_index = 1, end_index = 2, column_names = [firstname, lastname, address, date, employee_id], header=False\n",
					"            \n",
					"            Original dataframe:\n",
					"                999 1900-01-01 pla917 xxx\n",
					"                Simon Plancke \"Rue Belliard 25\"\n",
					"\n",
					"            Converted dataframe: \n",
					"                firstname   lastname    address             date        employee_id\n",
					"                Simon       Plancke     \"Rue Belliard 25\"   1900-01-01  pla917\n",
					"        \"\"\"\n",
					"\n",
					"        if hasattr(self, 'dataframe'):\n",
					"            if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Convert first row to separate columns...\")\n",
					"            # Before executing function: Execute some quality checks\n",
					"            \n",
					"            if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Validate column indexes...\")\n",
					"            self.validate_column_indexes(start_index=start_index, end_index=end_index, column_names=column_names, header=header)\n",
					"\n",
					"            # Dev-Info: \n",
					"            # Primary   = Columns that are already present in the dataframe (without column names)\n",
					"            # Secondary = Columns that need to be added to the dataframe\n",
					"\n",
					"            # From the list of column_names, get those that are not yet in the dataframe\n",
					"            count_secondary_columns = end_index - start_index + 1               # Get the count of the primary columns (add +1, since it's EXCL that number)\n",
					"            secondary_columns       = column_names[-count_secondary_columns:]   # Get the secondary columns (= extra columns you're going to add)\n",
					"\n",
					"            # From the list of column_names, get those that are already in the dataframe\n",
					"            ## count_primary_columns will always be the amount of columns + 1, (e.g. 'count_primary_columns is 5, it will take the first 4 columns)\n",
					"            count_primary_columns = len(column_names) - count_secondary_columns \n",
					"            primary_columns = column_names[0:count_primary_columns]\n",
					"            if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Primary columns: {primary_columns} ; Count: {count_primary_columns}\")\n",
					"            if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Secondary columns: {secondary_columns}; Count: {count_secondary_columns}\")\n",
					"\n",
					"\n",
					"            # Create a local dataframe that only contains the primary columns and add the primary column names\n",
					"            primary_column_dataframe = self.dataframe.select(self.dataframe.columns[0:count_primary_columns]) # Create a local dataframe with only the primary columns\n",
					"            primary_column_dataframe = primary_column_dataframe.toDF(*primary_columns)\n",
					"            \n",
					"            # Since the first row contains values to be added to the dataframe, extract the first row data\n",
					"            first_row_data = list(primary_column_dataframe.collect()[0][start_index:end_index+1])\n",
					"            \n",
					"            # Map the names of the columns with the literal values that need to be added: {column_name:literal_value}\n",
					"            mapped_headers = dict(zip(secondary_columns, first_row_data))\n",
					"            if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass] Mapped headers: {mapped_headers}\")\n",
					"\n",
					"            # Add the columns to the dataframe, with each row containing the same literal value\n",
					"            for key in list(mapped_headers.keys()):\n",
					"                primary_column_dataframe = primary_column_dataframe.withColumn(key, lit(mapped_headers[key]))\n",
					"                \n",
					"            # Remove the first row from the dataframe -&gt; 0: First row only in this case\n",
					"            # [IngestionFunctions]\n",
					"            self.dataframe = removeRowsDF(primary_column_dataframe, [0])\n",
					"\n",
					"\n",
					"            self.write_dataframe(write_format='csv', write_mode='overwrite', destination_path=remove_extension(string_with_extension=destination_path, extension_value=file_extension), column_delimiter=column_delimiter)\n",
					"            self.rename_dataframe_file(source_path=remove_extension(string_with_extension=destination_path, extension_value=file_extension), file_extension='csv')\n",
					"        else:\n",
					"            raise AttributeError(\"[SparkDataFrameClass] There is no dataframe attribute defined. Cannot convert first row to columns. Failing task...\")\n",
					"\n",
					"    def replace_dataframe_columns(self, replacing_dataframe:DataFrame, columns_to_replace:list):\n",
					"        \"\"\"\n",
					"        Replace one or more columns from self.dataframe by columns from another dataframe\n",
					"\n",
					"        :param replacing_dataframe: Dataframe that contains the columns to replace the self.dataframe columns by\n",
					"        :param columns_to_repalce:  List of columns that will be replaced\n",
					"        \"\"\" \n",
					"\n",
					"        # Dev-Note: This function has not been integrated in the ingestion framework as there need to be more unit testing\n",
					"        #   1. Do both dataframes actually have the same order of rows, or can the rows be read in a different order?\n",
					"        # Because of the current uncertainty over the above question, the function is not integrated.\n",
					"        # If there would be a need for this function, improvements can also be made to the current functionality of the function\n",
					"        # The funtion is being unit tested during deployments (Test_Dataframes_v2), but tests can be improved.\n",
					"\n",
					"        if self.debug: self.dataframe.show()\n",
					"        if self.debug: replacing_dataframe.show()\n",
					"        # Check: Dataframes need to be the same lenght\n",
					"        row_count_df = self.dataframe.count()\n",
					"        row_count_repl_df = replacing_dataframe.count()\n",
					"\n",
					"        if row_count_df != row_count_repl_df:\n",
					"            raise ValueError(\"[Dataframes_v2:SparkDataFrameClass] Number of rows in the original dataframe do not match the number of rows in the replace dataframe\")\n",
					"\n",
					"\n",
					"        # Get the current list of columns\n",
					"        column_list = self.dataframe.columns\n",
					"\n",
					"        # Add a row_number to the dataframes\n",
					"        self.dataframe      = self.dataframe.withColumn(\"row_num\", monotonically_increasing_id())\n",
					"        replacing_dataframe = replacing_dataframe.withColumn(\"row_num\", monotonically_increasing_id())\n",
					"\n",
					"        # drop the columns to replace from self.dataframe\n",
					"        for col_name in columns_to_replace:\n",
					"            self.dataframe = self.dataframe.drop(self.dataframe[col_name])\n",
					"\n",
					"\n",
					"        # Select the subset of columns to replace from the replace_dataframe\n",
					"        columns_to_replace += ['row_num']\n",
					"        replacing_dataframe_subset = replacing_dataframe.select(columns_to_replace)\n",
					"\n",
					"        # Use join to replace the columns for self.dataframe\n",
					"        self.dataframe = self.dataframe.join(replacing_dataframe_subset, on='row_num', how='left').select(column_list)\n",
					""
				],
				"execution_count": 51
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class SparkDataFrameChecks(SparkDataFrameClass):\n",
					"\n",
					"    def __init__(self, source_path:str, header:bool, separator:str, file_kind:str, column_names:list, data_types:list, checks:list, column_information:list,\n",
					"                skip_lines:int=None, debug:bool=False, deploy:bool=False, source_kind:str=None, escape_character:str='\\\\', quote_character:str='\"'):\n",
					"        \"\"\"\n",
					"        Initialize a SparkDataFrameChecks instance and load the dataframe object if the call is not from a deploy-stage\n",
					"\n",
					"        :param source_path:         Abfss-path to the source file that is being checked\n",
					"        :param header:              Boolean value indicating whether the source file contains a headerline\n",
					"        :param separator:           Column delimiter used in the source file\n",
					"        :param file_kind:           The type of file that is being dealth with (csv, json, parquet, etc.)\n",
					"        :param column_names:        The list of column names in the source file\n",
					"        :param data_types:          The list of datatypes of the columns\n",
					"        :param checks:              The list of data quality checks that need to be done on the source file\n",
					"        :param skip_lines:          Value indicating whether the source file contains rows to be skipped during dataframe load\n",
					"        :param debug:               Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"        :param deploy:              Boolean indicating whether the class is called in deploy-mode\n",
					"        :param source_kind:         When doing checks on raw data (parquet), what was the original type of file (csv, json, parquet, etc.)\n",
					"        :param qoute_character:     Character that is used to indicate string values in the source file\n",
					"        :param escapce_character:   Character that is used to escape special characters in the source file\n",
					"\n",
					"        :note deploy-mode\n",
					"            The deploy argument indiciates whether the instance has to load in a dataframe or whether a dataframe exists already.\n",
					"            If not deploy   -&gt; Use the other arguments to load the source file as a dataframe\n",
					"            If deploy       -&gt; Set the dataframe-argument after creating the class instance\n",
					"\n",
					"            Dev-Note it would be possible to call load_dataframe separatly from the __init__ function and remove the deploy-mode option\n",
					"        \n",
					"        \"\"\"\n",
					"        self.debug = debug\n",
					"\n",
					"        if not deploy:\n",
					"            # [Dataframes_v2:SparkDataFrameClass] Load the dataframe from the source path\n",
					"            self.dataframe = self.load_dataframe(\n",
					"                source_path=source_path, \n",
					"                header=header, \n",
					"                separator=separator, \n",
					"                file_kind=file_kind, \n",
					"                skip_lines=skip_lines, \n",
					"                escape_character=escape_character, \n",
					"                quote_character=quote_character\n",
					"            )\n",
					"\n",
					"        # Set class-member values\n",
					"        self.source_path = source_path\n",
					"        self.header:bool = header\n",
					"        self.separator = separator\n",
					"        self.file_kind = file_kind\n",
					"        self.column_names:list = column_names\n",
					"        self.data_types:list = data_types \n",
					"        self.column_information:list = column_information\n",
					"        self.checks:list = checks\n",
					"        self.source_kind:str = source_kind\n",
					"\n",
					"        # Add the mandatory checks to the self.checks list\n",
					"        self.add_mandatory_checks()\n",
					"    \n",
					"\n",
					"    def add_mandatory_checks(self:object):\n",
					"        \"\"\"\n",
					"        Add a list of mandatory checks to the self.checks member of the Checks-class\n",
					"        \"\"\"\n",
					"\n",
					"        # List of mandatory checks for all the phases\n",
					"        mandatory_checks = ['header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n",
					"\n",
					"        # If the mandatory checks are not in self.checks, add them\n",
					"        for mandatory_check in mandatory_checks:\n",
					"            if mandatory_check not in self.checks:\n",
					"                self.checks.append(mandatory_check)\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------ ORCHESTRATE CHECK EXECUTION -------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def start_checks(self, phase:str, parameters:dict=None):\n",
					"        \"\"\"\n",
					"        Orchestrator method: Calls the necessary methods to execute the checks\n",
					"        \n",
					"        :param phase:       Medallion architecture phase: landing, raw, silver -&gt; Distinguish between the possible tests per phase\n",
					"        :param parameters:  Optional parameters needed for check-execution\n",
					"\n",
					"        \"\"\"\n",
					"\n",
					"        # Given the phase: return a phase-specific list of configured checks to be executed\n",
					"        filtered_checks:dict = self.get_checks_for_phase(phase=phase)\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks]: Executing checks for phase {phase}: {filtered_checks}\")\n",
					"        # If there are checks to be executed:\n",
					"        if (len(filtered_checks) &gt; 0):\n",
					"\n",
					"            # Before executing the checks:\n",
					"            # If parameters are passed to start_checks(), make sure the parameter-keys are valid\n",
					"            if parameters:\n",
					"                self.validate_parameter_keys(parameters=parameters)\n",
					"            \n",
					"            # Loop over all the configured tests\n",
					"            for check in filtered_checks: \n",
					"                filtered_checks[check]()\n",
					"            \n",
					"        # If no checks to execute\n",
					"        else:\n",
					"            if self.debug: print(f\"[Dataframes_v2:SparkDataFrameClass]: No checks were executed for phase {phase}.\")\n",
					"           \n",
					"        return 0\n",
					"\n",
					"    # ------------------------------------------------------------- VALIDATE THE ARGUMENTS PASSED TO START_CHECKS() -------------------------------------------------------------\n",
					"\n",
					"    def validate_phase(self, phase:str):\n",
					"        \"\"\"\n",
					"        Validate that the phase-argument passed to start_checks() is valid\n",
					"        \"\"\"\n",
					"\n",
					"        # Validate that the given value for self.phase is one of the expected/allowed values\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks]: Validate phase: {phase}\")\n",
					"\n",
					"        # Set the list of allowed values\n",
					"        allowed_phases = ['landing', 'raw', 'silver']\n",
					"\n",
					"        # [Functions/GenericFunctions] Validate that the argument for self.phase is an item in the allowed_phases list\n",
					"        validate_argument('phase', phase, allowed_phases)\n",
					"\n",
					"    def validate_parameter_keys(self, parameters: dict):\n",
					"        \"\"\"\n",
					"        Validate that the parameters-argument passed to start_checks() is valid\n",
					"        \"\"\"\n",
					"        # Dev-note: Move this to the DataConfig module to avoid having unexpected parameters in configuration\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks]: Validate checks_parameters\")\n",
					"\n",
					"        # Set the list of allowed values\n",
					"        allowed_parameters = ['landing_rows_expected', 'primary_key_columns']\n",
					"        \n",
					"        # [Functions/GenericFunctions] Validate that the keys for the parameters-object are in the allowed_parameters list\n",
					"        validate_argument_list('check_parameters', list(parameters.keys()), allowed_parameters)\n",
					"\n",
					"        # If no error is thrown: Set class-member self.parameters\n",
					"        self.parameters:dict = parameters\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------ PREPARE FOR CHECK EXECUTION -------------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    def get_checks_for_phase(self, phase:str):\n",
					"        \"\"\"\n",
					"        Filter the self.checks to return only the checks for a specific phase\n",
					"        \"\"\"\n",
					"\n",
					"        # Validate that the phase-argument is an expected argument\n",
					"        self.validate_phase(phase=phase) # Allowed phase-values: [landing, raw, silver]\n",
					"\n",
					"        # If the phase is landing: Filter self.checks to only return the possible checks for landing\n",
					"        if phase == 'landing':\n",
					"            possible_landing_checks = ['landing_rows']\n",
					"            filtered_checks:list = self.filter_checks(possible_phase_checks=possible_landing_checks)\n",
					"\n",
					"        # If the phase is landing: Filter self.checks to only return the possible checks for raw\n",
					"        elif phase == 'raw':\n",
					"            possible_raw_checks = ['header','default_replace', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n",
					"            filtered_checks:list = self.filter_checks(possible_phase_checks=possible_raw_checks)\n",
					"\n",
					"        # If the phase is allowed, but not found in the if-structure, then that means there are no tasks available (yet) for that phase\n",
					"        else:\n",
					"            raise AttributeError(f\"[Dataframes_v2:SparkDataFrameClass] There are no checks implemented for phase '{phase}'\")\n",
					"\n",
					"        return filtered_checks\n",
					"\n",
					"\n",
					"    def filter_checks(self, possible_phase_checks: list) -&gt; dict:\n",
					"        \"\"\"\n",
					"        Return a list of checks and their corresponding method to be executed for a phase and file\n",
					"\n",
					"        :param possible_phase_checks: List of all the checks that could be executed for a certain phase\n",
					"        \"\"\"\n",
					"\n",
					"        # Map the possible check-values to a corresponding function \n",
					"        checks_functions = { \n",
					"            'header': self.check_dataframe_headers, \n",
					"            'default_replace': self.check_values_to_replace,\n",
					"            'data_type': self.check_dataframe_datatypes, \n",
					"            'landing_rows': self.check_minimum_rowcount,\n",
					"            'primary_keys': self.check_primary_keys,\n",
					"            'duplicate_primary_keys': self.check_duplicate_primary_keys  \n",
					"        }\n",
					"                \n",
					"        # filtered_checks will contain the function to be executed for a phase, with corresponding function: {check_name: function_name}\n",
					"        filtered_checks = dict()\n",
					"\n",
					"        # Loop over the list of possible checks for the phase\n",
					"        for check_name in possible_phase_checks:\n",
					"            # If the check_name is in the self.checks list, map the check with corresponding function to the filtered_checks-dictionary\n",
					"            if (check_name in self.checks):\n",
					"                try:\n",
					"                    filtered_checks[check_name] = checks_functions[check_name]\n",
					"                except Exception:\n",
					"                    # If extraction from checks_functions fails, then the available and allowed key is not yet mapped to a method in the class\n",
					"                    raise KeyError(f\"Check '{check_name}' is not mapped to a check method. Check not (yet) implemented.\")\n",
					"\n",
					"        # Return dictionary {check_name: function_name}\n",
					"        return filtered_checks\n",
					"    \n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"    # --------------------------------------------------------------------------------------- CHECK FUNCTIONS ----------------------------------------------------------------\n",
					"    # ------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    def check_minimum_rowcount(self):\n",
					"        \"\"\"\n",
					"        Check if the dataframe contains a minimal number of rows\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks]: Check minimum row count for {self.source_path}\")\n",
					"\n",
					"        actual_amount_rows:int = self.dataframe.count()\n",
					"        \n",
					"        if self.parameters.get(\"landing_rows_expected\") is not None:\n",
					"            if actual_amount_rows &lt; self.parameters[\"landing_rows_expected\"]:\n",
					"                raise AssertionError(f\"[Dataframes_v2:SparkDataFrameChecks] Minimum rows is {self.parameters['landing_rows_expected']}, but got {actual_amount_rows}\")\n",
					"        else:\n",
					"            raise ValueError(\"[Dataframes_v2:SparkDataFrameChecks] Cannot check minimum rowcount without landing_rows_expected parameter\")\n",
					"\n",
					"        return 0\n",
					"\n",
					"    \n",
					"    def check_dataframe_headers(self):\n",
					"        \"\"\"\n",
					"        Headers = True: Check if the configured column names match with the headers of the dataframe\n",
					"        Headers = False: Add the configured column names and validate that they are not present on the first line of the dataframe\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks]: Check header-line for {self.source_path} for header={self.header}\")\n",
					"\n",
					"        # If header is present:\n",
					"        if self.header == True:\n",
					"            \n",
					"            # If the column_names in the configuration match the column names of the dataframe -&gt; Continue\n",
					"            if self.column_names == self.dataframe.columns:\n",
					"                return 0 ## For unit test\n",
					"            \n",
					"            # If the column_names in the configuration match the column names of the dataframe AND the first column is _c0 -&gt; Continue\n",
					"            # Current configuration: Allow this as the first column is most likely an index column with no column name\n",
					"            # Dev-Note: Might be necessary to add the _c0 column to the configuration to avoid potential future bugs on this use case\n",
					"            elif self.column_names == self.dataframe.columns[1:] and self.dataframe.columns[0] == '_c0':\n",
					"                # Dev-Info: If the first column is empty, it will be named _c&lt;number&gt;, and it won't match with the configured names in the config files\n",
					"                return 0 ## For unit test\n",
					"            \n",
					"            # If the column_names in the configuration match the column names of the dataframe, BUT the sequence is not the same -&gt; Continue if json; Error if other: Configuration needs to be changed\n",
					"            # Dev-Info: When working with json files, column names will be stored alphabethically since the schema is often not fixed.\n",
					"            elif sorted(self.column_names) == sorted(self.dataframe.columns):\n",
					"                if self.source_kind == 'json':\n",
					"                    return 0\n",
					"                else:\n",
					"                    raise ConfigurationError(\n",
					"                        custom_message=f\"Column sequence in configuration ({self.column_names}) does not match column sequence in dataframe ({self.dataframe.columns})\",\n",
					"                        notebook_name=\"Dataframes_v2\",\n",
					"                        class_name= \"SparkDataFrameChecks\",\n",
					"                        function_name= \"check_dataframe_headers\"\n",
					"                    )\n",
					"            # If the column_names in the configuration match the column names of the dataframe, BUT the sequence is not the same AND the first column is _c0 -&gt; Error: Configuration needs to be changed\n",
					"            elif sorted(self.column_names) == sorted(self.dataframe.columns[1:]) and self.dataframe.columns[0] == '_c0':\n",
					"                raise ConfigurationError(\n",
					"                    custom_message=f\"Column sequence in configuration ({self.column_names}) does not match column sequence in dataframe ({self.dataframe.columns[1:]}) (case: index column _c0)\",\n",
					"                    notebook_name=\"Dataframes_v2\",\n",
					"                    class_name= \"SparkDataFrameChecks\",\n",
					"                    function_name= \"check_dataframe_headers\"\n",
					"            )\n",
					"            # If none of the previous cases match:\n",
					"            else:\n",
					"                # Use self.column_information to only get a list of mandatory column names\n",
					"                local_column_names:list = self.column_names\n",
					"                local_column_information:list = self.column_information\n",
					"                column_information_dict = dict(zip(local_column_names, local_column_information))\n",
					"                if self.debug: print(\"Column information dictionary: \", column_information_dict)\n",
					"\n",
					"                mandatory_columns = return_filtered_keys(column_information_dict, 'optional', reverse=True)\n",
					"                # If a non-empty list is returned\n",
					"                if self.debug: print(\"Mandatory columns: \",mandatory_columns)\n",
					"                if self.debug: print(\"Dataframe columns': \", self.dataframe.columns)\n",
					"                if self.debug: print(\"Configured columns': \", local_column_names)\n",
					"\n",
					"\n",
					"                if mandatory_columns:\n",
					"                    # If the mandatory_list is a subset of the the dataframe_columns: Successful execution\n",
					"                    if set(mandatory_columns).issubset(set(self.dataframe.columns)):\n",
					"                        if set(self.dataframe.columns).issubset(set(local_column_names)):\n",
					"                            return 0\n",
					"                        else:\n",
					"                            unconfigured_columns = list(filter(lambda item: item not in local_column_names, self.dataframe.columns))\n",
					"                            raise ConfigurationError(\n",
					"                                custom_message=f\"The following columns were found in the source file but not in the configuration: {unconfigured_columns}\",\n",
					"                                notebook_name=\"Dataframes_v2\",\n",
					"                                class_name= \"SparkDataFrameChecks\",\n",
					"                                function_name= \"check_dataframe_headers\"\n",
					"                            )\n",
					"                    else:\n",
					"                        missing_mandatory_columns = list(filter(lambda item: item not in self.dataframe.columns, mandatory_columns))\n",
					"                        raise ConfigurationError(\n",
					"                            custom_message=f\"The following columns are non-optional in the configuration but were not found in the source file: {missing_mandatory_columns}\",\n",
					"                            notebook_name=\"Dataframes_v2\",\n",
					"                            class_name= \"SparkDataFrameChecks\",\n",
					"                            function_name= \"check_dataframe_headers\"\n",
					"                        )\n",
					"\n",
					"\n",
					"\n",
					"            # If no return happened yet: Raise error\n",
					"            raise ConfigurationError(\n",
					"                custom_message=f\"Header mismatch: ACTUAL ({self.dataframe.columns}) vs EXPECTED ({self.column_names}).\",\n",
					"                notebook_name=\"Dataframes_v2\",\n",
					"                class_name= \"SparkDataFrameChecks\",\n",
					"                function_name= \"check_dataframe_headers\"\n",
					"            )\n",
					"\n",
					"        # If header is not present:\n",
					"        elif self.header == False:\n",
					"            # Replace the '_cX' headers with the actual header-line from the configuration\n",
					"            self.dataframe = self.dataframe.toDF(*self.column_names) ## Unpack the list with *, so that it doesn't include one column with a list of the column names\n",
					"            \n",
					"            # Get the first row of the dataframe\n",
					"            first_row = self.dataframe.head(1)[0] # returns a row-object\n",
					"            first_row_values:list = list(first_row.asDict().values())\n",
					"\n",
					"            # Validate that the first row does not contain the column names\n",
					"            if all(x in self.column_names for x in first_row_values):\n",
					"                raise ConfigurationError(\n",
					"                    custom_message=f\"File {self.source_path} was not expected to contain a header-line but it does\",\n",
					"                    notebook_name=\"Dataframes_v2\",\n",
					"                    class_name= \"SparkDataFrameChecks\",\n",
					"                    function_name= \"check_dataframe_headers\"\n",
					"            )\n",
					"\n",
					"            else:\n",
					"                return 0 ## For unit test\n",
					"\n",
					"        else:\n",
					"            # Dev-Note: Check on datatype enforcement -&gt; No need to have this 'else' case if function enforces boolean\n",
					"            raise ValueError(f\"[Dataframes_v2:SparkDataFrameChecks] Invalid configuration for header: {self.header}\")\n",
					"\n",
					"\n",
					"    def check_primary_keys(self):\n",
					"        \"\"\"\n",
					"        Validate that the Primary Keys are not null\n",
					"        \"\"\"\n",
					"        if self.debug: print(\"[Dataframes_v2:SparkDataFrameChecks] Check primary keys for NOT NULL\")\n",
					"        if self.parameters.get('primary_key_columns') is not None:\n",
					"            primary_key_dataframe = self.dataframe.select(self.parameters['primary_key_columns'])\n",
					"\n",
					"            null_values = False\n",
					"            column_nullcount:dict = dict()\n",
					"            for column in self.parameters['primary_key_columns']:\n",
					"                null_count = primary_key_dataframe.filter(\n",
					"                    col(column).isNull()\n",
					"                ).count()\n",
					"\n",
					"                if null_count &gt; 0:\n",
					"                    primary_key_value=self.check_pk_values_to_replace(column)\n",
					"                    if primary_key_value==1 :\n",
					"                        null_values = True\n",
					"\n",
					"                column_nullcount[column] = null_count\n",
					"\n",
					"            if null_values:\n",
					"                    raise AssertionError(f\"[Checks:PrimaryKeys] Primary keys should not contain NULL values. Number of null values for PK columns: {column_nullcount}\")\n",
					"            else:\n",
					"                return 0\n",
					"\n",
					"        else:\n",
					"            raise ValueError(f\"[Checks:PrimaryKeys]: Cannot execute primary_key check without list of primary keys\")\n",
					"\n",
					"    def check_duplicate_primary_keys(self):\n",
					"\n",
					"        if self.parameters.get('primary_key_columns') is not None:\n",
					"            duplcate_pk_dataframe = self.dataframe.groupby(self.parameters['primary_key_columns']).count().where('count &gt; 1')\n",
					"            if duplcate_pk_dataframe.isEmpty():\n",
					"                return 0\n",
					"\n",
					"            duplcate_pk_dataframe.show()\n",
					"            raise MiddleWareError(\n",
					"                custom_message=f\"[Checks:DuplicatPrimaryKeys] Duplicate primary keys found in the source file for {duplcate_pk_dataframe.count()} instances\",\n",
					"                notebook_name=\"Dataframes_v2\",\n",
					"                class_name=\"SparkDataFrameChecks\",\n",
					"                function_name=\"check_duplicate_primary_keys\")\n",
					"            \n",
					"\n",
					"        \n",
					"        else:\n",
					"            raise ValueError(f\"[Checks:DuplicatPrimaryKeys]: Cannot execute duplicate_primary_keys check without list of primary keys\")\n",
					"\n",
					"\n",
					"    def check_values_to_replace(self):\n",
					"        \"\"\"\n",
					"        Replace invalid values for a certain datatype by a default value\n",
					"\n",
					"        :Dev-Info\n",
					"            Certain values cannot be read by spark and will be replaced by default/NULL values before ingesting\n",
					"            Example: Dates before 1900 cannot be read by spark and are therefore set to NULL -&gt; This is a limitation that is present in many modern environmnets (SQL, etc.).\n",
					"            Since no exclusion can be given that the resulting delta tables will only be ready using spark, a use-case can be made for the implementation of the below method\n",
					"            Note that this is part of test-class so that the configuration contains an explicit reference to this replace method, making it more visible to developers/managers.\n",
					"\n",
					"        \"\"\"\n",
					"        if self.debug: print(\"[Dataframes_v2:SparkDataFrameChecks] Replace dataframe values with defaults\")\n",
					"        local_column_names:list = self.column_names\n",
					"        local_datatypes:list = self.data_types\n",
					"        column_datatype_dict:dict = dict(zip(local_column_names, local_datatypes))\n",
					"\n",
					"        for column_name, data_type in column_datatype_dict.items():\n",
					"            if data_type == 'timestamp':\n",
					"                self.dataframe = self.dataframe.withColumn(column_name, when(col(column_name) &lt; '1900-01-01', None).otherwise(col(column_name)))\n",
					"            if data_type == 'date':\n",
					"                self.dataframe = self.dataframe.withColumn(column_name, when(col(column_name) &lt; '1900-01-01', None).otherwise(col(column_name)))\n",
					"            # if 'decimal' in data_type:\n",
					"            #     self.dataframe = self.dataframe.withColumn(column_name, sql.functions.regexp_replace(col(column_name), \",\", \".\"))\n",
					"\n",
					"\n",
					"\n",
					"    def check_dataframe_datatypes(self):\n",
					"        \"\"\"\n",
					"        Check if the columns can be casted into the configured datatypes\n",
					"        \n",
					"        :Dev-Info: Spark converts values to NULL if casting cannot be done, but this method will throw an explicit error when conversion cannot be done\n",
					"        \"\"\"\n",
					"        # Define method-lists for data_types and column_names\n",
					"        # Changes to the lists might happen in the method, but the changes are only relevant for the method itself\n",
					"        local_data_types:list   = self.data_types\n",
					"        local_column_names:list = self.column_names\n",
					"        local_column_information:list = self.column_information\n",
					"\n",
					"        # Cases might occur where the first column of a dataframe contains the indices indicating the row, but this is not configured in the metadata\n",
					"        # During spark-load, this unnamed column will be renamed to _c0 (if self.header == True)\n",
					"        # To avoid errors against this: Add column _c0 to the column_names and give it an integer datatype\n",
					"        if (len(self.dataframe.columns) == (len(local_column_names) + 1) and (self.dataframe.columns[0] == '_c0')):\n",
					"            local_data_types = ['integer'] + local_data_types\n",
					"            local_column_names = ['_c0'] + local_column_names\n",
					"            local_column_information = [None] + local_column_information\n",
					"\n",
					"        # Map the columns with the data_types: {column_name: data_type}\n",
					"        column_datatype_dict    = dict(zip(local_column_names, local_data_types))\n",
					"        column_information_dict = dict(zip(local_column_names, local_column_information))\n",
					"        if self.debug: print(\"[Dataframes_v2:SparkDataFrameChecks] column_datatype_dict:\", column_datatype_dict)\n",
					"        if self.debug: print(\"[Dataframes_v2:SparkDataFrameChecks] column_information_dict:\", column_information_dict)\n",
					"\n",
					"        # If the number of columns in the dataframe does not match the number of configured columns -&gt; Error\n",
					"        optional_columns:list = return_filtered_keys(column_information_dict, 'optional', reverse=False)\n",
					"\n",
					"        if len(self.dataframe.columns) != len(local_column_names):\n",
					"            present_and_optional_columns = list(set(self.dataframe.columns) | set(optional_columns))\n",
					"            if len(present_and_optional_columns) != len(local_column_names):\n",
					"                raise ConfigurationError(\n",
					"                    custom_message=f\"Total number of configured columns does not match total number of source file columns: \\n{local_column_names} ({len(local_column_names)}) versus {self.dataframe.columns} ({len(self.dataframe.columns)})\",\n",
					"                    notebook_name=\"Dataframes_v2\" ,\n",
					"                    class_name= \"SparkDataFrameChecks\",\n",
					"                    function_name= \"check_dataframe_datatypes\"\n",
					"                )\n",
					"\n",
					"        # [Dataframes_v2:SparkDataFrameChecks] Cast each column to the expected datatype and validate no NULL-conversions\n",
					"        result = dict()\n",
					"        SuccessfullCasting = True\n",
					"        for column in local_column_names:\n",
					"            if column in self.dataframe.columns:\n",
					"                try:\n",
					"                    # Try to cast each column individually to its expected datatype. If it is not possible, an error will be thrown\n",
					"                    # Catch the error and continue conversion. A dictionary will return all the columns (and the number of rows) that could not be converted to the correct datatype\n",
					"                    result[column] = self.cast_column(dataframe_subset=self.dataframe.select(column), column=column, data_type=column_datatype_dict[column], formatting = column_information_dict[column])\n",
					"                except AssertionError as casting_mismatch:\n",
					"                    result[column] = casting_mismatch\n",
					"                    SuccessfullCasting = False\n",
					"            else:\n",
					"                if column in optional_columns:\n",
					"                    if self.debug: print(\"Skip column check for {} as it is not in the dataframe and is flagged as optional\")\n",
					"                    result[column] = 'optional_skip'\n",
					"                else: ConfigurationError(\n",
					"                    custom_message=\"Skipping datatype check for column {} is not possible as it is optional, but it is also not present in the dataframe\",\n",
					"                    notebook_name=\"Dataframes_v2\",\n",
					"                    class_name=\"SparkDataframeChecks\",\n",
					"                    function_name=\"check_dataframe_datatypes\"\n",
					"                )\n",
					"        \n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks] Casting results: {result}\")\n",
					"        if not SuccessfullCasting:\n",
					"            # Example result: {column_name: 0, column_name2: 3, etc.} -&gt; 3 values of column_name2 could not be converted to the configured datatype\n",
					"            raise ValueError(f\"Error during casting: {result}\")\n",
					"        return result\n",
					"\n",
					"\n",
					"    def cast_column(self:object, dataframe_subset, column:str, data_type:str, formatting:dict=dict()):\n",
					"        \"\"\"\n",
					"        Cast one column to a datatype\n",
					"\n",
					"        :param dataframe_subset:    Dataframe with only 1 column\n",
					"        :param data_type:           The datatype to which the column needs to be casted\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Dataframes_v2:SparkDataFrameChecks] Casting column {column} with datatype {data_type}\")\n",
					"\n",
					"        # data_type: This is a string value that resembles a data type in Spark (string, int, array, etc.)\n",
					"        #   Not all the values allowed in the metadata-configuration are recognized by spark (eg. long_integer)\n",
					"        #   For these use cases, a specific if-statement has been included\n",
					"        #\n",
					"        #   Additionally, PySpark does not recognize varchar(x) but the metadata-configuration gives support for this datatype\n",
					"        #   As a hack, using an udf-function, the metadata of the column is configured to enforce the varchar(x) constraint anyway\n",
					"        #   Dev-Note: udf's have an impact on the performance of the spark code -&gt; Would be nice to find a way around the udf if possible.\n",
					"\n",
					"        default_formatting = {\n",
					"            \"format\": \"yyyy-MM-dd HH:mm:ss\",\n",
					"            \"locale\": \"en-US\",\n",
					"            \"thousand_separator\": \"\",\n",
					"            \"decimal_separator\": \".\"\n",
					"        }\n",
					"        if not formatting:\n",
					"            formatting = dict()\n",
					"        local_formatting  = {**default_formatting, **formatting}\n",
					"        if self.debug: print(local_formatting)\n",
					"\n",
					"        try:\n",
					"            # If data_type == 'long_integer' -&gt; Cast column to LongType()\n",
					"            if data_type == 'long_integer':\n",
					"                dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", col(column).cast(sql.types.LongType()))\n",
					"\n",
					"            # If datatype == varchar(max)-&gt; Cast column to StringType()\n",
					"            elif data_type == 'varchar(max)':\n",
					"                dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", col(column).cast('string'))\n",
					"\n",
					"            # If datatype == varchar(&lt;xx&gt;) (not varchar(max) as this is handled above) -&gt; use a UDF enforce_varchar_constraint on the column to enforce a \"max_length\"\n",
					"            elif 'varchar' in data_type:\n",
					"                # Create the UDF defined in the IngestionFunctions notebook\n",
					"                enforce_length_udf = enforce_varchar_constraint({'__CHAR_VARCHAR_TYPE_STRING': data_type})\n",
					"                # Apply the UDF to the column and add the metadata\n",
					"                dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", enforce_length_udf(dataframe_subset[column]).alias(column, metadata={'__CHAR_VARCHAR_TYPE_STRING': data_type}))\n",
					"            elif 'decimal' in data_type:\n",
					"                # The decimal separator has been defined :\n",
					"                if (local_formatting['decimal_separator'] != '.'):\n",
					"                    if local_formatting['thousand_separator'] != \"\":\n",
					"                        self.dataframe = self.dataframe.withColumn(column, regexp_replace(column, local_formatting['thousand_separator'], ''))\n",
					"                    self.dataframe = self.dataframe.withColumn(column, regexp_replace(column, local_formatting['decimal_separator'], '.'))\n",
					"                    dataframe_subset = self.dataframe.select(column)\n",
					"                dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", col(column).cast(data_type))\n",
					"\n",
					"            elif data_type == 'timestamp':\n",
					"                timestamp_format = local_formatting['format']\n",
					"                if timestamp_format == 'epoch':\n",
					"                    dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", from_unixtime(dataframe_subset[column]))\n",
					"                else:\n",
					"                    dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", to_timestamp(dataframe_subset[column], timestamp_format))\n",
					"\n",
					"                # The original dataframe (self.dataframe) did not load the original columns using the format\n",
					"                # When merging the values into the deltaTable, the values that cannot be casted properly will be set to NULL\n",
					"                # To prevent this, overwrite the columns of self.dataframe with the columns that were loaded using the formatting-setting\n",
					"                if timestamp_format != \"yyyy-MM-dd HH:mm:ss\":\n",
					"                    if timestamp_format == 'epoch':\n",
					"                        self.dataframe = self.dataframe.withColumn(f\"{column}\", from_unixtime(dataframe_subset[column]))\n",
					"                    else:\n",
					"                        self.dataframe = self.dataframe.withColumn(f\"{column}\", to_timestamp(dataframe_subset[column], timestamp_format))\n",
					"                \n",
					"\n",
					"            elif data_type == 'date':\n",
					"                date_format = local_formatting['format']\n",
					"                if date_format == \"yyyy-MM-dd HH:mm:ss\": date_format = \"yyyy-MM-dd\"\n",
					"                dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", to_date(dataframe_subset[column], date_format))\n",
					"                \n",
					"                # The original dataframe (self.dataframe) did not load the original columns using the specified format\n",
					"                # When merging the values into the deltaTable, the values that cannot be casted properly will be set to NULL\n",
					"                # To prevent this, overwrite the columns of self.dataframe with the columns that were loaded using the locale-setting\n",
					"                if data_type != 'yyyy-MM-dd':\n",
					"                    self.dataframe = self.dataframe.withColumn(f\"{column}\", to_date(dataframe_subset[column], date_format))\n",
					"                \n",
					"            else:\n",
					"                dataframe_subset = dataframe_subset.withColumn(f\"casted_{column}\", col(column).cast(data_type))\n",
					"\n",
					"        # If a data_type is passed for which spark has no support and no specific if-statement has been built in -&gt; Error\n",
					"        except Exception as e:\n",
					"            raise ValueError(f'Casting error for casting column {column} to data type {data_type}: {e}')\n",
					"\n",
					"        # [Dataframes_v2:SparkDataFrameChecks] Check if the casted dataframe matches with the original\n",
					"        result = self.count_miscasted_column_values(dataframe_subset, column)\n",
					"        return result\n",
					"\n",
					"\n",
					"    def count_miscasted_column_values(self, casting_dataframe: pyspark.sql.DataFrame, column:str):\n",
					"        \"\"\"\n",
					"        Compare 2 columns (one casted, one not) and check if there were casting issues by checking if there are NULL values in one column but not in the other\n",
					"\n",
					"        :param casting_dataframe:   Dataframe with 2 columns, one casted and one not casted\n",
					"        :param column:              Name of the original column\n",
					"        \"\"\"\n",
					"        if self.debug: print(\"[Classes:Checks] Casted dataframe:\")\n",
					"        if self.debug: casting_dataframe.show()\n",
					"        if self.debug: print(f\"Column: {column}\")\n",
					"        # Compare the casted column and the original columns: Error when casted column is NULL and original contains a value\n",
					"        mismatch_count = casting_dataframe.filter(\n",
					"            col(f'casted_{column}').isNull() &amp;\n",
					"            col(column).isNotNull()\n",
					"        ).count()\n",
					"        if self.debug: print(\"Finish count\")\n",
					"        ## If count &gt; 0 -&gt; Error: Casting problem\n",
					"        if mismatch_count &gt; 0:\n",
					"            raise AssertionError(f\"Casting error: NULL values in column {column} after casting: Could not convert {mismatch_count} columns\")\n",
					"        else:\n",
					"            return 0\n",
					"    def check_pk_values_to_replace (self,column_name:str):\n",
					"        \"\"\"\n",
					"        Replace null values in a Dataframe column with specified values when the replace_value option is provided for that column.\n",
					"        :param column_name : Name of the Dataframe column \n",
					"        \"\"\"\n",
					"        column_information_dict = dict(zip(self.column_names, self.column_information))\n",
					"        column_information_object=column_information_dict[column_name]\n",
					"        filter=\"replace_value\"\n",
					"        if isinstance(column_information_object, dict):\n",
					"            if filter in column_information_object.keys():\n",
					"                replace_value=column_information_object[filter]\n",
					"                self.dataframe=self.dataframe.fillna({column_name: replace_value})\n",
					"                return 0\n",
					"        return 1\n",
					"\n",
					""
				],
				"execution_count": 1
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\DeltaTables.json">
{
	"name": "DeltaTables",
	"properties": {
		"folder": {
			"name": "Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "7cc89550-de4b-4933-a98e-2c231ec2b653"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import delta.tables as dt"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Dataframes_v2"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/GenericFunctions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class DeltaTableClass_v2(SparkDataFrameClass):\n",
					"    \"\"\"\n",
					"    Class defined for methods related to DeltaTables\n",
					"    This class inherits all methods defined in class SparkDataFrameClass\n",
					"    \"\"\"\n",
					"    \n",
					"    def __init(self:object, debug:bool=False):\n",
					"        self.debug = debug\n",
					"        # Dev-Note: Define source_path argument when creating class-instance\n",
					"\n",
					"\n",
					"    def load_delta_table(self, source_path:str):\n",
					"        \"\"\"\n",
					"        Load a delta table in memory as an instance of class DeltaTable\n",
					"\n",
					"        :param source_path: Abfss-path to the delta table\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Dataframes:DeltaTableOptimizeClass] Initialize delta table object for {source_path}\")\n",
					"        \n",
					"        # Load the delta table from the silver-container\n",
					"        self.delta_table = dt.DeltaTable.forPath(spark, source_path)\n",
					"        return self.delta_table # Used for unit testing\n",
					"\n",
					"    def create_delta_table(self:object, destination_path:str, table_description:str, column_objects:object, partition_objects:dict=[]):\n",
					"        \"\"\"\n",
					"        Create an empty (partitioned) delta table with columns and datatypes defined\n",
					"\n",
					"        :param destination_path:    Abfss-path on the silver container of where the delta table needs to be created\n",
					"        :param table_description:   General description of the delta table and its contents\n",
					"        :param column_objects:      A list of objects, representing the metadata needed to create a StructField object\n",
					"        :param partition_objects:   Dictionary containing all the names of the delta table on which to partition the delta table\n",
					"        \"\"\"\n",
					"\n",
					"        # [Dataframes_v2:SparkDataFrameClass] Create an empty dataframe with column names\n",
					"        self.create_dataframe(column_information=column_objects, table_description=table_description)\n",
					"\n",
					"        if partition_objects:\n",
					"            # Add the partitioning columns to the dataframe and write a delta table with partitioning\n",
					"            self.dataframe, partitioning_columns = create_partioning_list(dataframe=self.dataframe, partitioning_objects=partition_objects) \n",
					"            self.write_dataframe(write_format='delta', write_mode='overwrite', destination_path=destination_path, partitioning_columns=partitioning_columns) \n",
					"        else:\n",
					"            # Write a delta table without partitioning \n",
					"            self.write_dataframe(write_format='delta', write_mode='overwrite', destination_path=destination_path)\n",
					"\n",
					"    def count_delta_table (self):\n",
					"        \"\"\"\n",
					"        For each Delta table, count the total number of rows, as well as the number of rows that have been inserted and updated for the last operation\n",
					"        \"\"\"\n",
					"\n",
					"        inserted_row=self.delta_table.history().filter(\"operation = 'MERGE'\").select(\"operationMetrics.numTargetRowsInserted\").collect()[0][0]\n",
					"        updated_row= self.delta_table.history().filter(\"operation = 'MERGE'\").select(\"operationMetrics.numTargetRowsUpdated\").collect()[0][0]\n",
					"        source_row=  self.delta_table.history().filter(\"operation = 'MERGE'\").select(\"operationMetrics.numSourceRows\").collect()[0][0]\n",
					"        return inserted_row,updated_row,source_row\n",
					"\n",
					"    @staticmethod\n",
					"    def add_notnull_constraints(source_path:str, column_dimensions:dict):\n",
					"        \"\"\"\n",
					"        For all primary key columns, add NOT NULL constraints to the delta table\n",
					"            \n",
					"        :param source_path:         Path to the delta table\n",
					"        :param column_dimensions:   Dictionary with keys = column_names and values = dimension of the column\n",
					"        :example {\"column_name\": PK/SCD2/etc.}\n",
					"        \"\"\"\n",
					"        # Get all the constraints defined for the delta table\n",
					"        constraints = spark.sql(f\"DESCRIBE DETAIL  delta.`{source_path}`\").select('properties').collect()[0][0]\n",
					"        constraint_names = list(constraints.keys())\n",
					"\n",
					"        # Loop over the column_dimensions\n",
					"        for column_name, dimension in column_dimensions.items():\n",
					"            # If the dimension == \"PK\" -&gt; Add a not-null constraint\n",
					"            if dimension == 'PK':\n",
					"                constraint_name = f'pk__notnull_{column_name}'\n",
					"                if f'delta.constraints.{constraint_name}' not in constraint_names:\n",
					"                    spark.sql(f\"ALTER TABLE delta.`{source_path}` ADD CONSTRAINT {constraint_name} CHECK ({column_name} IS NOT NULL)\")\n",
					"                else:\n",
					"                    spark.sql(f\"ALTER TABLE delta.`{source_path}` DROP CONSTRAINT {constraint_name}\")\n",
					"                    spark.sql(f\"ALTER TABLE delta.`{source_path}` ADD CONSTRAINT {constraint_name} CHECK ({column_name} IS NOT NULL)\")\n",
					"\n",
					"    def validate_delta_table(self, table_name:str, container_name:str, storage_account:str, column_objects:list):\n",
					"        \"\"\"\n",
					"        When a delta table is already expected to exist, execute some validation checks to make sure the table is still up to par\n",
					"\n",
					"        :param table_name:      Name of the delta table\n",
					"        :param container_name:  Name of the container where the delta table is located\n",
					"        :param storage_account: Name of the storage account where the delta table is located\n",
					"        :param column_objects:  List of objects containing metatadata for each column in the delta table\n",
					"        \"\"\"\n",
					"\n",
					"        # Path to the delta table, given the parameter arguments\n",
					"        delta_path:str = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n",
					"\n",
					"        # Validate that the metadata of the delta table is as expected\n",
					"        self.validate_deltatable_metadata(delta_path=delta_path, table_name=table_name, container_name=container_name, storage_account=storage_account)\n",
					"        self.validate_columnnames(delta_path=delta_path, column_objects=column_objects)\n",
					"    \n",
					"    def validate_deltatable_metadata(self, delta_path:str, table_name:str, container_name:str, storage_account:str):\n",
					"        \"\"\"\n",
					"        Go into the metadata of the delta table and execute some validation checks \n",
					"\n",
					"        :param delta_path:      Abfss-path to the delta table\n",
					"        :param table_name:      Name of the delta table\n",
					"        :param container_name:  Name of the container where the delta table is located\n",
					"        :param storage_account: Name of the storage account where the delta table is located\n",
					"        \"\"\"        \n",
					"        \n",
					"        # Get the delta table metadata\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path}`\")\n",
					"\n",
					"        # Print the metadata objects when running in debug-mode\n",
					"        if self.debug: \n",
					"            for detail in table_details.columns:\n",
					"                expression = table_details.selectExpr(f'first({detail})').collect()[0][0]\n",
					"                print(str(detail), ':', str(expression))\n",
					"\n",
					"        # Get the relevant metadata\n",
					"        sql_table_format            = table_details.selectExpr(f\"first(format)\").collect()[0][0]\n",
					"        sql_table_name              = table_details.selectExpr(f\"first(name)\").collect()[0][0]\n",
					"        sql_table_location          = table_details.selectExpr(f\"first(location)\").collect()[0][0]\n",
					"        sql_table_partitionColumns  = table_details.selectExpr(f\"first(partitionColumns)\").collect()[0][0]\n",
					"        \n",
					"        # Execute some quality checks on the table\n",
					"        if sql_table_format != 'delta':\n",
					"            raise TypeError(\"The format of the table is not a delta table\")\n",
					"        # Dev-Note: This might cause issues when, for example, the storage_account has the same name as the table or container\n",
					"        if storage_account not in sql_table_location:\n",
					"            raise ValueError(f\"The table is not stored on the given storage account {storage_account}. The storage location is {sql_table_location}\")\n",
					"        # Dev-Note: This might cause issues when, for example, the database has the same name as the table or storage_account\n",
					"        if container_name not in sql_table_location:\n",
					"            raise ValueError(f\"The table is not stored on the expected container {container_name}. The storage location is {sql_table_location}\")\n",
					"        if not dt.DeltaTable.isDeltaTable(spark, delta_path):\n",
					"            raise TypeError(f\"The files stored on path {delta_path} for table {table_name} are not a delta table\")\n",
					"        return\n",
					"\n",
					"    def validate_columnnames(self, delta_path:str, column_objects:list):\n",
					"        \"\"\"\n",
					"        Validate if all columns on column_objects are in the delta table, and add them if not the case\n",
					"        Method: An empty database is made with all the columns that need to be added, and the partition columns\n",
					"\n",
					"        :param delta_path:      Abfss-path to the delta table that is being checked\n",
					"        :param column_objects:  List of objects containing metadata about the columns of the delta table\n",
					"\n",
					"        :example column_objects\n",
					"            column_objects = [\n",
					"                {\"column_name\": \"column_1\", \"dimension\": \"PK\",      \"data_type\": \"string\"},\n",
					"                {\"column_name\": \"column_2\", \"dimension\": \"SCD2\",    \"data_type\": \"string\"},\n",
					"                {\"column_name\": \"column_3\", \"dimension\": \"SCD2\",    \"data_type\": \"string\"}\n",
					"            ]\n",
					"        \"\"\"\n",
					"\n",
					"        # Load the existing delta table into memory\n",
					"        self.load_delta_table(delta_path) # initiate self.delta_table\n",
					"        # Get a list of existing columns of the delta table \n",
					"        existing_columns:list = self.delta_table.toDF().columns\n",
					"\n",
					"        # Loop over the column_objects\n",
					"        # If column_name not in columns -&gt; Add the object to columns_to_add\n",
					"        columns_to_add = []\n",
					"        for column_object in column_objects:\n",
					"            if column_object['column_name'] not in existing_columns:\n",
					"                columns_to_add.append(column_object)\n",
					"        \n",
					"        # If there are new columns to add to the delta table:\n",
					"        if columns_to_add:\n",
					"            # Dev-Info: If the delta table is partitioned, the partition columns need to be part of the dataframe that will be added\n",
					"            # Get the list of partition_columns of the existing delta table\n",
					"            table_details = spark.sql(f\"DESCRIBE DETAIL delta.`{delta_path}`\")\n",
					"            sql_table_partitionColumns  = table_details.selectExpr(f\"first(partitionColumns)\").collect()[0][0]\n",
					"            # Add the partition columns to the columns_to_add object\n",
					"            for partition_column in sql_table_partitionColumns:\n",
					"                column_object = {\"column_name\": partition_column, \"data_type\": \"string\", \"dimension\": \"SCD2\"}\n",
					"                columns_to_add.append(column_object)\n",
					"\n",
					"            # Create a dataframe with the columns that need to be added\n",
					"            self.create_dataframe(column_information=columns_to_add)\n",
					"            if self.debug: print(f\"Adding the following columns to delta table {delta_path}\")\n",
					"            if self.debug: self.dataframe.show()\n",
					"\n",
					"            # Add the dataframe to the existing delta table\n",
					"            self.write_dataframe(write_format='delta', write_mode='append', destination_path=delta_path, merge_schema=True)\n",
					"        return"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class OptimizeDeltaTable(DeltaTableClass_v2):\n",
					"    \"\"\"\n",
					"    Class defined for methods related to DeltaTable-optimizations\n",
					"    This class should only contain methods that can be executed on DeltaOptimizeBuilder-objects\n",
					"    \"\"\"\n",
					"    def __init__(self:object, table_name:str, env_code:str, debug:bool=False):\n",
					"        \"\"\"\n",
					"        Initialize an object of class OptimizeDeltaTable and create instances of class DeltaTable and DeltaOptimizeBuilder during creation\n",
					"\n",
					"        :param table_name:  Name of the delta table to optimize\n",
					"        :param env_code:    Environment code indicating which environment the tables are created in (dev, int, tst, acc, prd)\n",
					"        :param debug:       Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"\n",
					"        :return delta_table_path:       Path to the delta table (Assume table is located on silver-container)\n",
					"        :return delta_table:            Instance of class DeltaTable\n",
					"        :return delta_table_optimized:  Instance of class DeltaOptimizeBuilder\n",
					"        \"\"\"\n",
					"        self.debug      = debug\n",
					"        self.env_code   = env_code        \n",
					"        self.table_name = table_name\n",
					"\n",
					"        self.delta_table_path = f'abfss://silver@{self.env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\n",
					"        # Create object DeltaTable\n",
					"        self.delta_table =self.load_delta_table(source_path=self.delta_table_path)\n",
					"        # Create object DeltaOptimizeBuilder\n",
					"        self.optimize_deltatable()\n",
					"\n",
					"\n",
					"    def optimize_deltatable(self):\n",
					"        \"\"\"\n",
					"        Execute the .optimize() method on the delta table\n",
					"        This will create a DeltaOptimizeBuilder-object instance, which can be used for optimization methods\n",
					"        \"\"\"\n",
					"        \n",
					"        if self.debug: print(f\"[Dataframes:OptimizeDeltaTable]: Optimization object for {self.table_name}...\")\n",
					"        self.delta_table_optimized = self.delta_table.optimize()\n",
					"        if self.debug: print(f\"[Dataframes:OptimizeDeltaTable]: Optimized Delta table {self.delta_table_optimized}...\")\n",
					"\n",
					"\n",
					"    def compact_deltatable(self):\n",
					"\n",
					"        \"\"\"        \n",
					"        Execute the .executeCompaction() method on the DeltaOptimizeBuilder-object\n",
					"            \n",
					"        :Dev-Info: \n",
					"            This method will group together multiple smaller files into one bigger file based on the limits set in the configuration\n",
					"            The default limit of a spark session configuration is 1GB.\n",
					"            executeCompaction() will use a bin-packing method to sort the data in files of an equal size, respecting the 1GB limit\n",
					"        \"\"\"\n",
					"\n",
					"        # First: Validate that the DeltaOptimizeBuilder-object exists as a class-member\n",
					"        if hasattr(self, \"delta_table_optimized\"):\n",
					"            if isinstance(self.delta_table_optimized, dt.DeltaOptimizeBuilder):\n",
					"                if self.debug: print(f\"[Dataframes:OptimizeDeltaTable]: Compact delta table {self.table_name}...\")\n",
					"                # Execute compaction on the delta table\n",
					"                compaction_metrics:object = self.delta_table_optimized.executeCompaction()\n",
					"            else:\n",
					"                raise TypeError(\"[Dataframes:OptimizeDeltaTable] delta_table_optimized member is not of class DeltaOptimizeBuilder for compaction\")\n",
					"        \n",
					"        else:\n",
					"            raise ValueError(\"[Dataframes:OptimizeDeltaTable] Cannot execute compaction when optimize object does not exist\")\n",
					"\n",
					"        # Print a select set of metrics that give an initial insight in the compaction\n",
					"        if self.debug: compaction_metrics.select(\"path\", \"metrics.numFilesAdded\", \"metrics.numFilesRemoved\").show(truncate=False)\n",
					"\n",
					"    def z_order_deltatable(self, z_order_columns:list):\n",
					"        \"\"\"\n",
					"        Execute the .executeZOrderBy() method on the DeltaOptimizeBuilder-object\n",
					"        \n",
					"        :Dev-Info: \n",
					"            This method will cluster a set of rows that belong together within a file\n",
					"            ZOrder provides clustering of related data inside the files (for a specific partition, if applicable) that may contain multiple possible values for given column.\n",
					"            This will also allow for data skipping when querying the data, improving overall performance\n",
					"        \"\"\"\n",
					"        # First: Validate that the DeltaOptimizeBuilder-object exists as a class-member\n",
					"        if hasattr(self, \"delta_table_optimized\"):\n",
					"            if isinstance(self.delta_table_optimized, dt.DeltaOptimizeBuilder):\n",
					"                if self.debug: print(f\"[Dataframes:OptimizeDeltaTable]: Z-order delta table {self.table_name} on column(s): {z_order_columns}...\")\n",
					"                # Execute Z-order on the delta table\n",
					"                self.delta_table_optimized.executeZOrderBy(z_order_columns=z_order_columns)\n",
					"            else:\n",
					"                raise TypeError(\"[Dataframes:OptimizeDeltaTable] delta_table_optimized member is not of class DeltaOptimizeBuilder for Z-order\")\n",
					"        \n",
					"        else:\n",
					"            raise ValueError(\"[Dataframes:OptimizeDeltaTable] Cannot execute Z-order when optimize object does not exist\")\n",
					"\n",
					"\n",
					"    def optimize_table_storage(self, z_order_columns:list=None):\n",
					"        \"\"\"        \n",
					"        Orchestrator function that will create a DeltaOptimizeBuilder-object and execute optimization methods (compaction and z-order) on the object\n",
					"        \"\"\"\n",
					"\n",
					"        if self.debug: print(f\"[Dataframes:OptimizeDeltaTable] Optimize table storage...\")\n",
					"\n",
					"        # Create the DeltaOptimizeBuilder-object\n",
					"        self.optimize_deltatable()\n",
					"        # Execute compaction on the object\n",
					"        self.compact_deltatable()\n",
					"\n",
					"        # If a list of columns is given, make sure to also execute the executeZOrderBy-method\n",
					"        if z_order_columns:\n",
					"            self.z_order_deltatable(z_order_columns=z_order_columns)\n",
					"\n",
					"    def vacuum_deltatable(self, retention_period:int):\n",
					"        \"\"\"\n",
					"        Remove files that are not being used anymore. This will often be related to files that have been compacted into larger files\n",
					"        \"\"\"\n",
					"        if self.debug: print(f\"[Dataframes:OptimizeDeltaTable] Vacuum table storage...\")\n",
					"        \n",
					"        # Dev-Note: Set \"spark.databricks.delta.vacuum.parallelDelete.enabled\" to \"true\" if vacuum takes to much time\n",
					"        self.delta_table.vacuum(retention_period)\n",
					"\n",
					"        # Print a select set of metrics that give an initial insight in the vacuum\n",
					"        # if self.debug: print(f\"[Dataframes:OptimizeDeltaTable]: Vacuum  metrics:\")\n",
					"        # if self.debug: vacuum_metrics.select(\"numDeletedFiles\", \"numVacuumedDirectories\", \"numFilesToDelete\").show(truncate=False)\n",
					"        "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class DeployDeltaTable(DeltaTableClass_v2):\n",
					"    \"\"\"\n",
					"    Class to call methods and functions relevant during the deployment of delta tables\n",
					"    \"\"\"\n",
					"\n",
					"    def __init__(self:object, table_name:str, storage_account:str, container_name:str, table_description:str, column_info:str, partitioning: dict=[], debug:bool=False):\n",
					"        \"\"\"\n",
					"        Set a list of arguments to use during delta table deployment: silver_path and column_objects\n",
					"\n",
					"        :param table_name:          Name of the delta table to deploy/check\n",
					"        :param storage_account:     Name of the storage account where to deploy the table\n",
					"        :param container_name:      Name of the container where to deploy the table\n",
					"        :param table_description:   General description of the table and its contents\n",
					"        :param column_info:         String that resembles a list of objects that contain the necessary values to create StructField objects\n",
					"        :param partitioning:        Dictionary containing the names of the columns on which to partition the delta table\n",
					"        :param debug:               Boolean indicating whether the class is initiated in debug mode. This will output informational print-statements in the terminal\n",
					"\n",
					"        :result silver_path:        Abfss-path of where the delta table found\n",
					"        :result column_objects:     List of objects that contain the necessary values to create StructField objects (Same as column_info but converted to a proper list)\n",
					"        \"\"\"\n",
					"        self.debug = debug\n",
					"\n",
					"        # Initialize the class-object parameters\n",
					"        self.container_name = container_name\n",
					"        self.storage_account = storage_account\n",
					"        self.table_description = table_description\n",
					"        self.column_info = column_info\n",
					"        self.partitioning = partitioning\n",
					"        self.table_name = table_name\n",
					"\n",
					"\n",
					"        # Based on the given parameters, create a set of derived parameters                                                     \n",
					"        # self.full_name = self.container_name + '.' + self.table_name                                                            # Name of the delta table wit its schema/namespace: &lt;namespace&gt;.&lt;table_name&gt;\n",
					"        self.silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.container_name, self.storage_account, self.table_name) # Path to the table on the data lake storage account\n",
					"\n",
					"        # Create a list of json objects with all the column information\n",
					"        # Structure: {&lt;column_metadata&gt;: &lt;metadata_value&gt;}\n",
					"        # Example: \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'BK', 'data_type': 'string' }]\"\n",
					"        self.column_objects = process_string_to_json(string_object=self.column_info)\n",
					"\n",
					"        # # Create a dictionary matching the column_names and their data types\n",
					"        # # Structure: {&lt;column_name&gt;: &lt;column_datatype&gt;}\n",
					"        # # Example: { 'column_1': 'string', 'column_2': 'integer', ...}\n",
					"        # self.column_datatypes   = {column_object['column_name']: column_object['data_type'] for column_object in self.column_objects}\n",
					"\n",
					"    def deploy_delta_table(self):\n",
					"        \"\"\"\n",
					"        Orchestrator function that will call a set of methods depending on whether the delta table already exists or not\n",
					"        \"\"\"\n",
					"        print(f\"Starting Table deployment for table {self.table_name}\")\n",
					"        \n",
					"        # Check if the delta-table already exists and execute methods accordingly\n",
					"        #   True  -&gt; Validate\n",
					"        #   False -&gt; Create new table\n",
					"        if mssparkutils.fs.exists(self.silver_path):\n",
					"            print(f\"Table {self.table_name} exists already, validating structure...\")\n",
					"            self.validate_delta_table(table_name=self.table_name, storage_account=self.storage_account, container_name=self.container_name, column_objects=self.column_objects)\n",
					"\n",
					"        else:\n",
					"            print(f\"Table {self.table_name} does not exist, creating table...\")\n",
					"            column_dimensions  = {column_object['column_name']: column_object['dimension'] for column_object in self.column_objects}\n",
					"            self.create_delta_table(destination_path=self.silver_path, table_description=self.table_description, column_objects=self.column_objects, partition_objects=self.partitioning)\n",
					"            self.add_notnull_constraints(source_path=self.silver_path, column_dimensions=column_dimensions)\n",
					"        "
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\DeployDeltaTable.json">
{
	"name": "DeployDeltaTable",
	"properties": {
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "4a01aeda-bd7e-4ca4-b95c-1ff0aea25a37"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# DeployDeltaTable\r\n",
					"This script is called upon when changes are made to the configuration data. During the deployment of the configuration data, this script validates whether or not the configured table exists. If this is the case, some validitiy checks are executed to make sure that the table resembles the configured metadata. If the table does not exist, it is created."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 76
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Import necessary packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import necessary packages\r\n",
					"import delta.tables as dt\r\n",
					"# from pyspark.sql.functions import col\r\n",
					"# from pyspark.sql import Row, DataFrame\r\n",
					"import pyspark.sql as sql\r\n",
					"import json\r\n",
					"import re\r\n",
					"import delta.tables as dt"
				],
				"execution_count": 77
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 2. Define the script parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Parameters that need to be given to create the delta table\r\n",
					"table_name = 'delta_test'                                                                                               # Name of the delta table\r\n",
					"database_name = 'silver'                                                                                                # Namespace of the delta table in the Synapse Database\r\n",
					"column_info = '[{\"column_name\": \"id\", \"data_type\": \"integer\"}, {\"column_name\": \"column_2\", \"data_type\": \"string\"}]'     # The datatypes of the columns\r\n",
					"storage_account = 'devdapstdala1'                                                                                       # The storage account where the delta table is expected to be stored\r\n",
					"table_description = \"\"                                                                                                  # A general description of the delta table: What are the contents of the table describing"
				],
				"execution_count": 78
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"json_column_info = json.loads(column_info)\r\n",
					"\r\n",
					"column_dict = dict()\r\n",
					"for structfield in json_column_info:\r\n",
					"  column_dict[structfield[\"column_name\"]] = structfield[\"data_type\"]"
				],
				"execution_count": 79
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print(\"Parameter values\")\r\n",
					"print(\"table_name: \", table_name)\r\n",
					"print(\"database_name: \", database_name)\r\n",
					"print(\"storage_account: \", storage_account)\r\n",
					"print(\"table_description: \", table_description)\r\n",
					"print(\"column dictionnary: \", column_dict)"
				],
				"execution_count": 80
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Based on the given parameters, create a set of derived parameters                                                                \r\n",
					"full_name = database_name + '.' + table_name                                                                                       # Name of the delta table wit its schema/namespace: &lt;namespace&gt;.&lt;table_name&gt;\r\n",
					"silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (database_name, storage_account, table_name)                               # Path to the table on the data lake storage account\r\n",
					"\r\n",
					"# List of the data types that can be read by spark\r\n",
					"# Dev-note: DecimalType has 'precision' (How many numbers long can my number be) and 'scale' (how many numbers do I allow after the comma?)\r\n",
					"# Dev-note: Currently this is by default \"38 numbers long\" and it allows \"4 numbers after the comma (23.23 gets turned into 23.2300)\" -&gt; Parameter in the future\r\n",
					"# https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.DecimalType.html\r\n",
					"types = {\"array\": sql.types.ArrayType(sql.types.StringType()), \"binary\": sql.types.BinaryType(), \r\n",
					"        \"boolean\": sql.types.BooleanType(), \"date\": sql.types.DateType(), \"string\": sql.types.StringType(), \r\n",
					"        \"timestamp\": sql.types.TimestampType(), \"decimal\": sql.types.DecimalType(38, 4), \"float\": sql.types.FloatType(), \r\n",
					"        \"byte\": sql.types.ByteType(), \"integer\": sql.types.IntegerType(), \"long_integer\":  sql.types.LongType()}"
				],
				"execution_count": 81
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_decimal_datatype(decimal_value, datatypes):\r\n",
					"\r\n",
					"    pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\r\n",
					"\r\n",
					"    if decimal_value == 'decimal':\r\n",
					"        return datatypes\r\n",
					"    elif re.match(pattern, decimal_value):\r\n",
					"        match = re.match(pattern, decimal_value)\r\n",
					"        num_characters = int(match.group(1))\r\n",
					"        num_post_comma = int(match.group(2))\r\n",
					"        # Dev-Note: Add check to make sure that num_characters does not exceed 38 as pyspark cannot handle more, and that num_characters &gt; num_post_comma\r\n",
					"        datatypes[decimal_value] = sql.types.DecimalType(num_characters, num_post_comma)\r\n",
					"        return datatypes\r\n",
					"    else:\r\n",
					"        raise ValueError(f'The decimal value {decimal_value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...')"
				],
				"execution_count": 82
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_datatypes(datatypes, column_dictionary):\r\n",
					"    # Some datatypes come with an additionanal (set of) parameters. This function will add those datatypes to the initial types-dictionary defined above.\r\n",
					"    for value in column_dictionary.values():\r\n",
					"        if 'decimal' in value:\r\n",
					"            datatypes = add_decimal_datatype(value, datatypes)\r\n",
					"\r\n",
					"    return types"
				],
				"execution_count": 83
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 3. Define the functions to validate and create a delta table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def validate_table():\r\n",
					"    table_details = spark.sql(f\"DESCRIBE DETAIL silver.{table_name}\")\r\n",
					"\r\n",
					"    # Print all the details of the table\r\n",
					"    # for detail in table_details.columns:\r\n",
					"    #     expression = table_details.selectExpr(f\"first({detail})\").collect()[0][0]\r\n",
					"    #     print(str(detail) +\": \"+ str(expression))\r\n",
					"\r\n",
					"    # Expressions to validate\r\n",
					"    sql_table_format = table_details.selectExpr(f\"first(format)\").collect()[0][0]\r\n",
					"    sql_table_name = table_details.selectExpr(f\"first(name)\").collect()[0][0]\r\n",
					"    sql_table_location = table_details.selectExpr(f\"first(location)\").collect()[0][0]\r\n",
					"    # sql_table_partitionColumns = table_details.selectExpr(f\"first(partitionColumns)\").collect()[0][0] -&gt; Uncomment after integration\r\n",
					"\r\n",
					"    if sql_table_format != 'delta':\r\n",
					"        raise TypeError(\"The format of the table is not a delta table\")\r\n",
					"\r\n",
					"    if sql_table_name != full_name:\r\n",
					"        raise TypeError(f\"The name of the table {table_name} or the namespace {database_name} does not match with the name of the table {sql_table_name}\")\r\n",
					"\r\n",
					"    if storage_account not in sql_table_location:\r\n",
					"        raise ValueError(f\"The table is not stored on the given storage account {storage_account}. The storage location is {sql_table_location}\")\r\n",
					"\r\n",
					"    if database_name not in sql_table_location:\r\n",
					"        raise ValueError(f\"The table is not stored on the expected container {database_name}. The storage location is {sql_table_location}\")\r\n",
					"\r\n",
					"    if not dt.DeltaTable.isDeltaTable(spark, silver_path):\r\n",
					"        raise TypeError(f\"The files stored on path {silver_path} for table {table_name} are not a delta table\")\r\n",
					"\r\n",
					"    # Dev-Note: Validate that the folder actually exists in the container\r\n",
					"    # Dev-Note: Make sure that all columns of the config are also defined for the table\r\n",
					"\r\n",
					"    print(\"All validity checks were passed.\")\r\n",
					"    return True"
				],
				"execution_count": 84
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_table():\r\n",
					"    print(\"Creating table schema\")\r\n",
					"    add_columns = list()\r\n",
					"\r\n",
					"    for column_name in column_dict:\r\n",
					"        add_columns.append(dt.StructField(name=column_name,dataType=types[column_dict[column_name]],nullable=True, metadata={column_name: f\"metadata about the column {column_name}\"}))\r\n",
					"\r\n",
					"    print(\"Creating delta table\")\r\n",
					"    (dt.DeltaTable.createIfNotExists(spark)\r\n",
					"        .tableName(full_name)\r\n",
					"        .addColumns(add_columns)\r\n",
					"        .comment(table_description)\r\n",
					"        .location(f\"{silver_path}\")\r\n",
					"        .execute()\r\n",
					"    )"
				],
				"execution_count": 85
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 4. Orchestration: Run the functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"if __name__ == \"__main__\":\r\n",
					"\r\n",
					"    types = add_datatypes(types, column_dict)\r\n",
					"\r\n",
					"    print(f\"Starting Table deployment notebook for {full_name}\")\r\n",
					"    table_list = []\r\n",
					"    for table in spark.sql(f'show tables in {database_name}').collect():\r\n",
					"        table_list.append(table[1])\r\n",
					"    print(table_list)\r\n",
					"\r\n",
					"    if table_name in table_list:\r\n",
					"        if mssparkutils.fs.exists(silver_path):\r\n",
					"            print(f\"Table {full_name} exists already, validating structure...\")\r\n",
					"            validate_table()\r\n",
					"        else:\r\n",
					"            print(f\"Table {full_name} does exist, but not inside of the silver container.\")\r\n",
					"            print(f\"Removing {full_name} from silver db, and creating a new table &amp; link\")\r\n",
					"            clean_table(table_name)\r\n",
					"            create_table()\r\n",
					"\r\n",
					"    else:\r\n",
					"        print(f\"Table {full_name} does not exist, creating table...\")\r\n",
					"        create_table()"
				],
				"execution_count": 86
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\DeployDeltaTable_v2.json">
{
	"name": "DeployDeltaTable_v2",
	"properties": {
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "24a137f4-13b3-4983-b084-1c23b2e77f0c"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# DeployDeltaTable\r\n",
					"This script is called upon when changes are made to the configuration data. During the deployment of the configuration data, this script validates whether or not the configured table exists. If this is the case, some validitiy checks are executed to make sure that the table resembles the configured metadata. If the table does not exist, it is created."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Import necessary packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import necessary packages\r\n",
					"import delta.tables as dt\r\n",
					"import pyspark.sql as sql\r\n",
					"import json\r\n",
					"import re"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 2. Define the functions to validate and create a delta table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_decimal_datatype(decimal_value:str, datatypes:object) -&gt; object:\r\n",
					"\r\n",
					"    # Regex pattern\r\n",
					"    # The decimal is expected to look like 'decimal(&lt;xx&gt;,&lt;yy&gt;)'\r\n",
					"    pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\r\n",
					"\r\n",
					"\r\n",
					"    # First check if the decimal contains additional parameters\r\n",
					"    # If not: Nothing changes and the decimal will be assigned parameters (38,4) (38 characters, 4 of which after the comma)\r\n",
					"    if decimal_value == 'decimal':\r\n",
					"        return datatypes\r\n",
					"    # Otherwise: Check if there is a match with the regex\r\n",
					"    elif re.match(pattern, decimal_value):\r\n",
					"        match = re.match(pattern, decimal_value)\r\n",
					"        num_characters = int(match.group(1))\r\n",
					"        num_post_comma = int(match.group(2))\r\n",
					"        # Dev-Note: Add check to make sure that num_characters does not exceed 38 as pyspark cannot handle more, and that num_characters &gt; num_post_comma\r\n",
					"        datatypes[decimal_value] = sql.types.DecimalType(num_characters, num_post_comma)\r\n",
					"\r\n",
					"        return datatypes\r\n",
					"    # If there is also no match: The decimal-value has been configured incorrectly\r\n",
					"    else:\r\n",
					"        raise ValueError(f'The decimal value {decimal_value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def configure_datatypes(column_dictionary:dict) -&gt; dict:\r\n",
					"    # Create a dictionary with all of the data types that can be read by spark\r\n",
					"    # The keys of the dictionary are the values used in the metadata configuration files (SQL Database)\r\n",
					"    # The values of the dictionary are pyspark.sql datatypes \r\n",
					"    # Dev-Note: The default definition used for decimals is: 38 characters total, with 4 after the comma. \r\n",
					"    #           If specifics are given in the column_info (eg. decimal(12,2)), a new input will be added to the types-dictionary to accomodate this datatype\r\n",
					"    # https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.DecimalType.html\r\n",
					"\r\n",
					"    datatypes = {\"array\": sql.types.ArrayType(sql.types.StringType()), \"binary\": sql.types.BinaryType(), \r\n",
					"            \"boolean\": sql.types.BooleanType(), \"date\": sql.types.DateType(), \"string\": sql.types.StringType(), \r\n",
					"            \"timestamp\": sql.types.TimestampType(), \"decimal\": sql.types.DecimalType(38, 4), \"float\": sql.types.FloatType(), \r\n",
					"            \"byte\": sql.types.ByteType(), \"integer\": sql.types.IntegerType(), \"long_integer\":  sql.types.LongType()}\r\n",
					"\r\n",
					"\r\n",
					"    # Some datatypes come with an additionanal (set of) parameters. This function will add those datatypes to the initial types-dictionary defined above.\r\n",
					"    for value in column_dictionary.values():\r\n",
					"        # If the column_dictionary value contains a value with substring 'decimal' -&gt; Invoke the add_decimal_datatype function\r\n",
					"        if 'decimal' in value:\r\n",
					"            datatypes = add_decimal_datatype(value, datatypes)\r\n",
					"\r\n",
					"        # Throw an error if the value is not part of the datatypes dictionary and there is no if-statement configured to add it to the dictionary\r\n",
					"        elif value not in datatypes.keys():\r\n",
					"            raise ValueError(f\"The value '{value}' has not been configured as a datatype in the current set-up.\")\r\n",
					"\r\n",
					"    return datatypes"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def process_column_info_string(string_object:str) -&gt; object:\r\n",
					"    # Before converting the string_object to a json-object, replace all '-characters by \" first\r\n",
					"    column_info_string = (string_object.replace('\\'', '\"'))\r\n",
					"    # Convert the string_object string to a json object\r\n",
					"    try:\r\n",
					"        column_info_object = json.loads(column_info_string)\r\n",
					"        # Return the JSON object\r\n",
					"        return column_info_object\r\n",
					"    except:\r\n",
					"        raise TypeError(f\"The string_object that was passed cannot be converted to a json object. Passed string: {string_object}\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 3. Class object: Define DeltaTable methods"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class DeltaTableClass(object):\r\n",
					"    # Class used to define methods needed when creating a new delta table or validating the structure of an existing one\r\n",
					"    def __init__(self, table_name:str, database_name:str, storage_account:str, table_description:str, column_info:object) -&gt; object:\r\n",
					"\r\n",
					"        # Initialize the class-object parameters\r\n",
					"        self.table_name = table_name\r\n",
					"        self.database_name = database_name\r\n",
					"        self.storage_account = storage_account\r\n",
					"        self.table_description = table_description\r\n",
					"        self.column_info = column_info\r\n",
					"\r\n",
					"\r\n",
					"        # Based on the given parameters, create a set of derived parameters                                                     \r\n",
					"        self.full_name = self.database_name + '.' + self.table_name                                                              # Name of the delta table wit its schema/namespace: &lt;namespace&gt;.&lt;table_name&gt;\r\n",
					"        self.silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.database_name, self.storage_account, self.table_name) # Path to the table on the data lake storage account\r\n",
					"\r\n",
					"        # Create a list of json objects with all the column information\r\n",
					"        # Structure: {&lt;column_metadata&gt;: &lt;metadata_value&gt;}\r\n",
					"        # Example: \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'PK', 'data_type': 'string' }]\"\r\n",
					"        column_info_object = process_column_info_string(self.column_info)\r\n",
					"\r\n",
					"        # Create a dictionary matching the column_names and their data types\r\n",
					"        # Structure: {&lt;column_name&gt;: &lt;column_datatype&gt;}\r\n",
					"        # Example: { 'column_1': 'string', 'column_2': 'integer', ...}\r\n",
					"        self.column_dict = dict()\r\n",
					"        for structfield in column_info_object:\r\n",
					"            self.column_dict[structfield[\"column_name\"]] = structfield[\"data_type\"]\r\n",
					"        \r\n",
					"        # Derive a dictionary of pyspark.sql.types functions to match the given datatypes\r\n",
					"        # Structure: { &lt;datatype_reference&gt;: &lt;pyspark_datatype_reference&gt; }\r\n",
					"        # Example: { 'string': pyspark.sql.type.StringType(), 'integer': pyspark.sql.type.IntegerType()}\r\n",
					"        self.types = configure_datatypes(self.column_dict)\r\n",
					"\r\n",
					"    \r\n",
					"    def deploy_delta_table(self):\r\n",
					"        # Orchestrator function\r\n",
					"        # Check if the delta-table already exists and execute methods accordingly\r\n",
					"        print(f\"Starting Table deployment for table {self.full_name}\")\r\n",
					"\r\n",
					"        # Get list of existing tables in the database\r\n",
					"        table_list = []\r\n",
					"        for table in spark.sql(f'show tables in {self.database_name}').collect():\r\n",
					"            table_list.append(table[1])\r\n",
					"\r\n",
					"        # If the delta_table object contains the to-be configured table\r\n",
					"        #   -&gt; Check if there exists a folder on the storage account\r\n",
					"        #       True -&gt; Validate\r\n",
					"        #       False -&gt; Clean table from database and recreate table\r\n",
					"        if self.table_name in table_list:\r\n",
					"            if mssparkutils.fs.exists(self.silver_path):\r\n",
					"                print(f\"Table {self.full_name} exists already, validating structure...\")\r\n",
					"                self.validate_table()\r\n",
					"            else:\r\n",
					"                print(f\"Table {self.full_name} does exist, but not inside of the silver container.\")\r\n",
					"                print(f\"Removing {self.full_name} from silver db, and creating a new table &amp; link\")\r\n",
					"                clean_table(self.table_name)\r\n",
					"                self.create_table()\r\n",
					"\r\n",
					"        # Else: Create table from scratch\r\n",
					"        else:\r\n",
					"            print(f\"Table {self.full_name} does not exist, creating table...\")\r\n",
					"            self.create_table()\r\n",
					"\r\n",
					"\r\n",
					"    def create_table(self):\r\n",
					"\r\n",
					"        # Add all columns to add to the table to a list of StructField-objects\r\n",
					"        add_columns = list()\r\n",
					"        for column_name in self.column_dict:\r\n",
					"            add_columns.append(dt.StructField(name=column_name,dataType=self.types[self.column_dict[column_name]],nullable=True, metadata={column_name: f\"metadata about the column {column_name}\"}))\r\n",
					"\r\n",
					"        # Create the table if it does not exist\r\n",
					"        (dt.DeltaTable.createIfNotExists(spark)\r\n",
					"            .tableName(self.full_name)\r\n",
					"            .addColumns(add_columns)\r\n",
					"            .comment(self.table_description)\r\n",
					"            .location(f\"{self.silver_path}\")\r\n",
					"            .execute()\r\n",
					"        )\r\n",
					"\r\n",
					"\r\n",
					"    def validate_table(self):\r\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL silver.{self.table_name}\")\r\n",
					"\r\n",
					"        # Print all the details of the table\r\n",
					"        # for detail in table_details.columns:\r\n",
					"        #     expression = table_details.selectExpr(f\"first({detail})\").collect()[0][0]\r\n",
					"        #     print(str(detail) +\": \"+ str(expression))\r\n",
					"\r\n",
					"        # Expressions to validate\r\n",
					"        sql_table_format = table_details.selectExpr(f\"first(format)\").collect()[0][0]\r\n",
					"        sql_table_name = table_details.selectExpr(f\"first(name)\").collect()[0][0]\r\n",
					"        sql_table_location = table_details.selectExpr(f\"first(location)\").collect()[0][0]\r\n",
					"        # sql_table_partitionColumns = table_details.selectExpr(f\"first(partitionColumns)\").collect()[0][0] -&gt; Uncomment after integration\r\n",
					"\r\n",
					"        if sql_table_format != 'delta':\r\n",
					"            raise TypeError(\"The format of the table is not a delta table\")\r\n",
					"\r\n",
					"        if sql_table_name != self.full_name:\r\n",
					"            raise TypeError(f\"The name of the table {self.table_name} or the namespace {self.database_name} does not match with the name of the table {sql_table_name}\")\r\n",
					"\r\n",
					"        # Dev-Note: This might cause issues when, for example, the storage_account has the same name as the table or container\r\n",
					"        if self.storage_account not in sql_table_location:\r\n",
					"            raise ValueError(f\"The table is not stored on the given storage account {self.storage_account}. The storage location is {sql_table_location}\")\r\n",
					"        \r\n",
					"        # Dev-Note: This might cause issues when, for example, the database has the same name as the table or storage_account\r\n",
					"        if self.database_name not in sql_table_location:\r\n",
					"            raise ValueError(f\"The table is not stored on the expected container {self.database_name}. The storage location is {sql_table_location}\")\r\n",
					"\r\n",
					"        if not dt.DeltaTable.isDeltaTable(spark, self.silver_path):\r\n",
					"            raise TypeError(f\"The files stored on path {self.silver_path} for table {self.table_name} are not a delta table\")\r\n",
					"\r\n",
					"        # Dev-Note: Validate that the folder actually exists in the container\r\n",
					"        # Dev-Note: Make sure that all columns of the config are also defined for the table\r\n",
					"\r\n",
					"        # print(\"All validity checks were passed.\")\r\n",
					"        return True"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\DeployDeltaTable_v3.json">
{
	"name": "DeployDeltaTable_v3",
	"properties": {
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "1bd537ea-4bd2-4ce2-8b25-feaf10036ec8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# DeployDeltaTable\n",
					"This script is called upon when changes are made to the configuration data. During the deployment of the configuration data, this script validates whether or not the configured table exists. If this is the case, some validitiy checks are executed to make sure that the table resembles the configured metadata. If the table does not exist, it is created."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Import necessary packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import necessary packages\n",
					"import delta.tables as dt\n",
					"import pyspark.sql as sql\n",
					"import json\n",
					"import re"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 2. Define the functions to validate and create a delta table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_varchar_datatype(varchar_value:str, datatypes:object) -&gt; object:\n",
					"\n",
					"    # Regex pattern\n",
					"    # The varchar is expected to look like 'varchar(&lt;xxxx&gt;)'\n",
					"    pattern = r\"varchar\\(\\d+\\)\"\n",
					"\n",
					"    # First check if the varchar value contains a 'max' argument\n",
					"    if varchar_value == 'varchar(max)':\n",
					"        return datatypes\n",
					"    # Otherwise: Check if there is a match with the regex\n",
					"    elif re.match(pattern, varchar_value):\n",
					"        # If there is a match, add the value to the datatypes-dictionary with a StringType() datatype\n",
					"        # During the assignment, the value for 'varchar_value' will be added as metadata about the column\n",
					"        datatypes[varchar_value] = sql.types.StringType()\n",
					"        return datatypes\n",
					"    # If there is also no match: The varchar-value has been configured incorrectly\n",
					"    else:\n",
					"        raise ValueError(f'The varchar value {varchar_value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...')"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_decimal_datatype(decimal_value:str, datatypes:object) -&gt; object:\n",
					"\n",
					"    # Regex pattern\n",
					"    # The decimal is expected to look like 'decimal(&lt;xx&gt;,&lt;yy&gt;)'\n",
					"    pattern = r'^decimal\\((\\d{1,2}),(\\d{1,2})\\)$'\n",
					"\n",
					"    # First check if the decimal contains additional parameters\n",
					"    # If not: Nothing changes and the decimal will be assigned parameters (38,4) (38 characters, 4 of which after the comma)\n",
					"    if decimal_value == 'decimal':\n",
					"        return datatypes\n",
					"    # Otherwise: Check if there is a match with the regex\n",
					"    elif re.match(pattern, decimal_value):\n",
					"        # If there is a match: Extract the numbers from the string\n",
					"        # Assign the numerical values as the total number of characters and the numbers past the comma\n",
					"        match = re.match(pattern, decimal_value)\n",
					"        num_characters = int(match.group(1))\n",
					"        num_post_comma = int(match.group(2))\n",
					"        # Dev-Note: Add check to make sure that num_characters does not exceed 38 as pyspark cannot handle more, and that num_characters &gt; num_post_comma\n",
					"        datatypes[decimal_value] = sql.types.DecimalType(num_characters, num_post_comma)\n",
					"        return datatypes\n",
					"    # If there is also no match: The decimal-value has been configured incorrectly\n",
					"    else:\n",
					"        raise ValueError(f'The decimal value {decimal_value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...')"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def configure_datatypes(column_dictionary:dict) -&gt; dict:\n",
					"    # Create a dictionary with all of the data types that can be read by spark\n",
					"    # The keys of the dictionary are the values used in the metadata configuration files (SQL Database)\n",
					"    # The values of the dictionary are pyspark.sql datatypes \n",
					"    # Dev-Note: The default definition used for decimals is: 38 characters total, with 4 after the comma. \n",
					"    #           If specifics are given in the column_info (eg. decimal(12,2)), a new input will be added to the types-dictionary to accomodate this datatype\n",
					"    # https://spark.apache.org/docs/3.1.3/api/python/reference/api/pyspark.sql.types.DecimalType.html\n",
					"\n",
					"    datatypes = {\"array\": sql.types.ArrayType(sql.types.StringType()), \"binary\": sql.types.BinaryType(), \n",
					"            \"boolean\": sql.types.BooleanType(), \"date\": sql.types.DateType(), \"string\": sql.types.StringType(), \"varchar(max)\": sql.types.StringType(), \n",
					"            \"timestamp\": sql.types.TimestampType(), \"decimal\": sql.types.DecimalType(38, 4), \"float\": sql.types.FloatType(), \n",
					"            \"byte\": sql.types.ByteType(), \"integer\": sql.types.IntegerType(), \"int\": sql.types.IntegerType(), \"long_integer\":  sql.types.LongType()}\n",
					"\n",
					"\n",
					"    # Some datatypes come with an additionanal (set of) parameters. This function will add those datatypes to the initial types-dictionary defined above.\n",
					"    for value in column_dictionary.values():\n",
					"        # If the column_dictionary value contains a value with substring 'decimal' -&gt; Invoke the add_decimal_datatype function\n",
					"        if 'decimal' in value:\n",
					"            datatypes = add_decimal_datatype(value, datatypes)\n",
					"        elif 'varchar' in value:\n",
					"            datatypes = add_varchar_datatype(value, datatypes)\n",
					"\n",
					"        # Throw an error if the value is not part of the datatypes dictionary and there is no if-statement configured to add it to the dictionary\n",
					"        elif value not in datatypes.keys():\n",
					"            raise ValueError(f\"The value '{value}' has not been configured as a datatype in the current set-up.\")\n",
					"\n",
					"    return datatypes"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def process_column_info_string(string_object:str) -&gt; object:\n",
					"    # Before converting the string_object to a json-object, replace all '-characters by \" first\n",
					"    column_info_string = (string_object.replace('\\'', '\"'))\n",
					"    # Convert the string_object string to a json object\n",
					"    try:\n",
					"        column_info_object = json.loads(column_info_string)\n",
					"        # Return the JSON object\n",
					"        return column_info_object\n",
					"    except:\n",
					"        raise TypeError(f\"The string_object that was passed cannot be converted to a json object. Passed string: {string_object}\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_structfield_logging_columns(structfield_list):\n",
					"    structfield_list.append(dt.StructField(name='t_load_date_raw' ,dataType=sql.types.TimestampType(),nullable=True, metadata={'t_load_date_raw': f\"Date of when the row was loaded into the raw layer table\"}))\n",
					"    structfield_list.append(dt.StructField(name='t_load_date_silver' ,dataType=sql.types.TimestampType(),nullable=True, metadata={'t_load_date_silver': f\"Date of when the row was loaded into the delta table\"}))\n",
					"    structfield_list.append(dt.StructField(name='t_file_name' ,dataType=sql.types.StringType(),nullable=True, metadata={'t_load_date': f\"Name of the file where the data is coming from\"}))\n",
					"    structfield_list.append(dt.StructField(name='t_plan_id'   ,dataType=sql.types.IntegerType(),nullable=False, metadata={'t_plan_id': f\"The id of the plan in the meta.log_plans table\"}))\n",
					"    structfield_list.append(dt.StructField(name='t_task_id'   ,dataType=sql.types.IntegerType(),nullable=False, metadata={'t_task_id': f\"The id of the task in the meta.log_tasks table\"}))\n",
					"    structfield_list.append(dt.StructField(name='t_file_id'   ,dataType=sql.types.IntegerType(),nullable=False, metadata={'t_file_id': f\"The id of the file in the meta.log_files table\"}))\n",
					"\n",
					"    return structfield_list"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\DeployOrchestrator.json">
{
	"name": "DeployOrchestrator",
	"properties": {
		"folder": {
			"name": "Deploy"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "5",
				"spark.autotune.trackingId": "01bce0d1-1cf0-4f19-9a7c-39b343326906"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Deploy Orchestrator\n",
					"This is an orchestrator notebook that is used to deploy a set of delta tables.\n",
					"The set of delta tables, and their properties, are passed as a string to this notebook. This string is converted into a list of objects, after which the for-loop will loop over each object. Each object will be converted into a class-object of the DeltaTable-class (See notebook: DeployDeltaTable_v2) and the necessary functions are called to deploy the object as a delta table.\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 1. Parameters\n",
					"Define the necessary parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import json"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Parameters\n",
					"## Get a list of objects, where each object resembles the metadata for a certain delta table\n",
					"table_definition_list = '''[\n",
					"    {\n",
					"        \"table_name\": \"deploy_test\",\n",
					"        \"container_name\": \"silver\",\n",
					"        \"storage_account\": \"devdapstdala1\",\n",
					"        \"table_description\": \"test if the deploy script works\",\n",
					"        \"target_options\": {},\n",
					"        \"column_info\": \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'PK', 'data_type': 'timestamp' }, {   'column_sequence': 2,   'column_name': 'column_2',   'dimension': 'SCD2',   'data_type': 'int' }, {   'column_sequence': 3,   'column_name': 'column_3',   'dimension': 'SCD2',   'data_type': 'decimal(12,3)' }]\"\n",
					"    },\n",
					"    {\n",
					"        \"table_name\": \"deploy_test\",\n",
					"        \"container_name\": \"silver\",\n",
					"        \"storage_account\": \"devdapstdala1\",\n",
					"        \"table_description\": \"test if the deploy script works\",\n",
					"        \"target_options\": {},\n",
					"        \"column_info\": \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'PK', 'data_type': 'varchar(10)' }, {   'column_sequence': 2,   'column_name': 'column_2',   'dimension': 'SCD2',   'data_type': 'int' }, {   'column_sequence': 3,   'column_name': 'column_3',   'dimension': 'SCD2',   'data_type': 'decimal(12,3)' }]\"\n",
					"    },\n",
					"    {\n",
					"        \"table_name\": \"deploy_test_v2\",\n",
					"        \"container_name\": \"silver\",\n",
					"        \"storage_account\": \"devdapstdala1\",\n",
					"        \"table_description\": \"test what happens to decimal datatypes\",\n",
					"        \"target_options\": {},\n",
					"        \"column_info\": \"[{ 'column_sequence': 1, 'column_name': 'column_1', 'dimension': 'PK', 'data_type': 'varchar(max)' }, {   'column_sequence': 2,   'column_name': 'column_2',   'dimension': 'SCD2',   'data_type': 'int' }, {   'column_sequence': 3,   'column_name': 'column_3',   'dimension': 'SCD2',   'data_type': 'decimal(12,5)' }]\"\n",
					"    }\n",
					"]'''"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 2. \"Import\" the relevant notebooks and their functions/methods"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/GenericFunctions"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Classes"
				],
				"execution_count": 10
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## 3. Execute\n",
					"Loop over all delta-table objects and run the deploy_delta_table method of the DeltaTable-class"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Convert the 'table_definition_list' parameter into a list of json objects\n",
					"table_definition_list:str = replace_string_value(table_definition_list, '\\\\', '\\\\\\\\')\n",
					"table_definition_objects = json.loads(table_definition_list)\n",
					"\n",
					"default_target_options = {'partitioning': []}\n",
					"# Loop over each object\n",
					"for table_definition in table_definition_objects:\n",
					"    # Create a dictionary that will overwrite default values if they are specified in the table_definition-dictionary\n",
					"\n",
					"    # Dev-Info: Dictionary unpacking or keyword argument unpacking using **-syntax\n",
					"    #   When you use the ** operator on a dictionary, it unpacks the dictionary's key-value pairs into separate keyword arguments\n",
					"    #       Example: d = {'a': 1, 'b': 2}} ; function_name(**d) will pass the dictionary values as a=1,b=2 instead of the dev having the explicitly define the arguments like this\n",
					"    #   When you use the {**dict1, **dict2} syntax, Python merges the key-value pairs of both dictionaries into a new dictionary. \n",
					"    #   If there are duplicate keys, the values from the second dictionary (dict2) override the values from the first dictionary (dict1).\n",
					"    target_options_dict = table_definition['target_options']\n",
					"    full_target_options_dict  = {**default_target_options, **target_options_dict}\n",
					"\n",
					"    # Create a DeltaTableClass-object with the defaulted_table_definition keys\n",
					"    delta_table = DeployDeltaTable(\n",
					"        table_name          = table_definition['table_name'], \n",
					"        container_name      = table_definition['container_name'], \n",
					"        storage_account     = table_definition['storage_account'],\n",
					"        table_description   = table_definition['table_description'], \n",
					"        column_info         = table_definition['column_info'],\n",
					"        partitioning        = full_target_options_dict['partitioning']\n",
					"    )\n",
					"\n",
					"    # Run the deploy_delta_table-method for each class-object\n",
					"    delta_table.deploy_delta_table()"
				],
				"execution_count": 11
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\ErrorHandling.json">
{
	"name": "ErrorHandling",
	"properties": {
		"folder": {
			"name": "Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore2",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "4c0bdd64-16d8-4170-b9be-d6d11c3c1bd0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore2",
				"name": "devdapsspcore2",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore2",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"def get_exception_information(thrown_error:str) -&gt; tuple:\r\n",
					"    \"\"\"\r\n",
					"    Get the underlying error class and message from an error_block thrown by Python.\r\n",
					"    When dealing with spark related issues, Py4JJavaError is the class of the error, which aims to say \"Java throw an error\"\r\n",
					"    To get the Java error, some scripting is necessary to figure out the specific issue\r\n",
					"\r\n",
					"    :param thrown_error:        The entire error that was thrown by the notebook\r\n",
					"\r\n",
					"    :return exception_class:    String with the name of the Error_Class (ValueError (python), FileNotFoundError (spark), ConfigurationError (custom), etc.)\r\n",
					"    :return execption_message:  String with the entire error message\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Get the class-type of the thrown error, and extract the name of the error\r\n",
					"    error_type = type(thrown_error).__name__\r\n",
					"    # If the error is of type Py4JJavaError:\r\n",
					"    #   Get the underlying argument 'java_exception', which contains the error class-name and error message\r\n",
					"    if error_type == 'Py4JJavaError':\r\n",
					"        exception_class = thrown_error.java_exception.getClass().getName().rsplit('.', 1)[-1]\r\n",
					"        exception_message = thrown_error.java_exception.getMessage()\r\n",
					"    # If the error is not a Java error: Get the message and error class\r\n",
					"    else:\r\n",
					"        exception_class = error_type\r\n",
					"        exception_message = str(thrown_error)\r\n",
					"    \r\n",
					"    return exception_class, exception_message"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def is_custom_error(error_class:str) -&gt; bool:\r\n",
					"    \"\"\"\r\n",
					"    Returns True if the error is a custom error, False otherwise.\r\n",
					"\r\n",
					"    :param error_class: Contains the class-type of the error (ValueError, FileNotFoundError, ConfigurationError, etc.)\r\n",
					"\r\n",
					"    :return boolean: Value indicating whether or not the error_class is a custom error \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Get the list of custom error classes\r\n",
					"    # Assumption: All the custom error classes are subclasses of GenericErrorClass\r\n",
					"    custom_error_classes = [cls.__name__ for cls in globals().values() if isinstance(cls, type) and issubclass(cls, GenericErrorClass)]\r\n",
					"    if error_class in custom_error_classes:\r\n",
					"        return True\r\n",
					"    return False"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def handle_exceptions(exception_class:str, exception_message:str, exception_object:object) -&gt; dict:\r\n",
					"    \"\"\"\r\n",
					"    If the exception_class is not a custom error, create a GenericErrorClass object with some custom messages\r\n",
					"    Otherwise, return the object that should already be created during the initial error-raise\r\n",
					"\r\n",
					"    :param exception_class:     The error-class of the error object\r\n",
					"    :param exception_message:   The error message thrown by the error class\r\n",
					"    :param exception_object:    The entire error object\r\n",
					"\r\n",
					"    :result error_object:       Object containing the error that will be returned in the logging tables of the SQL Meta Database\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # [ErrorHandling] Check if the exception_class is a custom error class or not\r\n",
					"    custom_error = is_custom_error(exception_class)\r\n",
					"\r\n",
					"    if not custom_error:\r\n",
					"        # If the error is not a custom error:\r\n",
					"        # 1. Define the below set of arguments\r\n",
					"        #   Dev-Info:   The idea is that when dealing with an exception that is not part of the custom classes, it has most likely something to do with a code-issue\r\n",
					"        #               The below arguments warn the user about this, and make sure that the exception is flagged as something that needs to be dealt with by the DAP Core Engineers\r\n",
					"        #               The final error-message will also contain the original error message, and a general guide for the DAP Core Engineers on how to replicate the error \r\n",
					"        custom_message = \"Exception: unknown exception. Rerun task %{task_name}% (task_id: %{task_id}% , plan_id: %{plan_id}%) in debug-mode to see where the error is coming from. After debugging, make sure to add the use case to the ErrorHandling notebook.\"\r\n",
					"        custom_error_class = \"Exception\"\r\n",
					"        notebook_name = \"MetaNotebook\"\r\n",
					"        responsible_team = \"DAP Core Engineers\"\r\n",
					"        exception_message = str(exception_message).replace(\"'\", \"\")\r\n",
					"\r\n",
					"        # 2. Pass the arguments to a GenericErrorClass object and set the error_object\r\n",
					"        generic_error_object = GenericErrorClass()\r\n",
					"        error_object = generic_error_object.__report__(custom_error_class=custom_error_class, custom_message=custom_message, notebook_name=notebook_name, responsible_team=responsible_team, python_error_class=exception_class, python_error_message=exception_message)\r\n",
					"        return error_object\r\n",
					"\r\n",
					"    # When dealing with a custom error, the error_object should already be made and can therefore be returned as a whole\r\n",
					"    else:\r\n",
					"        return exception_object.error_object"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class GenericErrorClass(Exception):\r\n",
					"    \"\"\"\r\n",
					"    This is the generic class that all custom errors should be based on\r\n",
					"    Purpose: Whenever a custom error (MiddlewareError, ConfigurationError, etc.) is thrown, it creates an error_object. This object will be returned to the logging tables on the SQL Server\r\n",
					"    \"\"\"\r\n",
					"    def __report__(self, custom_error_class:str, custom_message:str, notebook_name:str, class_name:str=None, function_name:str=None, *args:list, **kwargs:dict) -&gt; dict:\r\n",
					"        \"\"\"\r\n",
					"        The __report__ function will create the error_object, which is a \"report\" on the error that occured\r\n",
					"\r\n",
					"        :param custom_error_class:  The name of the custom error class (GenericErrorClass, MiddleWareError, ConfigurationError) \r\n",
					"        :param custom_message:      The custom error message thrown by the custom class\r\n",
					"        :param notebook_name:       The name of the notebook where the error is coming from\r\n",
					"        :param class_name:          The name of the class where the error is coming from (if any)\r\n",
					"        :param function_name:       The name of the function where the error is coming from (if any)\r\n",
					"        :param args:                Additional list of arguments that are passed by the user/system\r\n",
					"        :param kwargs:              Additional key-word (aka dictionary) arguments that are passed by the user/system\r\n",
					"\r\n",
					"        :return error_object:       Dictionary containing the most relevant information about the error that has been thrown\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        # Create dictionary 'full_message', containing the arguments that have been passed to the __report__ method\r\n",
					"        self.custom_error_class = custom_error_class\r\n",
					"        self.custom_message = custom_message.replace(\"'\", \"\")\r\n",
					"        self.error_location = f\"[{notebook_name or ''}:{class_name or ''}:{function_name or ''}]\"\r\n",
					"        self.full_message = {\r\n",
					"            \"custom_message\": self.custom_message,\r\n",
					"            \"custom_error_class\": self.custom_error_class,\r\n",
					"            \"error_location_in_notebooks\": self.error_location\r\n",
					"        }\r\n",
					"\r\n",
					"        # Unite the 'full_message' with the kwargs-dictionary to create a complete dictionary of \r\n",
					"        self.error_object = {**self.full_message, **kwargs}\r\n",
					"        \r\n",
					"        # Initialize an Exception-class with the error_object representing the kwargs-argument of that class\r\n",
					"        super().__init__(self.error_object)\r\n",
					"\r\n",
					"        # Return the object\r\n",
					"        return self.error_object"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class MiddleWareError(GenericErrorClass):\r\n",
					"    \"\"\"\r\n",
					"    Custom error class, to be used for errors related to the MiddleWare team \r\n",
					"    Examples:\r\n",
					"        1. Expected files were not landed\r\n",
					"        2. The name of the file does not match expectations\r\n",
					"        etc.\r\n",
					"    \"\"\"\r\n",
					"    def __init__(self, custom_message:str, notebook_name:str, class_name:str=None, function_name:str=None, *args:list, **kwargs:dict):\r\n",
					"        \"\"\"\r\n",
					"        :param custom_message:      The custom error message thrown by the custom class\r\n",
					"        :param notebook_name:       The name of the notebook where the error is coming from\r\n",
					"        :param class_name:          The name of the class where the error is coming from (if any)\r\n",
					"        :param function_name:       The name of the function where the error is coming from (if any)\r\n",
					"        :param args:                Additional list of arguments that are passed by the user/system\r\n",
					"        :param kwargs:              Additional key-word (aka dictionary) arguments that are passed by the user/system\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        custom_error_class=\"MiddleWareError\"\r\n",
					"        responsible_team = \"Middleware Team (add email)\"\r\n",
					"\r\n",
					"        # Call the __report__ method of the GenericErrorClass to create the error_object\r\n",
					"        super().__report__(\r\n",
					"            custom_error_class=custom_error_class, \r\n",
					"            custom_message=custom_message, \r\n",
					"            notebook_name=notebook_name, \r\n",
					"            class_name=class_name,\r\n",
					"            function_name=function_name,\r\n",
					"            responsible_team=responsible_team, \r\n",
					"            **kwargs\r\n",
					"        )   "
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class ConfigurationError(GenericErrorClass):\r\n",
					"    \"\"\"\r\n",
					"    Custom error class, to be used for error related to file configuration\r\n",
					"    Examples\r\n",
					"        1. The configured headers to not match with the headers in the source file\r\n",
					"        2. The datatype of a column does not match with configuration\r\n",
					"        etc.\r\n",
					"    \"\"\"\r\n",
					"    def __init__(self, custom_message:str, notebook_name:str, class_name:str=None, function_name:str=None, *args:list, **kwargs:dict):\r\n",
					"        \"\"\"\r\n",
					"        :param custom_message:      The custom error message thrown by the custom class\r\n",
					"        :param notebook_name:       The name of the notebook where the error is coming from\r\n",
					"        :param class_name:          The name of the class where the error is coming from (if any)\r\n",
					"        :param function_name:       The name of the function where the error is coming from (if any)\r\n",
					"        :param args:                Additional list of arguments that are passed by the user/system\r\n",
					"        :param kwargs:              Additional key-word (aka dictionary) arguments that are passed by the user/system\r\n",
					"\r\n",
					"        \"\"\"\r\n",
					"\r\n",
					"        custom_error_class=\"ConfigurationError\"\r\n",
					"        responsible_team = \"DAP Engineers (add email)\"\r\n",
					"\r\n",
					"        # Call the __report__ method of the GenericErrorClass to create the error_object\r\n",
					"        super().__report__(\r\n",
					"            custom_error_class=custom_error_class, \r\n",
					"            custom_message=custom_message, \r\n",
					"            notebook_name=notebook_name, \r\n",
					"            class_name=class_name,\r\n",
					"            function_name=function_name,\r\n",
					"            responsible_team=responsible_team, \r\n",
					"            **kwargs\r\n",
					"        )"
				],
				"execution_count": 19
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\FilWorker.json">
{
	"name": "FilWorker",
	"properties": {
		"folder": {
			"name": "Workers"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f4a3c2ff-eed4-4f6a-ae48-5301ff01eda9"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "python"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## FilWorker\n",
					"\n",
					"\n",
					"The FilWorker is used during preprocessing. The purpose of the worker is to:\n",
					"- Obtain a set of header-names  of a source file, and move them into separate columns"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def run_fil_worker(task:object, env_code:str, debug:bool=True):\n",
					"    '''\n",
					"    Inputs\n",
					"        task: An object of the Task-Class [Modules/Classes]\n",
					"        env_code: The code used to indicate the environment the functions are running in (dev, int, tst, acc, prd)\n",
					"        debug: Boolean indicating whether the session is being run in Debug mode. This will trigger more information (print statements) for the user\n",
					"\n",
					"    Functionality:\n",
					"        For each file that is handled by this worker, the first line of values will be extracted.\n",
					"        For the indicated indices, the values will be added as a new column to the file\n",
					"        A new set of header values is assigned to the file\n",
					"\n",
					"    Example:\n",
					"        Convert 2nd column to a row and rename the columns to [header_a, header_b, header_c, header_d]\n",
					"        File example:\n",
					"            column_a, column_b, column_c\n",
					"            value_a1, value_b1, value_c1\n",
					"            value_a2, value_b2, value_c2\n",
					"\n",
					"        Resulting dataframe:\n",
					"            header_a, header_b, header_c, header_d\n",
					"            value_a1, value_b1, value_c1, column_b\n",
					"            value_a2, value_b2, value_c2, column_b\n",
					"\n",
					"    '''\n",
					"\n",
					"\n",
					"    # Set variable 'data_lake_storage_account' using the env_code to link to the relevant storage account\n",
					"    data_lake_storage_account:str = f\"{env_code}dapstdala1\"\n",
					"    if debug: print(f\"[FilWorker] Data lake storage account: {data_lake_storage_account}\")\n",
					"\n",
					"\n",
					"\n",
					"    # Set list of the metadata that needs to be extracted from the SQL Meta DB\n",
					"    metadata_keys:list = [\n",
					"        'file_pattern', \n",
					"        'file_kind',\n",
					"        'source_folder', \n",
					"        'column_delimiter', \n",
					"        'file_extension', \n",
					"        'container_name', \n",
					"        'source_name', \n",
					"        'header', \n",
					"        'table_name',\n",
					"        'escape_character', \n",
					"        'quote_character'\n",
					"    ]\n",
					"    if debug: print(f\"[FilWorker] Metadata variables: {', '.join(metadata_keys)}\")\n",
					"\n",
					"    # [Modules/Classes] Run stored procedure 'usp_get_task_metadata' and set the task-object class-member values\n",
					"    task.get_task_metadata(metadata_keys)\n",
					"    task.set_dataset_variables(data_lake_storage_account)\n",
					"    if debug: print(f\"[FilWorker] Task variables: {task.variables}\")\n",
					"\n",
					"\n",
					"    if debug: print(f\"[FilWorker] Filtering directory contents: path = '{task.landing_path}';  file_pattern: {task.variables['file_pattern']}\")\n",
					"    \n",
					"    # [Classes:Task] Get the list of files to process\n",
					"    file_objects:list = task.get_files_objects(data_lake_storage_account)\n",
					"\n",
					"    # [Modules/Classes] Execute a set of functions to successfully execute the first-line conversion for each file\n",
					"    for file_object in file_objects:\n",
					"        print(f\"[FilWorker] Processing file: {file_object.file_name} ({datetime.datetime.now().strftime('%H:%M:%S')})\")\n",
					"        \n",
					"        if debug: print(f\"[FilWorker] Log file\")\n",
					"        file_object.log_file()\n",
					"\n",
					"        if debug: print(\"[FilWorker] Create object of SparkDataFrameClass-Class\")\n",
					"        # [Modules/Dataframes_v2] Create object of CalculatedFieldHeaders-Class\n",
					"        dataframe_object = SparkDataFrameClass(debug=debug)\n",
					"        dataframe_object.load_dataframe(source_path=file_object.landing_path, header=file_object.task.header, separator=file_object.task.separator, file_kind=file_object.task.file_kind, quote_character=file_object.task.quote_character, escape_character=file_object.task.escape_character)\n",
					"        if debug: print(\"[FilWorker] Convert first row to extra columns\")\n",
					"        destination_path = file_object.landing_path.replace(task.file_path, task.table_name)\n",
					"        # Dev-Note: start_index and end_index are hardcoded -&gt; Make them dynamic\n",
					"        dataframe_object.convert_firstrow_to_columns(start_index = 1, end_index=3, column_names=task.source_column_names, header=task.header, destination_path=destination_path, file_extension=task.file_extension, column_delimiter=task.separator)\n",
					"\n",
					"        if debug: print(f\"[FilWorker] Move file to ARCHIVE\")\n",
					"        file_object.landing_to_archive()\n",
					"        dataframe_object.unpersist_dataframe()"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\GenericFunctions.json">
{
	"name": "GenericFunctions",
	"properties": {
		"folder": {
			"name": "Functions"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "15ec3da0-b18e-4b91-8a23-b335c3db9303"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import re\r\n",
					"import json\r\n",
					"import py4j\r\n",
					"import os"
				],
				"execution_count": 196
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 197
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": 198
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define global variables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define a set of regex special characters\r\n",
					"REGEX_SPECIAL_CHARACTERS = r\"[*+?^${}()|[\\]]\"  # Note: forward slash is not included as regex-matching will be executed on path-like variables"
				],
				"execution_count": 199
			},
			{
				"cell_type": "code",
				"source": [
					"# Dev-Note: # https://github.com/allrod5/parameters-validation -&gt; Use this instead\n",
					"# Dev-Note: Not possible to install this module -&gt; Relevant for \n",
					"def validate_argument(argument_name, argument_value, allowed_list):\n",
					"    if argument_value not in allowed_list:\n",
					"        raise ValueError(f\"The {argument_name}-argument '{str(argument_value)}' is not listed in the allowed {argument_name} list: {allowed_list}\")"
				],
				"execution_count": 200
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def validate_argument_list(argument_name:str, argument_list:list, allowed_list:list):\n",
					"    # Create an empty object to store all 'not allowed parameters'\n",
					"    not_allowed_list = []\n",
					"\n",
					"    # Check for every item in the argument_list if it is contained in the allowed_list\n",
					"    # If not: Add the item to the not_allowed_list\n",
					"    for argument in argument_list:\n",
					"        if argument not in allowed_list:\n",
					"            not_allowed_list.append(argument)\n",
					"\n",
					"    # If not_allowed_list not empty: raise error\n",
					"    if len(not_allowed_list) &gt; 0:\n",
					"            raise ValueError(f\"[GenericFunctions] The {argument_name}-argument does not take the following list of give parameters: '{', '.join(not_allowed_list)}'; Allowed values: {', '.join(allowed_list)}\")\n",
					""
				],
				"execution_count": 201
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def validate_linked_service(linked_service):\n",
					"    # Validate that the linked_service value exist in the list of linked services\n",
					"    try:\n",
					"        mssparkutils.credentials.getPropertiesAll(linked_service)\n",
					"\n",
					"    except Exception as exception:\n",
					"        # If the error states that the linked_service does not exist, throw a custom error\n",
					"        if 'the linked service does not exist or is not published' in str(exception):\n",
					"            raise ValueError(f\"Linked service {linked_service} does not exist or has not been published\") from None\n",
					"        # If a different error is thrown, throw the entire error message\n",
					"        else:\n",
					"            raise exception"
				],
				"execution_count": 202
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def filter_list(full_list:list, pattern:str, extension:str=None) -&gt; list:\r\n",
					"    \"\"\"\r\n",
					"    Return a list based on a pattern and extension\r\n",
					"\r\n",
					"    :param full_list:       The full list of items that may or may not contain the pattern\r\n",
					"    :param pattern:         The pattern that will be used to filter on\r\n",
					"    :param extension:       The extension expected to be at the end of the item (if applicable)\r\n",
					"\r\n",
					"    :result patterned_list: The list of items that only contain the pattern and extension\r\n",
					"\r\n",
					"    :note   The inclusion of 'extension' is to avoid that the 'finished' files are also loaded into the silver layer\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # Error handling: If the configuration contains * for the match_pattern -&gt; Set it to .* to have it comply with regex rules\r\n",
					"    if pattern == '*':\r\n",
					"        pattern = '.*'\r\n",
					"\r\n",
					"    # Compile a regular expression pattern into a regular expression object, which can be used for matching\r\n",
					"    compiled_pattern = re.compile(pattern)\r\n",
					"    # Return a list of FileObjects that matches with the pattern\r\n",
					"    patterned_list= [file for file in full_list if compiled_pattern.search(file.name)]\r\n",
					"    \r\n",
					"    # If an extension is given:\r\n",
					"    if extension:\r\n",
					"        # Compile a regular expression pattern into a regular expression object, which can be used for matching\r\n",
					"        compiled_pattern = re.compile(extension)\r\n",
					"        # Return a list of FileObjects that matches with the extension\r\n",
					"        patterned_list= [file for file in patterned_list if compiled_pattern.search(file.name)]\r\n",
					"\r\n",
					"    # Return the filtered list with FileObjects\r\n",
					"    return patterned_list"
				],
				"execution_count": 203
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def contains_regex_special_chars(value:str) -&gt; str:\r\n",
					"    \"\"\"    \r\n",
					"    Check if any character in the string is a regex special character\r\n",
					"\r\n",
					"    :param value:       String value that does or does not have any regex characters\r\n",
					"\r\n",
					"    :result boolean:    Indication of whether or not the value-parameter contains a regex character\r\n",
					"    \"\"\"    \r\n",
					"    return any(char in REGEX_SPECIAL_CHARACTERS for char in value)"
				],
				"execution_count": 204
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def split_string_at_regex(value:str, split_character:str) -&gt; str:\r\n",
					"    \"\"\"\r\n",
					"    Given a string with separator values (;, /, etc.), split the string and only return the first part of the string until the first regex.\r\n",
					"    This function is part of a greater construct that gives a list of values that need to match with a regex, but the function lists to values iteratively.\r\n",
					"\r\n",
					"    :param value:           String value that (potentially) contains a regex-pattern\r\n",
					"    :param split_character: Character in the string of where to split the value  \r\n",
					"\r\n",
					"    :return path:           Splitted string value where no regex-like pattern is contained\r\n",
					"\r\n",
					"    :example\r\n",
					"        value = this/is/a/$w+/regex/path\r\n",
					"        split_character = /\r\n",
					"\r\n",
					"        path = this/is/a/\r\n",
					"\r\n",
					"    :example\r\n",
					"        value = 'this is *w+ a regex .* sentence'\r\n",
					"        split_character = ' ' (space)\r\n",
					"\r\n",
					"        path = this is\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # If value is empty, return ConfigurationError\r\n",
					"    if not value:\r\n",
					"        raise ConfigurationError(\r\n",
					"            custom_message=f\"User did not provide a value for the source_folder: {value=}\",\r\n",
					"            notebook_name = \"GenericFunctions\",\r\n",
					"            function_name = \"split_string_at_regex\"\r\n",
					"        )\r\n",
					"\r\n",
					"    # Separate the value-string using the split_character\r\n",
					"    parts: list = value.split(sep=split_character)\r\n",
					"    path_parts: list = []\r\n",
					"\r\n",
					"    # Loop over all parts of the string until a regex-like expression is picked up\r\n",
					"    # Break the loop when this happens\r\n",
					"    for part in parts:\r\n",
					"        if not contains_regex_special_chars(part):\r\n",
					"            path_parts.append(part)\r\n",
					"        else:\r\n",
					"            break\r\n",
					"    \r\n",
					"    # Re-join the path parts with the split character\r\n",
					"    path = split_character.join(path_parts)\r\n",
					"\r\n",
					"    # If the path is empty and the string did not contain any regex-character:\r\n",
					"    # Return the initial value -&gt; No split \r\n",
					"    if not path and all(contains_regex_special_chars(part) for part in parts):\r\n",
					"        return value\r\n",
					"\r\n",
					"    return path"
				],
				"execution_count": 205
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def locate_static_path(regex_path:str) -&gt; tuple:\r\n",
					"    \"\"\"\r\n",
					"    Given a regex abfss-path, return all the files that regex-match with the path after looping through the root-folder (non_regex_path)\r\n",
					"    After getting back all the files, a regex-search will only keep the files where the file-path matches the regex_path\r\n",
					"\r\n",
					"    :param regex_path:          Abfss-path with at least one regex character \r\n",
					"\r\n",
					"    :result remaining_files:    List of files that match with the regex_path\r\n",
					"    :result folders:            All folders in the non-regex path \r\n",
					"\r\n",
					"    :note There is no filter in the folder-list as this would remove all parent folders that are on a higher level than the regex_path\r\n",
					"        :example regex_path = &lt;parent&gt;/.*/&lt;child&gt; -&gt; folder &lt;parent&gt;/ACC does not match with the regex_path\r\n",
					"\r\n",
					"    :example\r\n",
					"        regex_path = this/is/a/$w+/path/with/regex\r\n",
					"        remaining_files = list of files where path complies with regex_path\r\n",
					"        folder = list of folders where path is like non_regex_path\r\n",
					"    \"\"\"\r\n",
					"    # Split string at recursive places\r\n",
					"    non_regex_path = split_string_at_regex(regex_path, split_character='/')\r\n",
					"    regex_path = re.compile(regex_path)\r\n",
					"    \r\n",
					"    # Look for directory content at the first part of the path (non regex)\r\n",
					"    files, folders = list_directory_content(non_regex_path, [], [])\r\n",
					"\r\n",
					"    # Use the regex-path to filter through the returned file list\r\n",
					"    remaining_files = [file for file in files if regex_path.search(file.path)]\r\n",
					"        \r\n",
					"    return remaining_files, folders"
				],
				"execution_count": 218
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def list_directory_content(path:str, files:list, folders:list) -&gt; tuple:\n",
					"    '''\n",
					"    List the contents of a path/container (non recursive)\n",
					"\n",
					"    :param path: contains the path used to search for folders and files\n",
					"    :param files: contains a list of files that should be found\n",
					"    :param folders:contains a list of folders that should be found\n",
					"\n",
					"    :result files,folders : return all the files/folders that can recursively be found in the container/path\n",
					"    '''\n",
					"\n",
					"    # If path has regex:\n",
					"    # Recall function w specific regex properties\n",
					"    if contains_regex_special_chars(path):\n",
					"        files, folders = locate_static_path(regex_path=path)\n",
					"        return files, folders # Return the lists\n",
					"\n",
					"    else:\n",
					"        try:\n",
					"            path_contents = mssparkutils.fs.ls(path)\n",
					"            for item in path_contents:\n",
					"                # If the content is a directory, append it to the folder-list\n",
					"                # Use the directory-path as a new starting point for a new iteration\n",
					"                if item.isDir:\n",
					"                    folders.append(item)\n",
					"                    files, folders = list_directory_content(item.path, files, folders)\n",
					"                # If the content is a file, append it to the files-list\n",
					"                elif item.isFile:\n",
					"                    files.append(item)\n",
					"            return files, folders # Return the lists\n",
					"        except Exception as e:\n",
					"            error_class, error_message = get_exception_information(e)\n",
					"            if error_class==\"FileNotFoundException\":\n",
					"                if not folders:\n",
					"                    return files, folders\n",
					"                raise MiddleWareError (custom_message=f\"There is an issue with the parent folder {folders}\",notebook_name=\"GenericFunctions\",class_name=\"list_directory_content\")"
				],
				"execution_count": 208
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Remove the file extension of a certain path / folder / ...\n",
					"    \n",
					"def remove_extension(string_with_extension: str, extension_value:str):\n",
					"    \"\"\"\n",
					"    Remove the extension (.csv, .json, .parquet, etc) from a string\n",
					"\n",
					"    :param string_with_extension:       A string value containing an extension that needs to be removed\n",
					"    :param extension_value:             The value to remove from the string\n",
					"\n",
					"    :result string_without_extension:   The string value without the extension\n",
					"    \"\"\"\n",
					"\n",
					"    if extension_value:\n",
					"        extension_with_dot = f'.{extension_value}'\n",
					"        # Check if the file string ends with the extension\n",
					"        if string_with_extension.endswith(extension_with_dot):\n",
					"            # Remove the extension if applcatble\n",
					"            string_without_extension =  string_with_extension[:-len(extension_with_dot)]\n",
					"            return string_without_extension\n",
					"\n",
					"        # If the string does not have the extension, something went wrong during the config\n",
					"        raise ConfigurationError(\n",
					"            custom_message = f\"Trying to remove extension {extension_value} from {string_with_extension}, but extension is not at the end of the string\",\n",
					"            notebook_name = \"GenericFunctions\",\n",
					"            function_name = \"remove_extension\"\n",
					"        )\n",
					"    \n",
					"    # If there is no extension to remove, return the initial string\n",
					"    else:\n",
					"        return string_with_extension"
				],
				"execution_count": 209
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def process_string_to_json(string_object:str) -&gt; object:\n",
					"    # Before converting the string_object to a json-object, replace all '-characters by \" first\n",
					"    column_info_string = (string_object.replace('\\'', '\"'))\n",
					"    # Convert the string_object string to a json object\n",
					"    try:\n",
					"        column_info_object = json.loads(column_info_string)\n",
					"        # Return the JSON object\n",
					"        return column_info_object\n",
					"    except:\n",
					"        raise TypeError(f\"The string_object that was passed cannot be converted to a json object. Passed string: {string_object}\")"
				],
				"execution_count": 210
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def rename_blob(file_path, file_format, new_filename):\n",
					"    path_contents = mssparkutils.fs.ls(dir=file_path)\n",
					"    content_names = [file_object.name for file_object in path_contents]\n",
					"    if len(path_contents) &gt; 2 and '_SUCCESS' not in content_names:\n",
					"        raise Exception(f\"Too many files in {file_path}. All files will be overwritten\")\n",
					"        \n",
					"    for item in path_contents:\n",
					"        print(item)\n",
					"        # If the content is a csv file, append it to the files-list\n",
					"        if item.isFile:\n",
					"            if item.name.endswith(f'.{file_format}'):\n",
					"                filename = f'{new_filename}.{file_format}'\n",
					"                item_path = os.path.split(item.path)[0]\n",
					"                mssparkutils.fs.mv(src=item.path, dest=f'{item_path}/{filename}', create_path=True, overwrite=True)"
				],
				"execution_count": 211
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def locate_dropfolder(path:str)-&gt;str :\r\n",
					"    '''\r\n",
					"    If the last folder is not a timestamp folder, the function will go up through the folders until it finds one that is a timestamp.\r\n",
					"    if it doesn't find any, it throws an error.\r\n",
					"    \r\n",
					"    :example for abfss://landing@devdapstdala1.dfs.core.windows.net/pdm/unzip/PRD/pdm/20240808_110331/PDM_extracts_20240630.gz, \r\n",
					"    the return path will be abfss://landing@devdapstdala1.dfs.core.windows.net/pdm/unzip/PRD/pdm/20240808_110331\r\n",
					"\r\n",
					"    :param path: contains the path where a folder of type timestamp must be found\r\n",
					"\r\n",
					"    :result path: a path whose last folder is a timestamp\r\n",
					"    '''\r\n",
					"\r\n",
					"    # The dropfolder is expected to be a timestamp of structure yyyyMMdd_HHmmss\r\n",
					"    dropfolder_pattern:str = r'^\\d{8}_\\d{6}$'\r\n",
					"    last_folder:str = os.path.basename(path)\r\n",
					"    len_path:int =len([f for f in path if os.path.isdir(os.path.join(path, f))])\r\n",
					"    \r\n",
					"    # If there are no directories references in the path, the parent folder has been reached\r\n",
					"    # There is no dropfolder, so throw an error\r\n",
					"    if len_path &lt;= 0:\r\n",
					"        raise Exception(\"You reached the parent folder without finding the drop folder\")\r\n",
					"    \r\n",
					"    # Check if the last folder follows the dropfolder_pattern\r\n",
					"    # If not, go one level higher and call the (recursively) function again \r\n",
					"    if re.search(dropfolder_pattern, last_folder):\r\n",
					"        folder = last_folder\r\n",
					"        return path\r\n",
					"    else:\r\n",
					"        folder = os.path.dirname(path)\r\n",
					"        return locate_dropfolder(folder)"
				],
				"execution_count": 212
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def remove_dropfolder(path:str, look_for_dropfolder:bool=True):\r\n",
					"    \"\"\"\r\n",
					"    This function will remove :\r\n",
					"        1. Empty folders\r\n",
					"        2. File called \"finished\" if it is the only one present in a folder\r\n",
					"        3. Folders that contain only one file called \"finished\"\r\n",
					"\r\n",
					"    :param path: contains the path of the folder(s) and file(s) that need to be deleted\r\n",
					"    :param look_for_dropfolder : the boolean value indicates if we are in the correct folder (look_for_dropfolder=False), \r\n",
					"           and if not(look_for_dropfolder=true), the locate_dropfolder will be used.\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # If not yet on the dropfolder -&gt; Change the path argument\r\n",
					"    if look_for_dropfolder:\r\n",
					"        path=locate_dropfolder(path)\r\n",
					"\r\n",
					"    # Get the contents of the path\r\n",
					"    files, folders = list_directory_content(path, list(), list())\r\n",
					"    \r\n",
					"    # Reserve the list of folders: Clean the lowest directories first\r\n",
					"    folders.sort(key=lambda x: x.path, reverse=True)\r\n",
					"\r\n",
					"    # If the path contains folders: Remove them first\r\n",
					"    if len(folders)!=0:\r\n",
					"        for folder in folders:\r\n",
					"            remove_dropfolder(folder.path,False)\r\n",
					"        files, folders = list_directory_content(path, list(), list())\r\n",
					"    \r\n",
					"    # If there are no files: Remove the folder\r\n",
					"    if len(files) == 0:\r\n",
					"        clean_folder(path)\r\n",
					"\r\n",
					"    # If there are files:\r\n",
					"    # If the file is called 'finished', remove the file and the folder\r\n",
					"    if len(files) == 1 and files[0].name==\"finished\":\r\n",
					"        clean_folder(files[0].path)\r\n",
					"        clean_folder(path)\r\n",
					"\r\n",
					"    # Else: Do not do anything"
				],
				"execution_count": 213
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def replace_string_value(string:str, value_to_replace:str, replace_value:str) -&gt; str:\r\n",
					"    \"\"\"\r\n",
					"    Replace a string value by another value\r\n",
					"\r\n",
					"    :param string:              The string value that contains the value_to_replace\r\n",
					"    :param value_to_replace:    The value that needs to be replaced in the string\r\n",
					"    :param replace_value:       The value that will replace the initial value in the string\r\n",
					"\r\n",
					"    :result string:             The function will return the full string, but with the values replaced\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    if (isinstance(string, str)):\r\n",
					"        return string.replace(value_to_replace, replace_value)\r\n",
					"    else:\r\n",
					"        raise GenericError(custom_message=f\"You are trying to exectue a string-replace of value {value_to_replace} to {replace_value}, but {str(string)} is not a string!\", notebook_name=\"GenericFunctions\", function_name=\"replace_string_value\")"
				],
				"execution_count": 214
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def return_filtered_keys(obj:dict, prop:str, reverse:bool=False) -&gt; list:\r\n",
					"    \"\"\"\r\n",
					"    Return a list of dictionary keys where the value contains property 'prop' that is 'True'\r\n",
					"\r\n",
					"    :param obj:     Dictionary argument that contains the keys and values to check\r\n",
					"    :param prop:    Property that is being looked for in the value of each key\r\n",
					"    :param reverse: Boolean property indicating if the resulting list should return the values with or without the prop-property\r\n",
					"\r\n",
					"    :return filtered_keys:  List of keys that do (not) have the prop-property\r\n",
					"\r\n",
					"    :example\r\n",
					"        obj: {key1: {prop:True}, key2: {prop:False}, key3: None}\r\n",
					"\r\n",
					"        if reverse = False: result: [key1]\r\n",
					"        if reverse = True:  result: [key2, key3]\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"    # Initialize an empty list\r\n",
					"    filtered_keys = list()\r\n",
					"\r\n",
					"    # Get all the keys in the object\r\n",
					"    key_list = obj.keys()\r\n",
					"    for key in key_list:\r\n",
					"        # If the key has a value (so if not None)\r\n",
					"        if obj[key]:\r\n",
					"            # Check if the property can be found in the value\r\n",
					"            if prop in obj[key]:\r\n",
					"                # Check if the value is true\r\n",
					"                if obj[key][prop]: \r\n",
					"                    filtered_keys.append(key)\r\n",
					"\r\n",
					"    # If reverse: Return all the keys that do not have the prop-property set to True\r\n",
					"    if reverse:\r\n",
					"        filtered_keys = [item for item in key_list if item not in filtered_keys]\r\n",
					"\r\n",
					"    return filtered_keys"
				],
				"execution_count": 215
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def create_json_object(**kwargs):\r\n",
					"    \"\"\"\r\n",
					"    Return a json string\r\n",
					"    :param **kwargs : Dictionnary argument that needs to be converted\r\n",
					"\r\n",
					"    :return info : a json string that has been converted\r\n",
					"    \"\"\"\r\n",
					"    info=json.dumps(kwargs)\r\n",
					"    return info"
				],
				"execution_count": 216
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\IngestionFunctions.json">
{
	"name": "IngestionFunctions",
	"properties": {
		"folder": {
			"name": "Functions"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b6ca4e0f-f5b4-4c8a-a595-b876db781f93"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# INGESTION FUNCTIONS\n",
					"This notebook contains all the functions used during the ingestion process. The main functions contained in the notebook are:\n",
					"1. merge_file: Merge a new data file into an existing delta table based on the primary keys of that table\n",
					"2. column_match: When there is a primary key match, update the row columns\n",
					"3. no_match: When there is no primary key match, insert the row columns\n",
					"4. list_directory_contents: Returns a list of files and a list of folders that are contained in a specific (directory of a) container\n",
					"\n",
					"**Language**: PySpark"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"source": [
					"# Import packages\n",
					"# import delta.tables as dt       \n",
					"import re                       # Regex package: Used to analyse patterns in strings (e.g. file names, col)\n",
					"import json\n",
					"import pyspark.sql as sql\n",
					"from pyspark.sql.functions import monotonically_increasing_id, lit, udf\n",
					"from pyspark.sql.functions import year,month,days,col,to_timestamp,date_format, regexp_extract\n",
					"\n",
					"import datetime"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Function should move to generic functions but would cause conflict with other development being done in that notebook -&gt; Adding it here temporarily\r\n",
					"\r\n",
					"def convert_lists_to_list_of_dicts(lists:list, keys:list) -&gt; list:\r\n",
					"    \"\"\"\r\n",
					"    Convert a set of lists [\"value11\", \"value12\"] and [\"value21\", \"value22\"] to a list of dictionaries: [{\"key1\": \"value11\", \"key2\": \"value21\"}, {\"key1\": \"value12\", \"key2\": \"value22\"}]\r\n",
					"\r\n",
					"    :param lists: Contains the list of lists that need to be converted\r\n",
					"    :param keys: List of dictionary keys that need to be used\r\n",
					"\r\n",
					"    :result dict_list: A list of dictionary objects\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    if len(lists) != len(keys):\r\n",
					"        raise ValueError(f\"Number of keys does not match with number of lists: \\n{keys}\\n{lists}\")\r\n",
					"\r\n",
					"    # Transpose the list of lists\r\n",
					"    transposed_lists = list(zip(*lists))\r\n",
					"\r\n",
					"    # Zip each list as a dictionary object\r\n",
					"    dict_list = [dict(zip(keys, inner_list)) for inner_list in transposed_lists]\r\n",
					"\r\n",
					"    return dict_list\r\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Function should move to generic functions but would cause conflict with other development being done in that notebook -&gt; Adding it here temporarily\r\n",
					"\r\n",
					"def replace_listitems_by_keyvalues(keys_to_replace:list, key_value_pairs:dict) -&gt; list:\r\n",
					"    \"\"\"\r\n",
					"    Return a list of items based on a dictionary:\r\n",
					"    \r\n",
					"    :param keys_to_replace: list of items that are keys in the key_value_pairs parameter\r\n",
					"    :param key_value_pairs: dictionary of key-value pairs\r\n",
					"\r\n",
					"    :return replaced_values: list of items that are values in the key_value_pairs\r\n",
					"\r\n",
					"    :example\r\n",
					"        keys_to_replace = [\"foo\", \"bar\"]\r\n",
					"        key_value_pairs = {\"test\": \"TEST\", \"foo\":\"FOO\", \"bar\": \"BAR\"}\r\n",
					"        replaced_values = [\"FOO\", \"BAR\"]\r\n",
					"\r\n",
					"    :note If a key in keys_to_replace is not in the key_value_pairs, an error will be thrown. This is to avoid a mix of keys and values, or missing items in the returned list\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    replaced_values = list()\r\n",
					"    for key in keys_to_replace:\r\n",
					"        if key in key_value_pairs:\r\n",
					"            replaced_values.append(key_value_pairs[key])\r\n",
					"        else:\r\n",
					"            raise ValueError(f\"Key {key} cannot be replaced by a value. Process is stopped for safety reasons...\")\r\n",
					"    \r\n",
					"    \r\n",
					"    return replaced_values"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Partitioning Functions"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Partitioning is a big part of reading and writing dataframes. The functions defined in this subsection are oriented towards this functionality"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# %run Modules/Classes # Should be \"active\", but causes an issue with referencing too many notebooks from TestNotebook. Assumption: This notebook will never be called individually but will always be called when Modules/Classes has been \"imported\""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Whenever a partition-object contains a reference to a datePart (year, month, day), a validation needs to occur to make sure that the referenced column is indeed convertable to a timestamp or date datatype\n",
					"# This function will \n",
					"#   1. Cast the column to a timestamp datatype\n",
					"#   2. Create a Checks-class object (Classes-notebook) \n",
					"#   3. Execute the casted_values-method of the Checks-class to validate correct datatype conversion\n",
					"\n",
					"def cast_partitioning_timestamp(dataframe, partition_key):\n",
					"    '''\n",
					"    Inputs:\n",
					"        dataframe: The dataframe containing a column on which datetime-partitioning needs to be done\n",
					"        partition_key: Name of the column that needs to be converted to a timestamp\n",
					"    '''\n",
					"\n",
					"    # Create an instance of the Checks-class and execute the casted_values-method for correct datatype conversion\n",
					"    checks_object = SparkDataFrameChecks(source_path='', header=True, separator='', file_kind='', column_names=list(), data_types=list(), checks=list(), skip_lines='', column_information=list(), deploy=True)\n",
					"    checks_object.dataframe = dataframe\n",
					"    checks_object.cast_column(dataframe_subset=dataframe.select(partition_key), column=partition_key, data_type='timestamp', formatting={\"format\": \"yyyy-MM-dd\"})\n",
					"\n",
					"    # Duplicate the partition-column (casted_&lt;name&gt;) and cast the original column\n",
					"    casted_dataframe = dataframe.select(partition_key).withColumn(f\"casted_{partition_key}\", col(partition_key))\n",
					"    casted_dataframe = casted_dataframe.withColumn(f\"{partition_key}\", to_timestamp(col(partition_key)))\n",
					"    # casted_dataframe.show()\n",
					"    return casted_dataframe"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Function called when dealing with partitioning of dataframes/delta tables\n",
					"# The function will return:\n",
					"#   1. A dataframe with potentially added columns related to timestamps\n",
					"#   2. A list of column names on which partitioning will be executed\n",
					"\n",
					"def create_partioning_list(dataframe, partitioning_objects: dict):\n",
					"    '''\n",
					"    Inputs:\n",
					"        dataframe: The data on which potentially new columns need to be added for partitioning\n",
					"        partitioning_objects: A json-object containing the names of the columns on which to partition\n",
					"\n",
					"    Output:\n",
					"        partition_column_list = [platformname, p_year, p_month]\n",
					"        dataframe = original dataframe + [col(p_year), col(p_month)]\n",
					"\n",
					"    Functionality:\n",
					"        The function will loop over the partitioning_objects dictionary\n",
					"        For each object, it will check if the key 'datePart' is present\n",
					"        If yes:\n",
					"            Cast the column to a timestamp and validate conversion (cast_partitioning_timestamp())\n",
					"            Add the 'p_' column to the dataframe and the partition_column_list\n",
					"        If no:\n",
					"            Add the column to partition_column_list\n",
					"        \n",
					"        Raise an error if the datePart-key is present but does not contain a valid value (year, month, day)\n",
					"\n",
					"\n",
					"    Example arguments:\n",
					"        partitioning_objects: [\n",
					"            {\"name\": \"platform_name\", \"sequence\": 1},\n",
					"            {\"name\": \"creation_date\", \"sequence\": 2, \"datePart\": \"year\"},\n",
					"            {\"name\": \"creation_date\", \"sequence\": 3, \"datePart\": \"month\"},\n",
					"        ]\n",
					"\n",
					"        -&gt; For each datePart-key, a new column will be added to the dataframe. This is because the datePart only takes a part of the data to partition on rather than the entire date\n",
					"            The added columns have prefix 'p_' to indicate that they are not part of the original dataframe and have been added as partitioning-columns\n",
					"\n",
					"\n",
					"    '''\n",
					"\n",
					"    # Initiate an empty list\n",
					"    partition_column_list = []\n",
					"\n",
					"    # Loop over all the objects in the partitioning_objects dictionary\n",
					"    for partition_key in partitioning_objects:\n",
					"        # Get the value for key datePart\n",
					"        datePart = partition_key.get('datePart')\n",
					"\n",
					"        # If value == 'year', add p_year to partition_column_list and dataframe\n",
					"        if datePart == 'year':\n",
					"            cast_partitioning_timestamp(dataframe, partition_key.get('name'))\n",
					"\n",
					"            partition_column_list.append('p_year')\n",
					"            dataframe = dataframe.withColumn(\"p_year\", date_format(col(partition_key.get('name')), \"yyyy\"))\n",
					"\n",
					"        # If value == 'month', add p_month to partition_column_list and dataframe\n",
					"        elif datePart == 'month':\n",
					"            cast_partitioning_timestamp(dataframe, partition_key.get('name'))\n",
					"\n",
					"            partition_column_list.append('p_month')\n",
					"            dataframe = dataframe.withColumn(\"p_month\", date_format(col(partition_key.get('name')), \"MM\"))\n",
					"\n",
					"        # If value == 'day', add p_day to partition_column_list and dataframe\n",
					"        elif datePart == 'day':\n",
					"            cast_partitioning_timestamp(dataframe, partition_key.get('name'))\n",
					"\n",
					"            partition_column_list.append('p_day')\n",
					"            dataframe = dataframe.withColumn(\"p_day\", date_format(col(partition_key.get('name')), \"dd\"))\n",
					"\n",
					"        # If value == None, add column_name to partition_column_list\n",
					"        elif datePart == None:\n",
					"            partition_column_list.append(partition_key.get('name'))\n",
					"\n",
					"        # If other, throw invalid error\n",
					"        else:\n",
					"            raise ValueError(f'Invalid key value for datePart: {datePart}. Only values year, month, and day are allowed.')\n",
					"\n",
					"    return dataframe, partition_column_list"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Standalone Functions"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"User Defined Function: Force varchar-constrains when casting a dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Purpose: Skip the first x lines of a dataframe\n",
					"def skip_first_lines(landing_path:str, delimiter:str, header:bool, skiplines:int):\n",
					"    '''\n",
					"    Inputs:\n",
					"        landing_path: String-value with the abfss-path to the source file in the landing zone\n",
					"        delimiter: The column-delimiter used in the source file\n",
					"        header: Boolean value indicating whether, after skipping the first lines, the headerline is the first row\n",
					"        skiplines: Integer value, indicating how many rows to skip, starting from the top\n",
					"\n",
					"    Output:\n",
					"        dataframe: A pyspark-dataframe object with the first x lines skipped        \n",
					"\n",
					"    Functionality:\n",
					"        Load the source file as a one-column text file\n",
					"        Add column 'row_id' and remove all the columns where row_id &lt;= skiplines\n",
					"        Split the resulting dataframe based on the delimter and header\n",
					"\n",
					"\n",
					"    Notes:\n",
					"        If the first line of the initial file is the headerline, the recommended use for this function is to include the headerline in the rows to skip and set headers=False in the dataset configuration\n",
					"        New headers (_c0, _c1, etc.) will be added to the returned dataframe and when moving from raw to silver, the headerline will be added again based on the dataset configuration\n",
					"\n",
					"    '''\n",
					"    # Read the full file as a one-column dataframe (column_name: value)\n",
					"    text_df = spark.read.text(landing_path)\n",
					"    \n",
					"    # Add column 'row_id' and remove the columns with a row_id lower than the skiplines argument\n",
					"    filtered_df = text_df.filter(text_df.value != '').withColumn('row_id', sql.functions.monotonically_increasing_id()).filter(sql.functions.col('row_id') &gt;= skiplines).drop('row_id')\n",
					"    \n",
					"    # Map the resulting dataframe based on the one-column (value) and return only the contents (basically: remove the column name 'value')\n",
					"    filtered_rdd = filtered_df.rdd.map(lambda x: x.value)\n",
					"\n",
					"    # Read the filtered dataframe with its proper delimiter and pass the first line as a headerline is header=True \n",
					"    dataframe = spark.read.options(delimiter=delimiter, header=header).csv(filtered_rdd)\n",
					"\n",
					"    return dataframe"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# The UDF (User defined function) below allows the enforce a varchar-constraint on a column\n",
					"#   This will be used during the casting-step, where the ingestion framework validates whether incoming data is of the correct datatype\n",
					"def enforce_varchar_constraint(metadata):\n",
					"    # First get the varchar(&lt;maxlenght&gt;) out of the metadata string, which should look like {..., '__CHAR_VARCHAR_TYPE_STRING': 'varchar(&lt;maxstring&gt;),...}\n",
					"    # Throw an error when the metadata string does not contain the expected metadata value (defined above)\n",
					"    try:\n",
					"        max_length = int(metadata.get('__CHAR_VARCHAR_TYPE_STRING', '').split('(')[1].split(')')[0])\n",
					"    except IndexError:\n",
					"        raise ValueError(f\"Metadata does not contain correct varchar-definition: {metadata}. Expected something like {{'__CHAR_VARCHAR_TYPE_STRING': 'varchar(xx)'}}\")\n",
					"\n",
					"    # Return the string-value when it is shorter than the max_length\n",
					"    # Return None is the string-value is longer than the max_length\n",
					"    def check_length(s):\n",
					"        if s is None:\n",
					"            return s\n",
					"        elif len(s) &gt; max_length:\n",
					"            return None\n",
					"        else:\n",
					"            return s\n",
					"            \n",
					"    # Return the udf-object as a combination of the check_lenght-function and a StringType-datatype\n",
					"    # When applying the udf-object on a column, \n",
					"    return udf(check_length, sql.types.StringType())"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def filter_directory_content(path: str, file_pattern: str):\n",
					"    # Get all the files / folders on a certain path, and filter them based on a certain file_pattern\n",
					"    files, folders = list_directory_content(path, list(), list())\n",
					"\n",
					"    patterned_files = []\n",
					"    if file_pattern != '*':\n",
					"        # If the file_pattern != wildcard, match them on the pattern\n",
					"        patterned_files = [file for file in files if re.match(re.compile(file_pattern), file.name)]\n",
					"    else:\n",
					"        # Else: just return the whole list\n",
					"        patterned_files = files\n",
					"\n",
					"    return patterned_files\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def removeRowsDF(dataframe: sql.DataFrame, rows_to_remove: list):\n",
					"    # Purpose: Add N-rows from a dataframe, based on an index starting from 0 to N\n",
					"\n",
					"    # PRE-CHECKS:\n",
					"    max_df_index = dataframe.rdd.count() - 1\n",
					"    for row in rows_to_remove:\n",
					"        ## There can only be digits in the rows_to_remove\n",
					"        if not isinstance(row, int):\n",
					"            raise TypeError(f\"removeRowsDF: Item '{row}' inside rows_to_remove is not an integer. Only integers are allowed\")\n",
					"\n",
					"        ## If the row you're trying to remove is greater than the last index of the dataframe, it will return an error\n",
					"        elif row &gt; max_df_index:\n",
					"            raise IndexError(f\"removeRowsDF: The max index of the dataframe is {max_df_index}. The index you're trying to remove '{row}' is OutOfBounds\")\n",
					"\n",
					"    # First, save the dataframe as a copy, and also add the index column at the end\n",
					"    # Add an incremental so you can start removing the rows you want\n",
					"    new_dataframe = dataframe.withColumn('remove_index', monotonically_increasing_id())\n",
					"\n",
					"    # Every row with the specific index number in 'rows_to_remove' will be removed (0 = first row, 1 = second row, ...)\n",
					"    # After that, you can drop the index columns as you will no longer need it\n",
					"    new_dataframe = new_dataframe.filter(~new_dataframe.remove_index.isin(rows_to_remove))\n",
					"    new_dataframe = new_dataframe.drop('remove_index')\n",
					"\n",
					"    return new_dataframe"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Merge functions\n",
					"**Main Function: merge_file()**\n",
					"This function does the main orchestration of merging a new raw file into an existing delta table.\n",
					"It calls on the following functions:\n",
					"1. **pk_match()**: Create a string that shows which primary key columns of the raw file match with the primary key columns of the existing delta table.\n",
					"2. **column_match()**: For a specific row: When there is a match between the primary keys of the raw data and the delta table, return a dictionary of what columns to update\n",
					"3. **no_match()**: For a specific row: When there is no match between the primary keys of the raw data and the delta labe, return a dictionary of what columns to insert"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def pk_match(old_alias: str, new_alias: str, pk_columns: dict):\n",
					"    \"\"\"\n",
					"    Based on a dictionary of primary key columns ({ delta lake : raw data }), create a statement that matches the primary keys from the delta table and the raw data\n",
					"\n",
					"    Parameters:\n",
					"    :param old_alias:   The alias used for the delta lake in the merge_file() function\n",
					"    :param new_alias:   The alias used to refer to the raw data in the merge_file() function\n",
					"    :param pk_columns:  A dictionary of the primary columns of the delta lake and the raw data and how they are paired up: { 'id' : 'ID', 'date': 'DATE' }\n",
					"\n",
					"    :result condition:  A concatenated string that matches the primary keys from the old_alias and the new_alias\n",
					"\n",
					"    :example\n",
					"        primary key delta table is [id, date] and for the raw data it is [ID, DATE]\n",
					"        The function will return: 'oldData.id = newData.ID AND oldData.date = newData.DATE'\n",
					"\n",
					"    :note\n",
					"        Notice that the new_alias columns contain ` at the beginning and end of the column name. This is because some column names could contain spaces which could mess up the concatenated string\n",
					"        This is not the case for the old_alias (=delta table columns) as the team has agreed on a standard that does not use spaces but _ instead.\n",
					"    \"\"\"\n",
					"    condition = ' AND '.join(f'{old_alias}.{pk} = {new_alias}.`{pk_columns[pk]}`' for pk in pk_columns)\n",
					"    return condition"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Return a dictionary with all the columns to update when a row already exists in the table\n",
					"# Now: All rows will be updated\n",
					"\n",
					"# Parameters:\n",
					"# Columns: List of the columns to be updated\n",
					"# Data: The alias used to refer to the raw data in the merge_file function\n",
					"\n",
					"def ColumnMatch(columns: dict, data: str):\n",
					"    match_dictionary = dict()\n",
					"    for column in columns:\n",
					"        # If there is a match -&gt; The row will be updated\n",
					"        # There is no need to \"touch\" the t_insert_date column\n",
					"        # Therefore, we do not include it in the resulting dictionary\n",
					"        if column == 't_insert_date':\n",
					"            continue\n",
					"        match_dictionary[column] = sql.functions.col(\"%s.%s\" %(data, columns[column]))\n",
					"\n",
					"    return match_dictionary"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Return a dictionary with all the columns to update when a row does not exists already in the table\n",
					"# Now: All rows will be updated\n",
					"\n",
					"# Parameters:\n",
					"# Columns: List of the columns to be updated\n",
					"# Data: The alias used to refer to the raw data in the merge_file function\n",
					"\n",
					"def NoMatch(columns: dict, data: str):\n",
					"    no_match_dictionary = dict()\n",
					"    for column in columns:\n",
					"        no_match_dictionary[column] = sql.functions.col(\"%s.%s\" %(data, columns[column]))\n",
					"    \n",
					"    return no_match_dictionary"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def fill_extraction_date(dataframe, target_options:dict):\r\n",
					"    \"\"\"\r\n",
					"    Set values for technical column t_extract_date\r\n",
					"\r\n",
					"    :param dataframe:       The current rawData that is being handled\r\n",
					"    :param target_options:  Dictionary with information about how to extract the data for t_extract_date\r\n",
					"\r\n",
					"    :result dataframe:      The current rawData but with the t_extract_date column non-empty\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    # If property 'extract_date' is defined: \r\n",
					"    if 'extract_date' in target_options.keys():\r\n",
					"        # Get the extract_info as a separate object\r\n",
					"        extract_info:dict = target_options['extract_date']\r\n",
					"\r\n",
					"        # Get the name of the column to use to determine the extraction time\r\n",
					"        extract_column:str = extract_info['column_name']\r\n",
					"\r\n",
					"        # If the column is part of the dataframe\r\n",
					"        if extract_column in dataframe.columns:\r\n",
					"            # Extract the date from the column\r\n",
					"            if 'regex_expression' in extract_info.keys():\r\n",
					"                # Define a regex expression that will allow to get the timestamp from the column\r\n",
					"                regex_expression:str = extract_info['regex_expression']\r\n",
					"\r\n",
					"                # Define extract_date_format: The format of the timestamp after regex extraction\r\n",
					"                if 'extract_date_format' in extract_info.keys():\r\n",
					"                    extract_date_format:str = extract_info['extract_date_format']\r\n",
					"                else:\r\n",
					"                    extract_date_format = 'yyyyMMdd_HHmmss'\r\n",
					"                \r\n",
					"                # Extract the timestamp using the regex expression and date_format\r\n",
					"                dataframe = dataframe.withColumn('t_extract_date', to_timestamp(regexp_extract(extract_column, regex_expression, 1), extract_date_format))\r\n",
					"                \r\n",
					"            else:\r\n",
					"                # Dev-Info: If no regex-expression is needed, the assumption is made that the column will already be of datatype 'timestamp'\r\n",
					"                #   This means that there is already a format given to the column and so the date_format is assumed not the be of any relevance\r\n",
					"                dataframe = dataframe.withColumn('t_extract_date', extract_column)\r\n",
					"\r\n",
					"            ## If t_extract_date could not be set -&gt; Throw an error\r\n",
					"            if dataframe.filter(col('t_extract_date').isNull()).count() &gt; 0:\r\n",
					"                raise ConfigurationError(\r\n",
					"                    custom_message=f\"Extract date could not be set using extract_column {extract_column}\", \r\n",
					"                    notebook_name=\"IngestionFunctions\", \r\n",
					"                    function_name=\"fill_extraction_date\"\r\n",
					"                )\r\n",
					"            \r\n",
					"            return dataframe\r\n",
					"        \r\n",
					"        # If the column name could not be found -&gt; Throw an error\r\n",
					"        raise ConfigurationError(\r\n",
					"            custom_message=f'Column {extract_column} cannot be found in the given dataframe', \r\n",
					"            notebook_name='IngestionFunctions', \r\n",
					"            function_name='fill_extraction_date'\r\n",
					"        )\r\n",
					"\r\n",
					"    # If the key is not defined\r\n",
					"    else:\r\n",
					"        # Dev-Note: This is a back-up action that needs to exist as long as the extract_date is not a mandatory field\r\n",
					"        #   Once the field is mandatory, an error should be thrown if the extract data cannot be found\r\n",
					"        current_time = datetime.datetime.now()\r\n",
					"        dataframe = dataframe.withColumn('t_extract_date', lit(current_time).cast(sql.types.TimestampType()))\r\n",
					"        return dataframe"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def MergeFile(deltaTable, rawData, pk_columns_dict:dict, column_names_dict:dict, target_options:dict={}):\n",
					"  \"\"\"\n",
					"  Execute the merge of a new raw data file into an existing delta table\n",
					"\n",
					"  :param  deltaTable:       A dataframe of the existing delta table\n",
					"  :param rawData:           A dataframe of the to-be ingested raw data file\n",
					"  :param pk_columns_dict:   A dictionary of the primary columns of the delta lake and the raw data and how they are paired up\n",
					"  :param column_names_dict: A dictionary of all the columns of the delta lake and the raw data and how they are paired up\n",
					"  :param target_options:    A dictionary object containing additional checks and configurations for writing to the delta table: partitioning, extract_date,  etc\n",
					"  \"\"\"\n",
					"  # Alias: Used to refer to the new raw data and the old/existing delta table\n",
					"  new_alias = \"newData\"\n",
					"  old_alias = \"oldData\"\n",
					"\n",
					"  ############################################################################################\n",
					"  #################################### t_load_date_silver ####################################\n",
					"\n",
					"  # Validate that column 't_load_date_silver' exist in the rawData dataframe before adding the timestamp\n",
					"  if 't_load_date_silver' not in rawData.columns:\n",
					"    raise ValueError(\"Column 't_load_date_silver' does not exist in the raw dataframe. Something must have gone wrong when writing from landing to raw...\")\n",
					"\n",
					"  # Add the current time to the t_load_date_silver column of the rawData dataframe\n",
					"  current_time = datetime.datetime.now()\n",
					"  rawData = rawData.withColumn('t_load_date_silver', lit(current_time).cast(sql.types.TimestampType()))\n",
					"\n",
					"  ############################################################################################\n",
					"\n",
					"  ############################################################################################\n",
					"  #################################### t_extract_date ########################################\n",
					"\n",
					"  # Validate that column 't_extract_date' exist in the rawData dataframe before adding the timestamp\n",
					"  if 't_extract_date' not in rawData.columns:\n",
					"    raise ValueError(\"Column 't_extract_date' does not exist in the raw dataframe. Something must have gone wrong when writing from landing to raw...\")\n",
					"\n",
					"  # Add the extract time to the t_extract_date column of the rawData dataframe\n",
					"  rawData = fill_extraction_date(dataframe=rawData, target_options=target_options)\n",
					"\n",
					"  ############################################################################################\n",
					"\n",
					"\n",
					"  ############################################################################################\n",
					"  ################################### optional_columns #######################################\n",
					"\n",
					"  # Some columns are optional, but still configured in the dataset configuration\n",
					"  # Given the mandatory checks on headers and datatypes, columns that do not occur in the rawData \n",
					"  #   can be removed from the column_names_dict\n",
					"  # If not removed, there will be an issue when matching oldData and newData columns\n",
					"  keys_to_remove = list()\n",
					"  for key, value in column_names_dict.items():\n",
					"    if value not in rawData.columns:\n",
					"      keys_to_remove.append(key)\n",
					"\n",
					"  column_names_dict = {k: v for k, v in column_names_dict.items() if k not in keys_to_remove}\n",
					"\n",
					"  ############################################################################################\n",
					"\n",
					"  ############################################################################################\n",
					"  ####################################### partitioning #######################################\n",
					"  \n",
					"  # rawData = rawData.withColumn(partitioning_timestamp,to_timestamp(col(partitioning_timestamp))).withColumn(\"day\", date_format(col(partitioning_timestamp), \"dd\")).withColumn(\"year\", date_format(col(partitioning_timestamp), \"yyyy\")).withColumn(\"month\", date_format(col(partitioning_timestamp), \"MM\"))\n",
					"  # Add columns for partitioning\n",
					"  # This needs to happen before the quality checks, as the \n",
					"  if 'partitioning' in target_options.keys():\n",
					"      # Get the list of columns on which partitioning is executed\n",
					"      partition_target_columns:list = [partition_object['name'] for partition_object in target_options['partitioning']]\n",
					"      # Return the list of columns of the source file on which partitioning will be executed\n",
					"      partition_source_columns:list = replace_listitems_by_keyvalues(partition_target_columns, column_names_dict)\n",
					"      # Create a list of objects where the references to the target columns are replaced by references to the source\n",
					"      source_partitioning:list = [{**dct, 'name': value} for dct, value in zip(target_options['partitioning'], partition_source_columns)]\n",
					"      # print(source_partitioning)\n",
					"           \n",
					"      rawData, partioning_column_list = create_partioning_list(rawData, source_partitioning)\n",
					"\n",
					"\n",
					"      if 'p_year' in partioning_column_list:\n",
					"        column_names_dict['p_year'] = 'p_year'\n",
					"\n",
					"      if 'p_month' in partioning_column_list:\n",
					"        column_names_dict['p_month'] = 'p_month'\n",
					"\n",
					"      if 'p_day' in partioning_column_list:\n",
					"        column_names_dict['p_day'] = 'p_day'\n",
					"      \n",
					"      # Repartitioing the dataframe based on the partitioning columns\n",
					"      # Dev-Info: This will benefit the optimize-functions as there will be less file-movements needed\n",
					"      #   Original DF: multiple nodes contain subsets of the dataframe. Each subset can contain multiple partitions\n",
					"      #     When writing the DF, a file will be created for each partition per node\n",
					"      #     Say you have 3 nodes and 4 partitions -&gt; 12 files will be created \n",
					"      #   Repartitioned DF: Each partition is grouped in one node (simplified)\n",
					"      #     When writing to the DF, a file will be created per partition (simplified)\n",
					"      #     Say you have 3 nodes and 4 partitions -&gt; 4 files will be created (1 for each partition)\n",
					"      # Note: Simplified: The partition might not be able to be contained in 1 node if it is too large, but this idea is abstracted in the explanation\n",
					"      rawData.repartition(*partioning_column_list)\n",
					"\n",
					"  else:\n",
					"    partioning_column_list = []\n",
					"  \n",
					"  target_partition_columns:list = deltaTable.detail().select(\"partitionColumns\").collect()[0][0]\n",
					"  source_partition_columns:list = replace_listitems_by_keyvalues(target_partition_columns, column_names_dict)\n",
					"\n",
					"  if source_partition_columns != partioning_column_list:\n",
					"    raise ValueError(f\"Given list of partition columns {partioning_column_list} does not match with the partition columns of the specified raw data: {source_partition_columns}\")\n",
					"\n",
					"  ############################################################################################\n",
					"\n",
					"\n",
					"  #####################################################################################################\n",
					"  ################################### t_update_date &amp; t_insert_date ###################################\n",
					"\n",
					"  # Validate that column 't_update_date' exist in the rawData dataframe before adding the timestamp\n",
					"  if 't_update_date' not in rawData.columns:\n",
					"    raise ValueError(\"Column 't_update_date' does not exist in the raw dataframe. Something must have gone wrong when writing from landing to raw...\")\n",
					"  \n",
					"  # Validate that column 't_insert_date' exist in the rawData dataframe before adding the timestamp\n",
					"  if 't_insert_date' not in rawData.columns:\n",
					"    raise ValueError(\"Column 't_insert_date' does not exist in the raw dataframe. Something must have gone wrong when writing from landing to raw...\")\n",
					"\n",
					"  # Add the current time to the t_update_data and t_insert_date column of the rawData dataframe\n",
					"  current_time = datetime.datetime.now()\n",
					"  rawData = rawData.withColumn('t_update_date', lit(current_time).cast(sql.types.TimestampType()))\n",
					"  rawData = rawData.withColumn('t_insert_date', lit(current_time).cast(sql.types.TimestampType()))\n",
					"\n",
					"\n",
					"  ############################################################################################\n",
					"  ########################################## Merge ###########################################\n",
					"  # Execute a merge of the raw data and the delta table based on the primary columns of both dataframes\n",
					"  \n",
					"  deltaTable.alias(old_alias) \\\n",
					"    .merge(\n",
					"      rawData.alias(new_alias), pk_match(old_alias, new_alias, pk_columns_dict)) \\\n",
					"    .whenMatchedUpdate(set=ColumnMatch(column_names_dict, new_alias)) \\\n",
					"    .whenNotMatchedInsert(values = NoMatch(column_names_dict, new_alias)) \\\n",
					"    .execute()\n",
					" \n",
					"# For potential future use (soft-delete, insert, update):\n",
					"#   (targetDF\n",
					"#   .merge(sourceDF, \"source.key = target.key\")\n",
					"#   .whenMatchedUpdateAll()\n",
					"#   .whenNotMatchedInsertAll()\n",
					"#   .whenNotMatchedBySourceUpdate()\n",
					"#   .execute()\n",
					"# )\n",
					"# -&gt; https://docs.delta.io/latest/delta-update.html"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Extract data from a subpath\n",
					"    \n",
					"def get_parent_folder(path: str):\n",
					"    # Do an rsplit (= start splitting from the right), on the character '/', to get the name of the folder that your file is in\n",
					"    # Your parent folder will be in the second place of your array, since you're splitting on the first two '/' starting from the right\n",
					"    if '/' not in path:\n",
					"        # If there is no '/', then there is no parent\n",
					"        raise Exception(f\"get_parent_folder: The path '{path}' doesn't qualify as a path or doesn't have a parent folder\")\n",
					"\n",
					"    if path.count('/') == 1:\n",
					"        # Only happens when your depth of folders is parent/child\n",
					"        path_part = path.rsplit('/', 1)[0]\n",
					"    else:\n",
					"        # This is for all the other cases (e.g. parent/parent/child, parent/parent/.../child)\n",
					"        path_part = path.rsplit('/', 2)[1]\n",
					"\n",
					"    return path_part"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Create a dictionary from a list of flat-json objects stored as strings\n",
					"\n",
					"def jsons_to_dict(json_list:list):\n",
					"    # Init the place you want to store the whole dictionary\n",
					"    big_dictionary = dict()\n",
					"\n",
					"    for json_str in json_list:\n",
					"        if (json_str.count('{') &gt; 1):\n",
					"            # If the count &gt; 1, then it means it has multiple json objects inside\n",
					"            # So: create an array to handle this situation\n",
					"\n",
					"            # First remove all the white space, so you won't have an object inside with whitespaces\n",
					"            # This will matter when you're using split(',')\n",
					"            # Then create an array to loop over\n",
					"            json_without_whitespaces = json_str.replace(\" \", \"\")\n",
					"            jsons_array = json_without_whitespaces.split(',')\n",
					"\n",
					"            for obj in jsons_array:\n",
					"                try:\n",
					"                    # Parse each object\n",
					"                    dict_part_array = json.loads(obj)\n",
					"\n",
					"                    # Add it to the dictionary\n",
					"                    big_dictionary.update(dict_part_array)\n",
					"                except Exception:\n",
					"                    raise TypeError(f\"Unable to parse string {obj} to JSON\")\n",
					"        else:\n",
					"            try:\n",
					"                # Parse each object\n",
					"                dict_part = json.loads(json_str)\n",
					"\n",
					"                # Add it to the dictionary\n",
					"                big_dictionary.update(dict_part)\n",
					"                \n",
					"            except Exception:\n",
					"                raise TypeError(f\"Unable to parse string {json_str} to JSON\")\n",
					"\n",
					"    return big_dictionary"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def list_files(path, to_remove):\n",
					"#     files = mssparkutils.fs.ls(path)\n",
					"#     for file in files:\n",
					"#         if file.isDir:\n",
					"#             new_files, new_remove = list_files(file.path, to_remove)\n",
					"#             files += new_files\n",
					"#             to_remove += new_remove\n",
					"#             to_remove.append(file)\n",
					"#     return files, to_remove"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def move_to_raw(variables, data_lake_storage_account, SQLCon, task):\n",
					"#     storage_account = data_lake_storage_account\n",
					"#     container_name = variables['Container_Name']\n",
					"#     file_path = variables['Source_Folder'].replace('*', '')\n",
					"#     header = variables['Header']\n",
					"#     separator = variables['Column_Delimiter']\n",
					"#     format = variables['File_Extension']\n",
					"\n",
					"#     path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, file_path)\n",
					"\n",
					"#     files, to_remove = list_files(path, [])\n",
					"#     [files.remove(file) for file in to_remove if file in files]\n",
					"#     if task.variables['File_Pattern'] != '*':\n",
					"#         patterned_files = [file for file in files if re.match(re.compile(task.variables['File_Pattern']),file.name) ]\n",
					"#     else:\n",
					"#         patterned_files = files\n",
					"#     found_files = {file.path:file.name.split('.')[0]  for file in patterned_files}\n",
					"#     # print(files, patterned_files, found_files, to_remove) \n",
					"#     file_objects = [File(task, SQLCon, file, found_files[file], file.split('/')[-2], storage_account) for file in found_files]\n",
					"#     for file in file_objects:\n",
					"#         landingData = spark.read.load(file.landing_path, header=header, sep=separator, format = format)\n",
					"#         landingData.write.mode(\"overwrite\").parquet(file.raw_path)\n",
					"#         file.update_file_activity(\"RAW\", True, file.raw_path)\n",
					"#     return file_objects"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def ingest_to_silver(variables: dict, file: object, data_lake_storage_account: str):\n",
					"#     storage_account = data_lake_storage_account\n",
					"#     table_name = variables['Table_Name']\n",
					"#     header = variables['Header']\n",
					"#     separator = variables['Column_Delimiter']\n",
					"#     format = 'parquet'\n",
					"#     dimensions = variables['Dimension']\n",
					"\n",
					"#     pk_column_dict = {variables['Sink_Name'][index]:variables['Source_Name'][index] for index, dimension in enumerate(dimensions) if dimension == 'PK'}\n",
					"#     column_names_dict = dict(zip(variables['Sink_Name'], variables['Source_Name']))\n",
					"#     silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', storage_account, table_name)\n",
					"#     #file.set_silver_path(silver_path)\n",
					"\n",
					"#     deltaTable = dt.DeltaTable.forPath(spark, silver_path)\n",
					"#     rawData = spark.read.load(file.raw_path, header=header, sep=separator, format = format)\n",
					"#     MergeFile(deltaTable, rawData, pk_column_dict, column_names_dict)\n",
					"#     file.update_file_activity(\"SILVER\", True, silver_path)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def move_to_archive(file):\n",
					"#     mssparkutils.fs.mv(file.landing_path, file.archive_path, True)\n",
					"#     file.update_file_activity(\"ARCHIVE\", True, file.archive_path)\n",
					"#     return"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# def merge_ingestion(variables, data_lake_storage_account):\n",
					"#     # Source and sink location\n",
					"#     storage_account = data_lake_storage_account\n",
					"#     container_name = variables['container_name']\n",
					"#     table_name = variables['table_name']\n",
					"#     file_path = variables['source_folder']\n",
					"\n",
					"#     # File layout variables\n",
					"#     file_name = variables['file_pattern']+'.'+variables['file_type']\n",
					"#     format = variables['file_type']\n",
					"#     header = variables['header']\n",
					"#     separator = variables['column_delimiter']\n",
					"\n",
					"\n",
					"#     # File path definitions\n",
					"#     source_path_full = \"abfss://%s@%s.dfs.core.windows.net/%s/%s\" % (container_name, storage_account, file_path, file_name)\n",
					"#     silver_path_full = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('silver', storage_account, table_name)\n",
					"\n",
					"#     # Column names and Primary keys\n",
					"#     pk_column_names_dict = {variables['sink_name'][index]:variables['source_name'][index] for index, dimension in enumerate(dimensions) if dimension == 'PK'}\n",
					"#     column_names_dict = dict(zip(variables['sink_name'], variables['source_name']))\n",
					"\n",
					"    # List the directories, including timestamps, found in the landing container (and following the source_path)\n",
					"    # This list will be used to dump files into raw and archive\n",
					"\n",
					"    # file_path_stripped = file_path.replace('*', '')\n",
					"    # file_list = mssparkutils.fs.ls(\"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, file_path_stripped))\n",
					"    # source_path_timestamps = list()\n",
					"    # raw_path_dict = dict()\n",
					"    # archive_path_dict = dict()\n",
					"    # for file in file_list:\n",
					"    #     source_path_timestamps.append(file.path)\n",
					"    #     raw_path_dict[file.path] = \"abfss://%s@%s.dfs.core.windows.net/%s/%s\" % ('raw', storage_account, file_path_stripped, file.name)\n",
					"    #     archive_path_dict[file.path] = \"abfss://%s@%s.dfs.core.windows.net/%s/%s\" % ('archive', storage_account, file_path_stripped, file.name)\n",
					"\n",
					"\n",
					"    # # Loop through all files within the landing folders\n",
					"    # for source_file in source_path_timestamps:\n",
					"    #     deltaTable = dt.DeltaTable.forPath(spark, silver_path_full)\n",
					"    #     newData = spark.read.load(source_file, header=header, sep=separator, format = format)\n",
					"    #     newData.write.mode(\"overwrite\").parquet(raw_path_dict[source_file])\n",
					"    #     MergeFile(deltaTable, newData, pk_column_names, column_names_dict)\n",
					"    #     mssparkutils.fs.mv(source_file, archive_path_dict[source_file], True)"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\IngestionWorker.json">
{
	"name": "IngestionWorker",
	"properties": {
		"folder": {
			"name": "Workers"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "a36328c8-c080-484e-b0fa-9e45437365ea"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# INGESTION WORKER\n",
					"The IngestionWorker is used during ingestion. The purpose of the worker is to:\n",
					"- Collect the list of files that should be processed by the task\n",
					"- Move all files from landing to raw\n",
					"- Ingest the files from a raw dataframe into a silver delta table\n",
					"- Move the processed files from landing to archive"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def run_ingestion_worker(task: object, env_code: str, debug:bool=True):\n",
					"    '''\n",
					"    Inputs\n",
					"        task: An object of the Task-Class [Modules/Classes]\n",
					"        env_code: The code used to indicate the environment the functions are running in (dev, int, tst, acc, prd)\n",
					"        debug: Boolean indicating whether the session is being run in Debug mode. This will trigger more information (print statements) for the user\n",
					"\n",
					"    Functionality:\n",
					"        Each file that is handled by this worker will:\n",
					"            - Move from landing to raw, and be converted to a parquet file in the process\n",
					"            - Be ingested into a silver delta table\n",
					"            - Move from landing to archive, clearing the landing container from any already processed files\n",
					"    '''\n",
					"\n",
					"    # Set variable 'data_lake_storage_account' using the env_code to link to the relevant storage account\n",
					"    data_lake_storage_account:str = f\"{env_code}dapstdala1\"\n",
					"    if debug: print(f\"[IngestionWorker] Data lake storage account: {data_lake_storage_account}\")\n",
					"\n",
					"    # Set list of the metadata that needs to be extracted from the SQL Meta DB\n",
					"    metadata_keys:list = [\n",
					"        'file_layout',\n",
					"        'file_pattern', \n",
					"        'file_kind',\n",
					"        'file_extension', \n",
					"        'target_options', \n",
					"        'skip_first_lines',\n",
					"        'column_delimiter', \n",
					"        'header', \n",
					"        'source_name', \n",
					"        'sink_name', \n",
					"        'dimension', \n",
					"        'source_folder', \n",
					"        'data_type', \n",
					"        'column_info',\n",
					"        'table_name', \n",
					"        'container_name',\n",
					"        'escape_character',\n",
					"        'quote_character'\n",
					"    ]\n",
					"    if debug: print(f\"[IngestionWorker] Metadata variables: {', '.join(metadata_keys)}\")\n",
					"\n",
					"    # [Modules/Classes] Run stored procedure 'usp_get_task_metadata' and set the task-object class-member values\n",
					"    task.get_task_metadata(metadata_keys)\n",
					"    task.set_dataset_variables(data_lake_storage_account)\n",
					"    task.set_ingestion_variables(data_lake_storage_account)\n",
					"    if debug: print(f\"[IngestionWorker] Task variables: {task.variables}\")\n",
					"            \n",
					"\n",
					"    # If the table_name is known, ingestion can start\n",
					"    # Without a target table_name, there is no landing location for the source file\n",
					"    if task.table_name:\n",
					"\n",
					"        # Retrieve the files that need to be ingested by the task at hand\n",
					"        if debug: print(\"[IngestionWorker] Set file objects\")\n",
					"        file_objects:list = task.get_files_objects(data_lake_storage_account)\n",
					"        sorted_file_objects = sorted(file_objects, key=lambda obj: obj.landing_path,reverse=False)\n",
					"        # Loop over each file and execute the file movements (and necessary data quality checks)\n",
					"        for file in sorted_file_objects:\n",
					"            print(f'[IngestionWorker] Processing file: {file.file_name} ({datetime.datetime.now().strftime(\"%H:%M:%S\")})')\n",
					"\n",
					"            try:\n",
					"                if debug: print('[IngestionWorker] Copy file to RAW')\n",
					"                file.landing_to_raw()\n",
					"            except Exception as error:\n",
					"                print(f'[IngestionWorker] this is the error in the RAW {error}')\n",
					"                e_class, e_msg=get_exception_information(error)\n",
					"                error_object = handle_exceptions(e_class, e_msg, error)\n",
					"                file.update_file_activity(activity=\"RAW\", success=False, info_message=json.dumps(error_object))\n",
					"                raise error\n",
					"\n",
					"            try:\n",
					"                if debug: print('[IngestionWorker] Ingest file into SILVER')\n",
					"                file.raw_to_silver()\n",
					"            except Exception as error:\n",
					"                print(f'[IngestionWorker] this is the error in the SILVER {error}')\n",
					"                e_class, e_msg=get_exception_information(error)\n",
					"                error_object = handle_exceptions(e_class, e_msg, error)        \n",
					"                file.update_file_activity(activity=\"SILVER\", success=False, info_message=json.dumps(error_object))\n",
					"                raise error\n",
					"\n",
					"            try:\n",
					"                if debug: print('[IngestionWorker] Move file to ARCHIVE')\n",
					"                file.landing_to_archive()\n",
					"            except Exception as error:\n",
					"                print(f'[IngestionWorker] this is the error in the ARCHIVE {error}')\n",
					"                e_class, e_msg=get_exception_information(error)\n",
					"                error_object = handle_exceptions(e_class, e_msg, error)\n",
					"                file.update_file_activity(activity=\"ARCHIVE\", success=False, info_message=json.dumps(error_object))\n",
					"                raise error\n",
					"\n",
					"\n",
					"        # After ingesting all the files into the delta table, optimize storage of the table\n",
					"        if debug: print('[IngestionWorker] Optimize table storage')\n",
					"        delta_table:object = OptimizeDeltaTable(table_name=task.table_name, env_code=env_code, debug=debug)\n",
					"        delta_table.optimize_table_storage()\n",
					"        if hasattr(task, 'target_options') and task.target_options.get('retention_time'):\n",
					"            retention_time=task.target_options.get('retention_time')\n",
					"        else :\n",
					"            retention_time=720\n",
					"        if debug : print('retention time is :', retention_time)\n",
					"        delta_table.vacuum_deltatable(retention_time)"
				],
				"execution_count": 1
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\MetaNotebook.json">
{
	"name": "MetaNotebook",
	"properties": {
		"folder": {
			"name": "Workers"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b9993b9d-c5a8-47e1-919d-a6e42a28d345"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# META NOTEBOOK\n",
					"The purpose of the MetaNotebook  is to create SQL Connection, plan, and task class-objects. For each task-object, a Worker script is be called depending on the specified task. The class-objects objects contain different methods, and separate scripts are created with functions that can be called by the Worker scripts. Functions and methods are defined in other scripts for the following reasons:\n",
					"1. Increase the flexibility of the script\n",
					"2. Have a clear separation between *worker scripts* that make things happen and *functional scripts* that define how things happen\n",
					"\n",
					"**Language**: PySpark (Python)\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Script logic\n",
					"1. Import the necessary packages\n",
					"2. Run the relevant scripts: Scripts that contain the functions and methods need to be \"run\" first before being able to use them\n",
					"3. Create Class-objects for SQL Connection, Plans and Tasks\n",
					"4. Loop over and execute all Task objects: Each task needs to be assigned to a Worker-script that will actually execute the task"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 1. Import Packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import packages\n",
					"import json\n",
					"import sys\n",
					"import datetime\n",
					"# from operator import itemgetter"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 2. Prepare the spark session\n",
					"The following section will prepare the Spark Session. That is, when invoking a notebook it starts with a set of default configurations and functions. As stated, all the notebooks, other than the MetaNotebook, contain functions and classes. They will not execute anything without being called upon. This can only happen when the functions/classes have been 'imported' into the spark session.\n",
					"\n",
					"Same goes for the parameter-list that is being passed when calling the MetaNotebook. Some preprocessing activities will be executed on the parameters as to \"prepare\" the environment for execution."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 2.1 Parameters\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. If testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"'''\n",
					"Inputs\n",
					"    task_list -&gt; Contains a set of task-objects, each of which has a set of metadata defined which will be used throughout execution for communication with the SQL Meta Database\n",
					"    plan_id -&gt; The ID of the plan which the task is part of\n",
					"    env_code -&gt; The code used to indicate the environment the functions are running in (dev, int, tst, acc, prd)\n",
					"    debug -&gt; Boolean indicating whether the session is being run in Debug mode. This will trigger more information (print statements) for the user\n",
					"\n",
					"    task_list = {\n",
					"        plan_id = The ID of the plan which the task is part of\n",
					"        plan_name = The name of the plan that started the task\n",
					"        task_group = The name of the task group which the task is part of (PREPROCESS, INGEST, etc.)\n",
					"        task_id = The ID of the task that is being invoked\n",
					"        task_name = The name of the task that is being invoked\n",
					"        worker_name = The name of the (SPARK NOTEBOOK) worker that will execute the task\n",
					"        task_sequence = Whithin the plan and task group, the sequence in which the task will be executed (arbitrary value since tasks should be independent of one another)\n",
					"        original_plan_id = The ID of the plan the originally tried executing the task (Should be the same as the plan_id if the run is not a 'Retry' run of failed tasks)\n",
					"        current_status = The latest status of the plan (PENDING/FAILED)\n",
					"    }\n",
					"'''\n",
					"\n",
					"# List of task-objects to execute\n",
					"task_list:str = '''[{\n",
					"                \"plan_id\": ,\n",
					"                \"plan_name\": \"\",\n",
					"                \"task_group\": \"INGEST\",\n",
					"                \"task_id\": ,\n",
					"                \"task_name\": \"\",\n",
					"                \"worker_name\": \"IngestionWorker\",\n",
					"                \"task_sequence\":,\n",
					"                \"original_plan_id\": ,\n",
					"                \"current_status\": \"PENDING\"\n",
					"            }]'''\n",
					"\n",
					"# ID of the plan being executed\n",
					"# Dev-Info: One notebook call can in theory handle multiple plans at the same times (depending on the task-objects being passed), but this is not recommended\n",
					"plan_id:int = 1\n",
					"\n",
					"## Environment base is the phase the notebook is currently in (dev/int/tst/acc/prod)\n",
					"env_code:str = \"dev\"\n",
					"\n",
					"# Boolean indicating whether the session is being run in Debug mode. This will trigger more information (print statements) for the user\n",
					"debug:bool = True"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Try converting the task_list (string) to a list of json-objects\n",
					"try:\n",
					"    task_list:list = json.loads(task_list)\n",
					"except:\n",
					"    raise ValueError(\"Could not convert task_list to a json object. Ending notebook session...\")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Use default port for SQL Server Connection\n",
					"port = 1433\n",
					"spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
					"spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", 'CORRECTED')"
				],
				"execution_count": 27
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 2.2. Load notebooks\n",
					"Import all the relevant notebooks into the spark session using %run. After this action, the function and Classes defined in these notebooks will be able to be called from within the Spark session. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"1. Modules"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Classes"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Dataframes_v2"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": 31
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"2. Functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/GenericFunctions"
				],
				"execution_count": 34
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"3. Workers"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Workers/IngestionWorker"
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Workers/FilWorker"
				],
				"execution_count": 36
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 3. Class objects\n",
					"1. SQLServerConnection: Class that creates a connection to a SQL server. This class-object will be passed to all other classes to be able to execute SQL queries, stored procedures, etc... while only having to define the connection once\n",
					"2. Plan: Class that creates a plan-object containing the plan_id and plan_name.\n",
					"3. Task: Class that creates a task-object containing the IDs and Names of the plan and task. "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Modules/Classes] Create a object of the SQLServerConnection class to the SQL Meta Database\n",
					"metadb:object = SQLServerConnection(env_code, port, debug)"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# [Modules/Classes] Create an object of the Plan class with the relevant plan-metadata \n",
					"plan:object = Plan(plan_id, task_list[0]['plan_name'], metadb, debug)"
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# [Modules/Classes] Create an object of the Task class for each task object with the relevant task-metadata\n",
					"task_objects:list = [Task(task['task_name'], task['task_id'], task['original_plan_id'], metadb, task[\"worker_name\"], debug) for task in task_list]"
				],
				"execution_count": 39
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 4. Execute all tasks\n",
					"\n",
					"Loop over the list of task-objects. For each object, a worker should be defined. This worker is linked to a Worker-notebook that will be executed.\n",
					"All the tasks will be executed, with the errorFlag being set to True when at least one task fails. This will not prevent other tasks (which should be independend from one another) from being executed."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": []
				},
				"source": [
					"# Flag to indicate whether a task has failed during the for-loop.\n",
					"# If set to true, an error will be thrown at the end of the for-loop to indicate that at least one of the tasks failed\n",
					"errorFlag:bool = False"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"for task in task_objects:\n",
					"    print(f'Start task {task.task_name}: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n",
					"    # [Modules/Classes] Set the status of the task to 'IN PROGRESS'\n",
					"    task.start_task()\n",
					"    try:\n",
					"        # Invoke the Worker needed to execute the task\n",
					"        if task.worker_name == 'FilWorker':\n",
					"            run_fil_worker(task, env_code, debug)\n",
					"\n",
					"        elif task.worker_name == 'IngestionWorker':\n",
					"            run_ingestion_worker(task, env_code, debug)\n",
					"\n",
					"        # DummyWorker: Case used during unit testing to validate the integration between pipelines abd notebooks\n",
					"        elif task.worker_name == 'DummyWorker':\n",
					"            pass\n",
					"        \n",
					"        # If the given worker name is not an existing configuration, raise an error\n",
					"        else:\n",
					"            raise ValueError(\"Worker cannot be found\", task.worker_name)\n",
					"        print(f'End task {task.task_name} successfully: {datetime.datetime.now().strftime(\"%H:%M:%S\")}\\n\\n')\n",
					"        # [Modules/Classes] Set the status of the task to 'SUCCESS'\n",
					"        task.end_task(True)\n",
					"        \n",
					"    # Catch all errors being thrown during execution\n",
					"    except Exception as GeneralError:\n",
					"        print(f'End task {task.task_name} unsuccessfully: {datetime.datetime.now().strftime(\"%H:%M:%S\")}')\n",
					"\n",
					"        # [ErrorHandling:GenericErrorClass] Convert the error to an object of class GenericErrorClass\n",
					"        e_class, e_msg=get_exception_information(GeneralError)\n",
					"        error_object = handle_exceptions(e_class, e_msg, GeneralError)\n",
					"        print(error_object)\n",
					"        # [Modules/Classes] Set the status of the task to 'FAILED' and pass the custom error_object\n",
					"        task.end_task(False, error_object)\n",
					"\n",
					"        errorFlag = True"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# If errorFlag = True: Make sure the notebook execution fails to alert users\n",
					"if (errorFlag):\n",
					"    sys.exit(1)"
				],
				"execution_count": 42
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\SchemaDriftTemplates.json">
{
	"name": "SchemaDriftTemplates",
	"properties": {
		"folder": {
			"name": "Templates"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "de2801b6-f53d-4e4b-8eaf-c3e0c16cb255"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# SchemaDriftTemplates\r\n",
					"This script contains a set of functions which can be used when there is a change in the schema of a specific source file. This is related to changing column names, data types, partitioning columns, etc. The functions allow for a standardized approach to changing the schema of an existing delta table without the risk of unforeseen implicit impact. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Resolve standard scenarios\r\n",
					"- Change column datatype (OK)\r\n",
					"- Add generic column to all delta tables (OK)\r\n",
					"- Change column name (OK)\r\n",
					"- Remove column name (OK)\r\n",
					"- Change PK column to regular column (OK)\r\n",
					"- Change column to PK column (OK)\r\n",
					"- Changed partitioning column (OK)\r\n",
					"\r\n",
					"Note: Adding a new column is already integrated in the framework and happens automatically when updating the configuration file"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Developer Notes\r\n",
					"When changing the schema of a delta table, there are some options. Depending on the type of change that needs to be executed, the following functions (options) exist in the Delta architecture:\r\n",
					"\r\n",
					"- mergeSchema\r\n",
					"\r\n",
					"This functionality can be used when there are two dataframes with a (potentially) different schema but without any impact on one another. That is, the columns with the same name are expected to have the same schema. If this is not the case, an error will be thrown.\r\n",
					"In theory, the function does allow for other types of changes (remove or rename columns), but this would require the settings of the table to change. This is not recommended practice as it has an implicit impact on other operations that can occur on the delta tables. \r\n",
					"https://learn.microsoft.com/en-us/azure/databricks/delta/table-properties\r\n",
					"\r\n",
					"- overwriteSchema\r\n",
					"\r\n",
					"This functionality fully overwrites the existing schema and the content of an existing delta table with that of a \"new\" dataframe. For this to execute without any underlying impact, the following must be considered:\r\n",
					"1. Do all StructField objects (or columns) have it's metadata described (This is especially important for the varchar-columns)\r\n",
					"2. Does the dataframe contain all the data (that needs to be kept) of the current delta table? \r\n",
					"3. Does the current delta table have any partition fields, and do they need to be replicated when overwriting the data\r\n",
					""
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Set-up environment\r\n",
					"- Import libraries and notebooks\r\n",
					"- Set global variables"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Standard Python packages\r\n",
					"import os\r\n",
					"import re\r\n",
					"\r\n",
					"# Spark and delta functionalities\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql.functions import col\r\n",
					"from pyspark.sql.types import StructField, StructType"
				],
				"execution_count": 69
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 70
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"print_statements = False    # Set to True if you want to print additional information\r\n",
					"env_code = 'dev'            # Change to the working environment (Will give a time-out connection when not properly set)\r\n",
					"container_name = 'silver'   # Name of the container where the delta tables are stored\r\n",
					"\r\n",
					"storage_account = f'{env_code}dapstdala1'   # Name of the storage account where the delta tables are stored\r\n",
					"delta_lake_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net'    # Full path to the container where the delta tables are stored"
				],
				"execution_count": 71
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Helper functions\r\n",
					"Additional functions that are used throughout the different schema-drift functions"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def is_delta_table(table_path:str) -&gt; bool:\r\n",
					"    \"\"\"\r\n",
					"    Check if a table_path is a Delta table\r\n",
					"\r\n",
					"    :param table_path:  Abfss-path to the delta lake\r\n",
					"    :return Boolean:    Indicating whether the table_path is to a delta table\r\n",
					"    \"\"\"\r\n",
					"    try:\r\n",
					"        if print_statements: print(table_path)\r\n",
					"        # Check if the table_path can be loaded as a delta table\r\n",
					"        delta_table = DeltaTable.forPath(spark, table_path)\r\n",
					"        # Get the table metadata and validate that the format is 'delta'\r\n",
					"        table_details = spark.sql(f\"DESCRIBE DETAIL delta.`{table_path}`\")\r\n",
					"        if table_details.selectExpr(f\"first(format)\").collect()[0][0] == 'delta':\r\n",
					"            if print_statements: print(\"    is a delta table\")\r\n",
					"            return True\r\n",
					"        if print_statements: print(\"    is NOT a delta table\")\r\n",
					"        return False\r\n",
					"    except Exception as e:\r\n",
					"        if print_statements: print(\"    is NOT a delta table\")\r\n",
					"        return False"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_partitioning_columns(table_path:str) -&gt; list:\r\n",
					"    \"\"\"\r\n",
					"    Return the list of columns on which partitioning is done\r\n",
					"\r\n",
					"    :param table_path:                  Abfss-path to the delta lake\r\n",
					"    :return sql_table_partitionColumns: List of columns on which the table is partitioned\r\n",
					"    \"\"\"\r\n",
					"    table_details = spark.sql(f\"DESCRIBE DETAIL delta.`{table_path}`\")\r\n",
					"    sql_table_partitionColumns:list = table_details.selectExpr(f\"first(partitionColumns)\").collect()[0][0]\r\n",
					"    if print_statements: print(f\"List of partition columns for {table_path}: {sql_table_partitionColumns}\")\r\n",
					"\r\n",
					"    return sql_table_partitionColumns"
				],
				"execution_count": 53
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_constraints(table_path:str) -&gt; list:\r\n",
					"    \"\"\"\r\n",
					"    Return the list of constraints that are placed on the table\r\n",
					"\r\n",
					"    :param table_path:          Abfss-path to the delta lake\r\n",
					"    :return constraint_names:   List of constraints that are enforced on the table\r\n",
					"    \"\"\"\r\n",
					"    constraints:dict = spark.sql(f\"DESCRIBE DETAIL  delta.`{table_path}`\").select('properties').collect()[0][0]\r\n",
					"    constraint_names:list = list(constraints.keys())\r\n",
					"    if print_statements: print(f\"List of constraints on {table_path}: {constraint_names}\")\r\n",
					"    return constraint_names"
				],
				"execution_count": 54
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def get_column_metadata(table_path:str):\r\n",
					"    \"\"\"\r\n",
					"    Print the StructField object of each column, including the metadata\r\n",
					"    Mainly used for manual validation of changes to the schema\r\n",
					"\r\n",
					"    :param table_path:          Abfss-path to the delta lake\r\n",
					"\r\n",
					"    :example\r\n",
					"        table_schema[column_id] = StructField(&lt;column_name&gt;, &lt;dataType&gt;, &lt;nullable&gt;)\r\n",
					"        table_schema[column_id].metadata = {&lt;dictionary with metadata about the column&gt;} = {\"&lt;column_name&gt;\": \"&lt;description of the purpose of the column&gt;\", \"__CHAR_VARCHAR_TYPE_STRING\": \"varchar(&lt;xx&gt;)\"}\r\n",
					"    \"\"\"    \r\n",
					"    delta_table = spark.read.load(path=table_path, format='delta')\r\n",
					"    table_schema:dict = delta_table.schema\r\n",
					"\r\n",
					"    for column_id in range (len(table_schema)):\r\n",
					"        if print_statements: print(table_schema[column_id])\r\n",
					"        if print_statements: print(table_schema[column_id].metadata)"
				],
				"execution_count": 55
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Schema drift functions"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"def add_generic_column_to_all_delta_tables(column_name:str, data_type:str, dimension:str='SCD2'):\r\n",
					"    \"\"\"\r\n",
					"    Add &lt;column_name&gt; with &lt;data_type&gt; to all existing delta tables that do not yet have that column\r\n",
					"\r\n",
					"    :param column_name: Name of the column that needs to be added to the delta tables\r\n",
					"    :param data_type:   Data Type of the column that needs to be added to the delta tables\r\n",
					"    :param dimension:   The dimension of the column (primary key, SCD2, etc.)\r\n",
					"\r\n",
					"    :notes\r\n",
					"        It is not recommended to use this function to add primary key (PK) columns!\r\n",
					"        Refer to function add_new_column and refactor_pk_to_data_column\r\n",
					"    \"\"\"\r\n",
					"    \r\n",
					"#region Get a list of all the existing delta tables\r\n",
					"    # Dev-Info: Assume all delta tables are defined directly at the delta_lake_path (so no recursive search is done)\r\n",
					"    found_folders = mssparkutils.fs.ls(delta_lake_path)\r\n",
					"    # Filter so that the list only contains the delta tables (and not any other files/folders)\r\n",
					"    delta_tables = [possible_delta_table_folder for possible_delta_table_folder in found_folders if is_delta_table(possible_delta_table_folder.path)]    \r\n",
					"    if print_statements: print(\"List of found delta tables:\")\r\n",
					"    if print_statements: [print(f\"  {delta_table}\") for delta_table in delta_tables]\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Loop through all delta tables and add the column if it does not exist in the table already\r\n",
					"    for delta_table in delta_tables:\r\n",
					"        add_new_column(delta_table.name, column_name=column_name, data_type=data_type, dimension=dimension)\r\n",
					"#endregion"
				],
				"execution_count": 56
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def add_new_column(table_name:str, column_name:str, data_type:str, dimension:str=\"SCD2\"):\r\n",
					"    \"\"\"\r\n",
					"    Add &lt;column_name&gt; with &lt;data_type&gt; to all existing delta tables that do not yet have that column\r\n",
					"\r\n",
					"    :param column_name: Name of the column that needs to be added to the delta tables\r\n",
					"    :param data_type:   Data Type of the column that needs to be added to the delta tables\r\n",
					"    :param dimension:   The dimension of the column (primary key, SCD2, etc.)\r\n",
					"\r\n",
					"    :notes\r\n",
					"        It is not recommended to use this function to add primary key (PK) columns!\r\n",
					"        Refer to function refactor_pk_to_data_column\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    \"\"\"\r\n",
					"    First get a list of all the existing delta tables\r\n",
					"    For each table: If the column is not yet part of the table:\r\n",
					"        1. Create a dataframe that contains the new column and the existing list of partition columns\r\n",
					"            Dev-Note: Without adding the partitioning columns, the merge cannot be executed properly\r\n",
					"        2. Merge the dataframe with the existing delta table \r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"    delta_table_path = f\"{delta_lake_path}/{table_name}\"\r\n",
					"    dataframe = spark.read.load(path=delta_table_path, format='delta')\r\n",
					"    if column_name not in dataframe.columns:\r\n",
					"    #region Create list &lt;columns_to_add&gt; of dictionaries that contain the column information of the new column\r\n",
					"        if print_statements: print(f\"Adding column {column_name} to {delta_table.name}...\")\r\n",
					"        \r\n",
					"        # Dev-Note: Refactor the function so that it would be possible to add multiple new columns at once\r\n",
					"        columns_to_add = [\r\n",
					"            {\"column_name\": column_name, \"data_type\": data_type, \"dimension\": dimension}\r\n",
					"        ]\r\n",
					"    #endregion\r\n",
					"\r\n",
					"    #region Add the partition columns to the column information object &lt;columns_to_add&gt;\r\n",
					"        sql_table_partitionColumns = get_partitioning_columns(table_path=path)\r\n",
					"        for partition_column in sql_table_partitionColumns:\r\n",
					"            column_object = {\"column_name\": partition_column, \"data_type\": \"string\", \"dimension\": \"SCD2\"}\r\n",
					"            columns_to_add.append(column_object)\r\n",
					"    #endregion\r\n",
					"\r\n",
					"    #region Crearte and merge new dataframe with existing delta table\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass:create_dataframe] Create a dataframe using the column_information object\r\n",
					"        df_object = SparkDataFrameClass()\r\n",
					"        df_object.create_dataframe(column_information=columns_to_add)\r\n",
					"\r\n",
					"        if print_statements: print(f\"Adding the following dataframe to delta table {path}\")\r\n",
					"        if print_statements: df_object.dataframe.show()\r\n",
					"\r\n",
					"        new_df = df_object.dataframe\r\n",
					"        # Add the dataframe to the existing delta table\r\n",
					"        new_df.write.format('delta').mode('append').option(\"mergeSchema\", True).save(path)\r\n",
					"    #endregion"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def change_column_datatype(table_name:str, column_name:str, new_data_type:str, validate_casting:bool=False):\r\n",
					"    \"\"\"\r\n",
					"    Change the data type of a specified column in a Delta table.\r\n",
					"\r\n",
					"    :param table_name:          The name of the Delta table.\r\n",
					"    :param column_name:         The name of the column to change.\r\n",
					"    :param new_data_type:       The new data type for the column.\r\n",
					"    :param validate_casting:    Whether to validate the casting of the column.\r\n",
					"    \"\"\"\r\n",
					"#region Define function variables\r\n",
					"    delta_table_path = f'{delta_lake_path}/{table_name}'\r\n",
					"    df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"    # Get the list of current partition_columns from the delta table\r\n",
					"    partition_columns:list = get_partitioning_columns(delta_table_path)\r\n",
					"    # Return the list of configuration datatypes and their spark-opposite\r\n",
					"    data_types:dict = class_obj.configure_datatypes([new_data_type])\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Define metadata field for the to be changed column\r\n",
					"    new_metadata = {column_name: f\"metadata about the column {column_name}\"}\r\n",
					"    varchar_pattern = r'^varchar\\((\\d{1,2})\\)$'\r\n",
					"    # If datatype = varchar(&lt;xx&gt;): Alter the metadata\r\n",
					"    if re.match(varchar_pattern, new_data_type):\r\n",
					"        # When dealing with a varchar, change the metadata of the column to add a condition\r\n",
					"        new_metadata['__CHAR_VARCHAR_TYPE_STRING'] = new_data_type\r\n",
					"        new_data_type = 'string'\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Create a new schema with the updated data type and metadata\r\n",
					"    new_schema = []\r\n",
					"    # Create a StructField object for each column, with an update for the new column\r\n",
					"    for field in df.schema.fields:\r\n",
					"        if field.name == column_name:\r\n",
					"            # Create a new StructField with the updated data type and metadata\r\n",
					"            new_field = StructField(field.name, data_types[new_data_type], field.nullable, new_metadata)\r\n",
					"            new_schema.append(new_field)\r\n",
					"        else:\r\n",
					"            new_field = StructField(field.name, field.dataType, field.nullable, field.metadata)\r\n",
					"            new_schema.append(new_field)\r\n",
					"\r\n",
					"    # Create a new StructType from the new schema\r\n",
					"    new_struct_type = StructType(new_schema)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Apply the new schema to the DataFrame\r\n",
					"    for field in new_schema:\r\n",
					"        df = df.withColumn(field.name, col(field.name).cast(field.dataType))\r\n",
					"        df = df.withMetadata(field.name, field.metadata)\r\n",
					"\r\n",
					"    if validate_casting:\r\n",
					"        class_obj = SparkDataFrameChecks(source_path=delta_table_path, header=True, separator=',', file_kind='', column_names='', data_types='', checks=[], column_information={},  deploy=True)\r\n",
					"        class_obj.dataframe = df\r\n",
					"        class_obj.cast_column(dataframe_subset=df.select(column_name), column=column_name, data_type=new_data_type)\r\n",
					"\r\n",
					"    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partition_columns).save(delta_table_path)\r\n",
					"#endregion"
				],
				"execution_count": 57
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def rename_column(table_name:str, current_column_name:str, new_column_name:str):\r\n",
					"    \"\"\"\r\n",
					"    Change the data type of a specified column in a Delta table.\r\n",
					"\r\n",
					"    :param table_name:          The name of the Delta table.\r\n",
					"    :param current_column_name: The name of the column to change.\r\n",
					"    :param new_column_name:     The new name of the column.\r\n",
					"    \"\"\"\r\n",
					"#region Define function variables\r\n",
					"    delta_table_path = f'{delta_lake_path}/{table_name}'\r\n",
					"    df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"    # Get the list of current partition_columns from the delta table\r\n",
					"    partition_columns = get_partitioning_columns(delta_table_path)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Check if current_column_name is a PK column\r\n",
					"    constraint_list = get_constraints(delta_table_path)\r\n",
					"    not_null_constraint = f'pk__notnull_{current_column_name}'\r\n",
					"\r\n",
					"    add_pk_constraint = False\r\n",
					"    if f'delta.constraints.{not_null_constraint}' in constraint_list:\r\n",
					"        add_pk_constraint = True\r\n",
					"        spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` DROP CONSTRAINT {not_null_constraint}\")\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Check if there is partitioning on the current_column_name\r\n",
					"    if current_column_name in partition_columns:\r\n",
					"        partition_columns = [x.replace('overwrite', 'debug') for x in partition_columns]\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region\r\n",
					"    # Rename the column (for example, renaming 'old_column_name' to 'new_column_name')\r\n",
					"    df = df.withColumnRenamed(current_column_name, new_column_name)\r\n",
					"    # Write the DataFrame back to the Delta table\r\n",
					"    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partition_columns).save(delta_table_path)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region\r\n",
					"    if add_pk_constraint:\r\n",
					"        new_not_null_constraint = f'pk__notnull_{new_column_name}'\r\n",
					"        spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` ADD CONSTRAINT {new_not_null_constraint} CHECK ({new_column_name} IS NOT NULL)\")\r\n",
					"    \r\n",
					"#endregion"
				],
				"execution_count": 59
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def drop_column_name(table_name:str, column_name:str):\r\n",
					"    \"\"\"\r\n",
					"    Change the data type of a specified column in a Delta table.\r\n",
					"\r\n",
					"    :param table_name:  The name of the Delta table.\r\n",
					"    :param column_name: The name of the column to drop.\r\n",
					"    \"\"\"\r\n",
					"#region Define function variables\r\n",
					"    delta_table_path = f'{delta_lake_path}/{table_name}'\r\n",
					"    df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Check if current_column_name is a PK column\r\n",
					"    constraint_list = get_constraints(delta_table_path)\r\n",
					"    not_null_constraint = f'pk__notnull_{column_name}'\r\n",
					"\r\n",
					"    add_pk_constraint = False\r\n",
					"    if f'delta.constraints.{not_null_constraint}' in constraint_list:\r\n",
					"        add_pk_constraint = True\r\n",
					"        spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` DROP CONSTRAINT {not_null_constraint}\")\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Check if there is partitioning on the current_column_name\r\n",
					"    if column_name in partition_columns:\r\n",
					"        partition_columns.remove(column_name)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region\r\n",
					"    # Rename the column (for example, renaming 'old_column_name' to 'new_column_name')\r\n",
					"    df = df.drop(column_name)\r\n",
					"    # Write the DataFrame back to the Delta table\r\n",
					"    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*partition_columns).save(delta_table_path)\r\n",
					"#endregion"
				],
				"execution_count": 60
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def refactor_pk_to_data_column(table_name:str, column_name:str, validate_unique:bool=True):\r\n",
					"    \"\"\"\r\n",
					"    Remove the NOT NULL constraint on the column\r\n",
					"\r\n",
					"    :param table_name:  The name of the Delta table.\r\n",
					"    :param column_name: The name of the PK column to refactor to a regular data_column.\r\n",
					"    \"\"\"\r\n",
					"#region Define function variables\r\n",
					"    delta_table_path = f'{delta_lake_path}/{table_name}'\r\n",
					"    df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Check if current_column_name is a PK column\r\n",
					"    constraint_list = get_constraints(delta_table_path)\r\n",
					"    not_null_constraint = f'pk__notnull_{column_name}'\r\n",
					"\r\n",
					"    if f'delta.constraints.{not_null_constraint}' in constraint_list:\r\n",
					"        spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` DROP CONSTRAINT {not_null_constraint}\")\r\n",
					"        # if validate_unique:\r\n",
					"\r\n",
					"#endregion"
				],
				"execution_count": 62
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def refactor_data_column_to_pk(table_name:str, column_name:str):\r\n",
					"    \"\"\"\r\n",
					"    Remove the NOT NULL constraint on the column\r\n",
					"\r\n",
					"    :param table_name:  The name of the Delta table.\r\n",
					"    :param column_name: The name of the PK column to refactor to a regular data_column.\r\n",
					"    \"\"\"\r\n",
					"#region Define function variables\r\n",
					"    delta_table_path = f'{delta_lake_path}/{table_name}'\r\n",
					"    df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region Check if current_column_name is a PK column\r\n",
					"    constraint_list = get_constraints(delta_table_path)\r\n",
					"    not_null_constraint = f'pk__notnull_{column_name}'\r\n",
					"\r\n",
					"    if f'delta.constraints.{not_null_constraint}' in constraint_list:\r\n",
					"        spark.sql(f\"ALTER TABLE delta.`{delta_table_path}` ADD CONSTRAINT {new_not_null_constraint} CHECK ({column_name} IS NOT NULL)\")\r\n",
					"#endregion"
				],
				"execution_count": 63
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"def change_partitioning_columns(table_name, new_partitioning_list:list):\r\n",
					"    \"\"\"\r\n",
					"    Alter the existing list of partitioning columns to a new list\r\n",
					"\r\n",
					"    :param table_name:              The name of the Delta table.\r\n",
					"    :param new_partitioning_list:   List of column names on which to partition on\r\n",
					"    \"\"\"\r\n",
					"\r\n",
					"#region Define function variables\r\n",
					"    delta_table_path = f'{delta_lake_path}/{table_name}'\r\n",
					"    df = spark.read.format(\"delta\").load(delta_table_path)\r\n",
					"#endregion\r\n",
					"\r\n",
					"#region\r\n",
					"    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").partitionBy(*new_partitioning_list).save(delta_table_path)\r\n",
					"#endregion"
				],
				"execution_count": 64
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\TestNotebook.json">
{
	"name": "TestNotebook",
	"properties": {
		"folder": {
			"name": "Workers"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "e39bc7ff-34b6-455f-b7a5-31cea916c1c8"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# TEST NOTEBOOK\n",
					"\n",
					"The purpose of the Test Notebook script is to run all the Test Classes for unit testing the Synapse Notebook section in one organised space. The notebook will \"import\" all the Test Classes written in the Test Notebooks (Test_Modules, Test_Classes, Test_IngestionFunctions, etc.) and invoke all the classes at once. The results of all the tests will be handled by the notebook and wil be written as a text-file to the 'logs' container of the relevant data lake storage account.\n",
					"\n",
					"This script can be compared to the MetaNotebook script, as it also serves as an orchestrator rather than an actual executor.\n",
					"\n",
					"**Language**: PySpark (Python)"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Script logic\n",
					"1. Import the necessary python packages\n",
					"2. Prepare the spark session: Set parameters and run all the test notebooks to \"import\" the Test Cases\n",
					"3. Create a test-case runner object and run all the TestCases that have just been \"imported\"\n",
					"4. Process the test results and write the result to a text file on the logs container"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 1. Import packages"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import unittest\n",
					"import os\n",
					"from notebookutils import mssparkutils\n",
					"\n",
					"# Dev-Info: import pyspark.testing.utils # Pyspark version 3.5 contains useful functions (assertDataFrameEqual, assertSchemaEqual...) but they do not exist in the current 3.3.1 pyspark version"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 2. Prepare the spark session\n",
					"The following section will prepare the Spark Session. That is, when invoking a notebook it starts with a set of default configurations and functions. All the notebooks used for testing, other than the TestNotebook, contain functions and classes. They will not execute anything without being called upon. This can only happen when the functions/classes have been 'imported' into the spark session.\n",
					"\n",
					"Same goes for the parameter-list that is being passed when calling the TestNotebook. Some preprocessing activities will be executed on the parameters as to \"prepare\" the environment for execution."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Parameters\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. If testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"'''\n",
					"Inputs:\n",
					"    environment_code:The code used to indicate the environment the functions are running in (dev, int)\n",
					"'''\n",
					"\n",
					"environment_code:str = ''"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"Set variables to use during the notebook execution, but that will not be passed as parameters."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"spark.conf.set('spark.environment_code', environment_code)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set the env_code variable based on the spark configuration\n",
					"# If configuration argument 'environment_code' exists, use this. If not, use the parameter that has been passed \n",
					"# Dev-Info: During Synapse Workspace deployment, each notebook has been forced on the core_configuration spark configuration which contains the spark.environment_code=&lt;target_environment&gt;\n",
					"env_code:str = spark.conf.get('spark.environment_code', environment_code)\n",
					"print(env_code)\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set the variables that will be used to export the results file to\n",
					"test_adls_instance:str = f'{env_code}dapstdala1'\n",
					"test_export_container:str = 'logs'\n",
					"test_output_folder:str = 'test_results'\n",
					"output_file:str = 'output.txt'"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 2.2. Load notebooks\n",
					"Import all the relevant notebooks into the spark session using %run. After this action, the function and Classes defined in these notebooks will be able to be called from within the Spark session. \n",
					"\n",
					"Dev-Info: The notebooks are all being invoked using the %run-option. There is an option to pass parameter-arguments with this function, but they do not take variables. That is, it is not possible to add {'env_code'=env_code} to the function call as shown below. Only static variables (that is, no variable names but actual string values) can be passed (so only {'env_code'='dev'} works). For this reason, as each test notebook contains the core_configuration and env_code:str = spark.conf.get('spark.environment_code', environment_code) code snippet. \n",
					"\n",
					"Notebooks without params\n",
					"```\n",
					"%run Functions/Test_Functions/Test_Functions\n",
					"```\n",
					"Notebook with params\n",
					"```\n",
					"%run Modules/Test_Modules/Test_Modules {'env_code': &lt;static_string&gt; }\n",
					"```"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/Test_Functions/Test_IngestionFunctions"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/Test_Functions/Test_CleanWorkspace"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/Test_Functions/Test_GenericFunctions"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Test_Modules/Test_Classes"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Test_Modules/Test_Dataframes_v2"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Test_Modules/Test_DeltaTables"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### 3. Run Test Cases\n",
					"\n",
					"Each test notebook contains a set of classes that inherit the functionalities of the unittest.TestCase.\n",
					"The runner created below will invoke all the TestCase-classes and thereby run all the configured tests"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Use the output_file to write all the tests results to\n",
					"log_file:str = output_file\n",
					"with open(log_file, \"w\") as f:\n",
					"    # Create a runner of the TextTestRunner object, with the output_file as the write-location of the runner\n",
					"    # This basically tells the unittest-class to convert the test results to a txt-format and write it to a specific file\n",
					"    # Dev-Info: Note that this file does not exist anywhere yet! An empty textfile-like template has been created but there is no location assigned to this textfile\n",
					"    runner = unittest.TextTestRunner(f, verbosity=2)\n",
					"\n",
					"    # Run all the TestCase classes and store the results in a findings-variable \n",
					"    findings = unittest.main(argv=[''], testRunner=runner, verbosity=2, exit=False, warnings='ignore')"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Open the log_file and read the contents. Write all the lines one by one to a location on the storage account\n",
					"with open(log_file, \"r\") as f:\n",
					"        test = f.read()\n",
					"        test_list = [test]\n",
					"\n",
					"        spark.createDataFrame(test_list, \"string\").coalesce(1).write.format(\"text\").option(\"header\", \"false\").mode(\"overwrite\").save(f'abfss://{test_export_container}@{test_adls_instance}.dfs.core.windows.net/{test_output_folder}')"
				],
				"execution_count": 27
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# If the findings-variable contains failures or errors, make sure an error is thrown for the user to be alerted\n",
					"if len(findings.result.failures) != 0 and len(findings.result.errors) != 0:\n",
					"        raise Exception(f\"Failed tests: {findings.result.failures}, Error tests: {findings.result.errors}\")\n",
					"elif len(findings.result.failures) != 0:\n",
					"        raise Exception(f\"Failed tests: {findings.result.failures}\")\n",
					"elif len(findings.result.errors) != 0:\n",
					"        raise Exception(f\"Error tests: {findings.result.errors}\")"
				],
				"execution_count": 29
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_Classes.json">
{
	"name": "Test_Classes",
	"properties": {
		"folder": {
			"name": "Modules/Test_Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "160bf83a-4a37-4a9d-b95b-ebdc510c8163"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Classes\n",
					"**Purpose**: This Notebook will using the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the Spark Notebooks in the folder *Modules*.\n",
					"\n",
					"Unit tests: To test if a method does what we expect it to do, a untit test is written. A unit test isolates a method from the overall environment and executes the method. The aim is to be able to define the expected outcome of the method and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the method successfully\n",
					"\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import unittest\n",
					"from unittest.mock import patch, MagicMock, Mock, call, ANY\n",
					"\n",
					"import pyspark.testing.utils # Pyspark version 3.5 contains useful functions (assertDataFrameEqual, assertSchemaEqual...) but they do not exist in the current 3.3.1 pyspark version\n",
					"\n",
					"from pyspark.sql.types import StructType,StructField, StringType, IntegerType\n",
					"from delta.tables import DeltaTable\n",
					"from pyspark.sql.functions import col\n",
					"import pyspark\n",
					"\n",
					"import pandas as pd\n",
					"from pyspark.sql import SparkSession\n",
					"\n",
					"from io import StringIO\n",
					"import os\n",
					"from py4j.java_gateway import JavaObject\n",
					"\n",
					"import time\n",
					"import datetime\n",
					"import delta.tables as dt   "
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. When testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\n",
					"environment_code = 'dev'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\n",
					"\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the methods that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Classes"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/GenericFunctions"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define Test Classes"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: SQLServerConnection()\n",
					"Class Purpose: "
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SQLServerConnection\n",
					"# This class will test the __init__ of the SQLServerConnection-class defined in the Classes notebook\n",
					"class Test_SQLServerConnection_initialisation(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown TestCass class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the __init__ method\n",
					"\n",
					"    # Success: Initialize a SQLServerConnection class-object with valid arguments \n",
					"    def test_initialisation_sqlserverconnection_success(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the happy path: If the given arguments are valid, the server_url and linked_service name should be retrievable from the object\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be passed to the SQLServerConnection object\n",
					"        port_value = 1433\n",
					"        env_code_value = self.env_code\n",
					"        \n",
					"        # Define the expected values for the class-instance members: \n",
					"        expected_server_url = f\"jdbc:sqlserver://{env_code_value}-dap-sql-core.database.windows.net:{port_value};database={env_code_value}-dap-sqldb-core-meta\"\n",
					"        expected_linked_service = 'ls_dap_sql_meta'\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Initiliaze a SQLServerConnection class-object with a valid set of argument values\n",
					"        class_object = SQLServerConnection(env_code=env_code_value, port=port_value)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the initialised object parameter 'server_url' matches the expected value\n",
					"        actual_server_url  = class_object.server_url\n",
					"        actual_linked_service = class_object.linked_service\n",
					"\n",
					"        self.assertEqual(expected_server_url, actual_server_url, f\"[Classes:SQLServerConnection:Init] Expected server url does not match actual server url: {expected_server_url} versus {actual_server_url}\")\n",
					"        self.assertEqual(expected_linked_service, actual_linked_service, f\"[Classes:SQLServerConnection:Init] Expected linked service does not match actual server url: {expected_linked_service} versus {actual_linked_service}\")\n",
					"\n",
					"    # Success: Validate function expected_validate_argument_calls when initializing a SQLServerConnection class-object with valid arguments \n",
					"    def test_initialisation_sqlserverconnection_expected_calls(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the happy path: If the given arguments are valid, the __init__ function should call a set of validation functions\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be passed to the SQLServerConnection object\n",
					"        port_value = 1433\n",
					"        env_code_value = self.env_code\n",
					"        linked_service = 'ls_dap_sql_meta'\n",
					"\n",
					"        ##  [GenericFunctions]: Define the list of expected calls to the validate_argument function\n",
					"        expected_validate_argument_calls = [\n",
					"            call('env_code', env_code_value, ['dev', 'int', 'tst', 'acc', 'prd']), \n",
					"            call('port', port_value, [1433])\n",
					"        ]\n",
					"        expected_validate_linked_service_calls = [call(linked_service)]\n",
					"\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Initiliaze a SQLServerConnection class-object with a valid set of argument values\n",
					"        with patch.object(SQLServerConnection, 'validate_env_argument') as mock_validate_env_argument, \\\n",
					"            patch.object(SQLServerConnection, 'validate_port_argument') as mock_validate_port_argument, \\\n",
					"            patch.object(SQLServerConnection, 'validate_sql_linked_service') as mock_validate_sql_linked_service, \\\n",
					"            patch(f'__main__.validate_argument') as mock_validate_argument, \\\n",
					"            patch(f'__main__.validate_linked_service') as mock_validate_linked_service:\n",
					"\n",
					"            class_object = SQLServerConnection(env_code=env_code_value, port=port_value)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the mocked functions have been called with the expected set of arguments\n",
					"        ##  [Classes:SQLServerConnection]: Validate that the methods of the SQLServerConnection-class have been called\n",
					"        mock_validate_env_argument.assert_called_once()\n",
					"        mock_validate_port_argument.assert_called_once()\n",
					"        mock_validate_sql_linked_service.assert_called_once()\n",
					"        ##  [GenericFunctions]: Validate that the methods of the GenericFunctions notebook have been called with specific arguments\n",
					"        mock_validate_linked_service.called_once_with(expected_validate_linked_service_calls)\n",
					"        mock_validate_argument.called_with(expected_validate_argument_calls)\n",
					"\n",
					"\n",
					"    # Failure: Initialize a SQLServerConnection class-object with an invalid value for the env_code-argument\n",
					"    def test_initialisation_sqlserverconnection_invalid_environment(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the unhappy path: If the given env_code-arguments is invalid, an error should be thrown\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Define the expected error to be thrown when trying to initialize a class-object\n",
					"        expected_error = f\"The env_code-argument 'invalid' is not listed in the allowed env_code list: ['dev', 'int', 'tst', 'acc', 'prd']\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Initiliaze a SQLServerConnection class-object, expecting an error to be thrown\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object = SQLServerConnection(env_code='invalid', port=1433)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"\n",
					"    # Failure: Initialize a SQLServerConnection class-object with an invalid value for the port-argument\n",
					"    def test_initialisation_sqlserverconnection_invalid_port(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the unhappy path: If the given port-arguments is invalid, an error should be thrown\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Define the expected error to be thrown when trying to initialize a class-object\n",
					"        expected_error = f\"The port-argument '-99' is not listed in the allowed port list: [1433]\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Initiliaze a SQLServerConnection class-object, expecting an error to be thrown\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object = SQLServerConnection(env_code='dev', port=-99)\n",
					"        \n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:SQLServerConnection:Init] Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SQLServerConnection_initialisation)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SQLServerConnection Initialisation tests, something went wrong!\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SQLServerConnection\n",
					"# This class will test the set_sql_connection() of the SQLServerConnection-class defined in the Classes notebook\n",
					"class Test_SQLServerConnection_SetSQLConnection(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing\n",
					"        self.env_code = env_code\n",
					"        # Set up a valid SQLServerConnection object instance to use during testing\n",
					"        self.sql_connection_object = SQLServerConnection(env_code=env_code, port=1433)\n",
					"\n",
					"        self.get_connection_patcher = patch.object(spark._sc._gateway.jvm.java.sql.DriverManager, 'getConnection')\n",
					"        self.mock_get_connection    = self.get_connection_patcher.start()\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown patch.objected functions\n",
					"        self.get_connection_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the set_sql_connection() method\n",
					"\n",
					"    # Success: Assert that set_sql_connection returns 2 non-empty objects\n",
					"    def test_setsqlconnection_sqlserverconnection_success(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the happy path: Call the sql_connection_object method and expect a non-empty value for access_token and connection-object to be returned\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Call the method under test\n",
					"        access_token, connection = self.sql_connection_object.set_sql_connection()\n",
					"\n",
					"        # VALIDATE\n",
					"        # Assert that the method returns the expected values\n",
					"        self.assertIsNotNone(access_token)\n",
					"        self.assertIsNotNone(connection)\n",
					"\n",
					"\n",
					"\n",
					"    # Failure: Pass a string value to the server_url member of the SQLServerConnection object and expect an error\n",
					"    def test_setsqlconnection_sqlserverconnection_failed_invalid_sqlserver_1(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the unhappy path: Try setting a connection using a string-value as the server_url \n",
					"            Expect error: No suitable driver found for SQL Server &lt;server_url&gt;\n",
					"        '''\n",
					"\n",
					"        # PREPROCESS\n",
					"        # Set an invalid url to use throughout the test\n",
					"        invalid_url = 'invalid_url'\n",
					"        self.sql_connection_object.server_url = invalid_url\n",
					"        # Expect the following error to be returned\n",
					"        expected_error = f'No suitable driver found for SQL Server {invalid_url}'\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Call the method under test\n",
					"        with self.assertRaises(Exception) as error:\n",
					"            access_token, connection = self.sql_connection_object.set_sql_connection()\n",
					"\n",
					"        # VALIDATE\n",
					"        # Validate that the returned error matches expectations\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:SQLServerConnection:SetSQLConnection] Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"\n",
					"\n",
					"    # Failure: Pass a non-existing server to the server_url member of the SQLServerConnection object and expect an error\n",
					"    def test_setsqlconnection_sqlserverconnection_failed_invalid_sqlserver_2(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the unhappy path: Try making a connection to a non-existing server or to a server that cannot be accessed using the access_token\n",
					"            Expect error: Connection to SQL Server &lt;server_url&gt; cannot be made. Validate url-value and check firewall/access settings\n",
					"        '''\n",
					"\n",
					"        # PREPROCESS\n",
					"        # Set an invalid url to use throughout the test\n",
					"        invalid_url = 'jdbc:sqlserver://%s:%s;database=%s' % ('non-existing-db.database.windows.net', 1433, 'does-not-matter')\n",
					"        self.sql_connection_object.server_url = invalid_url\n",
					"        # Expect the following error to be returned\n",
					"        expected_error = f'Connection to SQL Server {invalid_url} cannot be made. Validate url-value and check firewall/access settings'\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Call the method under test\n",
					"        with self.assertRaises(Exception) as error:\n",
					"            access_token, connection = self.sql_connection_object.set_sql_connection()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches expectations\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:SQLServerConnection:SetSQLConnection] Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"\n",
					"\n",
					"    # Success: Mock all the called functions and make sure that they are called with the expected arguments\n",
					"    def test_setsqlconnection_sqlserverconnection_expected_calls(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            Test the happy path: Call the sql_connection_object method and expect a set of (mocked) functions to be called\n",
					"                - Call set_access_token -&gt; call mssparkutils.credentials.getConnectionStringOrCreds\n",
					"                - Create a Properties-instance and call the putAll-method using a connection_object with the returned access_tokenb\n",
					"                - Create a DriverManager-instance and get the connection using the Properties-instance\n",
					"                - Validate the values of the returned values\n",
					"        '''\n",
					"\n",
					"        # PREPROCESS\n",
					"        ## Set the return_value for the access_token\n",
					"        expected_access_token = 'mocked_string'\n",
					"        expected_connection_object = 'mocked_connection_object'\n",
					"        # Define a MagicMock object that can call the putAll function\n",
					"        mock_properties = MagicMock(spec=['putAll'])\n",
					"        ## Expect the ConnectionProperties.putAll() function to be called with the following dictionary\n",
					"        expected_connection_properties:dict = {\n",
					"            'AccessToken': expected_access_token\n",
					"        }\n",
					"    \n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Run the sql_connection_object() and mock all the called functions\n",
					"        with patch.object(SQLServerConnection, 'set_access_token') as mock_setaccesstoken, \\\n",
					"            patch.object(mssparkutils.credentials, 'getConnectionStringOrCreds') as mock_getconnectionstringorcreds, \\\n",
					"            patch.object(spark._sc._gateway, 'jvm') as mock_jvm:\n",
					"\n",
					"            # Dev-Info: The side_effect is needed because getConnectionStringOrCreds is not directly called but through a mocked object.\n",
					"            #   The need for this side_effect is to validate that the getConnectionStringOrCreds is also called, and not only focus on the set_access_token()\n",
					"            #   Set_access_token will still try calling the original (non-mocked) version of mock_getconnectionstringorcreds. Side_effect forces the call to the mock\n",
					"            #   When refering to functions on main (see Functions-folder), the side-effect is not needed as the mock-object will actually be called and not the original\n",
					"            #   It is a bit confusing but just remember: Functions on main are mocked directly, while methods are mocked indirectly. When you want to refer to a mocked method, you need to do it explicitly \n",
					"            mock_setaccesstoken.side_effect = mock_getconnectionstringorcreds\n",
					"            \n",
					"            # Define the return_value for the mocked functions\n",
					"            mock_jvm.java.util.Properties.return_value = mock_properties\n",
					"            mock_getconnectionstringorcreds.return_value=expected_access_token\n",
					"            mock_jvm.java.sql.DriverManager.getConnection.return_value = expected_connection_object\n",
					"            \n",
					"            # [Classes:SQLServerConnection]: Call the method under test\n",
					"            actual_access_token, actual_connection_object = self.sql_connection_object.set_sql_connection()\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Make sure that all the mocked functions have been called as expected\n",
					"        ## set_access_token and mssparkutils.credentials.getConnectionStringOrCreds -&gt; Need to be called to set the access_token\n",
					"        mock_setaccesstoken.assert_called()\n",
					"        mock_getconnectionstringorcreds.assert_called()\n",
					"\n",
					"        ## Properties and DriverManager: Class-Instances need to be created to set up a connection-object to the SQL Server\n",
					"        mock_jvm.java.util.Properties.assert_called()\n",
					"\n",
					"        ## Properties.putAll and DriverManager.getConnection(): Use the returned access_token to set the connectionProperties\n",
					"        mock_properties.putAll.assert_called_with(expected_connection_properties)\n",
					"        mock_jvm.java.sql.DriverManager.getConnection.assert_called_with(self.sql_connection_object.server_url, mock_properties)\n",
					"\n",
					"        ## Validate that the final returned values match the expectations -&gt; Test if the return_values from the mocked functions match the values returned by set_sql_connection() \n",
					"        self.assertEqual(expected_access_token, actual_access_token, f\"[Classes:SQLServerConnection:SetSQLConnection] Returned access token does not match expectations: {actual_access_token} versus {expected_access_token}\")\n",
					"        self.assertEqual(expected_connection_object, actual_connection_object, f\"[Classes:SQLServerConnection:SetSQLConnection] Returned connection object does not match expectations: {actual_connection_object} versus {expected_connection_object}\")      \n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SQLServerConnection_SetSQLConnection)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SQLServerConnection\n",
					"# This class will test the check_access_token() of the SQLServerConnection-class defined in the Classes notebook\n",
					"class Test_SQLServerConnection_checkaccesstoken(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing\n",
					"        self.env_code = env_code\n",
					"        self.sql_connection_object = SQLServerConnection(env_code=self.env_code, port=1433)\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown patched functions\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_access_token method\n",
					"\n",
					"    # Success: Current access_token == None and new value needs to be set\n",
					"    def test_checkaccesstoken_sqlserverconnection_empty_token(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            access_token = None\n",
					"            -&gt; DO not check isValidToken as it does not exist\n",
					"            -&gt; Call set_sql_connection to set up a connection with access_token -&gt; Validate connection and access_token values\n",
					"\n",
					"        '''\n",
					"\n",
					"        # PREPROCESS\n",
					"        # Set the current access_token of the object = None\n",
					"        access_token = None\n",
					"        self.sql_connection_object.access_token = access_token\n",
					"        # Set expected return values for access_token and connection object\n",
					"        expected_connection_object =  'new_connection_object'\n",
					"        expected_access_token = 'new_token'\n",
					"       \n",
					"\n",
					"        # EXECUTE\n",
					"        # Invoke the check_access_token method of the SQLServerConnection class, and expect values actual_connection_object and actual_access_token to be returned\n",
					"        with patch.object(mssparkutils.credentials, 'isValidToken') as mock_isvalidtoken, \\\n",
					"            patch.object(SQLServerConnection, 'set_sql_connection') as mock_setsqlconnection:\n",
					"            # Set the return values of the mocked functions -&gt; Token is None and only the set_sql_object() should be called\n",
					"            mock_setsqlconnection.return_value = expected_access_token, expected_connection_object\n",
					"            actual_access_token, actual_connection_object = self.sql_connection_object.check_access_token()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the mocked functions have been called accordingly\n",
					"        ## Do not expect the isValidToken to be called since original access_token == None\n",
					"        mock_isvalidtoken.assert_not_called()\n",
					"        mock_setsqlconnection.assert_called_once()\n",
					"\n",
					"        # Validate that the returned connection-object and access_token match with the return_values of set_sql_connection()\n",
					"        self.assertEqual(expected_connection_object, actual_connection_object, f\"[Classes:SQLServerConnection:CheckAccessToken] Connection objects do not match: {expected_connection_object} versus  {actual_connection_object}\")\n",
					"        self.assertEqual(expected_access_token, actual_access_token, f\"[Classes:SQLServerConnection:CheckAccessToken] Access tokens do not match: {expected_access_token} versus  {actual_access_token}\")\n",
					"\n",
					"\n",
					"    # Success: Current access_token is not expired\n",
					"    def test_checkaccesstoken_sqlserverconnection_nonempty_token_valid(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            access_token = None and isValidToken = True\n",
					"            -&gt; Validate that isValidToken was called\n",
					"            -&gt; Do not call set_sql_connection since no new value needed -&gt; Validate no change in connection and access_token values\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Define a value for the token argument\n",
					"        access_token = 'token_value'\n",
					"        self.sql_connection_object.access_token = access_token\n",
					"        # Set expected return values\n",
					"        expected_connection_object =  'predefined_connection_object'\n",
					"        self.sql_connection_object.connection = expected_connection_object\n",
					"        expected_access_token = access_token\n",
					"        # Set return values for the set_sql_connection() -&gt; Should not be executed but will overwrite with these values if executed\n",
					"        not_expected_access_token       = 'new_token_value'\n",
					"        not_expected_connection_object  = 'new_connection_object'\n",
					"\n",
					"       \n",
					"\n",
					"        # EXECUTE\n",
					"        # Invoke the check_access_token method of the SQLServerConnection class, and expect values actual_connection_object and actual_access_token to be returned\n",
					"        with patch.object(mssparkutils.credentials, 'isValidToken') as mock_isvalidtoken, \\\n",
					"            patch.object(SQLServerConnection, 'set_sql_connection') as mock_setsqlconnection:\n",
					"            # Set the return values of the mocked functions -&gt; Token is valid and the original values should be returned\n",
					"            mock_isvalidtoken.return_value = True\n",
					"            mock_setsqlconnection.return_value = not_expected_access_token, not_expected_connection_object\n",
					"\n",
					"            actual_access_token, actual_connection_object = self.sql_connection_object.check_access_token()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the mocked functions have been called accordingly\n",
					"        ## Do not expect the set_sql_connection to be called since original access_token is still valid and does not need to be replaced\n",
					"        mock_isvalidtoken.assert_called_once_with(access_token)\n",
					"        mock_setsqlconnection.assert_not_called()\n",
					"\n",
					"        # Validate that the returned connection-object and access_token match with the original_values and not with the values returned from set_sql_connection()\n",
					"        self.assertEqual(expected_connection_object, actual_connection_object, f\"[Classes:SQLServerConnection:CheckAccessToken] Connection objects do not match: {expected_connection_object} versus  {actual_connection_object}\")\n",
					"        self.assertEqual(expected_access_token, actual_access_token, f\"[Classes:SQLServerConnection:CheckAccessToken] Token values do not match: {expected_access_token} versus  {actual_access_token}\")\n",
					"\n",
					"\n",
					"    # Success: Current access_token is expired and a new one needs to be set\n",
					"    def test_checkaccesstoken_sqlserverconnection_nonempty_token_invalid(self):\n",
					"        '''\n",
					"        Test logic:\n",
					"            access_token = None and isValidToken = False\n",
					"            -&gt; Validate that isValidToken was called\n",
					"            -&gt; Call set_sql_connection since no new value needed -&gt; Validate connection and access_token values\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Define a value for the token argument\n",
					"        access_token = 'token_value'\n",
					"        self.sql_connection_object.access_token = access_token\n",
					"        # Set expected return values and \n",
					"        expected_connection_object =  'new_connection_object'\n",
					"        expected_access_token = 'new_access_token'\n",
					"        self.sql_connection_object.connection = 'predefined_connection_object'\n",
					"       \n",
					"\n",
					"        # EXECUTE\n",
					"        # Invoke the check_access_token method of the SQLServerConnection class, and expect values actual_connection_object to be returned\n",
					"        with patch.object(mssparkutils.credentials, 'isValidToken') as mock_isvalidtoken, \\\n",
					"            patch.object(SQLServerConnection, 'set_sql_connection') as mock_setsqlconnection:\n",
					"            # Set the return values of the mocked functions -&gt; Token is invalid and needs to be recreated\n",
					"            mock_isvalidtoken.return_value = False\n",
					"            mock_setsqlconnection.return_value = expected_access_token, expected_connection_object\n",
					"\n",
					"            actual_access_token, actual_connection_object = self.sql_connection_object.check_access_token()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the mocked functions have been called accordingly\n",
					"        ## Do not expect the set_sql_connection to be called since original access_token is still valid and does not need to be replaced\n",
					"        mock_isvalidtoken.assert_called_once_with(access_token)\n",
					"        mock_setsqlconnection.assert_called_once()\n",
					"\n",
					"        # Validate that the returned connection-object and access_token match with the return_values of set_sql_connection()\n",
					"        self.assertEqual(expected_connection_object, actual_connection_object, f\"[Classes:SQLServerConnection:CheckAccessToken] Connection objects not equal: {expected_connection_object} versus  {actual_connection_object}\")\n",
					"        self.assertEqual(expected_access_token, actual_access_token, f\"[Classes:SQLServerConnection:CheckAccessToken] Access tokens not equal: {expected_access_token} versus {actual_access_token}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SQLServerConnection_checkaccesstoken)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SQLServerConnection\n",
					"# This class will test the execute_query() of the SQLServerConnection-class defined in the Classes notebook\n",
					"class Test_SQLServerConnection_ExecuteQuery(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing\n",
					"        self.env_code = env_code\n",
					"        self.sql_connection_object = SQLServerConnection(env_code=self.env_code, port=1433)\n",
					"        \n",
					"        ## [Classess:SQLServerConnection] Patch the functions that actually run SQL queries to prevent making connections to SQL during the testing\n",
					"        self.mock_execute_query_with_return_patcher = patch.object(SQLServerConnection, 'execute_query_with_return', return_value=True)\n",
					"        self.mock_execute_query_with_return = self.mock_execute_query_with_return_patcher.start()\n",
					"\n",
					"        self.mock_execute_query_without_return_patcher = patch.object(SQLServerConnection, 'execute_query_without_return')\n",
					"        self.mock_execute_query_without_return = self.mock_execute_query_without_return_patcher.start()\n",
					"\n",
					"        self.get_connection_patcher = patch.object(spark._sc._gateway.jvm.java.sql.DriverManager, 'getConnection')\n",
					"        self.mock_get_connection    = self.get_connection_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown patched functions\n",
					"        self.mock_execute_query_with_return_patcher.stop()\n",
					"        self.mock_execute_query_without_return_patcher.stop()\n",
					"        self.get_connection_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the __init__ method\n",
					"\n",
					"\n",
					"    # Success: Execute both staticmethods: once in the check_access_token and once for the actual execution\n",
					"    # Validate that both methods have been executed\n",
					"    def test_setsqlconnection_executequery_functionaltest(self):\n",
					"        ''' \n",
					"        Test Logic:\n",
					"            - Run the execute_query with statement 'test', primary_key=True and ExpectReturn=False\n",
					"            - Validate that the mocked functions have been called with the expected arguments\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        self.sql_connection_object.execute_query('test', True, False)\n",
					"\n",
					"        # EVALUATE\n",
					"        self.mock_execute_query_without_return.assert_called_once_with(ANY, 'test')\n",
					"        self.mock_execute_query_with_return.assert_called_once_with(ANY, 'SELECT TOP 1 * FROM meta.source_configuration')\n",
					"\n",
					"    # Success: Execute the primary checks and expect a return \n",
					"    def test_setsqlconnection_executequery_primarychecks_return(self):\n",
					"        ''' \n",
					"        Test Logic:\n",
					"            Run the execute_query with statement 'test', primary_key=True and ExpectReturn=True =&gt; Mock all functions\n",
					"            - Primary key = True -&gt; Call set_access_token and check_lifetime_db\n",
					"            - ExpectReturn = True -&gt; Call execute_query_with_return; Do not call execute_query_without_return\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Set the query-statement to execute\n",
					"        query               = 'test'\n",
					"        # Set the MagicMock-object for the createStatement-function -&gt; Make sure that it can still \"call\" the close-function\n",
					"        mock_statement      = MagicMock(spec='close')\n",
					"        # Set the MagicMock-object for the mock_checkaccesstoken-function that will return a connection-object -&gt; Make sure that it can still \"call\" the createStatement-function\n",
					"        mock_connection     = MagicMock(spec='createStatement')\n",
					"        # When using the mocked connection-object with method createStatement, set the returned object to the 'mock_statement' object defined above\n",
					"        mock_connection.createStatement = mock_statement\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Run the execute_query() and mock all the called functions\n",
					"        with patch.object(SQLServerConnection, 'check_access_token') as mock_checkaccesstoken, \\\n",
					"            patch.object(SQLServerConnection, 'check_lifetime_db') as mock_checklifetimedb:\n",
					"            # Set the output of the mock_checkaccesstoken to Mock (access_token) and mock_connection (connection_object)\n",
					"            mock_checkaccesstoken.return_value = Mock(), mock_connection\n",
					"\n",
					"            # [Classes:SQLServerConnection]: Call the method under test\n",
					"            self.sql_connection_object.execute_query(query, True, True)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the correct functions have been called\n",
					"        ## Primary checks == True -&gt; Call set_access_token and check_lifetime_db\n",
					"        mock_checkaccesstoken.assert_called_once()\n",
					"        mock_checklifetimedb.assert_called_once()\n",
					"        ## Expect_Return == True -&gt; Call with_return with the query\n",
					"        self.mock_execute_query_without_return.assert_not_called()\n",
					"        self.mock_execute_query_with_return.assert_called_once_with(mock_statement.return_value, query)\n",
					"\n",
					"    # Success: Do not execute the primary checks and expect a return\n",
					"    def test_setsqlconnection_executequery_noprimarychecks_return(self):\n",
					"        ''' \n",
					"        Test Logic:\n",
					"            Run the execute_query with statement 'test', primary_key=True and ExpectReturn=True =&gt; Mock all functions\n",
					"            - Primary key = False -&gt; Do not call set_access_token and check_lifetime_db\n",
					"            - ExpectReturn = True -&gt; Call execute_query_with_return; Do not call execute_query_without_return\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Set the query-statement to execute\n",
					"        query               = 'test'\n",
					"        # Set the MagicMock-object for the createStatement-function -&gt; Make sure that it can still \"call\" the close-function\n",
					"        mock_statement      = MagicMock(spec='close')\n",
					"        # Set the MagicMock-object for the mock_checkaccesstoken-function that will return a connection-object -&gt; Make sure that it can still \"call\" the createStatement-function\n",
					"        mock_connection     = MagicMock(spec='createStatement')\n",
					"        # When using the mocked connection-object with method createStatement, set the returned object to the 'mock_statement' object defined above\n",
					"        mock_connection.createStatement = mock_statement\n",
					"        # Primary_Checks=False -&gt;  set a connection as none will be set by primary checks \n",
					"        self.sql_connection_object.connection = mock_connection\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Run the execute_query() and mock all the called functions\n",
					"        with patch.object(SQLServerConnection, 'check_access_token') as mock_checkaccesstoken, \\\n",
					"            patch.object(SQLServerConnection, 'check_lifetime_db') as mock_checklifetimedb:\n",
					"            # [Classes:SQLServerConnection]: Call the method under test\n",
					"            mock_checkaccesstoken.return_value = Mock(), mock_connection\n",
					"            self.sql_connection_object.execute_query(query, False, True)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the correct functions have been called\n",
					"        ## Primary checks == False -&gt; Do not call set_access_token and check_lifetime_db\n",
					"        mock_checkaccesstoken.assert_not_called()\n",
					"        mock_checklifetimedb.assert_not_called()\n",
					"        ## Expect_Return == True -&gt; Call with_return with the query\n",
					"        self.mock_execute_query_without_return.assert_not_called()\n",
					"        self.mock_execute_query_with_return.assert_called_once_with(mock_statement.return_value, query)\n",
					"\n",
					"    # Success: Execute the primary checks and do not expect a return\n",
					"    def test_setsqlconnection_executequery_primarychecks_noreturn(self):\n",
					"        ''' \n",
					"        Test Logic:\n",
					"            Run the execute_query with statement 'test', primary_key=True and ExpectReturn=True =&gt; Mock all functions\n",
					"            - Primary key = True -&gt; Call set_access_token and check_lifetime_db\n",
					"            - ExpectReturn = False -&gt; Do not call execute_query_with_return; Call execute_query_without_return\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Set the query-statement to execute\n",
					"        query               = 'test'\n",
					"        # Set the MagicMock-object for the createStatement-function -&gt; Make sure that it can still \"call\" the close-function\n",
					"        mock_statement      = MagicMock(spec='close')\n",
					"        # Set the MagicMock-object for the mock_checkaccesstoken-function that will return a connection-object -&gt; Make sure that it can still \"call\" the createStatement-function\n",
					"        mock_connection     = MagicMock(spec='createStatement')\n",
					"        # When using the mocked connection-object with method createStatement, set the returned object to the 'mock_statement' object defined above\n",
					"        mock_connection.createStatement = mock_statement\n",
					"\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Run the execute_query() and mock all the called functions\n",
					"        with patch.object(SQLServerConnection, 'check_access_token') as mock_checkaccesstoken, \\\n",
					"            patch.object(SQLServerConnection, 'check_lifetime_db') as mock_checklifetimedb:\n",
					"            # [Classes:SQLServerConnection]: Call the method under test\n",
					"            mock_checkaccesstoken.return_value = Mock(), mock_connection\n",
					"            self.sql_connection_object.execute_query(query, True, False)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the correct functions have been called\n",
					"        ## Primary checks == True -&gt; Call set_access_token and check_lifetime_db\n",
					"        mock_checkaccesstoken.assert_called_once()\n",
					"        mock_checklifetimedb.assert_called_once()\n",
					"        ## Expect_Return == False -&gt; Call without_return with the query\n",
					"        self.mock_execute_query_with_return.assert_not_called()\n",
					"        self.mock_execute_query_without_return.assert_called_once_with(mock_statement.return_value, query)\n",
					"\n",
					"    # Success: Do not execute the primary checks and do not expect a return\n",
					"    def test_setsqlconnection_executequery_noprimarychecks_noreturn(self):\n",
					"        ''' \n",
					"        Test Logic:\n",
					"            Run the execute_query with statement 'test', primary_key=True and ExpectReturn=True =&gt; Mock all functions\n",
					"            - Primary key = False -&gt; Do not call set_access_token and check_lifetime_db\n",
					"            - ExpectReturn = False -&gt; Do not call execute_query_with_return; Call execute_query_without_return\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Set the query-statement to execute\n",
					"        query               = 'test'\n",
					"        # Set the MagicMock-object for the createStatement-function -&gt; Make sure that it can still \"call\" the close-function\n",
					"        mock_statement      = MagicMock(spec='close')\n",
					"        # Set the MagicMock-object for the mock_checkaccesstoken-function that will return a connection-object -&gt; Make sure that it can still \"call\" the createStatement-function\n",
					"        mock_connection     = MagicMock(spec='createStatement')\n",
					"        # When using the mocked connection-object with method createStatement, set the returned object to the 'mock_statement' object defined above\n",
					"        mock_connection.createStatement = mock_statement\n",
					"        # The primary checks will not set a connection so one needs to be defined upfront\n",
					"        self.sql_connection_object.connection = mock_connection\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Run the execute_query() and mock all the called functions\n",
					"        with patch.object(SQLServerConnection, 'check_access_token') as mock_checkaccesstoken, \\\n",
					"            patch.object(SQLServerConnection, 'check_lifetime_db') as mock_checklifetimedb:\n",
					"            # [Classes:SQLServerConnection]: Call the method under test\n",
					"            mock_checkaccesstoken.return_value = Mock(), mock_connection\n",
					"            self.sql_connection_object.execute_query(query, False, False)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the correct functions have been called\n",
					"        ## Primary checks == False -&gt; Do not call set_access_token and check_lifetime_db\n",
					"        mock_checkaccesstoken.assert_not_called()\n",
					"        mock_checklifetimedb.assert_not_called()\n",
					"        ## Expect_Return == False -&gt; Call without_return with the query\n",
					"        self.mock_execute_query_with_return.assert_not_called()\n",
					"        self.mock_execute_query_without_return.assert_called_once_with(mock_statement.return_value, query)\n",
					"\n",
					"\n",
					"    # Success: Do not execute the primary checks and do not expect a return\n",
					"    def test_setsqlconnection_executequery_no_connection(self):\n",
					"        ''' \n",
					"        Test Logic:\n",
					"            Run the execute_query with self.connection = None\n",
					"            - Primary_key is False but should be set to True as no connection_object exists -&gt; Call set_access_token and check_lifetime_db\n",
					"            - ExpectReturn = False -&gt; Do not call execute_query_with_return; Call execute_query_without_return\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Set the query-statement to execute\n",
					"        query               = 'test'\n",
					"        # Set the MagicMock-object for the createStatement-function -&gt; Make sure that it can still \"call\" the close-function\n",
					"        mock_statement      = MagicMock(spec='close')\n",
					"        # Set the MagicMock-object for the mock_checkaccesstoken-function that will return a connection-object -&gt; Make sure that it can still \"call\" the createStatement-function\n",
					"        mock_connection     = MagicMock(spec='createStatement')\n",
					"        # When using the mocked connection-object with method createStatement, set the returned object to the 'mock_statement' object defined above\n",
					"        mock_connection.createStatement = mock_statement\n",
					"        # The primary checks will not set a connection so one needs to be defined upfront\n",
					"        self.sql_connection_object.connection = None\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection]: Run the execute_query() and mock all the called functions\n",
					"        with patch.object(SQLServerConnection, 'check_access_token') as mock_checkaccesstoken, \\\n",
					"            patch.object(SQLServerConnection, 'check_lifetime_db') as mock_checklifetimedb:\n",
					"            # [Classes:SQLServerConnection]: Call the method under test\n",
					"            mock_checkaccesstoken.return_value = Mock(), mock_connection\n",
					"            self.sql_connection_object.execute_query(query, False, False)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the correct functions have been called\n",
					"        ## self.connection = None -&gt; Set Primary_Checks == True -&gt; Call set_access_token and check_lifetime_db\n",
					"        mock_checkaccesstoken.assert_called_once()\n",
					"        mock_checklifetimedb.assert_called_once()\n",
					"        ## Expect_Return == False -&gt; Call without_return with the query\n",
					"        self.mock_execute_query_with_return.assert_not_called()\n",
					"        self.mock_execute_query_without_return.assert_called_once_with(mock_statement.return_value, query)\n",
					"        \n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SQLServerConnection_ExecuteQuery)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SQLServerConnection\n",
					"# This class will test the check_lifetime_db() of the SQLServerConnection-class defined in the Classes notebook\n",
					"class Test_SQLServerConnection_CheckLifetimeDB(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing\n",
					"        self.env_code = env_code\n",
					"        self.sql_connection_object = SQLServerConnection(env_code=self.env_code, port=1433)\n",
					"        \n",
					"        # [Classes:SQLServerConnection] Mock the execute_query method as it will try and execute actual calls to the database\n",
					"        self.mock_execute_query_patcher = patch.object(SQLServerConnection, 'execute_query')\n",
					"        self.mock_execute_query = self.mock_execute_query_patcher.start()\n",
					"\n",
					"        # Mock time.sleep as the test will take too long if the sleep will actually be executed\n",
					"        self.mock_sleep_patcher = patch.object(time, 'sleep')\n",
					"        self.mock_sleep = self.mock_sleep_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown patched functions\n",
					"        self.mock_execute_query_patcher.stop()\n",
					"        self.mock_sleep_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_lifetime_db method\n",
					"\n",
					"    # Success: Execute the execute_query method with valid arguments\n",
					"    def test_checklifetimedb_sqlserverconnection_sleep_once(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:SQLServerConnection] execute_query: First call will throw an error, second call will be successful\n",
					"        self.mock_execute_query.side_effect = [ValueError, True]\n",
					"        # time.sleep: If time.sleep is called, set the wait time to None (instead of the pre-set 20 seconds)\n",
					"        self.mock_sleep.side_effect = [None]\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:SQLServerConnection] Call the method under test\n",
					"        self.sql_connection_object.check_lifetime_db()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called 2 times\n",
					"        self.mock_execute_query.assert_called()\n",
					"        self.assertEqual(self.mock_execute_query.call_count, 2)\n",
					"\n",
					"        self.mock_sleep.assert_called()\n",
					"        self.assertEqual(self.mock_sleep.call_count, 1)\n",
					"\n",
					"\n",
					"    # Success: Execute the execute_query method with valid arguments\n",
					"    def test_checklifetimedb_sqlserverconnection_fail_on_5_tries(self):\n",
					"        # PREPROCESS\n",
					"        # Call the execute_query function 5 times and throw an error:\n",
					"        expected_error = \"Connecting to SQL MetaDB takes too long. Validate the connection...\"\n",
					"        # [Classes:SQLServerConnection] execute_query: First 5 calls will throw an error, 6th call (which should not be reached) will be successful\n",
					"        self.mock_execute_query.side_effect = [Exception()] * 5 + [True]\n",
					"        # time.sleep: If time.sleep is called, set the wait time to None (instead of the pre-set 20 seconds) for each of the 5 calls that will be executed against it (5 is predefined in the check_lifetime_db method)\n",
					"        self.mock_sleep.side_effect = [None]*4\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Invoke the check_lifetime_db method of the SQLServerConnection class, and expect values actual_connection_object to be returned\n",
					"        with self.assertRaises(ConnectionError) as error:\n",
					"            # [Classes:SQLServerConnection] Call the method under test\n",
					"            self.sql_connection_object.check_lifetime_db()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the putAll-method has been called with the ConnectionDictionary defined in the function\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"Expected error does not match actual: {expected_error} versus {actual_error}\")\n",
					"        self.mock_execute_query.assert_called()\n",
					"        self.assertEqual(self.mock_execute_query.call_count, 5)\n",
					"        self.mock_sleep.assert_called()\n",
					"        self.assertEqual(self.mock_sleep.call_count, 4)\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SQLServerConnection_CheckLifetimeDB)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Plan_Instance(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        return\n",
					"        \n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the plan.__init__ method\n",
					"\n",
					"    def test_initialisation_plan(self):\n",
					"        ## This test exist to see if the instance variables were configured properly, based on the input into the object\n",
					"        # PREPROCESS:\n",
					"        expected_plan_id = -99\n",
					"        expected_sql_server_connection = Mock()\n",
					"        expected_plan_name = 'mock_plan'\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Plan:Init] Run the method under test\n",
					"        PlanInstance = Plan(expected_plan_id, expected_plan_name, expected_sql_server_connection)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Match actuals with expectations\n",
					"        actual_plan_id = PlanInstance.plan_id\n",
					"        actual_plan_name = PlanInstance.plan_name\n",
					"\n",
					"        self.assertEqual(expected_plan_id, actual_plan_id,              f\"[Classes:Plan:Init] Something went wrong during initialisation of plan_id: {expected_plan_id} versus {actual_plan_id}\")\n",
					"        self.assertEqual(expected_plan_name, actual_plan_name,          f\"[Classes:Plan:Init] Something went wrong during initialisation of plan_name: {expected_plan_name} versus {actual_plan_name}\")\n",
					"        self.assertIn('Mock', str(PlanInstance.SQLServerConnection),    f\"[Classes:Plan:Init] Something went wrong during initialisation of SQLServerConnection: {str(PlanInstance.SQLServerConnection)}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Plan_Instance)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_Instance(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the task.__init__ method\n",
					"\n",
					"    def test_initialisation_task(self):\n",
					"        ## This test exist to see if the instance variables were configured properly, based on the input into the object\n",
					"        # PREPROCESS:\n",
					"        expected_task_name = 'mock_task_name'\n",
					"        expected_task_id = -99\n",
					"        expected_plan_id = -88\n",
					"        expected_SQLServerConnection = Mock()\n",
					"        expected_worker_name = 'test'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:Init] Run the method under test\n",
					"        TaskInstance = Task(expected_task_name, expected_task_id, expected_plan_id, expected_SQLServerConnection, expected_worker_name)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Match actuals with expectations\n",
					"        actual_task_name = TaskInstance.task_name\n",
					"        actual_task_id = TaskInstance.task_id\n",
					"        actual_plan_id = TaskInstance.plan_id\n",
					"        actual_worker_name = TaskInstance.worker_name\n",
					"\n",
					"        self.assertEqual(expected_task_name, actual_task_name,          f\"[Classes:Task:Init] Something went wrong during initialisation of task_name: {expected_task_name} versus {actual_task_name}\")\n",
					"        self.assertEqual(expected_task_id, actual_task_id,              f\"[Classes:Task:Init] Something went wrong during initialisation of task_id: {expected_task_id} versus {actual_task_id}\")\n",
					"        self.assertEqual(expected_plan_id, actual_plan_id,              f\"[Classes:Task:Init] Something went wrong during initialisation of plan_id: {expected_plan_id} versus {actual_plan_id}\")\n",
					"        self.assertIn('Mock', str(TaskInstance.SQLServerConnection),    f\"[Classes:Task:Init] Something went wrong during initialisation of SQLServerConnection: str(TaskInstance.SQLServerConnection)\")\n",
					"        self.assertEqual(expected_worker_name, actual_worker_name,      f\"[Classes:Task:Init] Something went wrong during initialisation of worker_name: {expected_worker_name} versus {actual_worker_name}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_Instance)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 14
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_StartTask(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        self.task_id = -99\n",
					"        self.plan_id = -88\n",
					"\n",
					"        self.task_object = Task(Mock(), self.task_id, self.plan_id, Mock(), Mock(), Mock())\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the start_task method\n",
					"\n",
					"\n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_start_task'\n",
					"    def test_starttask_task_success(self):\n",
					"        # PREPROCESS\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        mock_sql_object.execute_query.return_value = True\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:StartTask] Run the method under test\n",
					"        self.task_object.start_task()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for the \n",
					"        mock_sql_object.execute_query.assert_called_once_with(f'EXEC meta.usp_start_task @task_id = {self.task_id}, @plan_id = {self.plan_id}')\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_StartTask)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_EndTask(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        self.task_id = -99\n",
					"        self.plan_id = -88\n",
					"\n",
					"        self.task_object = Task(Mock(), self.task_id, self.plan_id, Mock(), Mock(), Mock())\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the end_task method\n",
					"\n",
					"    # Success: Expect execute_query from the mock-object to be called with statement 'EXEC meta.usp_end_task' with a positive success_flag\n",
					"    def test_endtask_task_success(self):\n",
					"        # PREPROCESS\n",
					"        success_flag = True\n",
					"        comment = \"Comment with double quotes\"\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        mock_sql_object.execute_query.return_value = True\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:EndTask] Run the method under test\n",
					"        self.task_object.end_task(Success_Flag=success_flag, Comment=comment)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for the \n",
					"        mock_sql_object.execute_query.assert_called_once_with(f\"EXEC meta.usp_end_task @task_id = {self.task_id}, @plan_id = {self.plan_id}, @success_flag = {success_flag}, @comment = '{comment}'\")\n",
					"\n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_end_task' with a negative success_flag\n",
					"    def test_endtask_task_failure(self):\n",
					"        # PREPROCESS\n",
					"        success_flag = False\n",
					"        comment = 'Comment with single quotes'\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        mock_sql_object.execute_query.return_value = True\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:EndTask] Run the method under test\n",
					"        self.task_object.end_task(Success_Flag=success_flag, Comment=comment)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for the \n",
					"        mock_sql_object.execute_query.assert_called_once_with(f\"EXEC meta.usp_end_task @task_id = {self.task_id}, @plan_id = {self.plan_id}, @success_flag = {success_flag}, @comment = '{comment}'\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_EndTask)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_RunUspGetTaskMetadata(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        self.task_id = -99\n",
					"\n",
					"        self.task_object = Task(Mock(), self.task_id, Mock(), Mock(), Mock(), Mock())\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the start_task method\n",
					"\n",
					"\n",
					"    # Success: Expect execute_query from the mock-object to be called with statement 'EXEC meta.usp_get_task_metadata' and expect_return=True\n",
					"    def test_run_usp_getTaskMetadata_task_success(self):\n",
					"        # PREPROCESS\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        mock_sql_object.execute_query.return_value = True\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:run_usp_getTaskMetadata] Run the method under test\n",
					"        self.task_object.run_usp_getTaskMetadata()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for the \n",
					"        mock_sql_object.execute_query.assert_called_once_with(f'exec meta.usp_get_task_metadata @task_id={self.task_id}', expect_return=True)\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_RunUspGetTaskMetadata)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_GetTaskMetadata(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        self.task_id = -99\n",
					"        self.task_object = Task(Mock(), self.task_id, Mock(), Mock(), Mock())\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the start_task method\n",
					"\n",
					"\n",
					"    # Success: Expect execute_query from the mock-object to be called with statement 'EXEC meta.usp_get_task_metadata' and expect_return=True\n",
					"    def test_gettaskmetadata_task_function_calls(self):\n",
					"        '''\n",
					"        Test Logic:\n",
					"            - Expect get_task_metadata() to call run_usp_getTaskMetadata, which will call execute_query with:\n",
					"                statement = 'exec meta.usp_get_task_metadata @task_id = &lt;task_id&gt;' \n",
					"                expect_return = True\n",
					"        '''\n",
					"        \n",
					"        # PREPROCESS\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        # Set the return_value of execute_query to an empty list of json-objects\n",
					"        mock_sql_object.execute_query.return_value = [{}, {}]\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"        # Pass an empty list of variables as the test will only validate function calls\n",
					"        variables = []\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:get_task_metadata] Run the method under test\n",
					"        self.task_object.get_task_metadata(variables)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for once with the expected arguments \n",
					"        mock_sql_object.execute_query.assert_called_once_with(f'exec meta.usp_get_task_metadata @task_id={self.task_id}', expect_return=True)\n",
					"\n",
					"    # Success: Expect self.task_object.variables-dictionary to contain keys and values for the passed variables-list\n",
					"    def test_gettaskmetadata_task_set_simple_variables(self):\n",
					"        '''\n",
					"        Test Logic:\n",
					"            Pass a non-empty list of variables to the get_task_metadata method\n",
					"            Mock execute_query to return a list of json-objects for a set of columns\n",
					"            Validate that the variables-member of the Task-instance task_object contains the keys and values after executing get_task_metadata\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        # Exeucte_query returns a non-empty object in an array\n",
					"        mock_sql_object.execute_query.return_value = [\n",
					"            {\n",
					"                'column_1': '[{\"test\": \"value1\"}]', \n",
					"                'column_2': '[{\"unittest\": \"value2\"}]',\n",
					"                'column_3': '[{\"keys\": \"value3\"}]'\n",
					"            }\n",
					"        ]\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"        # Pass the below list to get_task_metadata and expect key-value pairs to be set in the self.task_object.variables dictionary\n",
					"        variables = ['test', 'unittest', 'keys']\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:get_task_metadata] Run the method under test\n",
					"        self.task_object.get_task_metadata(variables)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has successfully set values in self.task_object.variables\n",
					"        self.assertEqual(self.task_object.variables['test'], \"value1\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'test' does not equal expectations: '{self.task_object.variables['test']}' versus 'value1'\")\n",
					"        self.assertEqual(self.task_object.variables['unittest'], \"value2\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'unittest' does not equal expectations: '{self.task_object.variables['test']}' versus 'value2'\")\n",
					"        self.assertEqual(self.task_object.variables['keys'], \"value3\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'keys' does not equal expectations: '{self.task_object.variables['test']}' versus 'value3'\")\n",
					"        \n",
					"\n",
					"    def test_gettaskmetadata_task_set_complex_variables(self):\n",
					"        '''\n",
					"        Test Logic:\n",
					"            Some key-values need to be considered as lists, not as individual values\n",
					"            This test validates that the values of list_columns (see  [Classes:Task:GetTaskMetadata]) are actually set as list-values\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"\n",
					"        # Define the list of expected values for the list-specific keys\n",
					"        # See [Classes:Task:GetTaskMetadata]: list_columns = ['source_name', 'sink_name', 'dimension', 'data_type', 'check_name', 'config_params', 'column_info']\n",
					"        expected_source_names   =  [\"value211\", \"value221\", \"value231\"]\n",
					"        expected_sink_names     =  [\"value212\", \"value222\", \"value232\"]\n",
					"        expected_dimensions     =  [\"value213\", \"value223\", \"value233\"]\n",
					"        expected_datatypes      =  [\"value214\", \"value224\", \"value234\"]\n",
					"        expected_check_names    =  [\"value215\", \"value225\", \"value235\"]\n",
					"        expected_config_params  =  [\"value216\", \"value226\", \"value236\"]\n",
					"        expected_column_info    =  [\"value217\", \"value227\", \"value237\"]\n",
					"\n",
					"\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        # Exeucte_query returns a non-empty object in an array\n",
					"        mock_sql_object.execute_query.return_value = [\n",
					"{\n",
					"                'column_1': '[{\"test1\": \"value11\", \"test2\": \"value12\"}]', \n",
					"                'column_2': '''\n",
					"                            [{\"source_name\": \"value211\", \"sink_name\": \"value212\", \"dimension\": \"value213\", \"data_type\": \"value214\", \"check_name\": \"value215\", \"config_params\": \"value216\", \"column_info\": \"value217\"}, \n",
					"                             {\"source_name\": \"value221\", \"sink_name\": \"value222\", \"dimension\": \"value223\", \"data_type\": \"value224\", \"check_name\": \"value225\", \"config_params\": \"value226\", \"column_info\": \"value227\"}, \n",
					"                             {\"source_name\": \"value231\", \"sink_name\": \"value232\", \"dimension\": \"value233\", \"data_type\": \"value234\", \"check_name\": \"value235\", \"config_params\": \"value236\", \"column_info\": \"value237\"}]\n",
					"                            ''',\n",
					"                'column_3': '[{\"keys1\": \"value31\", \"keys2\": \"value32\"}]'\n",
					"            }\n",
					"        ]\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"        # Pass the below list to get_task_metadata and expect key-value pairs to be set in the self.task_object.variables dictionary\n",
					"        # Dev-Note: the list_columns will be added automatically, even though they are not mentioned in the variables-list. This should be changed, but the current code does not allow for it\n",
					"        variables = ['test1', 'test2', 'keys1', 'keys2']\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:get_task_metadata] Run the method under test\n",
					"        self.task_object.get_task_metadata(variables)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has successfully set values in self.task_object.variables\n",
					"        self.assertEqual(self.task_object.variables['test1'], \"value11\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'test1' does not equal expectations: '{self.task_object.variables['test1']}' versus 'value11'\")\n",
					"        self.assertEqual(self.task_object.variables['keys1'], \"value31\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'keys1' does not equal expectations: '{self.task_object.variables['test1']}' versus 'value31'\")\n",
					"        self.assertEqual(self.task_object.variables['test2'], \"value12\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'test2' does not equal expectations: '{self.task_object.variables['test2']}' versus 'value12'\")\n",
					"        self.assertEqual(self.task_object.variables['keys2'], \"value32\", f\"[Classes:Task:GetTaskMetadata]: Value for key 'keys2' does not equal expectations: '{self.task_object.variables['test2']}' versus 'value32'\")\n",
					"        \n",
					"        # Validate that the execute_query has successfully set values in self.task_object.variables for the list_column keys\n",
					"        self.assertEqual(self.task_object.variables['source_name'],     expected_source_names,  f\"[Classes:Task:GetTaskMetadata]: Value for key 'source_name' does not equal expectations: '{self.task_object.variables['source_name']}' versus {expected_source_names}\")\n",
					"        self.assertEqual(self.task_object.variables['sink_name'],       expected_sink_names,    f\"[Classes:Task:GetTaskMetadata]: Value for key 'sink_names' does not equal expectations: '{self.task_object.variables['sink_name']}' versus {expected_sink_names}\")\n",
					"        self.assertEqual(self.task_object.variables['dimension'],       expected_dimensions,    f\"[Classes:Task:GetTaskMetadata]: Value for key 'dimensions' does not equal expectations: '{self.task_object.variables['dimension']}' versus {expected_dimensions}\")\n",
					"        self.assertEqual(self.task_object.variables['data_type'],       expected_datatypes,     f\"[Classes:Task:GetTaskMetadata]: Value for key 'datatypes' does not equal expectations: '{self.task_object.variables['data_type']}' versus {expected_datatypes}\")\n",
					"        self.assertEqual(self.task_object.variables['check_name'],      expected_check_names,   f\"[Classes:Task:GetTaskMetadata]: Value for key 'check_names' does not equal expectations: '{self.task_object.variables['check_name']}' versus {expected_check_names}\")\n",
					"        self.assertEqual(self.task_object.variables['config_params'],   expected_config_params, f\"[Classes:Task:GetTaskMetadata]: Value for key 'config_params' does not equal expectations: '{self.task_object.variables['config_params']}' versus {expected_config_params}\")\n",
					"        self.assertEqual(self.task_object.variables['column_info'],     expected_column_info,   f\"[Classes:Task:GetTaskMetadata]: Value for key 'column_info' does not equal expectations: '{self.task_object.variables['column_info']}' versus {expected_column_info}\")\n",
					"\n",
					"\n",
					"    # Failure: Expect execute_query from the mock-object to be called with statement 'EXEC meta.usp_get_task_metadata' and expect_return=True\n",
					"    def test_gettaskmetadata_task_return_empty_query(self):\n",
					"        '''\n",
					"        Test Logic:\n",
					"            If the execute_query returns an empty result, an error should be thrown\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        expected_error = f\"No task_metadata found for for task_id {str(self.task_id)}\"\n",
					"        # Set up the SQLServerConnection-object of the task_object\n",
					"        # Define it as a MagicMock object of the SQLServerConnection -&gt; Still be able to call execute_query\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        # Exeucte_query returns None\n",
					"        mock_sql_object.execute_query.return_value = None\n",
					"        self.task_object.SQLServerConnection = mock_sql_object\n",
					"        # Pass an empty list of variables as the test will only validate function calls\n",
					"        variables = []\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:get_task_metadata] Run the method under test\n",
					"        with self.assertRaises(UnboundLocalError) as error:\n",
					"            self.task_object.get_task_metadata(variables)  \n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:Task:GetTaskMetadata]: Error does not match expectations: {actual_error} versus {expected_error}\")\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_GetTaskMetadata)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 18
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_SetIngestionVariables(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        task_name = 'task_name'\n",
					"        self.task_object = Task(task_name, Mock(), Mock(), Mock(), Mock(), Mock())\n",
					"        self.task_object.table_name = 'unittest'\n",
					"        return\n",
					"\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the start_task method\n",
					"\n",
					"\n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_start_task'\n",
					"    def test_set_ingestion_variables_task_success(self):\n",
					"        # PREPROCESS\n",
					"        storage_account             = 'unittest'\n",
					"        expected_silver_path        =  \"abfss://silver@%s.dfs.core.windows.net/%s\" % ( storage_account, self.task_object.table_name)\n",
					"        # expected_raw_format         = 'parquet'\n",
					"        expected_skip_first_lines   = -1\n",
					"        expected_target_options     = {}\n",
					"\n",
					"        self.task_object.variables = {\n",
					"            'skip_first_lines': expected_skip_first_lines,\n",
					"            'target_options': expected_target_options\n",
					"        }\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:SetIngestionVariables] Run the method under test\n",
					"        self.task_object.set_ingestion_variables(storage_account)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for the \n",
					"        self.assertEqual(self.task_object.silver_path,      expected_silver_path,        f\"[Classes:Task:GetTaskMetadata]: Value for key 'silver_path' does not equal expectations: '{self.task_object.silver_path}' versus '{expected_silver_path}'\")\n",
					"        # self.assertEqual(self.task_object.raw_format,       expected_raw_format,         f\"[Classes:Task:GetTaskMetadata]: Value for key 'raw_format' does not equal expectations: '{self.task_object.raw_format}' versus '{expected_raw_format}'\")\n",
					"        self.assertEqual(self.task_object.skip_first_lines, expected_skip_first_lines,   f\"[Classes:Task:GetTaskMetadata]: Value for key 'skip_first_lines' does not equal expectations: '{self.task_object.skip_first_lines}' versus '{expected_skip_first_lines}'\")\n",
					"        self.assertEqual(self.task_object.target_options,   expected_target_options,     f\"[Classes:Task:GetTaskMetadata]: Value for key 'target_options' does not equal expectations: '{self.task_object.target_options}' versus '{expected_target_options}'\")\n",
					"\n",
					"\n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_start_task'\n",
					"    def test_set_ingestion_variables_task_targetoptions(self):\n",
					"        # PREPROCESS\n",
					"        storage_account             = 'unittest'\n",
					"        expected_target_options     = '{\"valid\": \"json\", \"test\": {\"regex\": \"\\\\\\\\d{8}\"}}'\n",
					"\n",
					"        self.task_object.variables = {\n",
					"            'skip_first_lines': -1,\n",
					"            'target_options': expected_target_options\n",
					"        }\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:SetIngestionVariables] Run the method under test\n",
					"        self.task_object.set_ingestion_variables(storage_account)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the execute_query has been called for the \n",
					"        self.assertEqual(self.task_object.target_options,   json.loads(expected_target_options), f\"[Classes:Task:GetTaskMetadata]: Value for key 'target_options' does not equal expectations: '{self.task_object.target_options}' versus '{expected_target_options}'\")\n",
					"        \n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_start_task'\n",
					"    def test_set_ingestion_variables_task_invalid_targetoptions(self):\n",
					"        # PREPROCESS\n",
					"        storage_account             = 'unittest'\n",
					"        expected_target_options     = 'string'\n",
					"        expected_error = f\"Given target_options cannot be converted to JSON. Validate configuration for {self.task_object.task_name}: {expected_target_options}\"\n",
					"\n",
					"        self.task_object.variables = {\n",
					"            'skip_first_lines': -1,\n",
					"            'target_options': expected_target_options\n",
					"        }\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:SetIngestionVariables] Run the method under test\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            self.task_object.set_ingestion_variables(storage_account)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:Task:GetTaskMetadata]: Error does not match expectations: {actual_error} versus {expected_error}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_SetIngestionVariables)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_SetDatasetVariables(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        task_name = 'task_name'\n",
					"        self.task_object = Task(task_name, Mock(), Mock(), Mock(), Mock(), Mock())\n",
					"        self.task_object.table_name = 'unittest'\n",
					"        return\n",
					"\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the start_task method\n",
					"\n",
					"\n",
					"    # Success: Expect all dataset_variables to be set after running set_dataset_variables()\n",
					"    def test_set_dataset_variables_task_success(self):\n",
					"        # PREPROCESS\n",
					"        # Set all the expected values\n",
					"        storage_account                     = 'unittest'\n",
					"        expected_table_name                 = 'test_table'\n",
					"        expected_container_name             = 'test_container'\n",
					"        expected_file_path                  = 'test_file/test_folder/'\n",
					"        expected_header                     = True\n",
					"        expected_sink_column_names          = ['test', 'sink']\n",
					"        expected_source_column_names        = ['test', 'sink']\n",
					"        expected_column_datatypes           = ['test', 'sink']\n",
					"        expected_dimensions                 = ['test', 'sink']\n",
					"        expected_column_information         = [{\"test\": \"dict\"}, {\"key\": \"value\"}, 'null']\n",
					"        expected_separator                  = 'test_sep'\n",
					"        expected_file_extension             = 'test_file_extension'\n",
					"        expected_file_kind                  = 'test_file_kind'\n",
					"        expected_landing_path               = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (expected_container_name, storage_account, expected_file_path)\n",
					"        expected_file_pattern               = 'test_file_pattern'\n",
					"        expected_quote_character            = '\"'\n",
					"        expected_escape_character           = '\\\\'\n",
					"\n",
					"        # Create task_object-member variables: Dictionary with key:value pairs of dataset metadata\n",
					"        self.task_object.variables = {\n",
					"            'table_name':           expected_table_name,\n",
					"            'container_name':       expected_container_name,\n",
					"            'source_folder':        expected_file_path,\n",
					"            'header':               expected_header,\n",
					"            'sink_name':            expected_sink_column_names,\n",
					"            'source_name':          expected_source_column_names,\n",
					"            'data_type':            expected_column_datatypes,\n",
					"            'dimension':            expected_dimensions,\n",
					"            'column_info':          ['{\"test\": \"dict\"}', '{\"key\": \"value\"}', 'null'],\n",
					"            'column_delimiter':     expected_separator,\n",
					"            'file_extension':       expected_file_extension,\n",
					"            'file_kind':            expected_file_kind,\n",
					"            'file_pattern':         expected_file_pattern,\n",
					"            'quote_character':      expected_quote_character,\n",
					"            'escape_character':     expected_escape_character\n",
					"        }\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:SetDatasetVariables] Run the method under test\n",
					"        self.task_object.set_dataset_variables(storage_account)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that all members have been set correctly\n",
					"        self.assertEqual(self.task_object.table_name,           expected_table_name,            f\"[Classes:Task:GetTaskMetadata]: Value for key 'table_name' does not equal expectations: '{self.task_object.table_name}' versus '{expected_table_name}'\")\n",
					"        self.assertEqual(self.task_object.container_name,       expected_container_name,        f\"[Classes:Task:GetTaskMetadata]: Value for key 'container_name' does not equal expectations: '{self.task_object.container_name}' versus '{expected_container_name}'\")\n",
					"        self.assertEqual(self.task_object.file_path,            expected_file_path,             f\"[Classes:Task:GetTaskMetadata]: Value for key 'file_path' does not equal expectations: '{self.task_object.file_path}' versus '{expected_file_path}'\")\n",
					"        self.assertEqual(self.task_object.header,               expected_header,                f\"[Classes:Task:GetTaskMetadata]: Value for key 'header' does not equal expectations: '{self.task_object.header}' versus '{expected_header}'\")\n",
					"        self.assertEqual(self.task_object.sink_column_names,    expected_sink_column_names,     f\"[Classes:Task:GetTaskMetadata]: Value for key 'sink_column_names' does not equal expectations: '{self.task_object.sink_column_names}' versus '{expected_sink_column_names}'\")\n",
					"        self.assertEqual(self.task_object.source_column_names,  expected_source_column_names,   f\"[Classes:Task:GetTaskMetadata]: Value for key 'source_column_names' does not equal expectations: '{self.task_object.source_column_names}' versus '{expected_source_column_names}'\")\n",
					"        self.assertEqual(self.task_object.column_datatypes,     expected_column_datatypes,      f\"[Classes:Task:GetTaskMetadata]: Value for key 'column_datatypes' does not equal expectations: '{self.task_object.column_datatypes}' versus '{expected_column_datatypes}'\")\n",
					"        self.assertEqual(self.task_object.dimensions,           expected_dimensions,            f\"[Classes:Task:GetTaskMetadata]: Value for key 'dimensions' does not equal expectations: '{self.task_object.dimensions}' versus '{expected_dimensions}'\")\n",
					"        self.assertEqual(self.task_object.separator,            expected_separator,             f\"[Classes:Task:GetTaskMetadata]: Value for key 'separator' does not equal expectations: '{self.task_object.separator}' versus '{expected_separator}'\")\n",
					"        self.assertEqual(self.task_object.file_extension,       expected_file_extension,        f\"[Classes:Task:GetTaskMetadata]: Value for key 'file_extension' does not equal expectations: '{self.task_object.file_extension}' versus '{expected_file_extension}'\")\n",
					"        self.assertEqual(self.task_object.file_kind,            expected_file_kind,             f\"[Classes:Task:GetTaskMetadata]: Value for key 'file_kind' does not equal expectations: '{self.task_object.file_kind}' versus '{expected_file_kind}'\")\n",
					"        self.assertEqual(self.task_object.landing_path,         expected_landing_path,          f\"[Classes:Task:GetTaskMetadata]: Value for key 'landing_path' does not equal expectations: '{self.task_object.landing_path}' versus '{expected_landing_path}'\")\n",
					"        self.assertEqual(self.task_object.file_pattern,         expected_file_pattern,          f\"[Classes:Task:GetTaskMetadata]: Value for key 'file_pattern' does not equal expectations: '{self.task_object.file_pattern}' versus '{expected_file_pattern}'\")\n",
					"        self.assertEqual(self.task_object.escape_character,     expected_escape_character,      f\"[Classes:Task:GetTaskMetadata]: Value for key 'escape_character' does not equal expectations: '{self.task_object.escape_character}' versus '{expected_escape_character}'\")\n",
					"        self.assertEqual(self.task_object.quote_character,      expected_quote_character,       f\"[Classes:Task:GetTaskMetadata]: Value for key 'quote_character' does not equal expectations: '{self.task_object.quote_character}' versus '{expected_quote_character}'\")\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_SetDatasetVariables)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Task_GetFilesObjects(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        # Set-up an object-instance of the Task-class to use over the test-methods\n",
					"        self.task_object = Task(Mock(), Mock(), Mock(), Mock(), Mock(), Mock())\n",
					"        self.task_object.landing_path = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/Task/get_file_objects/'\n",
					"\n",
					"\n",
					"        self.landing_path_1 = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/Task/get_file_objects/test_file_1'\n",
					"        self.landing_path_2 = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/Task/get_file_objects/test_file_2'\n",
					"\n",
					"        dataframe_1 = spark.createDataFrame(\n",
					"            data=[],\n",
					"            schema=\"test INT, schema STRING\"\n",
					"        )\n",
					"\n",
					"        dataframe_1.repartition(1).write.mode('overwrite').csv(self.landing_path_1)\n",
					"        dataframe_1.repartition(1).write.mode('append').csv(self.landing_path_1)\n",
					"        dataframe_1.repartition(1).write.mode('overwrite').csv(self.landing_path_2)\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the start_task method\n",
					"\n",
					"\n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_start_task'\n",
					"    def test_get_files_objects_task_success(self):\n",
					"        # PREPROCESS\n",
					"        self.task_object.file_pattern = 'part'\n",
					"        self.task_object.file_extension = None\n",
					"\n",
					"        expected_calls = [\n",
					"            call(\n",
					"                task=self.task_object, \n",
					"                SQLServerConnection=self.task_object.SQLServerConnection, \n",
					"                landing_path=ANY,\n",
					"                filename= ANY, \n",
					"                timestamp_folder='test_file_1', \n",
					"                storage_account='unittest', \n",
					"                debug=False\n",
					"            ),\n",
					"            call(\n",
					"                task=self.task_object, \n",
					"                SQLServerConnection=self.task_object.SQLServerConnection, \n",
					"                landing_path=ANY,\n",
					"                filename= ANY, \n",
					"                timestamp_folder='test_file_1', \n",
					"                storage_account='unittest', \n",
					"                debug=False\n",
					"            ),\n",
					"            call(\n",
					"                task=self.task_object, \n",
					"                SQLServerConnection=self.task_object.SQLServerConnection, \n",
					"                landing_path=ANY,\n",
					"                filename= ANY, \n",
					"                timestamp_folder='test_file_2', \n",
					"                storage_account='unittest', \n",
					"                debug=False\n",
					"            )\n",
					"        ]\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:GetFilesObjects] Run the method under test\n",
					"        with patch(__name__ + '.File') as mock_file_class:\n",
					"            mock_file_class.return_value = -1\n",
					"            file_objects=self.task_object.get_files_objects('unittest')\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the File-object has been called 3 times, once for each of the dataframes\n",
					"        self.assertEqual(mock_file_class.call_count, 3)\n",
					"        mock_file_class.assert_has_calls(expected_calls, any_order=True)\n",
					"\n",
					"\n",
					"    # Success: Excpet execute_query from the mock-object to be called with statement 'EXEC meta.usp_start_task'\n",
					"    def test_get_files_objects_task_success(self):\n",
					"        # PREPROCESS\n",
					"        self.task_object.file_pattern = 'unfound_pattern'\n",
					"        self.task_object.file_extension = None\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:Task:GetFilesObjects] Run the method under test\n",
					"        with patch(f'{__name__}.File') as mock_file_class, \\\n",
					"            patch(f'{__name__}.list_directory_content') as mock_list_directory_content, \\\n",
					"            patch(f'{__name__}.filter_list') as mock_filter_list:\n",
					"\n",
					"            mock_list_directory_content.return_value = ['invalid_filename'], list()\n",
					"            mock_filter_list.return_value = list()\n",
					"\n",
					"            file_objects=self.task_object.get_files_objects('unittest')\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the mocked functions have been called with the expected arguments\n",
					"        mock_list_directory_content.assert_called_once_with(self.task_object.landing_path, list(), list())\n",
					"        mock_filter_list.assert_called_once_with(full_list=['invalid_filename'], pattern=self.task_object.file_pattern, extension=None)\n",
					"        # Since the filter will render an empty list for found_files, the File-object class should not be called\n",
					"        mock_file_class.assert_not_called()\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Task_GetFilesObjects)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: File\n",
					"# This class will test the __init__() of the File-class defined in the Classes notebook\n",
					"class Test_File_Initialisation(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing       \n",
					"        self.task = Task(task_name= Mock(), task_id= Mock(), plan_id=Mock(), SQLServerConnection = Mock(), worker_name=Mock())\n",
					"\n",
					"        self.task.column_datatypes = ['string', 'int']\n",
					"        self.task.source_column_names = ['UNIT', 'TEST']\n",
					"        self.task.sink_column_names = ['unit', 'test']\n",
					"        self.task.table_name = ['unit_test']   \n",
					"        self.task.header = Mock()\n",
					"        self.task.variables = {\"check_name\": [], \"config_params\": {}}\n",
					"        self.task.file_extension = 'csv'\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_access_token method\n",
					"\n",
					"    # Success: Test a successful initialisation of a new file-object\n",
					"    def test_init_success(self):\n",
					"        # PREPROCESS\n",
					"        expected_task_object            = self.task\n",
					"        expected_task_object.table_name = 'unittest_silver'\n",
					"\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        expected_sql_server_object      = mock_sql_object\n",
					"\n",
					"        expected_file_name              = 'unittest.csv'\n",
					"        expected_storage_account        = 'storage_account'\n",
					"        expected_landing_path           = f'abfss-path://landing@{expected_storage_account}.dfs.core.windows.net/{expected_file_name}'\n",
					"        expected_timestamp              = 'timestamp'\n",
					"        expected_raw_path               = f'abfss-path://raw@{expected_storage_account}.dfs.core.windows.net/unittest'\n",
					"        expected_silver_path            = f'abfss://silver@{expected_storage_account}.dfs.core.windows.net/{expected_task_object.table_name}'\n",
					"        expected_archive_path           = f'abfss-path://archive@{expected_storage_account}.dfs.core.windows.net/{expected_file_name}'\n",
					"        expected_extended_filename      = expected_file_name + '_' + expected_timestamp\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Initialise a File-object from the File-class\n",
					"        with patch.object(File, 'log_file') as mock_log_file:\n",
					"            file_object = File(expected_task_object, expected_sql_server_object, expected_landing_path, expected_file_name, expected_timestamp,  expected_storage_account)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the correct functions have been called with the expected set of arguments\n",
					"        mock_log_file.assert_called_once()\n",
					"\n",
					"        # Validate that a instance of the Checks class has been created\n",
					"\n",
					"        # Validate that the member-values have been set according to expectations\n",
					"        self.assertEqual(expected_sql_server_object,  file_object.SQLServerConnection,  f'[Classes:File:Init] Initialisation of member \"SQLServerConnection\" does not match expectations: {expected_sql_server_object} versus {file_object.SQLServerConnection}')                        \n",
					"        self.assertEqual(expected_file_name,          file_object.file_name,            f'[Classes:File:Init] Initialisation of member \"file_name\" does not match expectations: {expected_file_name} versus {file_object.file_name}')\n",
					"        self.assertEqual(expected_storage_account,    file_object.storage_account,      f'[Classes:File:Init] Initialisation of member \"storage_account\" does not match expectations: {expected_storage_account} versus {file_object.storage_account}')    \n",
					"        self.assertEqual(expected_landing_path,       file_object.landing_path,         f'[Classes:File:Init] Initialisation of member \"landing_path\" does not match expectations: {expected_landing_path} versus {file_object.landing_path}')\n",
					"        self.assertEqual(expected_extended_filename,  file_object.extended_filename,    f'[Classes:File:Init] Initialisation of member \"extended_filename\" does not match expectations: {expected_extended_filename} versus {file_object.extended_filename}')   \n",
					"        self.assertEqual(expected_raw_path,           file_object.raw_path,             f'[Classes:File:Init] Initialisation of member \"raw_path\" does not match expectations: {expected_raw_path} versus {file_object.raw_path}')\n",
					"        self.assertEqual(expected_silver_path,        file_object.silver_path,          f'[Classes:File:Init] Initialisation of member \"silver_path\" does not match expectations: {expected_silver_path} versus {file_object.silver_path}')\n",
					"        self.assertEqual(expected_archive_path,       file_object.archive_path,         f'[Classes:File:Init] Initialisation of member \"archive_path\" does not match expectations: {expected_archive_path} versus {file_object.archive_path}')\n",
					"\n",
					"\n",
					"    def test_init_query_execution(self):\n",
					"        # PREPROCESS\n",
					"        expected_task_object            = self.task\n",
					"        expected_task_object.table_name = 'unittest_silver'\n",
					"        expected_task_object.task_id    = -1\n",
					"        expected_task_object.plan_id    = -1\n",
					"\n",
					"        mock_sql_object = MagicMock(spec=SQLServerConnection)\n",
					"        expected_sql_server_object      = mock_sql_object\n",
					"        \n",
					"        expected_file_name              = 'unittest.csv'\n",
					"        expected_storage_account        = 'storage_account'\n",
					"        expected_landing_path           = f'abfss-path://landing@{expected_storage_account}.dfs.core.windows.net/{expected_file_name}'\n",
					"        expected_timestamp              = 'timestamp'\n",
					"        expected_extended_filename      = expected_file_name + '_' + expected_timestamp\n",
					"\n",
					"\n",
					"        \n",
					"        with patch.object(mock_sql_object, 'execute_query') as mock_execute_query:\n",
					"            mock_execute_query.return_value = [{\"file_id\": -2}]\n",
					"            file_object = File(expected_task_object, expected_sql_server_object, expected_landing_path, expected_file_name, expected_timestamp,  expected_storage_account)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that execute_query has been called with the expected statement\n",
					"        mock_execute_query.assert_called_once_with(f\"Exec meta.usp_new_file @filename='{expected_file_name}', @task_id={expected_task_object.task_id}, @plan_id ={expected_task_object.plan_id}, @extended_filename = '{expected_extended_filename}', @info_message = '{expected_landing_path}', @no_select = 0\", expect_return=True)\n",
					"        # Validate that log_files has set the expected file_id member value\n",
					"        self.assertEqual(-2, file_object.file_id, f'[Classes:File:Init] Initialisation of member \"file_id\" does not match expectations: {-2} versus {file_object.file_id}')\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_File_Initialisation)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: File\n",
					"# This class will test the __init__() of the File-class defined in the Classes notebook\n",
					"class Test_File_LogFile(unittest.TestCase):\n",
					"# ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing\n",
					"        \n",
					"        self.mock_file_class_patcher = patch.object(File, '__init__', return_value=None)\n",
					"        self.mock_file_class = self.mock_file_class_patcher.start()\n",
					"\n",
					"        self.file_object = File()\n",
					"        self.file_object.file_name = Mock()\n",
					"        self.file_object.debug = False\n",
					"        self.file_object.task = Mock()\n",
					"        self.file_object.task.plan_id = -98\n",
					"        self.file_object.task.task_id = -97\n",
					"        self.file_object.extended_filename = \"extended_filename\"\n",
					"        self.file_object.landing_path = Mock()\n",
					"\n",
					"        self.file_object.SQLServerConnection = Mock()\n",
					"        \n",
					"        self.mock_SQLServerConnection_execute_query_patcher = patch.object(self.file_object.SQLServerConnection, 'execute_query')\n",
					"        self.mock_SQLServerConnection_execute_query = self.mock_SQLServerConnection_execute_query_patcher.start()\n",
					"        \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        self.mock_file_class_patcher.stop()\n",
					"        self.mock_SQLServerConnection_execute_query_patcher.stop()\n",
					"        # tearDown patched functions\n",
					"        return\n",
					"\n",
					"        \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_access_token method\n",
					"\n",
					"\n",
					"    # Success: Stored procedure call has returned a file_id argument\n",
					"    def test_log_file_success(self):\n",
					"        # PREPROCESS\n",
					"        # Set the expected results and return_value for the mocked functions\n",
					"        expected_file_id = -99\n",
					"        self.mock_SQLServerConnection_execute_query.return_value = [{'file_id': expected_file_id}]\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function/method to test\n",
					"        self.file_object.log_file()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the file_id has been set as expected\n",
					"        actual_file_id = self.file_object.file_id\n",
					"        self.assertEqual(expected_file_id, actual_file_id, f\"Actual file id does not match with expectation: {actual_file_id} versus {expected_file_id}\")\n",
					"\n",
					"        # Validate that the correct functions have been called with the expected arguments\n",
					"        self.mock_SQLServerConnection_execute_query.assert_called_once_with(\n",
					"            f\"\"\"Exec meta.usp_new_file @filename='{self.file_object.file_name}', @task_id={self.file_object.task.task_id}, @plan_id ={self.file_object.task.plan_id}, @extended_filename = '{self.file_object.extended_filename}', @info_message = '{self.file_object.landing_path}', @no_select = 0\"\"\", \n",
					"            expect_return=True\n",
					"        )\n",
					"\n",
					"\n",
					"    # Failure: Stored procedure call did not return any result\n",
					"    def test_log_file_noResult(self):\n",
					"        # PREPROCESS\n",
					"        # Set the expected results and return_value for the mocked functions\n",
					"        self.mock_SQLServerConnection_execute_query.return_value = None\n",
					"        expected_exception = f\"No file_id returned when logging file {self.file_object.extended_filename} for plan_id ({str(self.file_object.task.plan_id)}) and task_id ({str(self.file_object.task.task_id)})\"\n",
					"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the function/method to test and expect an error to be thrown\n",
					"        with self.assertRaises(UnboundLocalError) as error:\n",
					"            self.file_object.log_file()\n",
					"        \n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the raised error matches the expected error\n",
					"        actual_exception = str(error.exception)\n",
					"        self.assertEqual(expected_exception, actual_exception, f\"The actual exception for the log_file function does not match with with the expected exception: {actual_exception} versus {expected_exception}\")\n",
					"\n",
					"    # Failure: Stored procedure call did not return a file_id argument\n",
					"    def test_log_file_noFileID(self):\n",
					"        # PREPROCESS\n",
					"        # Set the expected results and return_value for the mocked functions\n",
					"        self.mock_SQLServerConnection_execute_query.return_value = [{'empty': 'dictionary'}]\n",
					"        expected_exception = f\"There has been no file_id set for file {self.file_object.extended_filename} for plan_id ({str(self.file_object.task.plan_id)}) and task_id ({str(self.file_object.task.task_id)})\"\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function/method to test and expect an error to be thrown\n",
					"        with self.assertRaises(AttributeError) as error:\n",
					"            self.file_object.log_file()\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the raised error matches the expected error\n",
					"        actual_exception = str(error.exception)\n",
					"        self.assertEqual(expected_exception, actual_exception,f\"The actual exception for the log_file function does not match with with the expected exception: {actual_exception} versus {expected_exception}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_File_LogFile)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: File\n",
					"# This class will test the landing_to_raw() of the File-class defined in the Classes notebook\n",
					"class Test_File_LandingToRaw(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing \n",
					"        self.task_id = -1\n",
					"        self.plan_id = -1\n",
					"        self.task = Task(task_name='landig_to_raw', task_id= self.task_id, plan_id=self.task_id, SQLServerConnection = Mock(), worker_name=Mock())\n",
					"\n",
					"        self.task.column_datatypes      = ['string', 'int']\n",
					"        self.task.source_column_names   = ['UNIT', 'TEST']\n",
					"        self.task.sink_column_names     = ['unit', 'test']\n",
					"        self.task.column_information    = [None, None]\n",
					"        self.task.table_name            = ['unit_test']   \n",
					"        self.task.header                = True\n",
					"        self.task.variables             = {\"check_name\": [], \"config_params\": {}}\n",
					"        self.task.skip_first_lines      = None\n",
					"        self.task.separator             = ','\n",
					"        self.task.file_kind             = 'csv'\n",
					"        self.task.quote_character       = '\"'\n",
					"        self.task.escape_character      = '\\\\'\n",
					"        self.task.file_extension        = 'csv'\n",
					"        \n",
					"\n",
					"        # [Classes:SQLServerConnection] Mock the SQLServerConnection: Make sure no queries are executed\n",
					"        self.mock_sql_object            = MagicMock(spec=SQLServerConnection)\n",
					"        self.sql_server_object          = self.mock_sql_object\n",
					"\n",
					"        # [Classes:File] Make sure the log_file does not execute as it will throw an error -&gt; Cannot make a SQLServerConnection\n",
					"        self.log_file_patcher           = patch.object(File, 'log_file')\n",
					"        self.mock_log_file              = self.log_file_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        self.log_file_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the landing_to_raw method\n",
					"\n",
					"    # Success: Test a successful LandingToRaw of a new file-object\n",
					"    def test_landingtoraw_file_success(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                = self.task\n",
					"        task_object.table_name     = 'unittest_silver'\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                  = 'MOCK_DATA.csv'\n",
					"        storage_account            = f'{env_code}dapstdala1'\n",
					"        file_path                  = 'Classes/FileMovement/landing/move_to_raw'\n",
					"        container_name             = 'unittest'\n",
					"        landing_path               = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                  = 'timestamp'\n",
					"\n",
					"        file_object                = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        file_object.file_id        = -1\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:LandingToRaw]: Run the method under test\n",
					"        with patch.object(self.mock_sql_object, 'execute_query') as mock_execute_query:\n",
					"            file_object.landing_to_raw()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Get the list of files from the raw_filepath and validate that the expected files have been landed there\n",
					"        raw_filepath = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/Classes/FileMovement/raw/move_to_raw/MOCK_DATA'\n",
					"        actual_raw_files, actual_raw_folders = list_directory_content(raw_filepath, list(), list())\n",
					"        actual_raw_file_names = [file.name for file in actual_raw_files]\n",
					"        self.assertIn(\"_SUCCESS\", actual_raw_file_names, \"[Classes:File:LandingToRaw] Succes-file was not found on raw\")\n",
					"        self.assertTrue(len(actual_raw_file_names) == 2, f\"[Classes:File:LandingToRaw] The files found on raw are {actual_raw_file_names}, which is not equal to the 2 files that were expected.\")\n",
					"\n",
					"\n",
					"    # Success: Validate that all the expected function calls have been made\n",
					"    def test_landingtoraw_file_function_calls(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.table_name          = 'unittest_silver'\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'MOCK_DATA.csv'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/landing/move_to_raw'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        file_object.file_id        = -1\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:LandingToRaw]: Run the method under test\n",
					"        with patch.object(self.mock_sql_object, 'execute_query') as mock_execute_query, \\\n",
					"            patch.object(SparkDataFrameChecks, 'start_checks') as mock_start_checks, \\\n",
					"            patch.object(SparkDataFrameClass, 'add_logging_columns_to_dataframe') as mock_add_logging_columns_to_dataframe:\n",
					"            file_object.landing_to_raw()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Get the list of files from the raw_filepath and validate that the expected files have been landed there\n",
					"        raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/Classes/FileMovement/raw/move_to_raw/MOCK_DATA'\n",
					"        info=create_json_object(path=raw_path,raw_amount=20)\n",
					"        mock_execute_query.assert_called_once_with(f\"Exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=RAW, @success=True, @info_message = '{info}'\")\n",
					"        mock_start_checks.assert_called_once_with(phase='landing', parameters={})\n",
					"        mock_add_logging_columns_to_dataframe.assert_called_once_with(task_id=task_object.task_id, plan_id=task_object.plan_id, file_id=file_object.file_id, file_name=file_object.file_name)\n",
					"\n",
					"\n",
					"    # Success: Validate that all the expected function calls have been made\n",
					"    def test_landingtoraw_file_skipline(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.table_name          = 'unittest_silver'\n",
					"        task_object.skip_first_lines    = 1\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'MOCK_SKIPLINES_move.csv'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/landing/move_to_raw'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        file_object.file_id        = -1\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:LandingToRaw]: Run the method under test\n",
					"        with patch.object(self.mock_sql_object, 'execute_query') as mock_execute_query, \\\n",
					"            patch.object(SparkDataFrameChecks, 'start_checks') as mock_start_checks, \\\n",
					"            patch.object(SparkDataFrameClass, 'add_logging_columns_to_dataframe') as mock_add_logging_columns_to_dataframe:\n",
					"            file_object.landing_to_raw()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Get the list of files from the raw_filepath and validate that the expected files have been landed there\n",
					"        raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/Classes/FileMovement/raw/move_to_raw/MOCK_SKIPLINES_move'\n",
					"        info=create_json_object(path=raw_path,raw_amount=6)\n",
					"        mock_execute_query.assert_called_once_with(f\"Exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=RAW, @success=True, @info_message = '{info}'\")\n",
					"        mock_start_checks.assert_called_once_with(phase='landing', parameters={})\n",
					"        mock_add_logging_columns_to_dataframe.assert_called_once_with(task_id=task_object.task_id, plan_id=task_object.plan_id, file_id=file_object.file_id,file_name=file_object.file_name)\n",
					"\n",
					"\n",
					"\n",
					"    # Success: Validate that all the expected function calls have been made\n",
					"    def test_landingtoraw_file_skipline_functioncalls(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.table_name          = 'unittest_silver'\n",
					"        task_object.skip_first_lines    = 1\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'MOCK_SKIPLINES_move.csv'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/landing/move_to_raw'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        file_object.file_id        = -1\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:LandingToRaw]: Run the method under test\n",
					"        with patch.object(self.mock_sql_object, 'execute_query') as mock_execute_query, \\\n",
					"            patch.object(SparkDataFrameChecks, 'start_checks') as mock_start_checks, \\\n",
					"            patch.object(SparkDataFrameClass, 'add_logging_columns_to_dataframe') as mock_add_logging_columns_to_dataframe:\n",
					"            file_object.landing_to_raw()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Get the list of files from the raw_filepath and validate that the expected files have been landed there\n",
					"        raw_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/Classes/FileMovement/raw/move_to_raw/MOCK_SKIPLINES_move'\n",
					"        info=create_json_object(path=raw_path,raw_amount=6)\n",
					"        mock_execute_query.assert_called_once_with(f\"Exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=RAW, @success=True, @info_message = '{info}'\")\n",
					"        mock_start_checks.assert_called_once_with(phase='landing', parameters={})\n",
					"        mock_add_logging_columns_to_dataframe.assert_called_once_with(task_id=task_object.task_id, plan_id=task_object.plan_id, file_id=file_object.file_id, file_name=file_object.file_name)\n",
					"\n",
					"    # Success: Validate that all the expected function calls have been made\n",
					"    def test_landingtoraw_file_jsonmultiline(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.table_name          = 'unittest_silver'\n",
					"        task_object.file_extension      = 'json'\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'MOCK_MULTILINE_move.json'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/landing/move_to_raw'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account, True)\n",
					"        file_object.file_id        = -1\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:LandingToRaw]: Run the method under test\n",
					"        with patch.object(self.mock_sql_object, 'execute_query') as mock_execute_query: \n",
					"            file_object.landing_to_raw()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Get the list of files from the raw_filepath and validate that the expected files have been landed there\n",
					"        raw_filepath = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/Classes/FileMovement/raw/move_to_raw/MOCK_MULTILINE_move'\n",
					"        actual_raw_files, actual_raw_folders = list_directory_content(raw_filepath, list(), list())\n",
					"        actual_raw_file_names = [file.name for file in actual_raw_files]\n",
					"        self.assertIn(\"_SUCCESS\", actual_raw_file_names, \"[Classes:File:LandingToRaw] Succes-file was not found on raw for MOCK_MULTILINE_move\")\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_File_LandingToRaw)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: File\n",
					"# This class will test the raw_to_silver() of the File-class defined in the Classes notebook\n",
					"class Test_File_RawToSilver(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing \n",
					"        self.task_id = -1\n",
					"        self.plan_id = -1\n",
					"        self.task = Task(task_name='landig_to_raw', task_id= self.task_id, plan_id=self.task_id, SQLServerConnection = Mock(), worker_name=Mock())\n",
					"\n",
					"        self.task.column_datatypes      = [\"string\",\"string\", \"string\", \"string\", \"string\"]\n",
					"        self.task.source_column_names   = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n",
					"        self.task.sink_column_names     = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n",
					"        self.task.dimensions             = ['SCD2', 'PK','PK','SCD2','SCD2',]\n",
					"        self.task.column_information    = [None, None, None, None, None]\n",
					"        self.task.table_name            = ['unit_test']   \n",
					"        self.task.header                = True\n",
					"        self.task.variables             = {\"check_name\": [], \"config_params\": {}}\n",
					"        self.task.separator             = ';'\n",
					"        self.task.file_kind             = 'parquet'\n",
					"        self.task.quote_character       = '\"'\n",
					"        self.task.escape_character      = '\\\\'   \n",
					"        self.task.file_extension        = None\n",
					"\n",
					"        # [Classes:SQLServerConnection] Mock the SQLServerConnection: Make sure no queries are executed\n",
					"        self.mock_sql_object            = MagicMock(spec=SQLServerConnection)\n",
					"        self.sql_server_object          = self.mock_sql_object\n",
					"\n",
					"        # [Classes:File] Make sure the log_file does not execute as it will throw an error -&gt; Cannot make a SQLServerConnection\n",
					"        self.log_file_patcher           = patch.object(File, 'log_file')\n",
					"        self.mock_log_file              = self.log_file_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        self.log_file_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the raw_to_silver method\n",
					"\n",
					"\n",
					"    def test_rawtosilver_file_success(self):\n",
					"        '''\n",
					"        Test Logic:\n",
					"            Create an empty delta table\n",
					"            Move a parquet file from raw to silver\n",
					"            Validate that the contents of the file have been ingested in the silver table by doing a row_count\n",
					"        '''\n",
					"\n",
					"        # PREPROCESS\n",
					"        # Set expected values\n",
					"        expected_rowcount = 20\n",
					"\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.target_options      = {}\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'part-00000-ded9c119-09f3-4c0a-9415-3536ded9342f-c000.snappy.parquet'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/raw/move_to_silver'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                     = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        \n",
					"        # Overwrite file_object members with test-values \n",
					"        file_object.file_id             = -1\n",
					"        file_object.raw_path            = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        file_object.silver_path         = f\"abfss://silver@{storage_account}.dfs.core.windows.net/mocking_table\"\n",
					"\n",
					"\n",
					"        ## Create an empty delta table (with technical fields) to upload the rawData into\n",
					"        df = spark.createDataFrame(data=[], schema=dt.StructType([\n",
					"                dt.StructField(\"_c0\",                 sql.types.StringType()),\n",
					"                dt.StructField(\"first_name\",          sql.types.StringType()),\n",
					"                dt.StructField(\"last_name\",           sql.types.StringType()),\n",
					"                dt.StructField(\"age\",                 sql.types.StringType()),\n",
					"                dt.StructField(\"email\",               sql.types.StringType()),\n",
					"                dt.StructField(\"t_load_date_raw\",     sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_load_date_silver\",  sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_extract_date\",      sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_update_date\",       sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_insert_date\",       sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_plan_id\",           sql.types.IntegerType()),\n",
					"                dt.StructField(\"t_task_id\",           sql.types.IntegerType()),\n",
					"                dt.StructField(\"t_file_id\",           sql.types.IntegerType()),\n",
					"                dt.StructField(\"t_file_name\",         sql.types.StringType())\n",
					"            ])\n",
					"        )\n",
					"        df.write.format(\"delta\").mode('overwrite').save(file_object.silver_path)\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:RawToSilver] Run the method under test\n",
					"        file_object.raw_to_silver()\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the delta table no longer is empty\n",
					"        df = dt.DeltaTable.forPath(spark, file_object.silver_path)\n",
					"        actual_rowcount = df.toDF().count()\n",
					"\n",
					"        self.assertEqual(expected_rowcount, actual_rowcount, f'[Classes:File:RawToSilver]: Expected row count does not match actuals: {expected_rowcount} versus {actual_rowcount}')\n",
					"\n",
					"\n",
					"    def test_rawtosilver_file_function_calls(self):\n",
					"        '''\n",
					"        Test Logic: Validate the function calls being made by the method under test\n",
					"            - update_file_activity should be called, which in turn should call [Classes:SQLServerConnection] execute_query\n",
					"            - MergeFile should be called\n",
					"            - [Classes:Checks] Data quailty methods check_dataframe_headers() and check_dataframe_datatypes() should both be called whenever files are moving from raw to silver\n",
					"        '''\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.target_options      = {}\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'part-00000-ded9c119-09f3-4c0a-9415-3536ded9342f-c000.snappy.parquet'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/raw/move_to_silver'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                     = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        \n",
					"        # Overwrite file_object members with test-values \n",
					"        file_object.file_id             = -1\n",
					"        file_object.raw_path            = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        file_object.silver_path         = f\"abfss://silver@{storage_account}.dfs.core.windows.net/mocking_table\"\n",
					"\n",
					"\n",
					"        ## Create an empty delta table (with technical fields) to upload the rawData into\n",
					"        df = spark.createDataFrame(data=[], schema=dt.StructType([\n",
					"                dt.StructField(\"_c0\",                 sql.types.StringType()),\n",
					"                dt.StructField(\"first_name\",          sql.types.StringType()),\n",
					"                dt.StructField(\"last_name\",           sql.types.StringType()),\n",
					"                dt.StructField(\"age\",                 sql.types.StringType()),\n",
					"                dt.StructField(\"email\",               sql.types.StringType()),\n",
					"                dt.StructField(\"t_load_date_raw\",     sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_load_date_silver\",  sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_extract_date\",      sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_update_date\",       sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_insert_date\",       sql.types.TimestampType()),\n",
					"                dt.StructField(\"t_plan_id\",           sql.types.IntegerType()),\n",
					"                dt.StructField(\"t_task_id\",           sql.types.IntegerType()),\n",
					"                dt.StructField(\"t_file_id\",           sql.types.IntegerType()),\n",
					"                dt.StructField(\"t_file_name\",         sql.types.StringType())\n",
					"            ])\n",
					"        )\n",
					"        df.write.format(\"delta\").mode('overwrite').save(file_object.silver_path)\n",
					"\n",
					"        expected_checks_parameters = {'primary_key_columns': ['first_name', 'last_name']}\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Classes:File:RawToSilver] Run the method under test, mocking all/most of the function calls made\n",
					"        with patch.object(self.mock_sql_object, 'execute_query') as mock_execute_query, \\\n",
					"            patch.object(SparkDataFrameChecks, 'check_dataframe_headers') as mock_headers_checks, \\\n",
					"            patch.object(SparkDataFrameChecks, 'check_dataframe_datatypes') as mock_casting_check, \\\n",
					"            patch.object(SparkDataFrameChecks, 'check_primary_keys') as mock_primary_keys, \\\n",
					"            patch('__main__.MergeFile') as mock_mergefile: \n",
					"            # [Classes:File:RawToSilver] Run the method under test\n",
					"            file_object.raw_to_silver()\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the mocked functions have been called with the expected arguments\n",
					"        ## [Classes:Checks] Expect the header-checks and datatypes-checks to always be called during the raw_to_silver stage as they are mandatory\n",
					"        info=create_json_object(path=file_object.silver_path,row_inserted=0,row_updated=0,source_row=0)\n",
					"        mock_headers_checks.assert_called_once()\n",
					"        mock_casting_check.assert_called_once()\n",
					"        mock_primary_keys.assert_called_once()\n",
					"        ## [IngestionFunctions] Expect the MergeFile-function to be called\n",
					"        mock_mergefile.assert_called_once()\n",
					"        ## [Classes:SQLServerConnection] Expect the execute_query to be called to log file updates\n",
					"        mock_execute_query.assert_called_once_with(f\"Exec meta.usp_update_file_activity @extended_filename='{file_object.extended_filename}', @activity=SILVER, @success=True, @info_message = '{info}'\")\n",
					"\n",
					"    \n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_File_RawToSilver)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: File\n",
					"# This class will test the archive_to_silver() of the File-class defined in the Classes notebook\n",
					"class Test_File_LandingToArchive(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use later for testing \n",
					"        self.task_id = -1\n",
					"        self.plan_id = -1\n",
					"        self.task = Task(task_name='landig_to_archive', task_id= self.task_id, plan_id=self.task_id, SQLServerConnection = Mock(), worker_name=Mock())\n",
					"\n",
					"        self.task.column_datatypes      = [\"string\",\"string\", \"string\", \"string\", \"string\"]\n",
					"        self.task.source_column_names   = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n",
					"        self.task.sink_column_names     = [\"_c0\", \"first_name\", \"last_name\", \"age\", \"email\"]\n",
					"        self.task.dimensions            = ['SCD2', 'PK','SCD2','SCD2','SCD2',]\n",
					"        self.task.table_name            = ['unit_test']   \n",
					"        self.task.header                = True\n",
					"        self.task.variables             = {\"check_name\": [], \"config_params\": {}}\n",
					"        self.task.separator             = ';'\n",
					"        self.task.file_extension        = 'csv'\n",
					"        \n",
					"\n",
					"        # [Classes:SQLServerConnection] Mock the SQLServerConnection: Make sure no queries are executed\n",
					"        self.mock_sql_object            = MagicMock(spec=SQLServerConnection)\n",
					"        self.sql_server_object          = self.mock_sql_object\n",
					"\n",
					"        # [Classes:File] Make sure the log_file does not execute as it will throw an error -&gt; Cannot make a SQLServerConnection\n",
					"        self.log_file_patcher           = patch.object(File, 'log_file')\n",
					"        self.mock_log_file              = self.log_file_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        self.log_file_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the archive_to_silver method\n",
					"\n",
					"    def test_landingtoarchive_file_success(self):\n",
					"        # PREPROCESS\n",
					"        # [Classes:Task] Create a task-object from the Task-class\n",
					"        task_object                     = self.task\n",
					"        task_object.table_name          = 'unittest_silver'\n",
					"        task_object.skip_first_lines    = 1\n",
					"        task_object.target_options      = {}\n",
					"\n",
					"        # [Classes:File] Create a file-object from the File-class\n",
					"        file_name                       = 'MOCK_DATA_move.csv'\n",
					"        storage_account                 = f'{env_code}dapstdala1'\n",
					"        file_path                       = 'Classes/FileMovement/landing/move_to_archive/20240802_112046'\n",
					"        container_name                  = 'unittest'\n",
					"        landing_path                    = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{file_path}/{file_name}'\n",
					"        timestamp                       = 'timestamp'\n",
					"\n",
					"        file_object                     = File(self.task, self.sql_server_object, landing_path, file_name, timestamp,  storage_account)\n",
					"        # Overwrite file_object members with test-values \n",
					"        file_object.file_id             = -1\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Classes:File:LandingToArchive] Run the method under test\n",
					"        file_object.landing_to_archive()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the file has been moved to the archive container\n",
					"        actual_archive_files, actual_archive_folders = list_directory_content(file_object.archive_path, list(), list())\n",
					"        actual_archive_file_names = [file.name for file in actual_archive_files]\n",
					"        self.assertTrue(len(actual_archive_file_names) == 1, f\"[Classes:File:LandingToArchive] The files found on archive are {actual_archive_file_names}, which is not equal to the 1 files that were expected.\")\n",
					"        self.assertEqual(actual_archive_file_names[0], file_name)\n",
					"        \n",
					"        # Validate that the folders/file have been deleted in the landing\n",
					"        actual_landing_files, actual_landing_folders= list_directory_content(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/Classes/FileMovement/landing/move_to_archive', list(), list())\n",
					"        self.assertTrue(len(actual_landing_files) == 0, f\"[Classes:File:LandingToArchive] The file was still in landing, which is not what we expected.\")\n",
					"        self.assertTrue(len(actual_landing_folders) == 0, f\"[Classes:File:LandingToArchive] The folder still exist in landing, which is not what we expected.\")\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_File_LandingToArchive)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # Run all tests and see if there are some errors\n",
					"# findings = unittest.main(argv=[''], verbosity=2,exit=False, warnings='ignore')\n",
					"\n",
					"# if len(findings.result.failures) != 0 or len(findings.result.errors) != 0:\n",
					"#         raise Exception(\"Error in some tests, something went wrong!\")"
				],
				"execution_count": 27
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_CleanWorkspace.json">
{
	"name": "Test_CleanWorkspace",
	"properties": {
		"folder": {
			"name": "Functions/Test_Functions"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "75a4ba4e-5e53-4196-a432-f4d6b5db91af"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test CleanWorkspace\r\n",
					"This script will test all the functions coming from the CleanWorkspace notebook."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import unittest and unittest.mock modules\r\n",
					"import unittest\r\n",
					"from unittest.mock import patch, MagicMock, call\r\n",
					"import os\r\n",
					"\r\n",
					"# Import pyspark specific testing package\r\n",
					"# import pyspark.testing.utils # Pyspark version 3.5 contains useful functions (assertDataFrameEqual, assertSchemaEqual...) but they do not exist in the current 3.3.1 pyspark version\r\n",
					"\r\n",
					"# Import DeltaTable module to read a blob-folder as a delta table object\r\n",
					"from delta.tables import DeltaTable"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\r\n",
					"environment_code = 'dev'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Set environment parameter 'env_code'\r\n",
					"# First, check if there is a value in the spark configuration called 'environment_code'. If not, use the parameter defined above.\r\n",
					"env_code = spark.conf.get('spark.environment_code', environment_code)\r\n",
					"\r\n",
					"\r\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\r\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\r\n",
					"if env_code not in ['dev', 'int']:\r\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define global parameters to use over the entire notebook\r\n",
					"container_name = 'unittest'\r\n",
					"storage_account = f'{env_code}dapstdala1'"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the methods that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define the test classes"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Function: clean_containers"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CleanContainers\r\n",
					"# This class will run a set of methods against the clean_containers function \r\n",
					"class Test_Function_CleanContainers(unittest.TestCase):\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    # Patch python functions: The clean_containers function needs to be run in isolation. \r\n",
					"    # Other function-calls need to be mocked to avoid getting errors on things that do not need to be tested.\r\n",
					"    def setUp(self):\r\n",
					"\r\n",
					"        ## Method coming from library pyspark.sql\r\n",
					"        self.mock_mssparkutils_fs_rm_patcher = patch.object(mssparkutils.fs, 'rm')\r\n",
					"        self.mock_mssparkutils_fs_rm = self.mock_mssparkutils_fs_rm_patcher.start()\r\n",
					"\r\n",
					"    # After the tests have been executed, the patched functions can be stopped\r\n",
					"    def tearDown(self):\r\n",
					"        self.mock_mssparkutils_fs_rm_patcher.stop()\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the clean_containers function\r\n",
					"\r\n",
					"    # Success: Test that the patched function is called when the given arguments are acceptable\r\n",
					"    # Clean 1 container\r\n",
					"    def test_clean_containers_one_container(self):\r\n",
					"        clean_containers(env_code='dev', containers=['logs'])\r\n",
					"        self.mock_mssparkutils_fs_rm.assert_called_once()\r\n",
					"\r\n",
					"    # Success: Test that the patched function is called when the given arguments are acceptable\r\n",
					"    # Clean multiple containers using 1 function call\r\n",
					"    def test_clean_containers_multiple_containers(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set variable 'expected_calls': \r\n",
					"        #   The following calls are expected to be executed when calling the function clean_containers\r\n",
					"        # Concept: The clean_containers function is expected to be called twice using the below parameters\r\n",
					"        expected_calls = [ call(dir='abfss://logs@devdapstdala1.dfs.core.windows.net/', recurse=True), \r\n",
					"                call(dir='abfss://silver@devdapstdala1.dfs.core.windows.net/', recurse=True)\r\n",
					"                ]\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        clean_containers(env_code='dev', containers=['logs', 'silver'])\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Make sure that all the calls have been made\r\n",
					"        self.mock_mssparkutils_fs_rm.assert_has_calls(expected_calls, any_order=True)\r\n",
					"\r\n",
					"\r\n",
					"    # Failure: When the container-argument is of the datatype 'string', expect an error to be thrown\r\n",
					"    def test_clean_containers_invalid_container_1(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # The below error is expected to be thrown\r\n",
					"        expected_error= \"List of containers (logs) contains one or more unexpected containers: ['l', 'o', 'g', 's']\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            clean_containers(env_code='dev', containers='logs')\r\n",
					"\r\n",
					"        # EVALUATE        \r\n",
					"        # Check if the thrown error matches the expeced error\r\n",
					"        self.assertEqual(str(error.exception), expected_error)\r\n",
					"\r\n",
					"    # Failure: When the given list of containers contains an unexpected container-name, expect an error to be thrown\r\n",
					"    def test_clean_containers_invalid_container_2(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # The below error is expected to be thrown\r\n",
					"        expected_error= \"List of containers (['logs', 'unittest']) contains one or more unexpected containers: ['unittest']\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            clean_containers(env_code='dev', containers=['logs', 'unittest'])\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Check if the thrown error matches the expeced error\r\n",
					"        self.assertEqual(str(error.exception), expected_error)\r\n",
					"\r\n",
					"    # Failure: When the given environment argument 'env_code' is not 'dev' or 'int', expect an error to be thrown\r\n",
					"    def test_invalid_environment(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # The below error is expected to be thrown\r\n",
					"        expected_error= \"Not allowed to clean containers for environment tst. Only dev and int cleanups are allowed\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            clean_containers(env_code='tst', containers=['logs'])\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Check if the thrown error matches the expeced error\r\n",
					"        self.assertEqual(str(error.exception), expected_error)\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_CleanContainers)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in clean containers test, something went wrong!\")"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Function: clean_delta_table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CleanDeltaTable\r\n",
					"# This class will run a set of methods against the clean_delta_table function \r\n",
					"class Test_Function_CleanDeltaTable(unittest.TestCase):\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    # Patch python functions: The clean_containers function needs to be run in isolation. \r\n",
					"    # Other function-calls need to be mocked to avoid getting errors on things that do not need to be tested.\r\n",
					"    def setUp(self):\r\n",
					"\r\n",
					"        ## Method coming from library pyspark.sql\r\n",
					"        self.mock_mssparkutils_fs_rm_patcher = patch.object(mssparkutils.fs, 'rm')\r\n",
					"        self.mock_mssparkutils_fs_rm = self.mock_mssparkutils_fs_rm_patcher.start()\r\n",
					"\r\n",
					"        # Patch the print-function\r\n",
					"        # Purpose: Evaluate the calls made to the print-function of Python and evaluate the printed message\r\n",
					"        self.mock_print_patcher = patch('builtins.print')\r\n",
					"        self.mock_print = self.mock_print_patcher.start()\r\n",
					"\r\n",
					"\r\n",
					"    # After the tests have been executed, the patched functions can be stopped\r\n",
					"    def tearDown(self):\r\n",
					"        self.mock_mssparkutils_fs_rm_patcher.stop()\r\n",
					"        self.mock_print_patcher.stop()\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the clean_delta_table function\r\n",
					"\r\n",
					"    # Success: Test that, for a set of valid arguments, the function calls mssparkutils.fs.rm with the correct set of arguments\r\n",
					"    def test_clean_delta_table_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # The following call is expected to be made \r\n",
					"        expected_calls = [ call(dir=f'abfss://silver@devdapstdala1.dfs.core.windows.net/test', recurse=True) ]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Execute the clean_delta_table function with a valid set of arguments\r\n",
					"        clean_delta_table(env_code='dev', delta_lake='silver', table_name='test')\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the mssparkuitls.fs.rm function has been called with the expected set of arguments\r\n",
					"        self.mock_mssparkutils_fs_rm.assert_has_calls(expected_calls)\r\n",
					"\r\n",
					"\r\n",
					"    # Failure: Argument 'delta_lake' can only take value 'silver' or 'unittest'\r\n",
					"    def test_clean_delta_table_invalid_deltalake(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Expect the below error to be thrown\r\n",
					"        expected_error= 'Only allowed to clean delta lake \\'silver\\', not landing'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function with an invalid value for the delta_lake argument\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            clean_delta_table(env_code='dev', delta_lake='landing', table_name='test_table')\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expected error was thrown\r\n",
					"        self.assertEqual(str(error.exception), expected_error)\r\n",
					"\r\n",
					"\r\n",
					"    def test_clean_delta_table_invalid_environment(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Expect the below error to be thrown\r\n",
					"        expected_error= \"Not allowed to clean containers for environment tst. Only dev and int cleanups are allowed\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function with an invalid value for the env_code argument\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            clean_delta_table(env_code='tst', delta_lake='silver', table_name='test_table')\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expected error was thrown\r\n",
					"        self.assertEqual(str(error.exception), expected_error)\r\n",
					"\r\n",
					"\r\n",
					"    def test_clean_delta_table_patherror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Mock the output of the mssparkutils.fs.rm function\r\n",
					"        # The returned output will be a FileNotFoundError with text-output \"PathNotFoundException\"\r\n",
					"        # Purpose: When a file that does not exists is being deleted, the clean_delta_table function will return a print-statement 'Delta table does not exist' instead of throwing an error\r\n",
					"        self.mock_mssparkutils_fs_rm.side_effect = FileNotFoundError(\"PathNotFoundException\")\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        clean_delta_table(env_code='dev', delta_lake='silver', table_name='test_table')\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expected print statement is returned\r\n",
					"        self.mock_print.assert_called_with(\"Delta table does not exist\")\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_CleanDeltaTable)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in clean delta table test, something went wrong!\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Function: clean_table"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CleanTable\r\n",
					"# This class will run a set of methods against the clean_delta_table function \r\n",
					"class Test_Function_CleanTable(unittest.TestCase):\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    # Patch python functions: The clean_table function needs to be run in isolation. \r\n",
					"    # Other function-calls need to be mocked to avoid getting errors on things that do not need to be tested.\r\n",
					"    def setUp(self):\r\n",
					"\r\n",
					"        ## Method coming from library pyspark.sql\r\n",
					"        self.mock_spark_sql_patcher = patch.object(spark, 'sql')\r\n",
					"        self.mock_spark_sql = self.mock_spark_sql_patcher.start()\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        self.mock_spark_sql_patcher.stop()\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the clean_table function\r\n",
					"\r\n",
					"    # Success: Call function spark.sql for table 'mock' \r\n",
					"    def test_clean_table_success(self):\r\n",
					"        clean_table(\"mock\")\r\n",
					"        self.mock_spark_sql.assert_called_once_with(\"DROP TABLE IF EXISTS silver.mock\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_CleanTable)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in clean table test, something went wrong!\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Function: clean_folder"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Function_CleanFolder(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        return\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the clean_folder function\r\n",
					"\r\n",
					"    # Success:the folder should be deleted\r\n",
					"    def test_CleanFolder_success_delete_empty_folder(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path_folder_empty=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/CleanWorkspace/folder_empty'\r\n",
					"        mssparkutils.fs.rm(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/CleanWorkspace/folder_empty/file_to_be_deleted')\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        path=clean_folder(path_folder_empty)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"        self.assertTrue(not os.path.exists(path_folder_empty), f\"[Classes:File:LandingToArchive] The folder still exist in landing, which is not what we expected.\")\r\n",
					"    #Success:the folder shouldn't be deleted\r\n",
					"    def test_CleanFolder_failed_delete_not_empty_folder(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path_folder_not_empty=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/CleanWorkspace/folder_not_empty'\r\n",
					"        mock_mssparkutils_fs = MagicMock()\r\n",
					"        mock_mssparkutils_fs.rm.side_effect = Exception(\"The recursive query parameter value must be true to delete a non-empty directory.\")\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        with patch.dict('sys.modules', {'mssparkutils': MagicMock(fs=mock_mssparkutils_fs)}):\r\n",
					"            with self.assertRaises(Exception) as error:\r\n",
					"                clean_folder(path_folder_not_empty)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"            self.assertEqual(str(error.exception), f\"The path could not be deleted {path_folder_not_empty}\")\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_CleanFolder)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in clean folder test, something went wrong!\")"
				],
				"execution_count": 10
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_Dataframes.json">
{
	"name": "Test_Dataframes",
	"properties": {
		"description": "This contains the tests for the notebook \"Dataframes\" inside of the Modules folder",
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c04fe967-3236-4b38-9af2-984f809c687d"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"import unittest\r\n",
					"from unittest.mock import patch, MagicMock, Mock, ANY\r\n",
					"\r\n",
					"from types import SimpleNamespace # Used to mimic JSON-object as a returned object\r\n",
					"\r\n",
					"import json\r\n",
					"import datetime\r\n",
					"import delta.tables as dt\r\n",
					"from delta.tables import DeltaOptimizeBuilder, DeltaTable"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\r\n",
					"environment_code = ''"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\r\n",
					"\r\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\r\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\r\n",
					"if env_code not in ['dev', 'int']:\r\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")\r\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Dataframes"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class TestCalculatedFieldHeaders(unittest.TestCase):\r\n",
					"    # Dev-Note: Tests are currently pretty basic (input checks, ...), so we have to expand our tests for that\r\n",
					"    def setUp(self):\r\n",
					"        self.mock_filter_directory_content_patcher = patch(\"__main__.filter_directory_content\")\r\n",
					"        self.mock_filter_directory_content = self.mock_filter_directory_content_patcher.start()\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        self.mock_filter_directory_content_patcher.stop()\r\n",
					"\r\n",
					"    @patch.object(CalculatedFieldHeaders, \"object_checks\")\r\n",
					"    def test_init_pass(self, mock_objects_checks):\r\n",
					"        # Test: Mandatory params are set correctly, and optional parameters (= end_index) is set to None, and validation function is called\r\n",
					"\r\n",
					"        all_columns = ['mock_col1', 'mock_col2']\r\n",
					"        path = f\"abfss://landing@{env_code}dapstdala1.dfs.core.windows.net/mock\"\r\n",
					"        df_preproc_object = CalculatedFieldHeaders(path, \"mock\", \"mock/test\",\"False\", \"\\t\", \"csv\", all_columns, 0)\r\n",
					"\r\n",
					"        self.assertEqual(f\"abfss://landing@{env_code}dapstdala1.dfs.core.windows.net/mock\", df_preproc_object.path)\r\n",
					"        self.assertEqual(f\"abfss://landing@{env_code}dapstdala1.dfs.core.windows.net/mock/test\", df_preproc_object.preprocess_path)\r\n",
					"        self.assertEqual(\"False\", df_preproc_object.header)\r\n",
					"        self.assertEqual(\"\\t\", df_preproc_object.column_delimiter)\r\n",
					"        self.assertEqual(\"csv\", df_preproc_object.file_extension)\r\n",
					"        self.assertEqual(all_columns, df_preproc_object.columns)\r\n",
					"        self.assertEqual(0, df_preproc_object.start_index)\r\n",
					"        self.assertEqual(None, df_preproc_object.end_index)\r\n",
					"\r\n",
					"        mock_objects_checks.assert_called_once()\r\n",
					"\r\n",
					"    def test_init_validation_header_error(self):\r\n",
					"        # Test: Header can only be 'False'\r\n",
					"        all_columns = ['mock_col1', 'mock_col2']\r\n",
					"        \r\n",
					"        with self.assertRaisesRegex(Exception, \"^CalculatedFieldHeaders.*[dD]ataframe.*header.*Expected.*False.*\"):\r\n",
					"            df_preproc_object = CalculatedFieldHeaders(\"test_path.csv\", \"mock\", \"mock/test\", \"True\", Mock(), Mock(), all_columns, 0)\r\n",
					"\r\n",
					"    def test_init_validation_end_index_pass(self):\r\n",
					"        # Test: If header is set properly, and end_index is not provided, it should be set to the len(columns) - 1 -&gt; (last index in list)\r\n",
					"        all_columns = ['mock_col1', 'mock_col2']\r\n",
					"        df_preproc_object = CalculatedFieldHeaders(Mock(), Mock(), Mock(), \"False\", Mock(), Mock(), all_columns, 0)\r\n",
					"\r\n",
					"        self.assertEqual(1, df_preproc_object.end_index)\r\n",
					"\r\n",
					"    def test_init_validation_end_index_error1(self):\r\n",
					"        # Test: If header is set properly, and end_index &gt; (len(self.columns) - 1), expect error\r\n",
					"        all_columns = ['mock_col1', 'mock_col2']\r\n",
					"\r\n",
					"        with self.assertRaisesRegex(IndexError, \"^CalculatedFieldHeaders.*end.*index.*2.*cannot.*be.*higher.*than.*the.*last.*index.*1.*\"):\r\n",
					"            df_preproc_object = CalculatedFieldHeaders(Mock(), \"mock\", \"mock/test\", \"False\", Mock(), Mock(), all_columns, 0, 2)\r\n",
					"\r\n",
					"    def test_init_end_index_error2(self):\r\n",
					"        # Test: If header is set properly, and end_index &lt; self.start_index, expect error\r\n",
					"        all_columns = ['mock_col1', 'mock_col2']\r\n",
					"\r\n",
					"        with self.assertRaisesRegex(Exception, \"^CalculatedFieldHeaders.*end.*index.*-1.*cannot.*be.*smaller.*than.*the.*starting.*index.*0.*\"):\r\n",
					"            df_preproc_object = CalculatedFieldHeaders(Mock(), \"mock\", \"mock/test\", \"False\", Mock(), Mock(), all_columns, 0, -1)\r\n",
					"\r\n",
					"    def test_init_start_index_error(self):\r\n",
					"        # Test: If header is set properly, and self.start_index &lt; 0, expect error\r\n",
					"        all_columns = ['mock_col1', 'mock_col2']\r\n",
					"\r\n",
					"        with self.assertRaisesRegex(Exception, \"^CalculatedFieldHeaders.*start.*index.*-1.*cannot.*be.*lower.*than.*0.*\"):\r\n",
					"            df_preproc_object = CalculatedFieldHeaders(Mock(), \"mock\", \"mock/test\", \"False\", Mock(), Mock(), all_columns, -1, 1)\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(TestCalculatedFieldHeaders)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: InitialisationDeltaTable\r\n",
					"# This class will test the __init__ function of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\r\n",
					"class TestDataframeClass(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        # Define a dataframe that will be used for testing\r\n",
					"        self.test_dataframe = spark.createDataFrame(\r\n",
					"            data=[(1, \"dev\"), (2, \"int\"),(3, \"tst\")],\r\n",
					"            schema=[\"id\", \"label\"]\r\n",
					"        )\r\n",
					"\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # No functions to tearDown. Keep the method for consistency and potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"\r\n",
					"    def test_DataframeClass_init(self):\r\n",
					"        # RUN\r\n",
					"        # Create a class-object of the DataframeClass \r\n",
					"        dataframe_object = DataframeClass(self.test_dataframe)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the object has a dataframe-argument initialised\r\n",
					"        self.assertIsNotNone(dataframe_object.dataframe)\r\n",
					"    \r\n",
					"    def test_add_logging_columns(self):\r\n",
					"        # PREPARE\r\n",
					"        # Create a class-object of the DataframeClass\r\n",
					"        dataframe_object = DataframeClass(self.test_dataframe)\r\n",
					"        # Define the list of expected columns that \r\n",
					"        expected_columns = ['id', 'label', 't_load_date_raw', 't_load_date_silver', 't_plan_id', 't_task_id', 't_file_id', 't_file_name']\r\n",
					"\r\n",
					"        # RUN\r\n",
					"        # Execute method add_logging_columns() \r\n",
					"        returned_dataframe = dataframe_object.add_logging_columns(1, 1, 1, 'test')\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the actual columns match with the expected columns\r\n",
					"        self.assertEqual(expected_columns, returned_dataframe.columns, f'The actual columns ({returned_dataframe.columns}) did not the expected columns ({expected_columns})')\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(TestDataframeClass)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")   "
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeltaTableOptimizeClass_Initialisation\r\n",
					"# This class will test the __init__ function of the DeltaTableOptimizeClass-class defined in the Dataframe notebook\r\n",
					"class Test_DeltaTableOptimizeClass_Initialisation(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        # Create an empty dataframe that can be used by the test function\r\n",
					"        self.table_name = 'initialize_deltatable_test'\r\n",
					"        self.env_code = env_code\r\n",
					"        self.delta_table_path = f'abfss://silver@{env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\r\n",
					"        dataframe = spark.createDataFrame(\r\n",
					"            data=[],\r\n",
					"            schema=\"test INT, schema STRING\"\r\n",
					"        )\r\n",
					"\r\n",
					"        dataframe.write.mode('overwrite').format('delta').save(self.delta_table_path)\r\n",
					"        return\r\n",
					"\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # No functions to tearDown. Keep the method for consistency and potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"    def test_initization_DeltaTableOptimizeClass_success(self):\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Run the method under test\r\n",
					"        deltatable_object = DeltaTableOptimizeClass(table_name=self.table_name, env_code=self.env_code)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the object has a dataframe-argument initialised\r\n",
					"        self.assertIsNotNone(deltatable_object.delta_table_path)\r\n",
					"        self.assertEqual(self.delta_table_path, deltatable_object.delta_table_path)\r\n",
					"        self.assertIsInstance(deltatable_object.delta_table, dt.DeltaTable)\r\n",
					"\r\n",
					"    def test_initialization_DeltaTableOptimizeClass_function_calls(self):\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Run the method under test\r\n",
					"        with patch.object(dt.DeltaTable, 'forPath') as mock_forpath:\r\n",
					"            deltatable_object = DeltaTableOptimizeClass(table_name=self.table_name, env_code=self.env_code)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expected functions have been called with the expected arguments\r\n",
					"        mock_forpath.assert_called_once()\r\n",
					"        mock_forpath.assert_called_once_with(ANY, self.delta_table_path)\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeltaTableOptimizeClass_Initialisation)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")   "
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeltaTableOptimizeClass_Initialisation\r\n",
					"# This class will test the optimize_table_storage function (and related functions) of the DeltaTableOptimizeClass defined in the Dataframes notebook\r\n",
					"class Test_DeltaTableOptimizeClass_OptimizeTableStorage(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        # Create an empty dataframe that can be used by the test function\r\n",
					"        self.table_name = 'optimize_deltatable_test'\r\n",
					"        self.env_code = env_code\r\n",
					"        self.delta_table_path = f'abfss://silver@{env_code}dapstdala1.dfs.core.windows.net/{self.table_name}'\r\n",
					"        dataframe = spark.createDataFrame(\r\n",
					"            data=[],\r\n",
					"            schema=\"test INT, schema STRING\"\r\n",
					"        )\r\n",
					"\r\n",
					"        dataframe.write.mode('overwrite').format('delta').save(self.delta_table_path)\r\n",
					"\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Create an instance of the DeltaTableOptimizeClass to use for testing the class-methods\r\n",
					"        self.delta_table_object = DeltaTableOptimizeClass(self.table_name, self.env_code)\r\n",
					"        return\r\n",
					"\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # No functions to tearDown. Keep the method for consistency and potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the optimize_table_storage() method\r\n",
					"\r\n",
					"    def test_optimizetablestorage_DeltaTableOptimizeClass_success(self):\r\n",
					"        '''\r\n",
					"        Test Logic:\r\n",
					"            Run 'orchestrator' method optimize_table_storage without any patched objects\r\n",
					"            Expect an object to be created of class DeltaOptimizeBuilder\r\n",
					"        '''\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Execute method under test\r\n",
					"        self.delta_table_object.optimize_table_storage()\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Validate the creation of the DeltaOptimizeBuilder-class instance\r\n",
					"        self.assertIsInstance(self.delta_table_object.delta_table_optimized, DeltaOptimizeBuilder)\r\n",
					"\r\n",
					"\r\n",
					"    def test_optimizetablestorage_DeltaTableOptimizeClass_function_calls(self):\r\n",
					"        '''\r\n",
					"        Test Logic:\r\n",
					"            Run 'orchestrator' method optimize_table_storage with patching all the DeltaOptimizeBuilder-class methods\r\n",
					"            Expect the class methods to (not) be called based on the given arguments\r\n",
					"        '''\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass]: Execute method under test\r\n",
					"        with patch.object(delta.tables.DeltaOptimizeBuilder, 'executeCompaction') as mock_compaction, \\\r\n",
					"            patch.object(delta.tables.DeltaOptimizeBuilder, 'executeZOrderBy') as mock_zorderby:\r\n",
					"            self.delta_table_object.optimize_table_storage()\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Validate that underlying functions have been called as expected\r\n",
					"        mock_compaction.assert_called_once()\r\n",
					"        mock_zorderby.assert_not_called()\r\n",
					"\r\n",
					"\r\n",
					"    def test_optimizetablestorage_DeltaTableOptimizeClass_function_calls(self):\r\n",
					"        '''\r\n",
					"        Test Logic:\r\n",
					"            Run 'orchestrator' method optimize_table_storage with patching all the DeltaOptimizeBuilder-class methods\r\n",
					"            Expect the class methods to (not) be called based on the given arguments\r\n",
					"        '''\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Execute method under test\r\n",
					"        with patch.object(delta.tables.DeltaOptimizeBuilder, 'executeCompaction') as mock_compaction, \\\r\n",
					"            patch.object(delta.tables.DeltaOptimizeBuilder, 'executeZOrderBy') as mock_zorderby:\r\n",
					"            self.delta_table_object.optimize_table_storage(['test'])\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Validate that underlying functions have been called as expected\r\n",
					"        mock_compaction.assert_called_once()\r\n",
					"        mock_zorderby.assert_called_once_with(z_order_columns=['test'])\r\n",
					"\r\n",
					"    def test_compactdeltatable_DeltaTableOptimizeClass_nonexist_deltaoptimizebuilder(self):\r\n",
					"        '''\r\n",
					"        Test Logic:\r\n",
					"            Run method compact_deltatable(), but with DeltaTableOptimizeClass-member 'delta_table_optimized' set to None\r\n",
					"            Expect an error to be thrown: Compaction cannot be executed if DeltaOptimizeBuilder-instance does not exist\r\n",
					"        '''\r\n",
					"\r\n",
					"        # PREPROCESS\r\n",
					"        # Set the delta_table_optimized class-member to None\r\n",
					"        self.delta_table_object.delta_table_optimized = None\r\n",
					"        # Define the error that is expected to be thrown \r\n",
					"        expected_error = \"[Dataframes:DeltaTableOptimizeClass] delta_table_optimized member is not of class DeltaOptimizeBuilder for compaction\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Execute method under test\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            self.delta_table_object.compact_deltatable()\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expected error matches the actual\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:DeltaTableOptimizeClass] Expected error does not match actual: {expected_error} versus {actual_error}\")\r\n",
					"\r\n",
					"    def test_zorderdeltatable_DeltaTableOptimizeClass_nonexist_deltaoptimizebuilder(self):\r\n",
					"        '''\r\n",
					"        Test Logic:\r\n",
					"            Run method z_order_deltatable(), but with DeltaTableOptimizeClass-member 'delta_table_optimized' set to None\r\n",
					"            Expect an error to be thrown: Z-OrderBy cannot be executed if DeltaOptimizeBuilder-instance does not exist\r\n",
					"        '''\r\n",
					"\r\n",
					"        # PREPROCESS\r\n",
					"        # Set the delta_table_optimized class-member to None\r\n",
					"        self.delta_table_object.delta_table_optimized = None\r\n",
					"        # Define the error that is expected to be thrown \r\n",
					"        expected_error = \"[Dataframes:DeltaTableOptimizeClass] delta_table_optimized member is not of class DeltaOptimizeBuilder for Z-order\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes:DeltaTableOptimizeClass] Execute method under test\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            self.delta_table_object.z_order_deltatable(z_order_columns=list())\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expected error matches the actual\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:DeltaTableOptimizeClass] Expected error does not match actual: {expected_error} versus {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeltaTableOptimizeClass_OptimizeTableStorage)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")   "
				],
				"execution_count": 37
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_Dataframes_v2.json">
{
	"name": "Test_Dataframes_v2",
	"properties": {
		"folder": {
			"name": "Modules/Test_Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "45c805c9-4ff0-45b8-9ad1-c41617e95f25"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Dataframes_v2\n",
					"This notebook will test all the classes defined in notebook Dataframes_v2.\n",
					"The structure of the notebook looks as follows:\n",
					"    \n",
					"- Section: Markdown cells are used to distinguish between the different classes that are being tested.\n",
					"- Code cells: Each code cell tests one specific method of the class being tested of a section.\n",
					"\n",
					"**Notes**\n",
					"1. At the end of each cell, a piece of commented code can be found. This code can be uncommented if the tests of that specific cell need to be run. This can be useful during debugging. Make sure that, before completing a pull request, these pieces of code are commented again, as tests will otherwise be executed multiple times during the same test-job of the CI pipeline.\n",
					"2. The approach taken is to always define a setUp and tearDown method at the beginning of each Test Case. This is done to keep the code consistent over the different classes. These methods can be used if the entire Test Case will use the same set of variables or if a function/method needs to be mocked/patched for the entire Test Case.\n",
					"3. Each test method uses a similar structure, with a PREPROCESS, EXECUTE, and EVALUATE step. This is to keep the tests readable and consistent."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary methods of the unittest library"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import the unittest-library \n",
					"import unittest\n",
					"# Import some specific functions of the library defined under the mock-method\n",
					"from unittest.mock import patch, MagicMock, Mock, ANY, call\n",
					"\n",
					"# Import the DataFrame class\n",
					"# Reason: The Dataframes_v2 notebook will on some occasions create class-instances of DataFrame. Assertion are executed to validate whether the returned objects are actualy of this class.\n",
					"from pyspark.sql import DataFrame\n",
					"from pyspark.sql.functions import lit\n",
					"\n",
					"# Import other functions used during testing\n",
					"from pyspark.sql.types import StructField, StructType\n",
					"import delta.tables as dt\n",
					"import json\n",
					"import ast"
				],
				"execution_count": 2
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Initialize the environment_code parameter\n",
					"In almost all notebooks, a reference to this parameter will be found. This is because the CI/CD pipelines will not overwrite specific environment-references defined in notebooks. Since the notebooks will be deployed to several environments, the use of the environment_code parameter has been chosen. \n",
					"The environment_code will be used when the spark session has no environment_code-argument defined. This argument is linked to the core_configuration, which can be found under Manage -&gt; Apache Spark Configurations\n",
					"The use of these parameters and arguments is so that, during deployment, there is only 1 place where all references need to be overwritten."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\n",
					"environment_code = ''"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\n",
					"\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")\n",
					""
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define global parameters to use over the entire notebook\n",
					"container_name = 'unittest'\n",
					"storage_account = f'{env_code}dapstdala1'"
				],
				"execution_count": 5
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the Dataframes_v2 notebook"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Dataframes_v2"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the load_dataframe() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_loaddataframe(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the load_dataframe() method\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------    \n",
					"    # --------------------------------------------- TEST SUCCESSFULL CALLS ---------------------------------------------    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    # Success: Call load_dataframe with csv-file_kind \n",
					"    def test_loaddataframe_sparkdataframeclass_else_case(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the load_dataframe() method\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/load_dataframe/load_csv_test/csv_test.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind       = 'csv'\n",
					"        skip_lines = None\n",
					"        \n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Define a set of expected values\n",
					"        ### 1. Expect the total number of rows in the dataframe to be 3\n",
					"        expected_row_count = 3\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe() method with a valid set of arguments\n",
					"        class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## 1. The dataframe class-member should be an instance of class pyspark.sql.DataFrame\n",
					"        self.assertIsInstance(class_object.dataframe, DataFrame,    f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe was not initialized correctly as a DataFrame\")\n",
					"        ## 2. The dataframe should contain a total of 3 rows\n",
					"        actual_row_count = class_object.dataframe.count()\n",
					"        self.assertEqual(expected_row_count, actual_row_count,      f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe did not contain the expected rowcount {expected_row_count} after initialization\")\n",
					"\n",
					"    # Success: Call load_dataframe with json-file_kind \n",
					"    def test_loaddataframe_sparkdataframeclass_json(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the load_dataframe() method\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/load_dataframe/load_json_test/json_test_multiobject.json'\n",
					"        header          = True\n",
					"        separator       = ','\n",
					"        file_kind       = 'json'\n",
					"        skip_lines = None\n",
					"\n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Define a set of expected values\n",
					"        ### 1. Expect the total number of rows in the dataframe to be 3\n",
					"        expected_row_count = 3\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe() method with a valid set of arguments\n",
					"        class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## 1. The dataframe class-member should be an instance of class pyspark.sql.DataFrame\n",
					"        self.assertIsInstance(class_object.dataframe, DataFrame,    f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe was not initialized correctly as a DataFrame\")\n",
					"        ## 2. The dataframe should contain a total of 3 rows\n",
					"        actual_row_count = class_object.dataframe.count()\n",
					"        self.assertEqual(expected_row_count, actual_row_count,      f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe did not contain the expected rowcount '{expected_row_count}' after initialization\")\n",
					"\n",
					"    # Success: Call load_dataframe with skip_lines\n",
					"    def test_loaddataframe_sparkdataframeclass_skipfirstlines(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the load_dataframe() method\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/load_dataframe/load_skipfirstlines_test/skipfirstlines_test.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind       = 'csv'\n",
					"        skip_lines = 4\n",
					"\n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Define a set of expected values\n",
					"        ### 1. Expect the total number of rows in the dataframe to be 3\n",
					"        expected_row_count = 3\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe() method with a valid set of arguments\n",
					"        class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## 1. The dataframe class-member should be an instance of class pyspark.sql.DataFrame\n",
					"        self.assertIsInstance(class_object.dataframe, DataFrame,    f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe was not initialized correctly as a DataFrame\")\n",
					"        \n",
					"        ## 2. The dataframe should contain a total of 3 rows\n",
					"        actual_row_count = class_object.dataframe.count()\n",
					"        self.assertEqual(expected_row_count, actual_row_count,      f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe did not contain the expected rowcount '{expected_row_count}' after initialization\")\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------    \n",
					"    # ----------------------------------------------- TEST FUNCTION CALLS ----------------------------------------------    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    # Success: Call load_dataframe with skip_lines \n",
					"    def test_loaddataframe_sparkdataframeclass_skipfirstlines_functioncall_called(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the load_dataframe() method\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/load_dataframe/load_skipfirstlines_test/skipfirstlines_test.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind       = 'csv'\n",
					"        skip_lines = 3\n",
					"\n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe() method with a valid set of arguments\n",
					"        with patch(\"__main__.skip_first_lines\") as mock_skip_first_lines:\n",
					"            class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## 1. Validate that the skip_first_lines function has been called\n",
					"        mock_skip_first_lines.assert_called_once()\n",
					"\n",
					"    # Success: Call load_dataframe without skip_first_lines \n",
					"    def test_loaddataframe_sparkdataframeclass_skipfirstlines_functioncall_notcalled(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the load_dataframe() method\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/load_dataframe/load_csv_test/csv_test.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind       = 'json'\n",
					"        skip_lines = None\n",
					"\n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe() method with a valid set of arguments\n",
					"        with patch(\"__main__.skip_first_lines\") as mock_skip_first_lines:\n",
					"            class_object.load_dataframe(source_path, header, separator, file_kind, skip_lines)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## 1. Validate that the skip_first_lines function has not been called\n",
					"        mock_skip_first_lines.assert_not_called()\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_loaddataframe)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the create_dataframe() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_createdataframe(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown TestCass class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the create_dataframe() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_createdataframe_sparkdataframeclass_default_datatypes(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the SparkDataFrameClass object\n",
					"        column_dict = [\n",
					"            {\"column_sequence\": 1,  \"column_name\": \"column_1\",  \"dimension\": \"SCD2\",  \"data_type\": \"array\"},\n",
					"            {\"column_sequence\": 2,  \"column_name\": \"column_2\",  \"dimension\": \"SCD2\",  \"data_type\": \"binary\"},\n",
					"            {\"column_sequence\": 3,  \"column_name\": \"column_3\",  \"dimension\": \"SCD2\",  \"data_type\": \"boolean\"},\n",
					"            {\"column_sequence\": 4,  \"column_name\": \"column_4\",  \"dimension\": \"SCD2\",  \"data_type\": \"date\"},\n",
					"            {\"column_sequence\": 5,  \"column_name\": \"column_5\",  \"dimension\": \"PK\",    \"data_type\": \"string\"},\n",
					"            {\"column_sequence\": 6,  \"column_name\": \"column_6\",  \"dimension\": \"SCD2\",  \"data_type\": \"varchar(max)\"},\n",
					"            {\"column_sequence\": 7,  \"column_name\": \"column_7\",  \"dimension\": \"SCD2\",  \"data_type\": \"timestamp\"},\n",
					"            {\"column_sequence\": 8,  \"column_name\": \"column_8\",  \"dimension\": \"SCD2\",  \"data_type\": \"decimal\"},\n",
					"            {\"column_sequence\": 9,  \"column_name\": \"column_9\",  \"dimension\": \"SCD2\",  \"data_type\": \"float\"},\n",
					"            {\"column_sequence\": 10, \"column_name\": \"column_10\", \"dimension\": \"SCD2\",  \"data_type\": \"byte\"},\n",
					"            {\"column_sequence\": 11, \"column_name\": \"column_11\", \"dimension\": \"SCD2\",  \"data_type\": \"integer\"},\n",
					"            {\"column_sequence\": 12, \"column_name\": \"column_12\", \"dimension\": \"SCD2\",  \"data_type\": \"int\"},\n",
					"            {\"column_sequence\": 13, \"column_name\": \"column_13\", \"dimension\": \"SCD2\",  \"data_type\": \"long_integer\"}\n",
					"        ]\n",
					"        table_description = \"Table to test the basic functionalities of the create table function\"\n",
					"        \n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Define a set of expected values:\n",
					"        ### 1. The expected column_names of the final dataframe after creation -&gt; Original fields and Technical fields \n",
					"        expected_column_names = [\n",
					"            \"column_1\", \"column_2\", \"column_3\", \"column_4\", \"column_5\", \"column_6\", \"column_7\", \n",
					"            \"column_8\", \"column_9\", \"column_10\", \"column_11\",  \"column_12\", \"column_13\",\n",
					"            \"t_load_date_raw\", \"t_load_date_silver\", \"t_extract_date\", \"t_update_date\", \"t_insert_date\", \"t_file_name\", \"t_plan_id\", \"t_task_id\", \"t_file_id\"\n",
					"        ]\n",
					"        ### 2. The expected data_types of each column of the dataframe -&gt; Original fields and Technical fields\n",
					"        expected_datatypes = [\n",
					"            ArrayType(StringType()), BinaryType(), BooleanType(), DateType(), StringType(), StringType(), \n",
					"            TimestampType(), DecimalType(38, 4), FloatType(), ByteType(), IntegerType(), IntegerType(), LongType(),\n",
					"            TimestampType(), TimestampType(), TimestampType(), TimestampType(), TimestampType(), StringType(), IntegerType(), IntegerType(), IntegerType()\n",
					"        ]\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the create_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.create_dataframe(column_dict, table_description)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## 1. The dataframe class-member should be an instance of class pyspark.sql.DataFrame\n",
					"        self.assertIsInstance(class_object.dataframe, DataFrame,   f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe was not initialized correctly as a DataFrame\")\n",
					"        ## 2. The column names of the dataframe should match the list of expected column_names\n",
					"        actual_column_names:list = class_object.dataframe.columns\n",
					"        self.assertEqual(expected_column_names, actual_column_names,    f\"[Dataframes:SparkDataFrameClass:LoadDataframe] Expected column names do not match with actuals: {expected_column_names} versus {actual_column_names}\")\n",
					"        ## 3. The datatypes of the dataframe should match the list of expected datatypes\n",
					"        actual_datatypes:list = [class_object.dataframe.select(column_name).schema[0].dataType for column_name in expected_column_names]\n",
					"        self.assertEqual(expected_datatypes, actual_datatypes,          f\"[Dataframes:SparkDataFrameClass:LoadDataframe] Expected data types do not match with actuals: {expected_datatypes} versus {actual_datatypes}\")\n",
					"\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_createdataframe_sparkdataframeclass_specialdatatypes(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be passed to the SparkDataFrameClass object\n",
					"        column_dict = [\n",
					"            {\"column_sequence\": 1, \"column_name\": \"column_1\", \"dimension\": \"PK\",    \"data_type\": \"varchar(4)\"},\n",
					"            {\"column_sequence\": 2, \"column_name\": \"column_2\", \"dimension\": \"SCD2\",  \"data_type\": \"varchar(10)\"},\n",
					"            {\"column_sequence\": 3, \"column_name\": \"column_3\", \"dimension\": \"SCD2\",  \"data_type\": \"decimal(12,4)\"},\n",
					"        ]\n",
					"        table_description = \"Table to test the basic functionalities of the create table function\"\n",
					"        \n",
					"        ## Define a class-object \n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        # Define the expected values for the class-instance members:\n",
					"        # For column_1: the expectation is that there is a metadata-field that limits the total number of characters that can be added\n",
					"        expected_metadata_column1 = {\"dataType\": StringType(), \"metadata\": {'column_1': 'metadata about the column column_1', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(4)'}}\n",
					"        # For column_2: the expectation is that there is a metadata-field that limits the total number of characters that can be added\n",
					"        expected_metadata_column2 = {\"dataType\": StringType(), \"metadata\": {'column_2': 'metadata about the column column_2', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(10)'}}\n",
					"        # For column_3: the expectation is that there is a metadata-field that limits the total number of characters that can be added\n",
					"        expected_metadata_column3 = {\"dataType\": DecimalType(12,4), \"metadata\": {'column_3': 'metadata about the column column_3'}}\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Initiliaze a SparkDataFrameClass class-object with a valid set of argument values\n",
					"        class_object.create_dataframe(column_dict, table_description)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the initialised object members match the expected values\n",
					"        self.assertIsInstance(class_object.dataframe, DataFrame,   f\"[Dataframes:SparkDataFrameClass:LoadDataframe] class-member dataframe was not initialized correctly as a DataFrame\")\n",
					"        \n",
					"        actual_metadata_column1 = class_object.dataframe.select(\"column_1\").schema[0]\n",
					"        actual_metadata_column2 = class_object.dataframe.select(\"column_2\").schema[0]\n",
					"        actual_metadata_column3 = class_object.dataframe.select(\"column_3\").schema[0]\n",
					"\n",
					"        # Validate that these match the expected values\n",
					"        self.assertEqual(expected_metadata_column1[\"dataType\"], actual_metadata_column1.dataType)\n",
					"        self.assertEqual(expected_metadata_column1[\"metadata\"], actual_metadata_column1.metadata)\n",
					"        self.assertEqual(expected_metadata_column2[\"dataType\"], actual_metadata_column2.dataType)\n",
					"        self.assertEqual(expected_metadata_column2[\"metadata\"], actual_metadata_column2.metadata)\n",
					"        self.assertEqual(expected_metadata_column3[\"dataType\"], actual_metadata_column3.dataType)\n",
					"        self.assertEqual(expected_metadata_column3[\"metadata\"], actual_metadata_column3.metadata)\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_createdataframe)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the write_dataframe() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_writedataframe(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"\n",
					"        self.column_dict = [\n",
					"            {\"column_sequence\": 1,  \"column_name\": \"column_1\",  \"dimension\": \"PK\",    \"data_type\": \"string\"},\n",
					"            {\"column_sequence\": 2,  \"column_name\": \"column_2\",  \"dimension\": \"SCD2\",  \"data_type\": \"binary\"},\n",
					"            {\"column_sequence\": 3,  \"column_name\": \"column_3\",  \"dimension\": \"SCD2\",  \"data_type\": \"boolean\"}\n",
					"        ]\n",
					"    \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown TestCass class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the write_dataframe() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_writedataframe_sparkdataframeclass_partitioning(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the SparkDataFrameClass object\n",
					"        table_description = \"Table to test the write_dataframe method when using partitioning\"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        class_object.create_dataframe(self.column_dict, table_description)\n",
					"\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/write_dataframe/partitioning_test'\n",
					"        write_format = 'delta'\n",
					"        write_mode = 'overwrite'\n",
					"        partitioning_columns = ['column_1']\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the write_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n",
					"\n",
					"        # EVALUATE\n",
					"        detailDF = dt.DeltaTable.forPath(spark, destination_path).detail()\n",
					"        actual_partitioning_columns = detailDF.select(\"partitionColumns\").collect()[0][0] \n",
					"        self.assertEqual(partitioning_columns,actual_partitioning_columns, f\"[Dataframes:SparkDataFrameClass:WriteDataframe] Expected partitioning columns were not found in delta table metadata: Expected {partitioning_columns}; Actual {actual_partitioning_columns}\" )\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_writedataframe_sparkdataframeclass_no_partitioning(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the SparkDataFrameClass object\n",
					"        table_description = \"Table to test the write_dataframe method when using partitioning\"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        class_object.create_dataframe(self.column_dict, table_description)\n",
					"\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/write_dataframe/no_partitioning_test'\n",
					"        write_format = 'delta'\n",
					"        write_mode = 'overwrite'\n",
					"        partitioning_columns = None\n",
					"        expected_partitioning_columns = list()\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the write_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n",
					"\n",
					"        # EVALUATE\n",
					"        detailDF = dt.DeltaTable.forPath(spark, destination_path).detail()\n",
					"        actual_partitioning_columns = detailDF.select(\"partitionColumns\").collect()[0][0] \n",
					"        self.assertEqual(expected_partitioning_columns,actual_partitioning_columns, f\"[Dataframes:SparkDataFrameClass:WriteDataframe] Expected partitioning columns were not found in delta table metadata: Expected {expected_partitioning_columns}; Actual {actual_partitioning_columns}\" )\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_writedataframe_sparkdataframeclass_multiplepartitioning(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be passed to the SparkDataFrameClass object\n",
					"        table_description = \"Table to test the write_dataframe method when using partitioning\"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        class_object.create_dataframe(self.column_dict, table_description)\n",
					"\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/write_dataframe/multiple_partitioning_test'\n",
					"        write_format = 'delta'\n",
					"        write_mode = 'overwrite'\n",
					"        partitioning_columns = ['column_1', 'column_2']\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the write_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n",
					"\n",
					"        # EVALUATE\n",
					"        detailDF = dt.DeltaTable.forPath(spark, destination_path).detail()\n",
					"        actual_partitioning_columns = detailDF.select(\"partitionColumns\").collect()[0][0] \n",
					"        self.assertEqual(partitioning_columns,actual_partitioning_columns, f\"[Dataframes:SparkDataFrameClass:WriteDataframe] Expected partitioning columns were not found in delta table metadata: Expected {partitioning_columns}; Actual {actual_partitioning_columns}\" )\n",
					"\n",
					"    # Failure: Throw error when dataframe-attribute does not exist\n",
					"    def test_writedataframe_sparkdataframeclass_throwerror(self):\n",
					"        # PREPROCESS\n",
					"        destination_path = 'test_path'\n",
					"        write_format = ''\n",
					"        write_mode = ''\n",
					"        partitioning_columns = None\n",
					"        expected_error = f\"[SparkDataFrameClass] There is no dataframe attribute defined. Cannot write the dataframe to destination {destination_path}. Failing task...\"\n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the write_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        with self.assertRaises(AttributeError) as error:\n",
					"            class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns)\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:SparkDataFrameClass:WriteDataframe] Expected error to be thrown when dataframe does not exist when writing dataframe. Got {actual_error}\")\n",
					"\n",
					"\n",
					"    # Failure: Throw an error when trying to merge_schema while using overwrite_method 'overwrite'\n",
					"    def test_writedataframe_sparkdataframeclass_failure_overwriteandmerge(self):\n",
					"        # PREPROCESS\n",
					"        # Set the arguments used to call the function with\n",
					"        destination_path = 'test_path'\n",
					"        write_format = ''\n",
					"        write_mode = 'overwrite'\n",
					"        merge_schema = True\n",
					"        partitioning_columns = None\n",
					"        expected_error = f\"[Dataframes_v2:SparkDataFrameClass] Not allowed to mergeSchema when write_mode is not 'append'. This will overwrite the entire dataframe\"\n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the method under test\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns, merge_schema=merge_schema)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the thrown error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:SparkDataFrameClass:WriteDataframe] Expected error to be thrown when dataframe does not exist when writing dataframe. Got {actual_error}\")\n",
					"\n",
					"    # Success: Append a new column to an existing dataframe\n",
					"    def test_writedataframe_sparkdataframeclass_success_appendandmerge_nopartitioning(self):\n",
					"        # PREPROCESS\n",
					"        # Create an empty delta table\n",
					"        delta_table_object = DeltaTableClass_v2()\n",
					"\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/schema_evolution'\n",
					"        table_description = 'test schema evolution'\n",
					"        column_objects = [\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\n",
					"        ]\n",
					"        partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\n",
					"\n",
					"        delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\n",
					"\n",
					"        # Create an empty dataframe\n",
					"        class_object = SparkDataFrameClass()\n",
					"        column_objects = [\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\n",
					"            {\"column_name\": \"column_3\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\n",
					"        ]\n",
					"        class_object.create_dataframe(column_information=column_objects)\n",
					"\n",
					"        # Set the arguments used to call the function with\n",
					"        write_format = 'delta'\n",
					"        write_mode = 'append'\n",
					"        partitioning_columns = None\n",
					"        expected_error = f\"[Dataframes_v2:SparkDataFrameClass] Not allowed to mergeSchema when write_mode is not 'append'. This will overwrite the entire dataframe\"\n",
					"\n",
					"        # Define expectations\n",
					"        expected_columns = ['column_1', 'column_2', 'column_3']\n",
					"        # EXECUTE\n",
					"        # Run the method under test\n",
					"        class_object.write_dataframe(destination_path=destination_path, write_format=write_format, write_mode=write_mode, partitioning_columns=partitioning_columns, merge_schema=True)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the delta table contains the 3 columns (excluding the technical fields)\n",
					"        delta_table = delta_table_object.load_delta_table(source_path=destination_path)\n",
					"        table_columns = delta_table.toDF().columns\n",
					"        \n",
					"        actual_columns = []\n",
					"        for expected_column in expected_columns:\n",
					"            if expected_column in table_columns:\n",
					"                actual_columns.append(expected_column)\n",
					"\n",
					"        self.assertEqual(expected_columns, actual_columns, f'[Dataframes:SparkDataFrameClass:WriteDataframe] Expected columns to be appended to the delta table, but there are missing columns: \\nEXPECTED {expected_columns} \\nMISSING {actual_columns}')\n",
					"        \n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_writedataframe)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")    "
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the add_logging_columns_to_dataframe() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_addloggingcolumnstodataframe(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"\n",
					"        self.column_dict = [\n",
					"            {\"column_sequence\": 1,  \"column_name\": \"column_1\",  \"dimension\": \"PK\",    \"data_type\": \"string\"},\n",
					"            {\"column_sequence\": 2,  \"column_name\": \"column_2\",  \"dimension\": \"SCD2\",  \"data_type\": \"binary\"},\n",
					"            {\"column_sequence\": 3,  \"column_name\": \"column_3\",  \"dimension\": \"SCD2\",  \"data_type\": \"boolean\"}\n",
					"        ]\n",
					"    \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown TestCass class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the add_logging_columns_to_dataframe() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_addloggingcolumnstodataframe_sparkdataframeclass_success(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        table_description = \"Table to test the add_logging_columns_to_dataframe method\"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        class_object.create_dataframe(self.column_dict, table_description)\n",
					"\n",
					"        task_id     = -1\n",
					"        plan_id     = -1\n",
					"        file_id     = -1\n",
					"        file_name   = 'unittest'\n",
					"\n",
					"        expected_columns = ['column_1', 'column_2', 'column_3', 't_load_date_raw', 't_load_date_silver', 't_extract_date', 't_update_date', 't_insert_date', 't_file_name', 't_plan_id', 't_task_id', 't_file_id']\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the add_logging_columns_to_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.add_logging_columns_to_dataframe(task_id=task_id, plan_id=plan_id, file_id=file_id, file_name=file_name)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_columns = class_object.dataframe.columns\n",
					"        self.assertEqual(expected_columns, actual_columns, f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToDataframe] Dataframe did not contain the expected column names: Expected {expected_columns} ; Actual {actual_columns}\")\n",
					"\n",
					"\n",
					"    # Failure: Throw error when dataframe-attribute does not exist\n",
					"    def test_addloggingcolumnstodataframe_sparkdataframeclass_throwerror(self):\n",
					"        # PREPROCESS\n",
					"        class_object = SparkDataFrameClass()\n",
					"        expected_error = \"[SparkDataFrameClass] There is no dataframe attribute defined. Cannot add logging columns. Failing task...\"\n",
					"        task_id     = -1\n",
					"        plan_id     = -1\n",
					"        file_id     = -1\n",
					"        file_name   = 'unittest'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the write_dataframe() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        with self.assertRaises(AttributeError) as error:\n",
					"            class_object.add_logging_columns_to_dataframe(task_id=task_id, plan_id=plan_id, file_id=file_id, file_name=file_name)\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToDataframe] Expected error to be thrown when dataframe does not exist when adding logging columns. Got {actual_error}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_addloggingcolumnstodataframe)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the add_logging_columns_to_schema() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_addloggingcolumnstoschema(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):   \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown TestCass class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the add_logging_columns_to_schema() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_addloggingcolumnstoschema_sparkdataframeclass_emptylist(self):\n",
					"        # PREPROCESS\n",
					"        class_object = SparkDataFrameClass()\n",
					"        expected_column_names   = ['t_load_date_raw', 't_load_date_silver', 't_extract_date', 't_update_date', 't_insert_date', 't_file_name', 't_plan_id', 't_task_id', 't_file_id']\n",
					"        expected_datatypes      = [TimestampType(), TimestampType(), TimestampType(), TimestampType(), TimestampType(), StringType(), IntegerType(), IntegerType(), IntegerType()]\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the add_logging_columns_to_schema() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        structfield_objects:list = class_object.add_logging_columns_to_schema(structfields=list())\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that each instance is an instance of class StructField\n",
					"        [self.assertIsInstance(structfield, StructField) for structfield in structfield_objects]\n",
					"\n",
					"        ## Get the list of column_names and datatypes\n",
					"        actual_column_names = [structfield.name     for structfield in structfield_objects]\n",
					"        actual_datatypes =    [structfield.dataType for structfield in structfield_objects]\n",
					"        ## Compare the lists with the expected values\n",
					"        self.assertEqual(expected_column_names, actual_column_names,    f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToSchema] The names of the columns did not match expectations: Expected {expected_column_names} versus Actual {actual_column_names}\")\n",
					"        self.assertEqual(expected_datatypes,    actual_datatypes,       f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToSchema] The datatypes of the columns did not match expectations: Expected {expected_datatypes} versus Actual {actual_datatypes}\")\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_addloggingcolumnstoschema_sparkdataframeclass_nonemptylist(self):\n",
					"        # PREPROCESS\n",
					"        class_object = SparkDataFrameClass()\n",
					"        structfields            = [StructField(name=\"unittest\", dataType=StringType())]\n",
					"        expected_column_names   = [\"unittest\", 't_load_date_raw', 't_load_date_silver', 't_extract_date', 't_update_date', 't_insert_date', 't_file_name', 't_plan_id', 't_task_id', 't_file_id']\n",
					"        expected_datatypes      = [StringType(), TimestampType(), TimestampType(), TimestampType(), TimestampType(), TimestampType(), StringType(), IntegerType(), IntegerType(), IntegerType()]\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the add_logging_columns_to_schema() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        structfield_objects:list = class_object.add_logging_columns_to_schema(structfields=structfields)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that each instance is an instance of class StructField\n",
					"        [self.assertIsInstance(structfield, StructField) for structfield in structfield_objects]\n",
					"\n",
					"        ## Get the list of column_names and datatypes\n",
					"        actual_column_names = [structfield.name     for structfield in structfield_objects]\n",
					"        actual_datatypes =    [structfield.dataType for structfield in structfield_objects]\n",
					"        ## Compare the lists with the expected values\n",
					"        self.assertEqual(expected_column_names, actual_column_names,    f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToSchema] The names of the columns did not match expectations: Expected {expected_column_names} versus Actual {actual_column_names}\")\n",
					"        self.assertEqual(expected_datatypes,    actual_datatypes,       f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToSchema] The datatypes of the columns did not match expectations: Expected {expected_datatypes} versus Actual {actual_datatypes}\")\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_addloggingcolumnstoschema)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the create_structfield_objects() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_createstructfieldobjects(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        self.configure_datatypes_patcher = patch.object(SparkDataFrameClass, 'configure_datatypes')\n",
					"        self.mock_configure_datatypes    = self.configure_datatypes_patcher.start()\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        self.configure_datatypes_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the create_structfield_objects() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_createstructfieldobjects_sparkdataframeclass_basicfields(self):\n",
					"        # PREPROCESS\n",
					"        column_information = [\n",
					"            {\"column_sequence\": 1,  \"column_name\": \"column_1\",  \"dimension\": \"PK\",    \"data_type\": \"string\"},\n",
					"            {\"column_sequence\": 2,  \"column_name\": \"column_2\",  \"dimension\": \"SCD2\",  \"data_type\": \"binary\"},\n",
					"            {\"column_sequence\": 3,  \"column_name\": \"column_3\",  \"dimension\": \"SCD2\",  \"data_type\": \"boolean\"}\n",
					"        ]\n",
					"\n",
					"        self.mock_configure_datatypes.return_value = {\"binary\": BinaryType(), \"boolean\": BooleanType(),\"string\": StringType()}\n",
					"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        expected_column_names   = [\"column_1\", \"column_2\", \"column_3\"]\n",
					"        expected_datatypes      = [StringType(), BinaryType(), BooleanType()]\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the create_structfield_objects() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        structfield_objects:list = class_object.create_structfield_objects(column_information=column_information)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that each instance is an instance of class StructField\n",
					"        [self.assertIsInstance(structfield, StructField) for structfield in structfield_objects]\n",
					"\n",
					"        ## Get the list of column_names and datatypes\n",
					"        actual_column_names = [structfield.name for structfield in structfield_objects]\n",
					"        actual_datatypes =    [structfield.dataType for structfield in structfield_objects]\n",
					"        ## Compare the lists with the expected values\n",
					"        self.assertEqual(expected_column_names, actual_column_names,    f\"[Dataframes:SparkDataFrameClass:createstructfieldobjects] The names of the columns did not match expectations: Expected {expected_column_names} versus Actual {actual_column_names}\")\n",
					"        self.assertEqual(expected_datatypes,    actual_datatypes,       f\"[Dataframes:SparkDataFrameClass:createstructfieldobjects] The datatypes of the columns did not match expectations: Expected {expected_datatypes} versus Actual {actual_datatypes}\")\n",
					"        \n",
					"        self.mock_configure_datatypes.assert_called_once_with(table_data_types=[\"string\", \"binary\", \"boolean\"])\n",
					"\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_createstructfieldobjects_sparkdataframeclass_specialcasefields(self):\n",
					"        # PREPROCESS\n",
					"        column_information = [\n",
					"            {\"column_sequence\": 1,  \"column_name\": \"column_1\",  \"dimension\": \"PK\",    \"data_type\": \"string\"},\n",
					"            {\"column_sequence\": 2,  \"column_name\": \"column_2\",  \"dimension\": \"SCD2\",  \"data_type\": \"varchar(4)\"},\n",
					"            {\"column_sequence\": 3,  \"column_name\": \"column_3\",  \"dimension\": \"SCD2\",  \"data_type\": \"varchar(10)\"},\n",
					"            {\"column_sequence\": 3,  \"column_name\": \"column_4\",  \"dimension\": \"SCD2\",  \"data_type\": \"decimal(12,4)\"}\n",
					"        ]\n",
					"\n",
					"        self.mock_configure_datatypes.return_value = {\"string\": StringType(), \"varchar(4)\": StringType(),\"varchar(10)\": StringType(), \"decimal(12,4)\": DecimalType(12,4)}\n",
					"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        expected_column_names   = [\"column_1\", \"column_2\", \"column_3\", \"column_4\"]\n",
					"        expected_datatypes      = [StringType(), StringType(), StringType(), DecimalType(12,4)]\n",
					"        expected_metadata       = [\n",
					"            {\"column_1\": f\"metadata about the column column_1\"},\n",
					"            {\"column_2\": f\"metadata about the column column_2\", '__CHAR_VARCHAR_TYPE_STRING': \"varchar(4)\"},\n",
					"            {\"column_3\": f\"metadata about the column column_3\", '__CHAR_VARCHAR_TYPE_STRING': \"varchar(10)\"},\n",
					"            {\"column_4\": f\"metadata about the column column_4\"}\n",
					"        ]\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the create_structfield_objects() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        structfield_objects:list = class_object.create_structfield_objects(column_information=column_information)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that each instance is an instance of class StructField\n",
					"        [self.assertIsInstance(structfield, StructField) for structfield in structfield_objects]\n",
					"\n",
					"        ## Get the list of column_names and datatypes\n",
					"        actual_column_names = [structfield.name     for structfield in structfield_objects]\n",
					"        actual_datatypes =    [structfield.dataType for structfield in structfield_objects]\n",
					"        actual_metadata =     [structfield.metadata for structfield in structfield_objects]\n",
					"\n",
					"        ## Compare the lists with the expected values\n",
					"        self.assertEqual(expected_column_names, actual_column_names,    f\"[Dataframes:SparkDataFrameClass:createstructfieldobjects] The names of the columns did not match expectations: Expected {expected_column_names} versus Actual {actual_column_names}\")\n",
					"        self.assertEqual(expected_datatypes,    actual_datatypes,       f\"[Dataframes:SparkDataFrameClass:createstructfieldobjects] The datatypes of the columns did not match expectations: Expected {expected_datatypes} versus Actual {actual_datatypes}\")\n",
					"        self.assertEqual(expected_metadata,     actual_metadata,        f\"[Dataframes:SparkDataFrameClass:createstructfieldobjects] The metadata of the columns did not match expectations: Expected {expected_metadata} versus Actual {actual_metadata}\")\n",
					"        \n",
					"        self.mock_configure_datatypes.assert_called_once_with(table_data_types=[\"string\", \"varchar(4)\", \"varchar(10)\", \"decimal(12,4)\"])\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_createstructfieldobjects)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the add_decimal_datatype() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_adddecimaldatatype(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def setUp(self):\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # No tear-down needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"        \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the add_decimal_datatype() function\n",
					"\n",
					"    # Success: Add a decimal-datatype reference to the types-dictionary\n",
					"    def test_add_decimal_datatype(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be used to call the function with\n",
					"        types = dict()\n",
					"        value = 'decimal(12,5)'\n",
					"        \n",
					"        ## Define an object-instance of class SparkDataFrameClass\n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Set the expected results\n",
					"        expected_dictionary = {\"decimal(12,5)\": sql.types.DecimalType(12,5)}\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the add_decimal_datatype() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        actual_dictionary = class_object.add_decimal_datatype(value, types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the returned variables match expectations\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\n",
					"\n",
					"    # Failure: Deny the additional reference of an invalid decimal configuration to the types-dictionary \n",
					"    def test_invalid_value(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be used to call the function with\n",
					"        types = dict()\n",
					"        value = 'decimal(12)'\n",
					"        \n",
					"        ## Define an object-instance of class SparkDataFrameClass\n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Set the expected results\n",
					"        expected_error = f\"The decimal value decimal(12) is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...\"\n",
					"        \n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the add_decimal_datatype() method of the SparkDataFrameClass with an invalid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object.add_decimal_datatype(value, types)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the expected error is actually returned\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_decimal_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_adddecimaldatatype)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the add_varchar_datatype() of the SparkDataFrameClass-class defined in the DataFrames_v2 notebook\n",
					"class Test_SparkDataFrameClass_addvarchardatatype(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        return\n",
					"       \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the add_varchar_datatype() function\n",
					"\n",
					"    # Success: Add a varchar-datatype reference to the types-dictionary\n",
					"    def test_addvarchardatatype_sparkdataframeclass_success(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be used to call the function with\n",
					"        types = dict()\n",
					"        value = 'varchar(12)'\n",
					"        \n",
					"        ## Define an object-instance of class SparkDataFrameClass\n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Set the expected results\n",
					"        expected_dictionary = {\"varchar(12)\": StringType()}\n",
					"    \n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the add_varchar_datatype() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        actual_dictionary = class_object.add_varchar_datatype(varchar_value=value, datatypes=types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the returned variables match expectations\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"[Dataframes_v2:SparkDataFrameClass:AddVarcharDatatype]The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\n",
					"\n",
					"    # Failure: Deny the additional reference of an invalid varchar configuration to the types-dictionary \n",
					"    def test_addvarchardatatype_sparkdataframeclass_invalidargument(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the arguments that will be used to call the function with\n",
					"        types = dict()\n",
					"        value = 'varchar'\n",
					"        \n",
					"        ## Define an object-instance of class SparkDataFrameClass\n",
					"        class_object = SparkDataFrameClass()\n",
					"\n",
					"        ## Set the expected results\n",
					"        expected_error = f\"The varchar value varchar is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the add_varchar_datatype() method of the SparkDataFrameClass with an invalid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object.add_varchar_datatype(varchar_value=value, datatypes=types)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the expected error is returned\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_varchar_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_addvarchardatatype)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the configure_datatypes() method defined in the SparkDataFrameClass notebook\n",
					"class Test_SparkDataFrameClass_configuredatatypes(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # Patch the functions that are called by the validate_table-method of the DeltaTable-class\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\n",
					"\n",
					"        self.mock_add_decimal_datatype_patcher = patch.object(SparkDataFrameClass, 'add_decimal_datatype')\n",
					"        self.mock_add_decimal_datatype = self.mock_add_decimal_datatype_patcher.start()\n",
					"\n",
					"        self.mock_add_varchar_datatype_patcher = patch.object(SparkDataFrameClass, 'add_varchar_datatype')\n",
					"        self.mock_add_varchar_datatype = self.mock_add_varchar_datatype_patcher.start()\n",
					"\n",
					"        ## Define an object-instance of class SparkDataFrameClass -&gt; Reuse object throughout test case\n",
					"        self.class_object = SparkDataFrameClass()\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        self.mock_add_decimal_datatype_patcher.stop()\n",
					"        self.mock_add_varchar_datatype_patcher.stop()\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the configure_datatypes function\n",
					"\n",
					"\n",
					"    # Success: Pass an empty dictionary and return a dictionary of predefined datatypes\n",
					"    def test_configuredatatypes_sparkdataframeclass_returngenericdatatypes(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = list()\n",
					"        ## Define the expected return values\n",
					"        expected_dictionary = {\n",
					"            \"array\": ArrayType(StringType()), \"binary\": BinaryType(), \"boolean\": BooleanType(), \"date\": DateType(), \"string\": StringType(), \n",
					"            \"varchar(max)\": StringType(), \"timestamp\": TimestampType(), \"decimal\": DecimalType(38, 4), \"float\": FloatType(), \"byte\": ByteType(), \n",
					"            \"integer\": IntegerType(), \"int\": IntegerType(), \"long_integer\":  LongType()\n",
					"        }\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        actual_dictionary = self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the returned values match expectations\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"[Dataframes_v2:SparkDataFrameClass:ConfigureDatatypes] The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\n",
					"    \n",
					"    # Success: Validate that the datatype-specific conversion functions (add_decimal_datatype and add_varchar_datatype) are not invoked when there are no value-matches in the dictionary\n",
					"    def test_configuredatatypes_sparkdataframeclass_functioncalls_nocalls(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['string']\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the datatype-specific functions have not been called\n",
					"        self.mock_add_decimal_datatype.assert_not_called()\n",
					"        self.mock_add_varchar_datatype.assert_not_called()\n",
					"\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a value 'decimal(&lt;xx&gt;,&lt;yy&gt;', the function add_decimal_datatype is called\n",
					"    def test_configuredatatypes_sparkdataframeclass_functioncalls_decimalfunction(self):        \n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['decimal(12,5)']\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the function has been called exactly once\n",
					"        self.mock_add_decimal_datatype.assert_called_with(decimal_value='decimal(12,5)', datatypes=ANY)\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a generic value 'decimal' and specific 'decimal(&lt;xx&gt;,&lt;yy&gt;), \n",
					"    # the function add_decimal_datatype is called for each reference\n",
					"    def test_configuredatatypes_sparkdataframeclass_functioncalls_decimalfunctiontwice(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['decimal', 'decimal(12,5)']\n",
					"        ## Define the list of expected function-calls to add_decimal_datatype\n",
					"        expected_calls = [call(decimal_value='decimal', datatypes=ANY), call(decimal_value='decimal(12,5)', datatypes=ANY)]\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the function has been called for each instance of 'decimal' \n",
					"        self.mock_add_decimal_datatype.assert_has_calls(expected_calls, any_order=True)\n",
					"\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a generic value 'varchar(max)', the function add_varchar_datatype is called\n",
					"    def test_configuredatatypes_sparkdataframeclass_functioncalls_varcharfunction(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['varchar(max)']\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the function add_varchar_datatype() has been called exactly once\n",
					"        self.mock_add_varchar_datatype.assert_called_once()\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a specific value 'varchar(4)', the function add_varchar_datatype is called\n",
					"    def test_configuredatatypes_sparkdataframeclass_functioncalls_varcharfunction_2(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['varchar(4)']\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the function has been called exactly once\n",
					"        self.mock_add_varchar_datatype.assert_called_with(varchar_value='varchar(4)', datatypes=ANY)\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a generic value 'varchar(max)' and a specific value 'varchar(4)', \n",
					"    # the function add_varchar_datatype is called for each reference\n",
					"    def test_configuredatatypes_sparkdataframeclass_functioncalls_varcharfunctiontwice(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['varchar(max)', 'varchar(4)']\n",
					"\n",
					"        # Define the list of expected function-calls to add_varchar_datatype\n",
					"        expected_calls = [call(varchar_value='varchar(max)', datatypes=ANY), call(varchar_value='varchar(4)', datatypes=ANY)]\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the function has been called for each instance of 'varchar'\n",
					"        self.mock_add_varchar_datatype.assert_has_calls(expected_calls, any_order=True)\n",
					"\n",
					"\n",
					"    # Failure: Test that if an invalid (or non-configured) datatype is passed, an error is thrown\n",
					"    def test_configuredatatypes_sparkdataframeclass_failure_invaliddatatype(self):\n",
					"        # PREPROCESS\n",
					"        ## Define the argument to call the configure_datatypes method with\n",
					"        table_data_types = ['string', 'invalid_datatype']\n",
					"\n",
					"        # Expect the following error to be thrown:\n",
					"        expected_error = f\"The datatype 'invalid_datatype' has not been configured as a datatype in the current set-up.\"\n",
					"\n",
					"        # EXECUTE\n",
					"        ## [Dataframes_v2:SparkDataFrameClass]: Call the configure_datatypes() method of the SparkDataFrameClass with an invalid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            self.class_object.configure_datatypes(table_data_types=table_data_types)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the thrown error is equal to the expected thrown error\n",
					"        self.assertEqual(expected_error, actual_error,  f'[Dataframes_v2:SparkDataFrameClass:ConfigureDatatypes] The returned error from the add_decimal_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_configuredatatypes)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\n",
					"# This class will test the convert_firstrow_to_columns() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameClass_convertfirstrowtocolumns(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"\n",
					"        self.validate_column_indexes_patcher = patch.object(SparkDataFrameClass, 'validate_column_indexes')\n",
					"        self.mock_validate_column_indexes = self.validate_column_indexes_patcher.start()\n",
					"\n",
					"        self.write_dataframe_patcher = patch.object(SparkDataFrameClass, 'write_dataframe')\n",
					"        self.mock_write_dataframe = self.write_dataframe_patcher.start()\n",
					"\n",
					"        self.rename_dataframe_file_patcher = patch.object(SparkDataFrameClass, 'rename_dataframe_file')\n",
					"        self.mock_rename_dataframe_file = self.rename_dataframe_file_patcher.start()\n",
					"\n",
					"        self.source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/convert_firstrow_to_columns/firstrow_to_headers.fil'\n",
					"        \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        self.validate_column_indexes_patcher.stop()\n",
					"        self.rename_dataframe_file_patcher.stop()\n",
					"        self.write_dataframe_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the convert_firstrow_to_columns() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_convertfirstrowtocolumns_sparkdataframeclass_success(self):\n",
					"        # PREPROCESS\n",
					"        start_index         = 1\n",
					"        end_index           = 2\n",
					"        column_names        = [\"name\", \"type\", \"hp\", \"attack\", \"catch_date\", \"state\"]\n",
					"        header              = False\n",
					"        destination_path    = 'destination_path.csv'\n",
					"\n",
					"        expected_catch_date = '2024-08-19'\n",
					"        expected_status     = 'captured'\n",
					"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        class_object.load_dataframe(source_path=self.source_path, header=header, separator='\\t', file_kind='csv')\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the convert_firstrow_to_columns() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.convert_firstrow_to_columns(start_index=start_index, end_index=end_index, column_names=column_names, header=header, destination_path=destination_path, file_extension='csv', column_delimiter=Mock())\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_column_names = class_object.dataframe.columns\n",
					"        actual_catch_date   = class_object.dataframe.select(\"catch_date\").distinct().rdd.map(lambda x: x.catch_date).collect()\n",
					"        actual_status       = class_object.dataframe.select(\"state\").distinct().rdd.map(lambda x: x.state).collect()\n",
					"\n",
					"        self.assertEqual(column_names,          actual_column_names,    f\"[Dataframes_v2:SparkDataFrameClass:ConvertFirstRowToColumns] Expected column names were not found in final dataframe: Expected {column_names} versus Actual {actual_column_names}\")\n",
					"        self.assertEqual(1,                     len(actual_catch_date), f\"[Dataframes_v2:SparkDataFrameClass:ConvertFirstRowToColumns] Number of distinct values expected in literal column 'catch_date' is not 1: {actual_catch_date}\")\n",
					"        self.assertEqual(expected_catch_date,   actual_catch_date[0],   f\"[Dataframes_v2:SparkDataFrameClass:ConvertFirstRowToColumns] Expected literal for 'catch_date' did not match expectations:  {actual_catch_date}\")\n",
					"        self.assertEqual(1,                     len(actual_status),     f\"[Dataframes_v2:SparkDataFrameClass:ConvertFirstRowToColumns] Number of distinct values expected in literal column 'status' is not 1: {actual_status}\")\n",
					"        self.assertEqual(expected_status,       actual_status[0],       f\"[Dataframes_v2:SparkDataFrameClass:ConvertFirstRowToColumns] Expected literal for 'status' did not match expectations: {actual_status}\")\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \n",
					"    def test_convertfirstrowtocolumns_sparkdataframeclass_functioncalls(self):\n",
					"        # PREPROCESS\n",
					"        start_index         = 1\n",
					"        end_index           = 2\n",
					"        column_names        = [\"name\", \"type\", \"hp\", \"attack\", \"catch_date\", \"state\"]\n",
					"        header              = False\n",
					"        destination_path    = 'destination_path.csv'\n",
					"        file_extension      = 'csv'\n",
					"        column_delimiter    =Mock()\n",
					"\n",
					"        expected_catch_date = '2024-08-19'\n",
					"        expected_status     = 'captured'\n",
					"\n",
					"        class_object = SparkDataFrameClass()\n",
					"        class_object.load_dataframe(source_path=self.source_path, header=header, separator='\\t', file_kind='csv')\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the convert_firstrow_to_columns() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        class_object.convert_firstrow_to_columns(start_index=start_index, end_index=end_index, column_names=column_names, header=header, destination_path=destination_path, file_extension=file_extension, column_delimiter=column_delimiter)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that the mocked functions have been called\n",
					"        self.mock_validate_column_indexes.assert_called_once_with(start_index=start_index, end_index=end_index, column_names=column_names, header=header)\n",
					"        self.mock_write_dataframe.assert_called_once_with(write_format=ANY, write_mode='overwrite', destination_path='destination_path', column_delimiter=column_delimiter)\n",
					"        self.mock_rename_dataframe_file.assert_called_once_with(source_path='destination_path', file_extension=file_extension)\n",
					"\n",
					"\n",
					"    def test_convertfirstrowtocolumns_sparkdataframeclass_raiseerror(self):\n",
					"        # PREPROCESS\n",
					"        class_object        = SparkDataFrameClass()\n",
					"        expected_error      = \"[SparkDataFrameClass] There is no dataframe attribute defined. Cannot convert first row to columns. Failing task...\"\n",
					"        start_index         = 1\n",
					"        end_index           = 2\n",
					"        column_names        = [\"name\", \"type\", \"hp\", \"attack\", \"catch_date\", \"state\"]\n",
					"        header              = False\n",
					"        destination_path    = 'destination_path.csv'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the convert_firstrow_to_columns() method of the SparkDataFrameClass with a valid set of arguments\n",
					"        with self.assertRaises(AttributeError) as error:\n",
					"            class_object.convert_firstrow_to_columns(start_index=start_index, end_index=end_index, column_names=column_names, header=header, destination_path=destination_path, file_extension=Mock(), column_delimiter=Mock())\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:SparkDataFrameClass:AddLoggingColumnsToDataframe] Expected error to be thrown when dataframe does not exist when adding logging columns. Got {actual_error}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_convertfirstrowtocolumns)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\r\n",
					"# This class will test the load_dataframe_with_schema() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\r\n",
					"class Test_SparkDataFrameClass_loaddataframewithschema(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        self.source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/load_dataframe_with_schema/load_dataframe_with_schema.csv'\r\n",
					"        self.header = True\r\n",
					"\r\n",
					"        self.column_info = [\r\n",
					"            {\"column_name\": \"DecimalUS\", \"data_type\": \"decimal(5,3)\"},\r\n",
					"            {\"column_name\": \"DecimalFR\", \"data_type\": \"decimal(5,3)\"},\r\n",
					"            {\"column_name\": \"TimestampUS\", \"data_type\": \"decimal(5,3)\"},\r\n",
					"            {\"column_name\": \"TimestampFR\", \"data_type\": \"decimal(5,3)\"},\r\n",
					"            {\"column_name\": \"DateUS\", \"data_type\": \"decimal(5,3)\"},\r\n",
					"            {\"column_name\": \"DateFR\", \"data_type\": \"decimal(5,3)\"},\r\n",
					"            {\"column_name\": \"DateNoSeparator\", \"data_type\": \"decimal(5,3)\"}\r\n",
					"        ]            \r\n",
					"\r\n",
					"        self.separator:str=';'\r\n",
					"        \r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown Test Case class\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the load_dataframe_with_schema() method\r\n",
					"\r\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \r\n",
					"    def test_loaddataframewithschema_sparkdataframeclass_success_matchingFRlocale(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set class_object arguments\r\n",
					"        class_object        = SparkDataFrameClass()\r\n",
					"        structfield_objects = class_object.create_structfield_objects(self.column_info)\r\n",
					"        schema              = StructType(structfield_objects)\r\n",
					"        column_to_select    = \"DecimalFR\"\r\n",
					"        column_selection    = [column_to_select]\r\n",
					"        locale              = 'fr-FR'\r\n",
					"\r\n",
					"        # Set expected values\r\n",
					"        expected_corrupt_records = 1\r\n",
					"        expected_valid_records   = 2\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe_with_schema() method of the SparkDataFrameClass with a valid set of arguments\r\n",
					"        dataframe = class_object.load_dataframe_with_schema(source_path=self.source_path, header=self.header,schema=schema,column_selection=column_selection,separator=self.separator,locale=locale)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Get the actual number of corrupt and non-corrupt records\r\n",
					"        dataframe.cache()\r\n",
					"        actual_corrupt_records  = dataframe.filter(col('_corrupt_record').isNotNull()).count()\r\n",
					"        actual_valid_records    = dataframe.filter(col(column_to_select).isNotNull()).count()\r\n",
					"        dataframe.unpersist()\r\n",
					"        # Compare actuals to expectations\r\n",
					"        self.assertEqual(expected_corrupt_records,  actual_corrupt_records, f\"[Dataframes:SparkDataFrameClass:LoadDataframeWithSchema] 1. Expected number of corrupt records did not match actuals\")\r\n",
					"        self.assertEqual(expected_valid_records,    actual_valid_records,   f\"[Dataframes:SparkDataFrameClass:LoadDataframeWithSchema] 1. Expected number of valid records did not match actuals\")\r\n",
					"\r\n",
					"    def test_loaddataframewithschema_sparkdataframeclass_success_notmatchingLocale(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set class_object arguments\r\n",
					"        class_object        = SparkDataFrameClass()\r\n",
					"        structfield_objects = class_object.create_structfield_objects(self.column_info)\r\n",
					"        schema              = StructType(structfield_objects)\r\n",
					"        column_to_select    = \"DecimalFR\"\r\n",
					"        column_selection    = [column_to_select]\r\n",
					"        locale              = 'en-US'\r\n",
					"\r\n",
					"        # Set expected values\r\n",
					"        expected_corrupt_records = 2\r\n",
					"        expected_valid_records   = 1\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe_with_schema() method of the SparkDataFrameClass with a valid set of arguments\r\n",
					"        dataframe = class_object.load_dataframe_with_schema(source_path=self.source_path, header=self.header,schema=schema,column_selection=column_selection,separator=self.separator,locale=locale)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Get the actual number of corrupt and non-corrupt records\r\n",
					"        dataframe.cache()\r\n",
					"        actual_corrupt_records  = dataframe.filter(col('_corrupt_record').isNotNull()).count()\r\n",
					"        actual_valid_records    = dataframe.filter(col(column_to_select).isNotNull()).count()\r\n",
					"        dataframe.unpersist()\r\n",
					"\r\n",
					"        # Compare actuals to expectations\r\n",
					"        self.assertEqual(expected_corrupt_records,  actual_corrupt_records, f\"[Dataframes:SparkDataFrameClass:LoadDataframeWithSchema] 2. Expected number of corrupt records did not match actuals\")\r\n",
					"        self.assertEqual(expected_valid_records,    actual_valid_records,   f\"[Dataframes:SparkDataFrameClass:LoadDataframeWithSchema] 2. Expected number of valid records did not match actuals\")\r\n",
					"\r\n",
					"    def test_loaddataframewithschema_sparkdataframeclass_success_matchingUSLocale(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set class_object arguments\r\n",
					"        class_object        = SparkDataFrameClass()\r\n",
					"        structfield_objects = class_object.create_structfield_objects(self.column_info)\r\n",
					"        schema              = StructType(structfield_objects)\r\n",
					"        column_to_select    = \"DecimalUS\"\r\n",
					"        column_selection    = [column_to_select]\r\n",
					"        locale              = 'en-US'\r\n",
					"        \r\n",
					"        # Set expected values\r\n",
					"        expected_corrupt_records = 1\r\n",
					"        expected_valid_records   = 2\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the load_dataframe_with_schema() method of the SparkDataFrameClass with a valid set of arguments\r\n",
					"        dataframe = class_object.load_dataframe_with_schema(source_path=self.source_path, header=self.header,schema=schema,column_selection=column_selection,separator=self.separator,locale=locale)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Get the actual number of corrupt and non-corrupt records\r\n",
					"        dataframe.cache()\r\n",
					"        actual_corrupt_records  = dataframe.filter(col('_corrupt_record').isNotNull()).count()\r\n",
					"        actual_valid_records    = dataframe.filter(col(column_to_select).isNotNull()).count()\r\n",
					"        dataframe.unpersist()\r\n",
					"\r\n",
					"        # Compare actuals to expectations\r\n",
					"        self.assertEqual(expected_corrupt_records,  actual_corrupt_records, f\"[Dataframes:SparkDataFrameClass:LoadDataframeWithSchema] 3. Expected number of corrupt records did not match actuals\")\r\n",
					"        self.assertEqual(expected_valid_records,    actual_valid_records,   f\"[Dataframes:SparkDataFrameClass:LoadDataframeWithSchema] 3. Expected number of valid records did not match actuals\")\r\n",
					"\r\n",
					"   \r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_loaddataframewithschema)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameClass\r\n",
					"# This class will test the replace_dataframe_columns() of the SparkDataFrameClass-class defined in the Dataframes_v2 notebook\r\n",
					"class Test_SparkDataFrameClass_replacedataframecolumns(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        self.source_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameClass/replace_dataframe_columns/replace_dataframe_columns.csv'\r\n",
					"        self.header = True\r\n",
					"        self.separator:str=';'\r\n",
					"        self.kind = 'csv'\r\n",
					"        \r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown Test Case class\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the replace_dataframe_columns() method\r\n",
					"\r\n",
					"    # Success: Initialize a SparkDataFrameClass class-object with valid arguments \r\n",
					"    def test_replacedataframecolumns_sparkdataframeclass_success_replaceOneColumn(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set class_object arguments\r\n",
					"        class_object        = SparkDataFrameClass()\r\n",
					"        class_object.load_dataframe(source_path=self.source_path, header=self.header, separator=self.separator, file_kind=self.kind)\r\n",
					"        dataframe = class_object.dataframe\r\n",
					"\r\n",
					"        columns_to_replace= ['Column1']\r\n",
					"        dataframe = dataframe.withColumn('Column1', lit(\"default\"))\r\n",
					"        dataframe = dataframe.withColumn('Column2', lit(2))\r\n",
					"\r\n",
					"        # Set expected values\r\n",
					"        expected_total_rows         = 4\r\n",
					"        expected_default_occurences = 4\r\n",
					"        expected_2_occurences       = 0\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the replace_dataframe_columns() method of the SparkDataFrameClass with a valid set of arguments\r\n",
					"        class_object.replace_dataframe_columns(replacing_dataframe=dataframe, columns_to_replace=columns_to_replace)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Get the actual number of value occurances in the table\r\n",
					"        actual_total_rows               = class_object.dataframe.count()\r\n",
					"        actual_count_default_occurences = class_object.dataframe.filter(col('Column1') == 'default').count()\r\n",
					"        actual_count_2_occurences       = class_object.dataframe.filter(col('Column2') == 2).count()\r\n",
					"\r\n",
					"        # Compare actuals to expectations\r\n",
					"        self.assertEqual(expected_total_rows,           actual_total_rows,               f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 1. Expected number of total rows did not match actuals\")\r\n",
					"        self.assertEqual(expected_default_occurences,   actual_count_default_occurences, f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 1. Expected number of default values did not match actuals\")\r\n",
					"        self.assertEqual(expected_2_occurences,         actual_count_2_occurences,       f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 1. Expected number of 2 values did not match actuals\")\r\n",
					"\r\n",
					"\r\n",
					"    def test_replacedataframecolumns_sparkdataframeclass_success_replaceTwoColumns(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set class_object arguments\r\n",
					"        class_object        = SparkDataFrameClass()\r\n",
					"        class_object.load_dataframe(source_path=self.source_path, header=self.header, separator=self.separator, file_kind=self.kind)\r\n",
					"        dataframe = class_object.dataframe\r\n",
					"\r\n",
					"        columns_to_replace= ['Column1', 'Column2']\r\n",
					"        dataframe = dataframe.withColumn('Column1', lit(\"default\"))\r\n",
					"        dataframe = dataframe.withColumn('Column2', lit(2))\r\n",
					"\r\n",
					"        # Set expected values\r\n",
					"        expected_total_rows         = 4\r\n",
					"        expected_default_occurences = 4\r\n",
					"        expected_2_occurences       = 4\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the replace_dataframe_columns() method of the SparkDataFrameClass with a valid set of arguments\r\n",
					"        class_object.replace_dataframe_columns(replacing_dataframe=dataframe, columns_to_replace=columns_to_replace)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Get the actual number of value occurances in the table\r\n",
					"        actual_total_rows               = class_object.dataframe.count()\r\n",
					"        actual_count_default_occurences = class_object.dataframe.filter(col('Column1') == 'default').count()\r\n",
					"        actual_count_2_occurences       = class_object.dataframe.filter(col('Column2') == 2).count()\r\n",
					"\r\n",
					"        # Compare actuals to expectations\r\n",
					"        self.assertEqual(expected_total_rows,           actual_total_rows,               f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 2. Expected number of total rows did not match actuals\")\r\n",
					"        self.assertEqual(expected_default_occurences,   actual_count_default_occurences, f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 2. Expected number of default values did not match actuals\")\r\n",
					"        self.assertEqual(expected_2_occurences,         actual_count_2_occurences,       f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 2. Expected number of 2 values did not match actuals\")\r\n",
					"\r\n",
					"    def test_replacedataframecolumns_sparkdataframeclass_success_replaceNoColumns(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Set class_object arguments\r\n",
					"        class_object        = SparkDataFrameClass()\r\n",
					"        class_object.load_dataframe(source_path=self.source_path, header=self.header, separator=self.separator, file_kind=self.kind)\r\n",
					"        dataframe = class_object.dataframe\r\n",
					"\r\n",
					"        columns_to_replace= []\r\n",
					"        dataframe = dataframe.withColumn('Column1', lit(\"default\"))\r\n",
					"        dataframe = dataframe.withColumn('Column2', lit(2))\r\n",
					"\r\n",
					"        # Set expected values\r\n",
					"        expected_total_rows         = 4\r\n",
					"        expected_default_occurences = 0\r\n",
					"        expected_2_occurences       = 0\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # [Dataframes_v2:SparkDataFrameClass]: Call the replace_dataframe_columns() method of the SparkDataFrameClass with a valid set of arguments\r\n",
					"        class_object.replace_dataframe_columns(replacing_dataframe=dataframe, columns_to_replace=columns_to_replace)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Get the actual number of value occurances in the table\r\n",
					"        actual_total_rows               = class_object.dataframe.count()\r\n",
					"        actual_count_default_occurences = class_object.dataframe.filter(col('Column1') == 'default').count()\r\n",
					"        actual_count_2_occurences       = class_object.dataframe.filter(col('Column2') == 2).count()\r\n",
					"\r\n",
					"        # Compare actuals to expectations\r\n",
					"        self.assertEqual(expected_total_rows,           actual_total_rows,               f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 3. Expected number of total rows did not match actuals\")\r\n",
					"        self.assertEqual(expected_default_occurences,   actual_count_default_occurences, f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 3. Expected number of default values did not match actuals\")\r\n",
					"        self.assertEqual(expected_2_occurences,         actual_count_2_occurences,       f\"[Dataframes:SparkDataFrameClass:ReplaceDataframeColumns] 3. Expected number of 2 values did not match actuals\")\r\n",
					"   \r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameClass_replacedataframecolumns)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in SparkDataFrameClass loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Test Class: SparkDataFrameChecks"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the __init__() of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameChecks_initialization(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):   \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown TestCass class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the __init__() method\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameChecks class-object with valid arguments \n",
					"    def test_initialization_SparkDataFrameChecks_emptychecklist(self):\n",
					"        # PREPROCESS\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameChecks/initialization/initialize_checks_object.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind     = 'csv'\n",
					"        column_names    = ['date', 'string']\n",
					"        data_types      = ['timestamp', 'string']\n",
					"        checks          = []\n",
					"        column_information     = [{\"locale\": \"nl-NL\"}, {\"format\": \"yyyy-MM-dd\"}]\n",
					"\n",
					"\n",
					"        expected_checks =  ['header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n",
					"        expected_row_count = 3\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the __init__() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that each instance is an instance of class StructField\n",
					"        actual_checks       = class_object.checks\n",
					"        actual_column_names = class_object.dataframe.columns\n",
					"        actual_row_count    = class_object.dataframe.count()\n",
					"        \n",
					"        self.assertEqual(expected_checks,               actual_checks,       f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Class member 'checks' does not contain expected values: Expected {expected_checks} vs Actual {actual_checks}\")\n",
					"        self.assertEqual(column_names,                  actual_column_names, f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Loaded dataframe does not contain expected column names: Expected {column_names} vs Actual {actual_column_names}\")\n",
					"        self.assertEqual(expected_row_count,            actual_row_count,    f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Loaded dataframe does not contain expected row count: Expected {expected_row_count} vs Actual {actual_row_count}\")\n",
					"        self.assertIsInstance(class_object.dataframe,   DataFrame,           f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Loaded dataframe is not of class 'pyspark.sql.DataFrame'\")\n",
					"\n",
					"\n",
					"    # Success: Initialize a SparkDataFrameChecks class-object with valid arguments \n",
					"    def test_initialization_SparkDataFrameChecks_nonemptychecklist(self):\n",
					"        # PREPROCESS\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameChecks/initialization/initialize_checks_object.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind     = 'csv'\n",
					"        column_names    = ['date', 'string']\n",
					"        data_types      = ['timestamp', 'string']\n",
					"        checks          = ['landing_rows', 'header']\n",
					"        column_information     = [{\"locale\": \"nl-NL\"}, {\"format\": \"yyyy-MM-dd\"}]\n",
					"\n",
					"        expected_checks =  ['landing_rows', 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n",
					"        expected_row_count = 3\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the __init__() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Validate that each instance is an instance of class StructField\n",
					"        actual_checks       = class_object.checks\n",
					"        actual_column_names = class_object.dataframe.columns\n",
					"        actual_row_count    = class_object.dataframe.count()\n",
					"        \n",
					"        self.assertEqual(expected_checks,               actual_checks,       f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Class member 'checks' does not contain expected values: Expected {expected_checks} vs Actual {actual_checks}\")\n",
					"        self.assertEqual(column_names,                  actual_column_names, f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Loaded dataframe does not contain expected column names: Expected {column_names} vs Actual {actual_column_names}\")\n",
					"        self.assertEqual(expected_row_count,            actual_row_count,    f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Loaded dataframe does not contain expected row count: Expected {expected_row_count} vs Actual {actual_row_count}\")\n",
					"        self.assertIsInstance(class_object.dataframe,   DataFrame,           f\"[Dataframes_v2:SparkDataFrameChecks:Initialization] Loaded dataframe is not of class 'pyspark.sql.DataFrame'\")\n",
					"    \n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_initialization)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\r\n",
					"# This class will test the replace_value function of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\r\n",
					"class Test_SparkDataFrameChecks_replace_value(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):   \r\n",
					"        self.load_dataframe_patcher        = patch.object(SparkDataFrameChecks, 'load_dataframe')\r\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown TestCass class\r\n",
					"        self.load_dataframe_patcher.stop()\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful replace_value function\r\n",
					"    # Success:replace the null pk value in the dataframe \r\n",
					"    def test_check_pk_values_to_replace_with_a_integer(self):\r\n",
					"        # PREPROCESS\r\n",
					"        source_path = Mock()\r\n",
					"        header=True\r\n",
					"        separator=';'\r\n",
					"        file_kind='csv'\r\n",
					"        column_names=['primary_key','column_1']\r\n",
					"        data_types=['integer','string']\r\n",
					"        checks=[]\r\n",
					"        expected_replace_value=0\r\n",
					"        column_information=[\r\n",
					"                    {\r\n",
					"                        \"replace_value\": expected_replace_value\r\n",
					"                    },\r\n",
					"                    {\r\n",
					"                        None\r\n",
					"                    }]\r\n",
					"        self.schema = sql.types.StructType([\r\n",
					"            sql.types.StructField(\"primary_key\", sql.types.IntegerType(), True),\r\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True)\r\n",
					"        ])\r\n",
					"\r\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n",
					"        data_types=data_types, checks=checks, column_information=column_information)\r\n",
					"        class_object.dataframe = spark.createDataFrame(data=[(None,'test'),(1,'test1')], schema=self.schema)\r\n",
					"        # # EXECUTE\r\n",
					"        actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n",
					"        First_row_value = class_object.dataframe.first()\r\n",
					"        actual_replace_value = First_row_value.primary_key\r\n",
					"        # EVALUATE\r\n",
					"        ## Validate that the null pk has been replaced\r\n",
					"        self.assertEqual(expected_replace_value,actual_replace_value,f\"[Dataframes_v2:SparkDataFrameChecks:pk_values_to_replace] The dataframe did not change as expected for an integer value\")\r\n",
					"    \r\n",
					"    # Success:replace the null pk value in the dataframe \r\n",
					"    def test_check_pk_values_to_replace_with_a_string(self):\r\n",
					"        # PREPROCESS\r\n",
					"        source_path = Mock()\r\n",
					"        header=True\r\n",
					"        separator=';'\r\n",
					"        file_kind='csv'\r\n",
					"        column_names=['primary_key','column_1']\r\n",
					"        data_types=['string','string']\r\n",
					"        checks=[]\r\n",
					"        expected_replace_value=\"angry_bird\"\r\n",
					"        column_information=[\r\n",
					"                    {\r\n",
					"                        \"replace_value\": expected_replace_value\r\n",
					"                    },\r\n",
					"                    {\r\n",
					"                        None\r\n",
					"                    }]\r\n",
					"        self.schema = sql.types.StructType([\r\n",
					"            sql.types.StructField(\"primary_key\", sql.types.StringType(), True),\r\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True)\r\n",
					"        ])\r\n",
					"\r\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n",
					"        data_types=data_types, checks=checks, column_information=column_information)\r\n",
					"        class_object.dataframe = spark.createDataFrame(data=[(None,'test'),(1,'test1')], schema=self.schema)\r\n",
					"        # # EXECUTE\r\n",
					"        actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n",
					"        First_row_value = class_object.dataframe.first()\r\n",
					"        actual_replace_value = First_row_value.primary_key\r\n",
					"        # EVALUATE\r\n",
					"        ## Validate that the null pk has been replaced\r\n",
					"        self.assertEqual(expected_replace_value,actual_replace_value,f\"[Dataframes_v2:SparkDataFrameChecks:pk_values_to_replace] The dataframe did not change as expected for an string value\")\r\n",
					"    \r\n",
					"    # Success:replace the null pk value in the dataframe and return 0\r\n",
					"    def test_check_pk_values_to_replace_with_the_option(self):\r\n",
					"        # PREPROCESS\r\n",
					"        source_path = Mock()\r\n",
					"        header=True\r\n",
					"        separator=';'\r\n",
					"        file_kind='csv'\r\n",
					"        column_names=['primary_key','column_1']\r\n",
					"        data_types=['integer','string']\r\n",
					"        checks=[]\r\n",
					"        column_information=[\r\n",
					"                    {\r\n",
					"                        \"replace_value\": \"0\"\r\n",
					"                    },\r\n",
					"                    {\r\n",
					"                        None\r\n",
					"                    }]\r\n",
					"\r\n",
					"        expected_result=0\r\n",
					"\r\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n",
					"        data_types=data_types, checks=checks, column_information=column_information)\r\n",
					"        # EXECUTE\r\n",
					"        actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n",
					"        # EVALUATE\r\n",
					"        ## Validate that the null pk has been replaced\r\n",
					"        self.assertEqual(expected_result,actual_result,f\"[Dataframes_v2:SparkDataFrameChecks:pk_values_to_replace] the primary value to replace 'checks' didn't work as expected\")\r\n",
					"    \r\n",
					"    # Failure:the 'column_information' isn't a dict. We should return 1\r\n",
					"    def test_check_pk_values_to_replace_with_set(self):\r\n",
					"        # PREPROCESS\r\n",
					"        source_path = Mock()\r\n",
					"        header=True\r\n",
					"        separator=';'\r\n",
					"        file_kind='csv'\r\n",
					"        column_names=['primary_key','column_1']\r\n",
					"        data_types=['integer','string']\r\n",
					"        checks=[]\r\n",
					"        column_information=[\r\n",
					"                    {\r\n",
					"                        None\r\n",
					"                    },\r\n",
					"                    {\r\n",
					"                        None\r\n",
					"                    }]\r\n",
					"\r\n",
					"        expected_result=1\r\n",
					"\r\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n",
					"        data_types=data_types, checks=checks, column_information=column_information)\r\n",
					"        # EXECUTE\r\n",
					"        actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n",
					"        # EVALUATE\r\n",
					"        ## Validate that the null pk has been replaced\r\n",
					"        self.assertEqual(expected_result,actual_result,f\"[Dataframes_v2:SparkDataFrameChecks:pk_values_to_replace] the primary value to replace 'checks' didn't work as expected\")\r\n",
					"    \r\n",
					"    # Failure:the 'column_information' doesn't contain 'replace_value' key. We should return 1\r\n",
					"    def test_check_pk_values_to_replace_with_the_option(self):\r\n",
					"        # PREPROCESS\r\n",
					"        source_path = Mock()\r\n",
					"        header=True\r\n",
					"        separator=';'\r\n",
					"        file_kind='csv'\r\n",
					"        column_names=['primary_key','column_1']\r\n",
					"        data_types=['integer','string']\r\n",
					"        checks=[]\r\n",
					"        column_information=[\r\n",
					"                    {\r\n",
					"                        \"optional\": \"0\"\r\n",
					"                    },\r\n",
					"                    {\r\n",
					"                        None\r\n",
					"                    }]\r\n",
					"\r\n",
					"        expected_result=1\r\n",
					"\r\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, \r\n",
					"        data_types=data_types, checks=checks, column_information=column_information)\r\n",
					"        # EXECUTE\r\n",
					"        actual_result=class_object.check_pk_values_to_replace(\"primary_key\")\r\n",
					"        # EVALUATE\r\n",
					"        ## Validate that the null pk has been replaced\r\n",
					"        self.assertEqual(expected_result,actual_result,f\"[Dataframes_v2:SparkDataFrameChecks:pk_values_to_replace] the primary value to replace 'checks' didn't work as expected\")\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_replace_value)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Dev-Info**: The Test_SparkDataFrameChecks_startchecks class will implicitly test get_checks_for_phase and filter_checks. Therefore, no reparate Test Case for these functions has been implemented."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the start_checks() of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameChecks_startchecks(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Mock all check-methods -&gt; Check if the functions are called\n",
					"        self.check_minimum_rowcount_patcher     = patch.object(SparkDataFrameChecks, 'check_minimum_rowcount')\n",
					"        self.mock_check_minimum_rowcount        = self.check_minimum_rowcount_patcher.start()\n",
					"        \n",
					"        self.check_dataframe_headers_patcher    = patch.object(SparkDataFrameChecks, 'check_dataframe_headers')\n",
					"        self.mock_check_dataframe_headers       = self.check_dataframe_headers_patcher.start()\n",
					"\n",
					"        self.check_primary_keys_patcher         = patch.object(SparkDataFrameChecks, 'check_primary_keys')\n",
					"        self.mock_check_primary_keys            = self.check_primary_keys_patcher.start()\n",
					"\n",
					"        self.check_duplicate_primary_keys_patcher         = patch.object(SparkDataFrameChecks, 'check_duplicate_primary_keys')\n",
					"        self.mock_check_duplicate_primary_keys            = self.check_duplicate_primary_keys_patcher.start()\n",
					"\n",
					"        self.check_dataframe_datatypes_patcher   = patch.object(SparkDataFrameChecks, 'check_dataframe_datatypes')\n",
					"        self.mock_check_dataframe_datatypes     = self.check_dataframe_datatypes_patcher.start()\n",
					"\n",
					"        # Mock validate_parameters_keys -&gt; Since no checks will be executed, there is no need to validate the parameter-argument\n",
					"        self.validate_parameter_keys_patcher   = patch.object(SparkDataFrameChecks, 'validate_parameter_keys')\n",
					"        self.mock_validate_parameter_keys     = self.validate_parameter_keys_patcher.start() \n",
					"\n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \n",
					"\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        self.class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        # Stop all patcher-functions\n",
					"        self.check_minimum_rowcount_patcher.stop()\n",
					"        self.check_dataframe_headers_patcher.stop()\n",
					"        self.check_primary_keys_patcher.stop()\n",
					"        self.check_dataframe_datatypes_patcher.stop()\n",
					"        self.check_duplicate_primary_keys_patcher.stop()\n",
					"        self.load_dataframe_patcher.stop()\n",
					"        self.validate_parameter_keys_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the start_checks() method\n",
					"\n",
					"    # Success: Call start_checks() with a non-empty list of checks for phase 'landing'\n",
					"    def test_startchecks_SparkDataFrameChecks_executelandingchecks(self):\n",
					"        # PREPROCESS\n",
					"        # # Set the checks-argument to a non-empty list of checks\n",
					"        self.class_object.checks = ['landing_rows', 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n",
					"        # # Define the arguments to use when calling the start_checks() method\n",
					"        phase = 'landing'\n",
					"        parameters = 'parameter_object'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the start_checks() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.start_checks(phase=phase, parameters=parameters)\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        self.mock_check_minimum_rowcount.assert_called_once()\n",
					"        self.mock_check_dataframe_headers.assert_not_called()\n",
					"        self.mock_check_primary_keys.assert_not_called()\n",
					"        self.mock_check_dataframe_datatypes.assert_not_called()\n",
					"        self.mock_check_duplicate_primary_keys.assert_not_called()\n",
					"        self.mock_validate_parameter_keys.assert_called_once_with(parameters=parameters)\n",
					"\n",
					"    # Success: Call start_checks() with an empty list of checks for phase 'landing'\n",
					"    def test_startchecks_SparkDataFrameChecks_executelandingchecks_emptychecklist(self):\n",
					"        # PREPROCESS\n",
					"        # # Set the checks-argument to an empty list of checks\n",
					"        self.class_object.checks = []\n",
					"        # # Define the arguments to use when calling the start_checks() method\n",
					"        phase = 'landing'\n",
					"        parameters = 'parameter_object'\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the start_checks() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.start_checks(phase=phase, parameters=parameters)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        self.mock_check_minimum_rowcount.assert_not_called()\n",
					"        self.mock_check_dataframe_headers.assert_not_called()\n",
					"        self.mock_check_primary_keys.assert_not_called()\n",
					"        self.mock_check_duplicate_primary_keys.assert_not_called()\n",
					"        self.mock_check_dataframe_datatypes.assert_not_called()\n",
					"        self.mock_validate_parameter_keys.assert_not_called()\n",
					"\n",
					"    # Success: Call start_checks() on a clean class-instance with an empty list of checks for phase 'landing'\n",
					"    def test_startchecks_SparkDataFrameChecks_executelandingchecks_localclassobject(self):\n",
					"        # PREPROCESS\n",
					"        # # Initialize a clean class-instance and use the self.checks-argument defined during initialisation\n",
					"        # # Dev-Info: Expect self.checks = [ 'header', 'data_type', 'primary_keys']\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        local_class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # # Define the arguments to use when calling the start_checks() method\n",
					"        phase = 'landing'\n",
					"        parameters = 'parameter_object'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the start_checks() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.start_checks(phase=phase, parameters=parameters)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        self.mock_check_minimum_rowcount.assert_not_called()\n",
					"        self.mock_check_dataframe_headers.assert_not_called()\n",
					"        self.mock_check_primary_keys.assert_not_called()\n",
					"        self.mock_check_duplicate_primary_keys.assert_not_called()\n",
					"        self.mock_check_dataframe_datatypes.assert_not_called()\n",
					"        self.mock_validate_parameter_keys.assert_not_called()\n",
					"\n",
					"\n",
					"    # Success: Call start_checks() with a non-empty list of checks for phase 'landing'\n",
					"    def test_startchecks_SparkDataFrameChecks_executerawchecks(self):\n",
					"        # PREPROCESS\n",
					"        self.class_object.checks = ['landing_rows', 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys']\n",
					"        # # Define the arguments to use when calling the start_checks() method\n",
					"        phase = 'raw'\n",
					"        parameters = 'parameter_object'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the start_checks() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.start_checks(phase=phase, parameters=parameters)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        self.mock_check_minimum_rowcount.assert_not_called()\n",
					"        self.mock_check_dataframe_headers.assert_called_once()\n",
					"        self.mock_check_primary_keys.assert_called_once()\n",
					"        self.mock_check_duplicate_primary_keys.assert_called_once()\n",
					"        self.mock_check_dataframe_datatypes.assert_called_once()\n",
					"        self.mock_validate_parameter_keys.assert_called_once_with(parameters=parameters)\n",
					"\n",
					"\n",
					"    # Success: Call start_checks() on a clean class-instance with an empty list of checks for phase 'landing'\n",
					"    def test_startchecks_SparkDataFrameChecks_executerawchecks_localclassobject(self):\n",
					"        # PREPROCESS\n",
					"        # # Initialize a clean class-instance and use the self.checks-argument defined during initialisation\n",
					"        # # Dev-Info: Expect self.checks = [ 'header', 'data_type', 'primary_keys', 'duplicate_primary_keys]\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        local_class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # # Define the arguments to use when calling the start_checks() method\n",
					"        phase = 'raw'\n",
					"        parameters = 'parameter_object'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the start_checks() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        local_class_object.start_checks(phase=phase, parameters=parameters)\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        self.mock_check_minimum_rowcount.assert_not_called()\n",
					"        self.mock_check_dataframe_headers.assert_called_once()\n",
					"        self.mock_check_primary_keys.assert_called_once()\n",
					"        self.mock_check_duplicate_primary_keys.assert_called_once()\n",
					"        self.mock_check_dataframe_datatypes.assert_called_once()\n",
					"        self.mock_validate_parameter_keys.assert_called_once_with(parameters=parameters)\n",
					"\n",
					"    # Success: Call start_checks() and expect the validate_parameter_keys() to throw an error on an invalid phase\n",
					"    #   This test guarantees that the validate-function is called inexplicitly\n",
					"    def test_startchecks_SparkDataFrameChecks_failure_invalidphase(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the start_checks() method\n",
					"        phase = 'invalid_phase'\n",
					"        parameters = 'parameter_object'\n",
					"\n",
					"        # # Define the expected return values\n",
					"        expected_error = f\"The phase-argument '{str(phase)}' is not listed in the allowed phase list: ['landing', 'raw', 'silver']\"\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the start_checks() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            self.class_object.start_checks(phase=phase, parameters=parameters)\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes_v2:SparkDataFrameChecks:StartChecks] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual {actual_error}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_startchecks)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the validate_phase() of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameChecks_validatephase(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \n",
					"\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        self.class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        # Stop all patcher-functions\n",
					"        self.load_dataframe_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the validate_phase() method\n",
					"\n",
					"    # Success: Call validate_phase() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_landingphase(self):\n",
					"        # PREPROCESS\n",
					"        # # Set the checks-argument to a non-empty list of checks\n",
					"        # # Define the arguments to use when calling the validate_phase() method\n",
					"        phase = 'landing'\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_phase() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.validate_phase(phase=phase)\n",
					"\n",
					"    # Success: Call validate_phase() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_rawphase(self):\n",
					"        # PREPROCESS\n",
					"        # # Set the checks-argument to a non-empty list of checks\n",
					"        # # Define the arguments to use when calling the validate_phase() method\n",
					"        phase = 'raw'\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_phase() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.validate_phase(phase=phase)\n",
					"\n",
					"    # Success: Call validate_phase() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_silverphase(self):\n",
					"        # PREPROCESS\n",
					"        # # Set the checks-argument to a non-empty list of checks\n",
					"        # # Define the arguments to use when calling the validate_phase() method\n",
					"        phase = 'silver'\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_phase() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object.validate_phase(phase=phase)\n",
					"\n",
					"    # Success: Call validate_phase() and expect an error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_invalidphase(self):\n",
					"        # PREPROCESS\n",
					"        # # Set the checks-argument to a non-empty list of checks\n",
					"        # # Define the arguments to use when calling the validate_phase() method\n",
					"        phase = 'invalid_phase'\n",
					"\n",
					"        # # Define the expected return values\n",
					"        expected_error = f\"The phase-argument '{str(phase)}' is not listed in the allowed phase list: ['landing', 'raw', 'silver']\"\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_phase() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            self.class_object.validate_phase(phase=phase)\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Check that the check functions have (not) been called\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes_v2:SparkDataFrameChecks:StartChecks] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual {actual_error}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_validatephase)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the validate_parameter_keys() of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameChecks_validateparameterkeys(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        # Stop all patcher-functions\n",
					"        self.load_dataframe_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the validate_parameter_keys() method\n",
					"\n",
					"    # Success: Call validate_parameter_keys() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_emptyobject(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the validate_parameter_keys() method\n",
					"        parameters = dict()\n",
					"\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_parameter_keys() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        class_object.validate_parameter_keys(parameters=parameters)\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the parameters-attribute has been set\n",
					"        self.assertTrue(hasattr(class_object, 'parameters'))\n",
					"\n",
					"\n",
					"    # Success: Call validate_parameter_keys() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_validobject(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the validate_parameter_keys() method\n",
					"        parameters = {'landing_rows_expected': -1, 'primary_key_columns': ['column_name']}\n",
					"        \n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_parameter_keys() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        class_object.validate_parameter_keys(parameters=parameters)\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the parameter-attribute has been set\n",
					"        self.assertTrue(hasattr(class_object, 'parameters'))\n",
					"        actual_parameters = class_object.parameters\n",
					"        self.assertEqual(parameters, actual_parameters)\n",
					"\n",
					"    # Failure: Call validate_parameter_keys() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_invalidobject(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the validate_parameter_keys() method\n",
					"        parameters = {'invalid_key': -1, 'other_invalid_key': -1}\n",
					"        expected_error = \"[GenericFunctions] The check_parameters-argument does not take the following list of give parameters: 'invalid_key, other_invalid_key'; Allowed values: landing_rows_expected, primary_key_columns\"\n",
					"        \n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_parameter_keys() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object.validate_parameter_keys(parameters=parameters)\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes_v2:SparkDataFrameChecks:StartChecks] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_error}\")\n",
					"\n",
					"\n",
					"    # Failure: Call validate_parameter_keys() and expect no error to be thrown\n",
					"    def test_validatephase_SparkDataFrameChecks_invalidobject_2(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the validate_parameter_keys() method\n",
					"        parameters = {'invalid_key': -1, 'landing_rows_expected': -1}\n",
					"        expected_error = \"[GenericFunctions] The check_parameters-argument does not take the following list of give parameters: 'invalid_key'; Allowed values: landing_rows_expected, primary_key_columns\"\n",
					"        \n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the validate_parameter_keys() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            class_object.validate_parameter_keys(parameters=parameters)\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes_v2:SparkDataFrameChecks:StartChecks] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_error}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_validateparameterkeys)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the check_minimum_rowcount() of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameChecks_checkminimumrowcount(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        # PREPROCESS\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameChecks/initialization/initialize_checks_object.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind     = 'csv'\n",
					"        column_names    = ['date', 'string']\n",
					"        data_types      = ['timestamp', 'string']\n",
					"        column_information = [None, None]\n",
					"        checks          = ['landing_rows']\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the __init__() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        self.class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the check_minimum_rowcount() method\n",
					"\n",
					"    # Success: Call check_minimum_rowcount() and expect no error to be thrown\n",
					"    def test_checkminimumrowcount_SparkDataFrameChecks_success(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_minimum_rowcount() method\n",
					"        self.class_object.parameters = {'landing_rows_expected': 2}\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_minimum_rowcount() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        result = self.class_object.check_minimum_rowcount()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the parameters-attribute has been set\n",
					"        self.assertEqual(0, result)\n",
					"\n",
					"    # Failure: Call check_minimum_rowcount() and expect assertionerror to be thrown\n",
					"    def test_checkminimumrowcount_SparkDataFrameChecks_failure_assertionerror(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_minimum_rowcount() method\n",
					"        self.class_object.parameters = {'landing_rows_expected': 4}\n",
					"        expected_error = f\"[Dataframes_v2:SparkDataFrameChecks] Minimum rows is {self.class_object.parameters['landing_rows_expected']}, but got 3\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_minimum_rowcount() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        with self.assertRaises(AssertionError) as error:\n",
					"            self.class_object.check_minimum_rowcount()\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes_v2:SparkDataFrameChecks:StartChecks] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_error}\")\n",
					"\n",
					"    # Failure: Call check_minimum_rowcount() and expect valueerror to be thrown\n",
					"    def test_checkminimumrowcount_SparkDataFrameChecks_failure_valueerror(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_minimum_rowcount() method\n",
					"        self.class_object.parameters = dict()\n",
					"        expected_error = f\"[Dataframes_v2:SparkDataFrameChecks] Cannot check minimum rowcount without landing_rows_expected parameter\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_minimum_rowcount() method of the SparkDataFrameChecks with a valid set of arguments\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            self.class_object.check_minimum_rowcount()\n",
					"\n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes_v2:SparkDataFrameChecks:StartChecks] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_error}\")\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_checkminimumrowcount)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the check_dataframe_headers() of the SparkDataFrameChecks-class defined in the Dataframes_v2 notebook\n",
					"class Test_SparkDataFrameChecks_checkdataframeheaders(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        source_path     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameChecks/initialization/initialize_checks_object.csv'\n",
					"        header          = True\n",
					"        separator       = ';'\n",
					"        file_kind       = 'csv'\n",
					"        column_names    = list()\n",
					"        data_types      = list()\n",
					"        column_information = list()\n",
					"        checks          = ['headers']\n",
					"\n",
					"        self.class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        source_path_c0     = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/SparkDataFrameChecks/initialization/initialize_checks_object_c0.csv'\n",
					"        self.class_object_c0 = SparkDataFrameChecks(source_path=source_path_c0, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        self.class_object_headerless = SparkDataFrameChecks(source_path=source_path, header=False, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful expected_validate_argument_calls of the check_dataframe_headers() method\n",
					"\n",
					"    # Success: Call check_dataframe_headers() with correct column sequence\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_success(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['date', 'string']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.file_kind = 'csv'\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        result = self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        \n",
					"        self.assertEqual(0, result)\n",
					"\n",
					"    # Success: Call check_dataframe_headers() with incorrect column sequence\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_failonsequence(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['string', 'date']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.file_kind = 'csv'\n",
					"        \n",
					"        expected_error = f\"Column sequence in configuration ([string, date]) does not match column sequence in dataframe ([date, string])\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.assertEqual(expected_error, actual_message, f\"[Dataframes_v2:SparkDataFrameChecks:CheckDataframeHeaders] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_message}\")\n",
					"\n",
					"\n",
					"    # Success: Call check_dataframe_headers() with incorrect sequence but 'json' file_kind\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_jsonsequence(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['string', 'date']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.source_kind = 'json'\n",
					"        self.class_object.file_kind = 'parquet'\n",
					"        \n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        result = self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        \n",
					"        self.assertEqual(0, result)\n",
					"\n",
					"    # Success: Call check_dataframe_headers() with invalid column names\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_failoncolumns(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['invalid', 'columnname']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.file_kind = 'csv'\n",
					"        \n",
					"        expected_error = f\"Header mismatch: ACTUAL ([date, string]) vs EXPECTED ([invalid, columnname]).\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.assertEqual(expected_error, actual_message, f\"[Dataframes_v2:SparkDataFrameChecks:CheckDataframeHeaders] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_message}\")\n",
					"\n",
					"\n",
					"    # Success: Call check_dataframe_headers() with Column c0 and correct sequence\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_c0succes(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object_c0.column_names = ['date', 'string']\n",
					"        self.class_object_c0.header = True\n",
					"        self.class_object_c0.file_kind = 'csv'\n",
					"        \n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        result = self.class_object_c0.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        self.assertEqual(result, 0)\n",
					"    \n",
					"    \n",
					"    # Success: Call check_dataframe_headers() with Column C0 and incorrect sequence\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_c0failonsequence(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object_c0.column_names = ['string', 'date']\n",
					"        self.class_object_c0.header = True\n",
					"        self.class_object_c0.file_kind = 'csv'\n",
					"\n",
					"        expected_error = f\"Column sequence in configuration ([string, date]) does not match column sequence in dataframe ([date, string]) (case: index column _c0)\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            self.class_object_c0.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.assertEqual(expected_error, actual_message, f\"[Dataframes_v2:SparkDataFrameChecks:CheckDataframeHeaders] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_message}\")\n",
					"\n",
					"    # Success: Call check_dataframe_headers() for headerless file that has column names on first row\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_failheaderless(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object_headerless.column_names = ['date', 'string']\n",
					"        self.class_object_headerless.header = False\n",
					"        self.class_object_headerless.file_kind = 'csv'\n",
					"\n",
					"        expected_error = f\"File {self.class_object_headerless.source_path} was not expected to contain a header-line but it does\"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            self.class_object_headerless.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.assertEqual(expected_error, actual_message, f\"[Dataframes_v2:SparkDataFrameChecks:CheckDataframeHeaders] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_message}\")\n",
					"\n",
					"    # Success: Call check_dataframe_headers() for headerless file that does not have column names on first row\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_successheaderless(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object_headerless.column_names = ['different', 'columnnames']\n",
					"        self.class_object_headerless.header = False\n",
					"        self.class_object_headerless.file_kind = 'csv'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        result = self.class_object_headerless.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        self.assertEqual(0, result)\n",
					"\n",
					"    # Success: Call check_dataframe_headers() for a file where some columns are optional\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_success_optionalcolumn(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['date', 'string', 'test']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.file_kind = 'csv'\n",
					"        self.class_object.column_information = [\n",
					"            {\n",
					"                None\n",
					"            },\n",
					"            {\n",
					"                \"optional\": False\n",
					"            },\n",
					"            {\n",
					"                \"optional\": True\n",
					"            }           \n",
					"        ]\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        result = self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        self.assertEqual(0, result)\n",
					"\n",
					"\n",
					"    # Failure: Not all columns in the dataframe are in the configuration\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_failure_missingconfig(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['date']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.file_kind = 'csv'\n",
					"        self.class_object.column_information = [\n",
					"            {\n",
					"                \"optional\": False\n",
					"            }\n",
					"        ]\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        expected_error = f\"The following columns were found in the source file but not in the configuration: [string]\"\n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.assertEqual(expected_error, actual_message, f\"[Dataframes_v2:SparkDataFrameChecks:CheckDataframeHeaders] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_message}\")\n",
					"\n",
					"\n",
					"    # Failure: Not all mandatory (non-optional) columns are in the dataframe\n",
					"    def test_checkdataframeheaders_SparkDataFrameChecks_failure_missingmandatorycolumn(self):\n",
					"        # PREPROCESS\n",
					"        # # Define the arguments to use when calling the check_dataframe_headers() method\n",
					"        self.class_object.column_names = ['date', 'string', 'test', 'mandatory']\n",
					"        self.class_object.header = True\n",
					"        self.class_object.file_kind = 'csv'\n",
					"        self.class_object.column_information = [\n",
					"            {\n",
					"                None\n",
					"            },\n",
					"            {\n",
					"                \"optional\": False\n",
					"            },\n",
					"            {\n",
					"                \"optional\": True\n",
					"            },\n",
					"            {\n",
					"                \"optional\": False\n",
					"            }        \n",
					"        ]\n",
					"\n",
					"        # Define the expected error message\n",
					"        expected_error = f\"The following columns are non-optional in the configuration but were not found in the source file: [mandatory]\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # # [Dataframes_v2:SparkDataFrameChecks]: Call the check_dataframe_headers() method of the SparkDataFrameChecks\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            self.class_object.check_dataframe_headers()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # # Validate that the returned error is the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.assertEqual(expected_error, actual_message, f\"[Dataframes_v2:SparkDataFrameChecks:CheckDataframeHeaders] Expected error does not match actual error: \\nExpected {expected_error} ; \\nActual  {actual_message}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_checkdataframeheaders)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameChecks loaddataframe tests, something went wrong!\")"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the check_pirmary_keys() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_SparkDataFrameChecks_checkprimarykeys(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # PREPROCESS:\n",
					"        # Define the expected schema and metadata values\n",
					"        # The following schema is expected to be returned\n",
					"        self.schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"column_2\", sql.types.IntegerType(), True)\n",
					"        ])\n",
					"        \n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \n",
					"        self.replace_value_patcher             = patch.object(SparkDataFrameChecks, 'check_pk_values_to_replace')\n",
					"        self.mock_replace_value                = self.replace_value_patcher.start() \n",
					"        self.mock_replace_value.return_value = 1\n",
					"\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\n",
					"        checks = list()\n",
					"        self.class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        # Stop all patcher-functions\n",
					"        self.load_dataframe_patcher.stop()\n",
					"        self.replace_value_patcher.stop()\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_pirmary_keys method\n",
					"\n",
					"    # Success: Create a table with a certain schema and having a varchar(4) datatype\n",
					"    # The varchar(4) datatype is expected to be part of the metadata of the column \n",
					"    def test_checkprimarykey_sparkdataframechecks_successinteger(self):\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1),\n",
					"            (None, 2),\n",
					"            ('test', 3)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        self.class_object.dataframe = valid_dataframe\n",
					"        self.class_object.parameters = {'primary_key_columns': ['column_2']}\n",
					"\n",
					"        result = self.class_object.check_primary_keys()\n",
					"        self.assertEqual(0, result, \"[Classes:Checks:CheckPrimaryKeys]: Expected no errors but return value did not match with expected value 0\")\n",
					"\n",
					"    def test_checkprimarykey_sparkdataframechecks_successstring(self):\n",
					"        expected_error = \"[Checks:PrimaryKeys] Primary keys should not contain NULL values. Number of null values for PK columns: {'column_1': 1}\"\n",
					"        \n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1),\n",
					"            ('test', None),\n",
					"            ('test', 3)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        self.class_object.dataframe = valid_dataframe\n",
					"        self.class_object.parameters = {'primary_key_columns': ['column_1']}\n",
					"        \n",
					"        result = self.class_object.check_primary_keys()\n",
					"        self.assertEqual(0, result, \"[Classes:Checks:CheckPrimaryKeys]: Expected no errors but return value did not match with expected value 0\")\n",
					"\n",
					"    def test_checkprimarykey_sparkdataframechecks_failinteger(self):\n",
					"        expected_error = \"[Checks:PrimaryKeys] Primary keys should not contain NULL values. Number of null values for PK columns: {'column_2': 1}\"\n",
					"        \n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1),\n",
					"            (None, None),\n",
					"            ('test', 3)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        self.class_object.dataframe = valid_dataframe\n",
					"        self.class_object.parameters = {'primary_key_columns': ['column_2']}\n",
					"\n",
					"        with self.assertRaises(AssertionError) as error:\n",
					"            self.class_object.check_primary_keys()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:Checks:CheckPrimaryKeys] Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"    def test_checkprimarykey_sparkdataframechecks_failstringcolumn(self):\n",
					"        expected_error = \"[Checks:PrimaryKeys] Primary keys should not contain NULL values. Number of null values for PK columns: {'column_1': 1}\"\n",
					"        \n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1),\n",
					"            (None, None),\n",
					"            ('test', 3)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        self.class_object.dataframe = valid_dataframe\n",
					"        self.class_object.parameters = {'primary_key_columns': ['column_1']}\n",
					"\n",
					"        with self.assertRaises(AssertionError) as error:\n",
					"            self.class_object.check_primary_keys()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:Checks:CheckPrimaryKeys] Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"\n",
					"    def test_checkprimarykey_sparkdataframechecks_noparameters(self):\n",
					"        expected_error = \"[Checks:PrimaryKeys]: Cannot execute primary_key check without list of primary keys\"\n",
					"        \n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1),\n",
					"            ('test', None),\n",
					"            ('test', 3)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        self.class_object.dataframe = valid_dataframe\n",
					"        self.class_object.parameters = {}\n",
					"\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            self.class_object.check_primary_keys()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned error matches the expected error\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Classes:Checks:CheckPrimaryKeys] Expected error does not match actual error: {expected_error} versus {actual_error} \")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_checkprimarykeys)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Dataframes_v2"
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\r\n",
					"# This class will test the check_duplicate_primary_keys() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\r\n",
					"class Test_SparkDataFrameChecks_checkduplicateprimarykeys(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define a reusable schema and metadata values\r\n",
					"        self.schema = sql.types.StructType([\r\n",
					"            sql.types.StructField(\"pk_column\", sql.types.StringType(), True),\r\n",
					"            sql.types.StructField(\"spare_column\", sql.types.StringType(), True)\r\n",
					"        ])\r\n",
					"        \r\n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\r\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\r\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \r\n",
					"\r\n",
					"        # Mock a reusable class-object \r\n",
					"        source_path, header, separator, file_kind, column_names, data_types, column_information = Mock(), Mock(), Mock(), Mock(), Mock(), Mock(), Mock()\r\n",
					"        checks = list()\r\n",
					"        self.class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\r\n",
					"\r\n",
					"\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown Test Case class\r\n",
					"        # Stop all patcher-functions\r\n",
					"        self.load_dataframe_patcher.stop()\r\n",
					"        return\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_duplicate_primary_keys method\r\n",
					"\r\n",
					"    # Success: Do not raise an error when there are no duplicates in a single PK instance\r\n",
					"    def test_checkduplicateprimarykey_sparkdataframechecks_success_singlepk(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Initialize a dataframe with no duplicate values\r\n",
					"        valid_dataframe = spark.createDataFrame(data=[\r\n",
					"            (12345, \"spare_value\"),\r\n",
					"            (67890, \"spare_value\"),\r\n",
					"            (54321, \"spare_value\")\r\n",
					"        ], schema=self.schema)\r\n",
					"\r\n",
					"        # Define the class-arguments\r\n",
					"        self.class_object.dataframe = valid_dataframe\r\n",
					"        self.class_object.parameters = {'primary_key_columns': ['pk_column']}\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        result = self.class_object.check_duplicate_primary_keys()\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Expect the function to return 0\r\n",
					"        self.assertEqual(0, result, \"[Classes:Checks:checkduplicateprimarykeys]: 1. Expected no errors but return value did not match with expected value 0\")\r\n",
					"\r\n",
					"    # Success: Do not raise an error when there are no duplicates in a combined PK instance\r\n",
					"    def test_checkduplicateprimarykey_sparkdataframechecks_success_combinedpk(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Initialize a dataframe with no duplicate values\r\n",
					"        valid_dataframe = spark.createDataFrame(data=[\r\n",
					"            (12345, \"spare_value\"),\r\n",
					"            (67890, \"spare_value\"),\r\n",
					"            (54321, \"spare_value\")\r\n",
					"        ], schema=self.schema)\r\n",
					"\r\n",
					"        # Define the class-arguments\r\n",
					"        self.class_object.dataframe = valid_dataframe\r\n",
					"        self.class_object.parameters = {'primary_key_columns': ['pk_column', 'spare_column']}\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        result = self.class_object.check_duplicate_primary_keys()\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Expect the function to return 0\r\n",
					"        self.assertEqual(0, result, \"[Classes:Checks:checkduplicateprimarykeys]: 2. Expected no errors but return value did not match with expected value 0\")\r\n",
					"\r\n",
					"\r\n",
					"    # Failure: Raise an error when there are duplicate PK values for a single PK instance\r\n",
					"    def test_checkduplicateprimarykey_sparkdataframechecks_failure_duplicatesinglepk(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Initialize a dataframe with duplicate values\r\n",
					"        valid_dataframe = spark.createDataFrame(data=[\r\n",
					"            (12345, \"spare_value\"),\r\n",
					"            (67890, \"spare_value\"),\r\n",
					"            (12345, \"spare_value\")\r\n",
					"        ], schema=self.schema)\r\n",
					"\r\n",
					"        # Define the expected error message\r\n",
					"        expected_message = \"[Checks:DuplicatPrimaryKeys] Duplicate primary keys found in the source file for 1 instances\"\r\n",
					"\r\n",
					"        # Define the class-arguments\r\n",
					"        self.class_object.dataframe = valid_dataframe\r\n",
					"        self.class_object.parameters = {'primary_key_columns': ['pk_column']}\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        with self.assertRaises(MiddleWareError) as error:\r\n",
					"            self.class_object.check_duplicate_primary_keys()\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned error is the expected error\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        actual_error = ast.literal_eval(actual_error)\r\n",
					"        actual_message = actual_error[\"custom_message\"]\r\n",
					"        \r\n",
					"        self.assertEqual(actual_message, expected_message, f\"[Classes:Checks:checkduplicateprimarykeys]: 3. Expected error message does not match actual: \\nEXPECTED: {expected_message} \\nACTUAL: {actual_message}\")\r\n",
					"\r\n",
					"    # Failure: Raise an error when there are duplicate PK values for a combined PK instance \r\n",
					"    def test_checkduplicateprimarykey_sparkdataframechecks_failure_duplicatecombinedpk(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Initialize a dataframe with duplicate values\r\n",
					"        valid_dataframe = spark.createDataFrame(data=[\r\n",
					"            (12345, \"spare_value\"),\r\n",
					"            (12345, \"spare_value\"),\r\n",
					"            (12345, \"spare_value2\"),\r\n",
					"            (12345, \"spare_value3\"),\r\n",
					"            (67890, \"spare_value\"),\r\n",
					"            (67890, \"spare_value\"),\r\n",
					"            (67890, \"spare_value2\"),\r\n",
					"            (67890, \"spare_value3\")\r\n",
					"        ], schema=self.schema)\r\n",
					"\r\n",
					"        # Define the expected error message\r\n",
					"        expected_message = \"[Checks:DuplicatPrimaryKeys] Duplicate primary keys found in the source file for 2 instances\"\r\n",
					"\r\n",
					"        # Define the class-arguments\r\n",
					"        self.class_object.dataframe = valid_dataframe\r\n",
					"        self.class_object.parameters = {'primary_key_columns': ['pk_column', 'spare_column']}\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        with self.assertRaises(MiddleWareError) as error:\r\n",
					"            self.class_object.check_duplicate_primary_keys()\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned error is the expected error\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        actual_error = ast.literal_eval(actual_error)\r\n",
					"        actual_message = actual_error[\"custom_message\"]\r\n",
					"        \r\n",
					"        self.assertEqual(actual_message, expected_message, f\"[Classes:Checks:checkduplicateprimarykeys]: 4. Expected error message does not match actual: \\nEXPECTED: {expected_message} \\nACTUAL: {actual_message}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_checkduplicateprimarykeys)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\r\n",
					"#     print(test_resuts)"
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: Checks\n",
					"# This class will test the check_values_to_replace() method of the DeltaTable-class defined in the checks_v3 notebook\n",
					"class Test_Checks_checkvaluestoreplace(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # PREPROCESS:\n",
					"        # Define the expected schema and metadata values\n",
					"        # The following schema is expected to be returned\n",
					"        self.schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"column_2\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"column_3\", sql.types.StringType(), True)\n",
					"        ])\n",
					"        \n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \n",
					"     \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        self.load_dataframe_patcher.stop()\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_values_to_replace method\n",
					"\n",
					"    # Success: Create a table with a certain schema and having a varchar(4) datatype\n",
					"    # The varchar(4) datatype is expected to be part of the metadata of the column \n",
					"    def test_checkvaluestoreplace_checks_noreplace(self):\n",
					"\n",
					"        source_path, header, separator, file_kind= Mock(), Mock(), Mock(), Mock()\n",
					"        column_names = ['column_1', 'column_2', 'column_3'] \n",
					"        data_types   = ['string', 'timestamp', 'date']\n",
					"        column_information = [None, None, None]\n",
					"        checks = list()\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        \n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', '2024-08-22 00:00:00', '2024-08-22'),\n",
					"            ('test', '2024-08-22 00:00:00', '2024-08-22'),\n",
					"            ('test', '2024-08-22 00:00:00', '2024-08-22')\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        class_object.dataframe = valid_dataframe\n",
					"\n",
					"        class_object.check_values_to_replace()\n",
					"\n",
					"    def test_checkvaluestoreplace_checks_invaliddatatype(self):\n",
					"        source_path, header, separator, file_kind= Mock(), Mock(), Mock(), Mock()\n",
					"        column_names = ['column_1', 'column_2', 'column_3'] \n",
					"        data_types   = ['string', 'timestamp', 'date']\n",
					"        column_information = [None, None, None]\n",
					"\n",
					"        checks = list()\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        \n",
					"        \n",
					"        valid_dataframe = spark.createDataFrame(data=[\n",
					"            ('test', '0000-01-01 00:00:00', '2024-08-22'),\n",
					"            ('test', '2024-08-22 00:00:00', '0000-01-01'),\n",
					"            ('test', '2024-08-22 00:00:00', '2024-08-22')\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        class_object.dataframe = valid_dataframe\n",
					"\n",
					"        result = class_object.check_values_to_replace()\n",
					"        expected_result = spark.createDataFrame(data=[\n",
					"            ('test', None,                  '2024-08-22'),\n",
					"            ('test', '2024-08-22 00:00:00', None),\n",
					"            ('test', '2024-08-22 00:00:00', '2024-08-22')\n",
					"        ],\n",
					"        schema=['column_1', 'column_2', 'column_3']\n",
					"        )\n",
					"\n",
					"        self.assertEqual([row for row in class_object.dataframe.collect()], expected_result.collect())\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Checks_checkvaluestoreplace)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Dev-Info**: The Test_SparkDataFrameChecks_checkdataframedatatypes class will implicitly test cast_column and count_miscasted_column_values. Therefore, no reparate Test Case for these functions has been implemented."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SparkDataFrameChecks\n",
					"# This class will test the check_dataframe_datatypes() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_SparkDataFrameChecks_checkdataframedatatypes(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # PREPROCESS:\n",
					"        # Define the expected schema and metadata values\n",
					"        # The following schema is expected to be returned\n",
					"        self.schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"string_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"integer_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"varchar_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"timestamp_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"date_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"decimal_column\", sql.types.StringType(), True)\n",
					"        ])\n",
					"        \n",
					"        # Mock load_dataframe() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_patcher             = patch.object(SparkDataFrameChecks, 'load_dataframe')\n",
					"        self.mock_load_dataframe                = self.load_dataframe_patcher.start() \n",
					"\n",
					"        # Mock load_dataframe_with_schema() -&gt; No need to create DataFrame object if no checks are executed\n",
					"        self.load_dataframe_with_schema_patcher = patch.object(SparkDataFrameChecks, 'load_dataframe_with_schema')\n",
					"        self.mock_load_dataframe_with_schema    = self.load_dataframe_with_schema_patcher.start() \n",
					"\n",
					"        # Mock replace_dataframe_columns() -&gt; No need to execute this funtion and replace columns\n",
					"        self.replace_dataframecolumns__patcher = patch.object(SparkDataFrameChecks, 'replace_dataframe_columns')\n",
					"        self.mock_replace_dataframe_columns    = self.replace_dataframecolumns__patcher.start() \n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case class\n",
					"        # Stop all patcher-functions\n",
					"        self.load_dataframe_patcher.stop()\n",
					"        self.load_dataframe_with_schema_patcher.stop()\n",
					"        self.replace_dataframecolumns__patcher.stop()\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the check_dataframe_datatypes method\n",
					"\n",
					"    # Success: Create a table with a certain schema and having a varchar(4) datatype\n",
					"    # The varchar(4) datatype is expected to be part of the metadata of the column \n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_successfulcasting(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 2, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 3, 'value', '1900-01-01 00:00:00', '1900-01-01', 3.14)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        expected_result = {\"string_column\": 0, \"integer_column\": 0, \"varchar_column\": 0, \"timestamp_column\": 0, \"date_column\": 0, \"decimal_column\": 0}\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names   = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n",
					"        data_types     = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None, {\"format\": \"yyyy-MM-dd HH:mm:ss\"}, {\"format\": \"yyyy-MM-dd\"},  {\"locale\": \"en-US\"}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        class_object.dataframe = dataframe\n",
					"\n",
					"        # EXECUTE\n",
					"        actual_result = class_object.check_dataframe_datatypes()\n",
					"\n",
					"        # EVALUATE\n",
					"        self.assertEqual(expected_result, actual_result)\n",
					"\n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_unsuccessfulcasting(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1,         'values',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 'string',  'value',    '1900-01-01 00:00:00', '1900-01-01', 3.1434),\n",
					"            ('test', 3,         'values',   '1900-01-01 00:00:00', '1900-01-01', 3.1234)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        expected_result = {\n",
					"            'string_column': 0, \n",
					"            'integer_column': AssertionError('Casting error: NULL values in column integer_column after casting: Could not convert 1 columns'), \n",
					"            'varchar_column': AssertionError('Casting error: NULL values in column varchar_column after casting: Could not convert 2 columns'), \n",
					"            'timestamp_column': 0, \n",
					"            'date_column': 0,\n",
					"            'decimal_column': 0\n",
					"        }\n",
					"        expected_error = f\"Error during casting: {expected_result}\"\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names   = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n",
					"        data_types     = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None, {\"format\": \"yyyy-MM-dd HH:mm:ss\"}, {\"format\": \"yyyy-MM-dd\"},  {\"locale\": \"en-US\"}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        class_object.dataframe = dataframe\n",
					"\n",
					"        # EXECUTE\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            actual_result = class_object.check_dataframe_datatypes()\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error)\n",
					"\n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_optionalcolumn(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 2,  'value',    '1900-01-01 00:00:00', '1900-01-01', 3.1434),\n",
					"            ('test', 3,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.1234)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names       = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"optional_column\"]\n",
					"        data_types         = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None, {\"format\": \"yyyy-MM-dd HH:mm:ss\"}, {\"format\": \"yyyy-MM-dd\"},  {\"locale\": \"en-US\"}, {\"optional\": True}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        class_object.dataframe = dataframe\n",
					"\n",
					"        class_object.column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"optional_column\"]\n",
					"        expected_result = {\"string_column\": 0, \"integer_column\": 0, \"varchar_column\": 0, \"timestamp_column\": 0, \"date_column\": 0, \"decimal_column\": 0, \"optional_column\": 'optional_skip'}\n",
					"\n",
					"        # EXECUTE\n",
					"        actual_result = class_object.check_dataframe_datatypes()\n",
					"\n",
					"        # EVALUATE\n",
					"        self.assertEqual(expected_result, actual_result)\n",
					"\n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_nonexistingdatatype(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 2,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 3,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.12)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names   = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n",
					"        data_types     = [\"non_existing_datatype\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None, {\"format\": \"yyyy-MM-dd HH:mm:ss\"}, {\"format\": \"yyyy-MM-dd\"},  {\"locale\": \"en-US\"}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        class_object.dataframe = dataframe\n",
					"\n",
					"        expected_error = f\"Casting error for casting column string_column to data type non_existing_datatype: .*\"\n",
					"        \n",
					"        # EXECUTE &amp; EVALUATE\n",
					"        with self.assertRaisesRegex(ValueError, expected_error) as error:\n",
					"            actual_result = class_object.check_dataframe_datatypes()\n",
					"\n",
					"\n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_datatype_functioncalls_datetimestamp(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 2,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 3,  'value',   '1900-01-01 00:00:00', '1900-01-01', 3.12)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names   = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n",
					"        data_types     = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None,{\"format\": \"yyyy-MM-dd HH:mm:ss\"},  {\"format\": \"yyyy-MM-dd\"},  {\"locale\": \"en-US\"}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information, debug=False)\n",
					"        # Set the dataframe-argument of the object to dataframe\n",
					"        class_object.dataframe = dataframe\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the function under test with mocked functions to_timestamp, to_date, and count_miscasted_column_values\n",
					"        # with patch.object(pyspark.sql.functions, \"to_timestamp\")                as mock_to_timestamp, \\\n",
					"        #     patch.object(pyspark.sql.functions, \"to_date\")                      as mock_to_date, \\\n",
					"        with    patch.object(SparkDataFrameChecks, \"count_miscasted_column_values\") as mock_mismatch_count:\n",
					"            class_object.check_dataframe_datatypes()\n",
					"\n",
					"        # EVALUATE\n",
					"        # mock_to_timestamp.assert_called_once()\n",
					"        # mock_to_date.assert_called_once()\n",
					"        self.mock_load_dataframe_with_schema.assert_not_called() # Decommissioned function\n",
					"\n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_datatype_functioncalls_decimal(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks\n",
					"        schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"string_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"integer_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"varchar_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"timestamp_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"date_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"decimal_column\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"decimal_column_dot\", sql.types.StringType(), True)\n",
					"        ])\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1,  'value',   '01-01-1999 00:00:00', '01-01-1999', 3.14, \"3,23\"),\n",
					"            ('test', 2,  'value',   '01-01-1999 00:00:00', '01-01-1999', 3.14, \"3,23\"),\n",
					"            ('test', 3,  'value',   '01-01-1999 00:00:00', '01-01-1999', 3.12, \"3,23\")\n",
					"        ], schema=schema)\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names   = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"decimal_column_dot\"]\n",
					"        data_types     = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None,{\"format\": \"dd-MM-yyyy HH:mm:ss\"},  {\"format\": \"dd-MM-yyyy\"},  {\"locale\": \"en-US\"}, {\"locale\": \"fr-FR\"}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information, debug=False)\n",
					"        class_object.dataframe = dataframe\n",
					"        \n",
					"        # EXECUTE\n",
					"        # with patch.object(pyspark.sql.functions, \"to_timestamp\")                as mock_to_timestamp, \\\n",
					"        #     patch.object(pyspark.sql.functions, \"to_date\")                      as mock_to_date, \\\n",
					"        with    patch.object(SparkDataFrameChecks, \"count_miscasted_column_values\") as mock_mismatch_count:\n",
					"            class_object.check_dataframe_datatypes()\n",
					"\n",
					"        # EVALUATE\n",
					"        # mock_to_timestamp.assert_called_once()\n",
					"        # mock_to_date.assert_called_once()\n",
					"        self.mock_load_dataframe_with_schema.assert_not_called() # Decommissioned function\n",
					"        self.mock_replace_dataframe_columns.assert_not_called()  # Decommissioned function\n",
					"\n",
					"    def test_checkdataframedatatypes_sparkdataframechecks_columnmismatch(self):\n",
					"        # PREPROCESS\n",
					"        # Create a reusable instance of SparkDataFrameChecks -&gt; Mock all arguments except checks (needed for add_mandatory_checks)\n",
					"        dataframe = spark.createDataFrame(data=[\n",
					"            ('test', 1,         'values',   '1900-01-01 00:00:00', '1900-01-01', 3.14),\n",
					"            ('test', 'string',  'value',    '1900-01-01 00:00:00', '1900-01-01', 3.1434),\n",
					"            ('test', 3,         'values',   '1900-01-01 00:00:00', '1900-01-01', 3.1234)\n",
					"        ], schema=self.schema)\n",
					"\n",
					"        source_path, header, separator, file_kind = Mock(), Mock(), Mock(), Mock()\n",
					"        column_names       = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\"]\n",
					"        data_types         = [\"string\", \"integer\", \"varchar(5)\", \"timestamp\", \"date\", \"decimal(3,2)\"]\n",
					"        column_information = [None, None, None, {\"format\": \"yyyy-MM-dd HH:mm:ss\"}, {\"format\": \"yyyy-MM-dd\"},  {\"locale\": \"en-US\"}]\n",
					"        checks = ['header']\n",
					"        class_object = SparkDataFrameChecks(source_path=source_path, header=header, separator=separator, file_kind=file_kind, column_names=column_names, data_types=data_types, checks=checks, column_information=column_information)\n",
					"        class_object.dataframe = dataframe\n",
					"\n",
					"        class_object.column_names = [\"string_column\", \"integer_column\", \"varchar_column\", \"timestamp_column\", \"date_column\", \"decimal_column\", \"additional_column\"]\n",
					"        expected_error = (f\"Total number of configured columns does not match total number of source file columns: \\n{str(class_object.column_names)} ({len(class_object.column_names)}) versus {str(column_names)} ({len(column_names)})\").replace(\"'\", \"\")\n",
					"\n",
					"        # EXECUTE\n",
					"        with self.assertRaises(ConfigurationError) as error:\n",
					"            actual_result = class_object.check_dataframe_datatypes()\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_error = str(error.exception)\n",
					"        actual_error = ast.literal_eval(actual_error)\n",
					"        actual_message = actual_error[\"custom_message\"]\n",
					"        self.maxDiff = None\n",
					"        self.assertEqual(expected_error, actual_message)\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_SparkDataFrameChecks_checkdataframedatatypes)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_DeltaTables.json">
{
	"name": "Test_DeltaTables",
	"properties": {
		"folder": {
			"name": "Modules/Test_Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "34f4f246-8ea3-417b-8a8b-cb4d7a042eb0"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test DeltaTables\n",
					"This notebook will test all the classes defined in notebook DeltaTables.\n",
					"The structure of the notebook looks as follows:\n",
					"    \n",
					"- Section: Markdown cells are used to distinguish between the different classes that are being tested.\n",
					"- Code cells: Each code cell tests one specific method of the class being tested of a section.\n",
					"\n",
					"**Notes**\n",
					"1. At the end of each cell, a piece of commented code can be found. This code can be uncommented if the tests of that specific cell need to be run. This can be useful during debugging. Make sure that, before completing a pull request, these pieces of code are commented again, as tests will otherwise be executed multiple times during the same test-job of the CI pipeline.\n",
					"2. The approach taken is to always define a setUp and tearDown method at the beginning of each Test Case. This is done to keep the code consistent over the different classes. These methods can be used if the entire Test Case will use the same set of variables or if a function/method needs to be mocked/patched for the entire Test Case.\n",
					"3. Each test method uses a similar structure, with a PREPROCESS, EXECUTE, and EVALUATE step. This is to keep the tests readable and consistent."
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary methods of the unittest library"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"# Import the unittest-library \n",
					"import unittest\n",
					"# Import some specific functions of the library defined under the mock-method\n",
					"from unittest.mock import patch, MagicMock, Mock, ANY, call\n",
					"\n",
					"# Import the DeltaTable and DeltaOptimizeBuilder class\n",
					"# Reason: The DeltaTables notebook will on some occasions create class-instances of DeltaTable/DeltaOptimizeBuilder. Assertion are executed to validate whether the returned objects are actualy of this class.\n",
					"from delta.tables import DeltaTable, DeltaOptimizeBuilder\n",
					"import delta.tables as dt"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Initialize the environment_code parameter\n",
					"In almost all notebooks, a reference to this parameter will be found. This is because the CI/CD pipelines will not overwrite specific environment-references defined in notebooks. Since the notebooks will be deployed to several environments, the use of the environment_code parameter has been chosen. \n",
					"The environment_code will be used when the spark session has no environment_code-argument defined. This argument is linked to the core_configuration, which can be found under Manage -&gt; Apache Spark Configurations\n",
					"The use of these parameters and arguments is so that, during deployment, there is only 1 place where all references need to be overwritten."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\n",
					"environment_code = ''"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\n",
					"\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define global parameters to use over the entire notebook\n",
					"container_name = 'unittest'\n",
					"storage_account = f'{env_code}dapstdala1'"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the DeltaTables notebook"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Test Class: DeltaTableClass_v2"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeltaTableClassv2\n",
					"# This class will test the __init__() of the DeltaTableClassv2-class defined in the DeltaTables notebook\n",
					"class Test_DeltaTableClassv2_initialization(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the __init__() method\n",
					"\n",
					"    # Success: Create an instance of class DeltaTableClassv2\n",
					"    def test_inititalization_deltatableclass_objectcreation(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # [DeltaTables:DeltaTableClassv2:__init__] Run the method under test\n",
					"        class_object = DeltaTableClass_v2(debug=True)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the debug-property has been set\n",
					"        self.assertTrue(class_object.debug)\n",
					"        # Validate that the class_object is an instance of class SparkDataFrameClass\n",
					"        self.assertIsInstance(class_object, SparkDataFrameClass)\n",
					"\n",
					"    # Success: Mock the init function and validate that the function was called when initializing a DeltaTableClass_v2 class-instance\n",
					"    def test_inititalization_deltatableclass_functioncalls(self):\n",
					"        # PREPROCESS\n",
					"        debug = False\n",
					"\n",
					"        # EXECUTE\n",
					"        # [DeltaTables:DeltaTableClassv2:__init__] Run the method under test\n",
					"        with unittest.mock.patch.object(DeltaTableClass_v2, '__init__', return_value = None) as mock_init:\n",
					"            DeltaTableClass_v2(debug=debug)\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the init function has been called with the debug-argument as False\n",
					"        mock_init.assert_called_once_with(debug=debug)\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeltaTableClassv2_initialization)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass initialization tests, something went wrong!\")"
				],
				"execution_count": 7
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Dev-Info**: The below Test Case tests method create_delta_table. load_delta_table is not been tested, as it only calls a spark-native function and there is no real point in writing a test for this method. However, the below Test Case uses the load_delta_table method in the validation-steps, so it is being tested implicitly in this case."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeltaTableClassv2\n",
					"# This class will test the create_delta_table() of the DeltaTableClassv2-class defined in the DeltaTables notebook\n",
					"class Test_DeltaTableClassv2_createdeltatable(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the create_delta_table() method\n",
					"\n",
					"    # Success: Create a new table and validate that it is a DeltaTable-object\n",
					"    def test_createdeltatable_deltatableclass_writenewtable(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be used to call the method with\n",
					"        table_name = 'DeltaTableClassv2/create_delta_table/new_table'\n",
					"        column_objects = [\n",
					"            { 'column_name': 'column_1', 'data_type': 'string', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }, \n",
					"            { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_4',  'data_type': 'varchar(4)', 'dimension':'SCD2' }\n",
					"        ]\n",
					"        table_description = 'test method create_delta_table'\n",
					"        class_object = DeltaTableClass_v2()\n",
					"\n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already        \n",
					"        delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            clean_delta_table(env_code=self.env_code, delta_lake=container_name, table_name=table_name)\n",
					"        \n",
					"        # Define expectations for the values after calling the method\n",
					"        expected_partitioning_columns = list()\n",
					"\n",
					"        # RUN\n",
					"        # [DeltaTables:DeltaTableClassv2:create_delta_table] Run the method under test\n",
					"        class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the new table is indeed a DeltaTable\n",
					"        delta_table = class_object.load_delta_table(source_path=delta_table_path)\n",
					"        self.assertIsInstance(delta_table, DeltaTable, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: New table {table_name} is not of class 'DeltaTable'...\")\n",
					"        # Validate that there has been no partitioning done on the table\n",
					"        actual_partitioning_columns = delta_table.detail().select(\"partitionColumns\").collect()[0][0]\n",
					"        self.assertEqual(expected_partitioning_columns, actual_partitioning_columns, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: No partitioning columns were expected to be defined for {table_name}\")\n",
					"\n",
					"    # Success: Create a new table with partitioning_columns and validate that the metadata of the DeltaTable contains the partitioning-columns\n",
					"    #   Additional check: Write column_4 as a varchar and validate that the metadata of the column contains a __CHAR_VARCHAR_TYPE_STRING-constraint\n",
					"    def test_createdeltatable_deltatableclass_writepartitionedtable(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be used to call the method with\n",
					"        table_name = 'DeltaTableClassv2/create_delta_table/partitioned_table'\n",
					"        column_objects = [\n",
					"            { 'column_name': 'column_1', 'data_type': 'string', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }, \n",
					"            { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_4',  'data_type': 'varchar(4)', 'dimension':'SCD2' }\n",
					"        ]\n",
					"        table_description = 'test method create_delta_table'\n",
					"        partition_objects = [\n",
					"            {'name': 'column_1', 'sequence': 1}\n",
					"        ]\n",
					"\n",
					"        class_object = DeltaTableClass_v2()\n",
					"        delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n",
					"        \n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(env_code=self.env_code, delta_lake=container_name, table_name=table_name)\n",
					"        \n",
					"        # Define expectations for the values after calling the method\n",
					"        expected_metadata_column4 = {'column_4': 'metadata about the column column_4', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(4)'}\n",
					"        expected_partitioning_columns = ['column_1']\n",
					"\n",
					"        # EXECUTE\n",
					"        # [DeltaTables:DeltaTableClassv2:create_delta_table] Run the method under test\n",
					"        class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the new table is indeed a DeltaTable\n",
					"        delta_table = class_object.load_delta_table(source_path=delta_table_path)\n",
					"        self.assertIsInstance(delta_table, DeltaTable, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: New table {table_name} is not of class 'DeltaTable'...\")\n",
					"        # Validate that the table contains the expected partitioning columns\n",
					"        actual_partitioning_columns = delta_table.detail().select(\"partitionColumns\").collect()[0][0]\n",
					"        self.assertEqual(expected_partitioning_columns, actual_partitioning_columns, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: Partitioning columns did not match expectations: \\nExpected: {expected_partitioning_columns}  \\nActual:   {actual_partitioning_columns}\")\n",
					"        # Validate that the metadata for column 4 contains the __CHAR_VARCHAR_TYPE_STRING-constraint\n",
					"        actual_metadata_column4 = delta_table.toDF().select(\"column_4\").schema[0].metadata\n",
					"        self.assertEqual(expected_metadata_column4, actual_metadata_column4, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: Metadata for column_4 does not match expectations: \\nExpected: {expected_metadata_column4}  \\nActual:   {actual_metadata_column4}\")\n",
					"\n",
					"    # Success: Create a new table with partitioning_columns for dateParts and validate that the metadata of the DeltaTable contains the datePart partitioning-columns\n",
					"    #   Additional check: Write column_4 as a varchar and validate that the metadata of the column contains a __CHAR_VARCHAR_TYPE_STRING-constraint\n",
					"    def test_createdeltatable_deltatableclass_writepartitionedtable_datepart(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be used to call the method with\n",
					"        table_name = 'DeltaTableClassv2/create_delta_table/partitioned_table_datepart'\n",
					"        column_objects = [\n",
					"            { 'column_name': 'column_1', 'data_type': 'timestamp', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }, \n",
					"            { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_4',  'data_type': 'varchar(4)', 'dimension':'SCD2' }\n",
					"        ]\n",
					"        table_description = 'test method create_delta_table' \n",
					"        partition_objects = [\n",
					"            {'name': 'column_1', 'sequence': 1, 'datePart': 'year'},\n",
					"            {'name': 'column_1', 'sequence': 2, 'datePart': 'month'}\n",
					"\n",
					"        ]\n",
					"        class_object = DeltaTableClass_v2()\n",
					"        delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n",
					"        \n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(env_code=self.env_code, delta_lake=container_name, table_name=table_name)\n",
					"\n",
					"        # Define expectations for the values after calling the method\n",
					"        expected_metadata_column4 = {'column_4': 'metadata about the column column_4', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(4)'}\n",
					"        expected_partitioning_columns = ['p_year', 'p_month']\n",
					"\n",
					"        # EXECUTE\n",
					"        # [DeltaTables:DeltaTableClassv2:create_delta_table] Run the method under test\n",
					"        class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\n",
					"\n",
					"        # EVALUATE\n",
					"        ## Check if a table has been created on the location\n",
					"        deltaTable = class_object.load_delta_table(source_path=delta_table_path)\n",
					"        self.assertIsInstance(dt.DeltaTable.forPath(spark, delta_table_path), DeltaTable, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: New table {table_name} is not of class 'DeltaTable'...\")\n",
					"        # Validate that the table contains the expected partitioning columns\n",
					"        actual_partitioning_columns = deltaTable.detail().select(\"partitionColumns\").collect()[0][0]\n",
					"        self.assertEqual(expected_partitioning_columns, actual_partitioning_columns, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: Partitioning columns did not match expectations: \\nExpected: {expected_partitioning_columns}  \\nActual:   {actual_partitioning_columns}\")\n",
					"        # Validate that the metadata for column 4 contains the __CHAR_VARCHAR_TYPE_STRING-constraint\n",
					"        actual_metadata_column4 = deltaTable.toDF().select(\"column_4\").schema[0].metadata\n",
					"        self.assertEqual(expected_metadata_column4, actual_metadata_column4, f\"[DeltaTables:DeltaTableClassv2:CreateDeltaTable]: Metadata for column_4 does not match expectations: \\nExpected: {expected_metadata_column4}  \\nActual:   {actual_metadata_column4}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeltaTableClassv2_createdeltatable)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass createdeltatable tests, something went wrong!\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeltaTableClassv2\n",
					"# This class will test the add_notnull_constraints() of the DeltaTableClassv2-class defined in the DeltaTables notebook\n",
					"class Test_DeltaTableClassv2_addnontnullconstraints(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the add_notnull_constraints() method\n",
					"    \n",
					"    # Success: Add not_null constraints to a delta table\n",
					"    def test_addnontnullconstraints_deltatableclass_addconstraintstotable(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be used to call the method with\n",
					"        # Note: An new table will be created first to make sure that the table does actually exist before adding constraints\n",
					"        table_name = 'DeltaTableClassv2/add_notnull_constraints/new_table'\n",
					"        column_objects = [\n",
					"            { 'column_name': 'column_1', 'data_type': 'string', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }, \n",
					"            { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_4',  'data_type': 'varchar(4)', 'dimension':'SCD2' }\n",
					"        ]\n",
					"        table_description = 'test method add_notnull_constraints'\n",
					"\n",
					"        column_dimensions = {'column_1': 'PK', 'column_2' :'SCD2', 'column_3': 'PK', 'column_4': 'SCD2'}\n",
					"        class_object = DeltaTableClass_v2()\n",
					"        delta_table_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/{table_name}'\n",
					"        \n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(self.env_code, container_name, table_name)\n",
					"\n",
					"        # Create an empty delta table first before adding the constraints\n",
					"        class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects)\n",
					"\n",
					"        # Define expectations for the values after calling the method\n",
					"        expected_constraint_names = ['delta.constraints.pk__notnull_column_1', 'delta.constraints.pk__notnull_column_3']\n",
					"        expected_constraint_values = ['column_1 IS NOT NULL', 'column_3 IS NOT NULL']\n",
					"\n",
					"        # EXECUTE\n",
					"        # [DeltaTables:DeltaTableClassv2:add_notnull_constraints] Run the method under test\n",
					"        class_object.add_notnull_constraints(source_path=delta_table_path, column_dimensions=column_dimensions)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the list of constraints that have been added to the table\n",
					"        constraints = spark.sql(f\"DESCRIBE DETAIL  delta.`{delta_table_path}`\").select('properties').collect()[0][0]\n",
					"        actual_constraint_names = list(constraints.keys())\n",
					"        actual_constraint_values = list(constraints.values())\n",
					"        # Validate that the names and values of the constraints match the expectations\n",
					"        self.assertEqual(expected_constraint_names, actual_constraint_names,   f\"[DeployDeltaTables:AddNotNullConstraints] Constraints were not initialised as expected: actual = {actual_constraint_names} versus expected = {expected_constraint_names}\")\n",
					"        self.assertEqual(expected_constraint_values, actual_constraint_values, f\"[DeployDeltaTables:AddNotNullConstraints] Constraints were not initialised as expected: actual = {actual_constraint_values} versus expected = {expected_constraint_values}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeltaTableClassv2_addnontnullconstraints)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass addnontnullconstraints tests, something went wrong!\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/DeltaTables"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeltaTableClassv2\r\n",
					"# This class will test the validate_columnnames() of the DeltaTableClassv2-class defined in the DeltaTables notebook\r\n",
					"class Test_DeltaTableClassv2_validatecolumnnames(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        # Define the env_code parameter to use throughout the class\r\n",
					"        self.env_code = env_code\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown Test Case\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the validate_columnnames() method\r\n",
					"    \r\n",
					"    # Success: Add columns to a non-partitioned table\r\n",
					"    def test_validatecolumnnames_deltatableclass_addcolumnstononpartitionedtable(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Create an empty delta table\r\n",
					"        delta_table_object = DeltaTableClass_v2()\r\n",
					"\r\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_unpartitioned_table'\r\n",
					"        table_description = 'test schema evolution'\r\n",
					"        column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"\r\n",
					"        delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects)\r\n",
					"        \r\n",
					"        new_column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_3\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"        # Define expectations\r\n",
					"        expected_columns = ['column_1', 'column_2', 'column_3']\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=new_column_objects)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the delta table contains the 3 columns (excluding the technical fields)\r\n",
					"        delta_table = delta_table_object.load_delta_table(source_path=destination_path)\r\n",
					"        table_columns = delta_table.toDF().columns\r\n",
					"        \r\n",
					"        actual_columns = []\r\n",
					"        for expected_column in expected_columns:\r\n",
					"            if expected_column in table_columns:\r\n",
					"                actual_columns.append(expected_column)\r\n",
					"\r\n",
					"        self.assertEqual(expected_columns, actual_columns, f'[Dataframes:SparkDataFrameClass:WriteDataframe] Expected columns to be appended to the delta table, but there are missing columns: \\nEXPECTED {expected_columns} \\nMISSING {actual_columns}')\r\n",
					"        \r\n",
					"    # Success: Add columns to a partitioned table\r\n",
					"    def test_validatecolumnnames_deltatableclass_addcolumnstopartitionedtable(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Create an empty delta table\r\n",
					"        delta_table_object = DeltaTableClass_v2()\r\n",
					"\r\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_partitioned_table'\r\n",
					"        table_description = 'test schema evolution'\r\n",
					"        column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"        partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\r\n",
					"\r\n",
					"        delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\r\n",
					"        \r\n",
					"        new_column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_3\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"        # Define expectations\r\n",
					"        expected_columns = ['column_1', 'column_2', 'column_3']\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=new_column_objects)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the delta table contains the 3 columns (excluding the technical fields)\r\n",
					"        delta_table = delta_table_object.load_delta_table(source_path=destination_path)\r\n",
					"        table_columns = delta_table.toDF().columns\r\n",
					"        \r\n",
					"        actual_columns = []\r\n",
					"        for expected_column in expected_columns:\r\n",
					"            if expected_column in table_columns:\r\n",
					"                actual_columns.append(expected_column)\r\n",
					"\r\n",
					"        self.assertEqual(expected_columns, actual_columns, f'[Dataframes:SparkDataFrameClass:WriteDataframe] Expected columns to be appended to the delta table, but there are missing columns: \\nEXPECTED {expected_columns} \\nMISSING {actual_columns}')\r\n",
					"        \r\n",
					"    # Success: Add columns to a non-empty table\r\n",
					"    def test_validatecolumnnames_deltatableclass_addcolumnstononemptytable(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Create an empty delta table\r\n",
					"        delta_table_object = DeltaTableClass_v2()\r\n",
					"\r\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_nonempty_table'\r\n",
					"        table_description = 'test schema evolution'\r\n",
					"        column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"        partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\r\n",
					"\r\n",
					"        delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\r\n",
					"        delta_table = delta_table_object.load_delta_table(destination_path)\r\n",
					"        # Add data to the delta table\r\n",
					"        dataframe = spark.createDataFrame(\r\n",
					"            data=[\r\n",
					"                [\"foo\", \"foo\", None, None, None, None, None, None, 1, 1, 1],\r\n",
					"                [\"bar\", \"bar\", None, None, None, None, None, None, 1, 1, 1]\r\n",
					"            ],\r\n",
					"            schema=\"column_1 string, column_2 string, t_load_date_raw string, t_load_date_silver string, t_extract_date string, t_update_date string, t_insert_date string, t_file_name string, t_plan_id string, t_task_id string, t_file_id string\"\r\n",
					"        )\r\n",
					"\r\n",
					"        pk_columns_dict = {\"column_1\": \"column_1\"}\r\n",
					"        column_names_dict = {\r\n",
					"            \"column_1\": \"column_1\", \r\n",
					"            \"column_2\": \"column_2\",\r\n",
					"            \"t_load_date_raw\": \"t_load_date_raw\", \r\n",
					"            \"t_load_date_silver\": \"t_load_date_silver\", \r\n",
					"            \"t_extract_date\": \"t_extract_date\", \r\n",
					"            \"t_update_date\": \"t_update_date\", \r\n",
					"            \"t_insert_date\": \"t_insert_date\", \r\n",
					"            \"t_file_name\": \"t_file_name\", \r\n",
					"            \"t_plan_id\":\"t_plan_id\" , \r\n",
					"            \"t_task_id\":\"t_task_id\" , \r\n",
					"            \"t_file_id\":\"t_file_id\"\r\n",
					"        }\r\n",
					"\r\n",
					"        MergeFile(delta_table, dataframe, pk_columns_dict=pk_columns_dict, column_names_dict=column_names_dict, target_options={\"partitioning\": [{\"name\": \"column_1\"}]})\r\n",
					"        \r\n",
					"        # Add new columns to the delta table\r\n",
					"        new_column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_3\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"        # Define expectations\r\n",
					"        expected_columns = ['column_1', 'column_2', 'column_3']\r\n",
					"        expected_count = 2\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=new_column_objects)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the delta table contains the 3 columns (excluding the technical fields)\r\n",
					"        delta_table = delta_table_object.load_delta_table(source_path=destination_path)\r\n",
					"        table_columns = delta_table.toDF().columns\r\n",
					"        actual_count = delta_table.toDF().count()\r\n",
					"        \r\n",
					"        actual_columns = []\r\n",
					"        for expected_column in expected_columns:\r\n",
					"            if expected_column in table_columns:\r\n",
					"                actual_columns.append(expected_column)\r\n",
					"\r\n",
					"        self.assertEqual(expected_columns,  actual_columns, f'[Dataframes:SparkDataFrameClass:WriteDataframe] Expected columns to be appended to the delta table, but there are missing columns: \\nEXPECTED {expected_columns} \\nMISSING {actual_columns}')\r\n",
					"        self.assertEqual(expected_count,    actual_count,   f'[Dataframes:SparkDataFrameClass:WriteDataframe] Expected number of columns does not match with actual number of columns: \\nEXPECTED {expected_count} \\ACTUAL {actual_count}')\r\n",
					"\r\n",
					"    # Success: No columns need to be added\r\n",
					"    \r\n",
					"    def test_validatecolumnnames_deltatableclass_functioncalls(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Create an empty delta table\r\n",
					"        delta_table_object = DeltaTableClass_v2()\r\n",
					"\r\n",
					"        destination_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/validate_columnnames_nonempty_table'\r\n",
					"        table_description = 'test schema evolution'\r\n",
					"        column_objects = [\r\n",
					"            {\"column_name\": \"column_1\", \"dimension\": \"PK\", \"data_type\": \"string\"},\r\n",
					"            {\"column_name\": \"column_2\", \"dimension\": \"SCD2\", \"data_type\": \"string\"}\r\n",
					"        ]\r\n",
					"        partition_objects = [{\"name\": \"column_1\", \"sequence\": 1}]\r\n",
					"\r\n",
					"        delta_table_object.create_delta_table(destination_path=destination_path, table_description=table_description, column_objects=column_objects, partition_objects=partition_objects)\r\n",
					"        \r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        with patch.object(SparkDataFrameClass, 'create_dataframe') as mock_create_dataframe, \\\r\n",
					"            patch.object(SparkDataFrameClass, 'write_dataframe') as mock_write_dataframe:\r\n",
					"            delta_table_object.validate_columnnames(delta_path=destination_path, column_objects=column_objects)\r\n",
					"\r\n",
					"        # EVALUTE\r\n",
					"        # Validate that the mocked functions have not been called\r\n",
					"        mock_write_dataframe.assert_not_called()\r\n",
					"        mock_create_dataframe.assert_not_called()\r\n",
					"    \r\n",
					"    def test_mergefile_count_amount_of_rows (self):\r\n",
					"        # PREPROCESS\r\n",
					"        # initialize the variables\r\n",
					"        inserted_row_start_expected='3'\r\n",
					"        updated_row_start_expected='0'\r\n",
					"        source_row_start_expected='3'\r\n",
					"        inserted_row_end_expected='1'\r\n",
					"        updated_row_end_expected='2'\r\n",
					"        source_row_end_expected='3'\r\n",
					"        path=\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"count_test\")\r\n",
					"        delta_schema = StructType([StructField('id', StringType(),True), StructField('info', StringType(), False)])\r\n",
					"        old_alias = 'test'\r\n",
					"        new_alias = 'debug'\r\n",
					"        pk_columns_dict = {'id': 'ID'}\r\n",
					"        column_names_dict = {'id': 'ID', \"info\": \"INFO\"} \r\n",
					"        #create DataFrame\r\n",
					"        deltaTable=spark.createDataFrame([],delta_schema)\r\n",
					"        deltaTable.write.mode('overwrite').format(\"delta\").save(path)\r\n",
					"        delta_table_class = DeltaTableClass_v2(debug=True)\r\n",
					"        #loading the first deltaTable\r\n",
					"        deltaTable = delta_table_class.load_delta_table(source_path=path)\r\n",
					"        dataframe=spark.createDataFrame(\r\n",
					"            data=[\r\n",
					"                (\"column1\",\"valeur1\"),\r\n",
					"                (\"column2\",\"valeur2\"),\r\n",
					"                (\"column3\",\"valeur3\")\r\n",
					"                ],\r\n",
					"            schema=\"id string, info string\"\r\n",
					"            )\r\n",
					"        #merge result\r\n",
					"        deltaTable.alias(old_alias) \\\r\n",
					"        .merge(\r\n",
					"        dataframe.alias(new_alias), pk_match(old_alias, new_alias, pk_columns_dict)) \\\r\n",
					"        .whenMatchedUpdate(set=ColumnMatch(column_names_dict, new_alias)) \\\r\n",
					"        .whenNotMatchedInsert(values = NoMatch(column_names_dict, new_alias)) \\\r\n",
					"        .execute()\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        inserted_row_start_actual,updated_row_start_actual,source_row_start_actual=delta_table_class.count_delta_table()\r\n",
					"        #loading the second deltaTable\r\n",
					"        dataframe=spark.createDataFrame(\r\n",
					"            data=[\r\n",
					"                (\"column2\",\"valeur1\"),\r\n",
					"                (\"column3\",\"valeur2\"),\r\n",
					"                (\"column4\",\"valeur3\")\r\n",
					"                ],\r\n",
					"            schema=\"id string, info string\"\r\n",
					"            )\r\n",
					"        #merge result\r\n",
					"        deltaTable.alias(old_alias) \\\r\n",
					"        .merge(\r\n",
					"        dataframe.alias(new_alias), pk_match(old_alias, new_alias, pk_columns_dict)) \\\r\n",
					"        .whenMatchedUpdate(set=ColumnMatch(column_names_dict, new_alias)) \\\r\n",
					"        .whenNotMatchedInsert(values = NoMatch(column_names_dict, new_alias)) \\\r\n",
					"        .execute()# EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        inserted_row_end_actual,updated_row_end_actual,source_row_end_actual=delta_table_class.count_delta_table()\r\n",
					"        # EVALUTE\r\n",
					"        self.assertEqual(inserted_row_start_expected, inserted_row_start_actual, f\"[DeltaTables:DeltaTableClassv2:count_delta_table]: Insert merge count did not match expectations: \\nExpected: {inserted_row_start_expected}  \\nActual:   {inserted_row_start_actual}\")\r\n",
					"        self.assertEqual(updated_row_start_expected, updated_row_start_actual, f\"[DeltaTables:DeltaTableClassv2:count_delta_table]: Update merge count did not match expectations: \\nExpected: {updated_row_start_expected}  \\nActual:   {updated_row_start_actual}\")\r\n",
					"        self.assertEqual(source_row_start_expected, source_row_start_actual, f\"[DeltaTables:DeltaTableClassv2:count_delta_table]: Merge count did not match expectations: \\nExpected: {source_row_start_expected}  \\nActual:   {source_row_start_actual}\")\r\n",
					"        self.assertEqual(inserted_row_end_expected, inserted_row_end_actual, f\"[DeltaTables:DeltaTableClassv2:count_delta_table]: Insert merge count did not match expectations: \\nExpected: {inserted_row_end_expected}  \\nActual:   {inserted_row_end_actual}\")\r\n",
					"        self.assertEqual(updated_row_end_expected, updated_row_end_actual, f\"[DeltaTables:DeltaTableClassv2:count_delta_table]: Update merge count did not match expectations: \\nExpected: {updated_row_end_expected}  \\nActual:   {updated_row_end_actual}\")\r\n",
					"        self.assertEqual(source_row_end_expected, source_row_end_actual, f\"[DeltaTables:DeltaTableClassv2:count_delta_table]: Merge count did not match expectations: \\nExpected: {source_row_end_expected}  \\nActual:   {source_row_end_actual}\")\r\n",
					"        \r\n",
					"    \r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeltaTableClassv2_validatecolumnnames)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in SparkDataFrameClass validatecolumnnames tests, something went wrong!\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Test Class: OptimizeDeltaTable"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: OptimizeDeltaTable\n",
					"# This class will test the __init__() of the OptimizeDeltaTable-class defined in the DeltaTables notebook\n",
					"class Test_OptimizeDeltaTable_initialization(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the __init__() method\n",
					"    \n",
					"    # Success: Initialize an OptimizeDeltaTable class instance\n",
					"    # Expect the creation of a DeltaTable and a DeltaOptimizeBuilder class instance as class-arguments\n",
					"    def test_initization_OptimizeDeltaTable_success(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments that will be used to call the method with\n",
					"        table_name = 'OptimizeDeltaTable/initialization/load_delta_table'\n",
					"\n",
					"        # Create an empty delta table in the silver container -&gt; This table will be loaded using the load_delta_table function\n",
					"        column_objects = [\n",
					"            { 'column_name': 'column_1', 'data_type': 'timestamp', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }\n",
					"        ]\n",
					"        table_description = 'test OptimizeDeltaTable class initialization'\n",
					"        class_object = DeltaTableClass_v2()\n",
					"\n",
					"        delta_table_path = f'abfss://silver@{storage_account}.dfs.core.windows.net/{table_name}'\n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(self.env_code, container_name, table_name)\n",
					"\n",
					"        class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=[])\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:OptimizeDeltaTable:__init__] Run the method under test\n",
					"        class_object = OptimizeDeltaTable(table_name=table_name, env_code=self.env_code)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that delta_table_path-argument has been initialised\n",
					"        self.assertIsNotNone(class_object.delta_table_path,                              f\"[DeltaTables:OptimizeDeltaTable:__init__] Argument delta_table_path was not initialized\")\n",
					"        self.assertEqual(delta_table_path, class_object.delta_table_path,                f\"[DeltaTables:OptimizeDeltaTable:__init__] Argument delta_table_path was not initialized correctly: \\nEXPECTED {delta_table_path} \\nACTUAL:  {class_object.delta_table_path}\")\n",
					"        # Validate that an instance of class DeltaTable has been initialised\n",
					"        self.assertIsNotNone(class_object.delta_table,                                   f\"[DeltaTables:OptimizeDeltaTable:__init__] Argument delta_table was not initialized\")\n",
					"        self.assertIsInstance(class_object.delta_table, DeltaTable,                      f\"[DeltaTables:OptimizeDeltaTable:__init__] Argument delta_table was not initialized as a DeltaTable instance\")\n",
					"        # Validate that an instance of class DeltaOptimizeBuilder has been initialised\n",
					"        self.assertIsNotNone(class_object.delta_table_optimized,                         f\"[DeltaTables:OptimizeDeltaTable:__init__] Argument delta_table_optimized was not initialized\")\n",
					"        self.assertIsInstance(class_object.delta_table_optimized, DeltaOptimizeBuilder,  f\"[DeltaTables:OptimizeDeltaTable:__init__] Argument delta_table_optimized was not initialized as a DeltaOptimizeBuilder instance\")\n",
					"\n",
					"    # Success: Validate that the correct functions have been called during initialization\n",
					"    def test_initialization_OptimizeDeltaTable_function_calls(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:OptimizeDeltaTable:__init__] Run the method under test with mocked functions\n",
					"        with patch.object(DeltaTable, 'forPath') as mock_forpath, \\\n",
					"            patch.object(OptimizeDeltaTable, 'optimize_deltatable') as mock_optimize_deltatable:\n",
					"            class_object = OptimizeDeltaTable(table_name='test', env_code=self.env_code)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the expected functions have been called with the expected arguments\n",
					"        mock_forpath.assert_called_once()\n",
					"        mock_forpath.assert_called_once_with(ANY, class_object.delta_table_path)\n",
					"        mock_optimize_deltatable.assert_called_once()\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_OptimizeDeltaTable_initialization)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass initialization tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"**Dev-Info**: The below Test Case will test the optimize_table_storage function, which calls compat_deltatable and z_order_deltatable. For each of the last 2 methods, one additional test has been added to validate that an error is thrown if there is no DeltaOptimizeBuilder object present. No separate Test Case is defined for these methods, as they call internal spark functions and do not necessarily contain any logic to test."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: OptimizeDeltaTable_Initialisation\n",
					"# This class will test the optimize_table_storage function (and related functions) of the OptimizeDeltaTable defined in the Dataframes notebook\n",
					"class Test_OptimizeDeltaTable_OptimizeTableStorage(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def setUp(self):\n",
					"        self.env_code = env_code\n",
					"        \n",
					"        # Create an empty delta table in the silver container -&gt; This table will be loaded using the load_delta_table function\n",
					"        self.table_name = 'OptimizeDeltaTable/optimize_table_storage/load_delta_table'\n",
					"        silver_container = 'silver'\n",
					"        delta_table_path = f'abfss://{silver_container}@{storage_account}.dfs.core.windows.net/{self.table_name}'\n",
					"        column_objects = [\n",
					"            { 'column_name': 'column_1', 'data_type': 'timestamp', 'dimension':'PK' }, \n",
					"            { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }\n",
					"        ]\n",
					"        table_description = 'test OptimizeDeltaTable class optimize_table_storage'\n",
					"\n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(delta_table_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(self.env_code, silver_container, self.table_name)\n",
					"        \n",
					"        class_object = DeltaTableClass_v2()\n",
					"        class_object.create_delta_table(destination_path=delta_table_path, table_description=table_description, column_objects=column_objects, partition_objects=[])\n",
					"\n",
					"\n",
					"        return\n",
					"\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the optimize_table_storage() method\n",
					"\n",
					"    # Success: Call the method under test without a z-order list and validate that a set of functions have been called with a set of arguments\n",
					"    def test_optimizetablestorage_OptimizeDeltaTable_function_calls(self):\n",
					"        # PREPROCESS\n",
					"        # Define a class-object that will be used to call the method with\n",
					"        class_object = OptimizeDeltaTable(table_name='test', env_code=self.env_code)\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:OptimizeDeltaTable:optimize_table_storage]: Execute method under test\n",
					"        with patch.object(DeltaOptimizeBuilder, 'executeCompaction') as mock_compaction, \\\n",
					"            patch.object(DeltaOptimizeBuilder, 'executeZOrderBy') as mock_zorderby:\n",
					"            class_object.optimize_table_storage()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # [Dataframes:OptimizeDeltaTable] Validate that underlying functions have been called as expected\n",
					"        mock_compaction.assert_called_once()\n",
					"        mock_zorderby.assert_not_called()\n",
					"\n",
					"    # Success: Call the method under test with a z-order list and validate that a set of functions have been called with a set of arguments\n",
					"    def test_optimizetablestorage_OptimizeDeltaTable_function_calls(self):\n",
					"        # PREPROCESS\n",
					"        class_object = OptimizeDeltaTable(table_name=self.table_name, env_code=self.env_code)\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:OptimizeDeltaTable] Execute method under test\n",
					"        with patch.object(DeltaOptimizeBuilder, 'executeCompaction') as mock_compaction, \\\n",
					"            patch.object(DeltaOptimizeBuilder, 'executeZOrderBy') as mock_zorderby:\n",
					"            class_object.optimize_table_storage(['test'])\n",
					"        \n",
					"        # EVALUATE\n",
					"        # [Dataframes:OptimizeDeltaTable] Validate that underlying functions have been called as expected\n",
					"        mock_compaction.assert_called_once()\n",
					"        mock_zorderby.assert_called_once_with(z_order_columns=['test'])\n",
					"\n",
					"    # Failure: Throw an error when the class-object does not contain a DeltaOptimizeBuilder object when calling compact_deltatable()\n",
					"    def test_compactdeltatable_OptimizeDeltaTable_nonexist_deltaoptimizebuilder(self):\n",
					"        # PREPROCESS\n",
					"        # Create a class_object and set delta_table_optimized to None\n",
					"        class_object = OptimizeDeltaTable(table_name=self.table_name, env_code=self.env_code)\n",
					"        class_object.delta_table_optimized = None\n",
					"\n",
					"        # Define the error that is expected to be thrown \n",
					"        expected_error = \"[Dataframes:OptimizeDeltaTable] delta_table_optimized member is not of class DeltaOptimizeBuilder for compaction\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Dataframes:OptimizeDeltaTable:compact_deltatable] Execute method under test\n",
					"        with self.assertRaises(TypeError) as error:\n",
					"            class_object.compact_deltatable()\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the expected error matches the actual\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:OptimizeDeltaTable:compact_deltatable] Expected error does not match actual: {expected_error} versus {actual_error}\")\n",
					"\n",
					"    # Failure: Throw an error when the class-object does not contain a DeltaOptimizeBuilder object when calling z_order_deltatable()\n",
					"    def test_zorderdeltatable_OptimizeDeltaTable_nonexist_deltaoptimizebuilder(self):\n",
					"        # PREPROCESS\n",
					"        # Create a class_object and set delta_table_optimized to None\n",
					"        class_object = OptimizeDeltaTable(table_name=self.table_name, env_code=self.env_code)\n",
					"        class_object.delta_table_optimized = None\n",
					"\n",
					"        # Define the error that is expected to be thrown \n",
					"        expected_error = \"[Dataframes:OptimizeDeltaTable] delta_table_optimized member is not of class DeltaOptimizeBuilder for Z-order\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # [Dataframes:OptimizeDeltaTable:z_order_deltatable] Execute method under test\n",
					"        with self.assertRaises(TypeError) as error:\n",
					"            class_object.z_order_deltatable(z_order_columns=list())\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the expected error matches the actual\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"[Dataframes:OptimizeDeltaTable:z_order_deltatable] Expected error does not match actual: {expected_error} versus {actual_error}\")\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_OptimizeDeltaTable_OptimizeTableStorage)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")   "
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Test Class: DeployDeltaTable"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeployDeltaTable\n",
					"# This class will test the __init__() of the DeployDeltaTable-class defined in the DeltaTables notebook\n",
					"class Test_DeployDeltaTable_initialization(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the __init__() method\n",
					"    \n",
					"    # Success: Initialize an DeployDeltaTable class instance\n",
					"    # Expect the creation of a DeltaTable and a DeltaOptimizeBuilder class instance as class-arguments\n",
					"    def test_initization_DeployDeltaTable_success(self):\n",
					"        # PREPROCESS\n",
					"        # Create an empty delta table in the silver container -&gt; This table will be loaded using the load_delta_table function\n",
					"        table_name = 'DeployDeltaTable/initialization/load_delta_table'\n",
					"        table_description = 'Initialize the DeployDeltaTable class'\n",
					"        column_info = \"\"\"[\n",
					"            {'column_name': 'test', 'dimension': 'PK', 'data_type': 'string'},\n",
					"            {'column_name': 'test2', 'dimension': 'SCD2', 'data_type': 'string'},\n",
					"            {'column_name': 'test3', 'dimension': 'SCD2', 'data_type': 'string'}\n",
					"        ]\"\"\"\n",
					"        env_code = self.env_code\n",
					"        partitioning = list() \n",
					"\n",
					"        expected_silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, table_name)  \n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:DeployDeltaTable] Run the method under test\n",
					"        class_object = DeployDeltaTable(\n",
					"            table_name=table_name, \n",
					"            storage_account=storage_account, \n",
					"            container_name=container_name, \n",
					"            table_description=table_description, \n",
					"            column_info=column_info,\n",
					"            partitioning=partitioning\n",
					"        )\n",
					"    \n",
					"        # EVALUATE\n",
					"        # Validate that the silver_path matches expectations\n",
					"        self.assertIsNotNone(class_object.silver_path)\n",
					"        self.assertEqual(expected_silver_path, class_object.silver_path)\n",
					"        # Validate that the column_objects has been converted to a list\n",
					"        self.assertTrue(isinstance(class_object.column_objects, list))\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeployDeltaTable_initialization)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass initialization tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeployDeltaTable\n",
					"# This class will test the __init__() of the DeployDeltaTable-class defined in the DeltaTables notebook\n",
					"class Test_DeployDeltaTable_initialization(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    \n",
					"    def setUp(self):\n",
					"        # Define the env_code parameter to use throughout the class\n",
					"        self.env_code = env_code\n",
					"\n",
					"        # Mock function mssparuktils.fs.exist\n",
					"        # This function will check if a delta table already exists\n",
					"        # Each test-function will mock the output of this function and test which functions have been called depending on the output (True/False)\n",
					"        self.mssparkutils_fs_exists_patcher = patch.object(mssparkutils.fs, 'exists')\n",
					"        self.mock_mssparkutils_fs_exists    = self.mssparkutils_fs_exists_patcher.start()\n",
					"\n",
					"        # Mock all functions related to table creation and validation\n",
					"        # These functions are part of the DeltaTableClass_v2 and have already been tested\n",
					"        # Depending on the output of mssparkutils.fs.exist, certain methods need to be called whilst others should not be\n",
					"        self.create_delta_table_patcher = patch.object(DeltaTableClass_v2, 'create_delta_table')\n",
					"        self.mock_create_delta_table    = self.create_delta_table_patcher.start()\n",
					"\n",
					"        self.validate_delta_table_patcher = patch.object(DeltaTableClass_v2, 'validate_delta_table')\n",
					"        self.mock_validate_delta_table    = self.validate_delta_table_patcher.start()\n",
					"\n",
					"        self.add_notnull_constraints_patcher = patch.object(DeltaTableClass_v2, 'add_notnull_constraints')\n",
					"        self.mock_add_notnull_constraints    = self.add_notnull_constraints_patcher.start()\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # tearDown Test Case\n",
					"        # Stop all patched functions\n",
					"        self.mssparkutils_fs_exists_patcher.stop()\n",
					"        self.add_notnull_constraints_patcher.stop()\n",
					"        self.create_delta_table_patcher.stop()\n",
					"        self.validate_delta_table_patcher.stop()\n",
					"        return\n",
					"    \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the __init__() method\n",
					"    \n",
					"    # Success: When mssparutils.fs.exists returns False, call create_delta_table and add_notnull_constraints\n",
					"    def test_initization_DeployDeltaTable_createtable(self):\n",
					"        # PREPROCESS\n",
					"        # Define return value for mssparkutils.fs.exists\n",
					"        self.mock_mssparkutils_fs_exists.return_value = False\n",
					"\n",
					"        # Create an instance of class DeployDeltaTable\n",
					"        column_info =\"\"\"[\n",
					"            {'column_name': 'test', 'dimension': 'PK', 'data_type': 'string'},\n",
					"            {'column_name': 'test2', 'dimension': 'SCD2', 'data_type': 'string'},\n",
					"            {'column_name': 'test3', 'dimension': 'SCD2', 'data_type': 'string'}\n",
					"        ]\"\"\"\n",
					"\n",
					"        class_object = DeployDeltaTable(\n",
					"            table_name=Mock(), \n",
					"            storage_account=Mock(), \n",
					"            container_name=Mock(), \n",
					"            table_description=Mock(), \n",
					"            column_info=column_info, \n",
					"            partitioning=Mock()\n",
					"        )\n",
					"\n",
					"        # Define expected return values\n",
					"        expected_column_dimensions = {'test': 'PK', 'test2': 'SCD2', 'test3': \"SCD2\"}\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:DeployDeltaTable] Run the method under test\n",
					"        class_object.deploy_delta_table()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that certain functions have (not) been called\n",
					"        self.mock_create_delta_table.assert_called_once()\n",
					"        self.mock_validate_delta_table.assert_not_called()\n",
					"        self.mock_add_notnull_constraints.assert_called_once()\n",
					"        # For add_notnull_constraints: Validate that the method has been called with a specific set of arguments\n",
					"        self.mock_add_notnull_constraints.assert_called_once_with(source_path=ANY, column_dimensions=expected_column_dimensions)\n",
					"\n",
					"    # Success: When mssparutils.fs.exists returns True, call validate_delta_table\n",
					"    def test_initization_DeployDeltaTable_validatetable(self):\n",
					"        # PREPROCESS\n",
					"        # Define return value for mssparkutils.fs.exists\n",
					"        self.mock_mssparkutils_fs_exists.return_value = True\n",
					"\n",
					"        # Create an instance of class DeployDeltaTable\n",
					"        class_object = DeployDeltaTable(\n",
					"            table_name='test_table', \n",
					"            storage_account='test_storage', \n",
					"            container_name='test_container', \n",
					"            table_description='test_description', \n",
					"            column_info='{\"test\": \"column\"}', \n",
					"            partitioning=Mock()\n",
					"        )\n",
					"\n",
					"        # EXECUTE\n",
					"        # [Dataframes:DeployDeltaTable] Run the method under test\n",
					"        class_object.deploy_delta_table()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that certain functions have (not) been called\n",
					"        self.mock_create_delta_table.assert_not_called()\n",
					"        self.mock_validate_delta_table.assert_called_once()\n",
					"        self.mock_validate_delta_table.assert_called_once_with(table_name='test_table', storage_account='test_storage', container_name='test_container', column_objects={\"test\": \"column\"})\n",
					"        self.mock_add_notnull_constraints.assert_not_called()\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeployDeltaTable_initialization)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in SparkDataFrameClass initialization tests, something went wrong!\")"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_Deploy.json">
{
	"name": "Test_Deploy",
	"properties": {
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "f5537ed4-78d0-44b9-b63d-67f4c7efe1ed"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Deploy\r\n",
					"**Purpose**: This Notebook will use the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the Spark Notebooks in the folder *Deploy*.\r\n",
					"\r\n",
					"Unit tests: To test if a method does what we expect it to do, a untit test is written. A unit test isolates a method from the overall environment and executes the method. The aim is to be able to define the expected outcome of the method and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the method successfully\r\n",
					"\r\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\r\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\r\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import unittest\r\n",
					"from unittest.mock import patch, MagicMock, Mock\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from io import StringIO"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters\r\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. When testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Parameters that need to be given to create the delta table\r\n",
					"environment_code = ''"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\r\n",
					"storage_account = f'{env_code}dapstdala1' # The storage account where the delta table is expected to be stored\r\n",
					"database_name = 'silver'"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the methods that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define Test Classes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_DeployDeltaTables(unittest.TestCase):\r\n",
					"    def setUp(self):\r\n",
					"        # Before everything else, remove tables from silver database\r\n",
					"        # Save the names so that you can use them later in the teardown function\r\n",
					"        self.tables = ['delta_mock_container', 'delta_mock', ]\r\n",
					"\r\n",
					"        for table in self.tables:\r\n",
					"            clean_table(table)\r\n",
					"\r\n",
					"        # Init basic variables\r\n",
					"        self.table_name = 'delta_mock'\r\n",
					"        self.database_name = database_name\r\n",
					"        self.column_info = '[{\"column_name\": \"mock_test1\", \"data_type\": \"string\"}, {\"column_name\": \"mock_test2\", \"data_type\": \"string\"}]'\r\n",
					"        self.env_code = env_code\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # After everything is done, remove all the created tables again\r\n",
					"        for table in self.tables:\r\n",
					"            clean_table(table)\r\n",
					"        \r\n",
					"        # And also clean the silver container\r\n",
					"        clean_containers(self.env_code,['silver'])\r\n",
					"\r\n",
					"    def test_columninfo_error1(self):\r\n",
					"        # Test if an error is thrown when the column_info argument has the right structure, but with \\n and \\r and white spaces in it\r\n",
					"        column_info = \"\\\"[\\\\r\\\\n  {\\\\r\\\\n    \\\\\\\"column_name\\\\\\\": \\\\\\\"id\\\\\\\",\\\\r\\\\n    \\\\\\\"dimension\\\\\\\": \\\\\\\"PK\\\\\\\",\\\\r\\\\n    \\\\\\\"data_type\\\\\\\": \\\\\\\"integer\\\\\\\"\\\\r\\\\n  },\\\\r\\\\n  {\\\\r\\\\n    \\\\\\\"column_name\\\\\\\": \\\\\\\"employees\\\\\\\",\\\\r\\\\n    \\\\\\\"dimension\\\\\\\": \\\\\\\"SCD2\\\\\\\",\\\\r\\\\n    \\\\\\\"data_type\\\\\\\": \\\\\\\"string\\\\\\\"\\\\r\\\\n  },\\\\r\\\\n  {\\\\r\\\\n    \\\\\\\"column_name\\\\\\\": \\\\\\\"company\\\\\\\",\\\\r\\\\n    \\\\\\\"dimension\\\\\\\": \\\\\\\"SCD2\\\\\\\",\\\\r\\\\n    \\\\\\\"data_type\\\\\\\": \\\\\\\"string\\\\\\\"\\\\r\\\\n  }\\\\r\\\\n]\\\"\"\r\n",
					"\r\n",
					"        with self.assertRaises(Exception) as error:\r\n",
					"            mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': self.table_name, 'database_name': self.database_name, 'column_info': column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"        # Error message is a bit dynamic, so just test on a substring\r\n",
					"        self.assertIn('string indices must be integers', str(error.exception))\r\n",
					"\r\n",
					"    def test_columninfo_error2(self):\r\n",
					"        # Test if an error is thrown when the column_info argument has the wrong structure\r\n",
					"        column_info = '[{\"column_names\": [\"id\", \"string_column\"], \"column_datatypes\": [\"integer\", \"string\", \"integer\"]}]'\r\n",
					"\r\n",
					"        with self.assertRaises(Exception) as error:\r\n",
					"            mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': self.table_name, 'database_name': self.database_name, 'column_info': column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"        # Error message is a bit dynamic, so just test on a substring\r\n",
					"        self.assertIn('data_type', str(error.exception))\r\n",
					"    \r\n",
					"    def test_table_creation_success(self):\r\n",
					"        # Test if you can successfully create a new table and that it exists\r\n",
					"        mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': self.table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"        spark.sql(f'SELECT * FROM {self.database_name}.delta_mock')\r\n",
					"    \r\n",
					"    def test_table_notexist_error(self):\r\n",
					"        # Test that the query returns an error when the table doesn't exist\r\n",
					"        # You don't have to include the name inside the clean-up function, because it should never be created\r\n",
					"        # If it was created, this test will break and that needs to happen\r\n",
					"        with self.assertRaises(Exception) as error:\r\n",
					"            spark.sql(f'SELECT * FROM {self.database_name}.delta_mock_error')\r\n",
					"        \r\n",
					"        self.assertIn('Table or view not found', str(error.exception))\r\n",
					"\r\n",
					"    def test_containerempty_tableexists_success(self):\r\n",
					"        # Test that after the creation of a delta table, the SILVER container is cleared and that it will not return an error\r\n",
					"        # Table should exist in silver\r\n",
					"        table_name = 'delta_mock_container'\r\n",
					"\r\n",
					"        # Creating the table will also create a folder inside of the SILVER container\r\n",
					"        mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"        # Keep the table, but clean the container\r\n",
					"        clean_containers(self.env_code,['silver'])\r\n",
					"\r\n",
					"        mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"        # This still will return a result, but just need to test it if it can be properly run\r\n",
					"        spark.sql(f'SELECT * FROM {self.database_name}.delta_mock_container')\r\n",
					"\r\n",
					"    # def test_containerempty_tableexists_emptycontainer(self):\r\n",
					"    #     # dev-note: https://stackoverflow.com/questions/73310653/python-unit-test-with-mocking-assert-called-for-function-outside-class\r\n",
					"    #     # dev-note: Create a class from DeployDeltaTables so you can test it properly (can't use %run to expose functions to patch them, and can't access sys.stdout from notebook.run)\r\n",
					"\r\n",
					"    #     # Test that after the creation of a delta table, the SILVER container is cleared and that it will return a specific error\r\n",
					"    #     table_name = 'delta_mock_container'\r\n",
					"\r\n",
					"    #     # Creating the table will also create a folder inside of the SILVER container\r\n",
					"    #     mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"    #     # Keep the table, but clean the container\r\n",
					"    #     clean_containers(self.env_code,['silver'])\r\n",
					"\r\n",
					"    #     with patch('sys.stdout', new = StringIO()) as mock_stdout:\r\n",
					"    #         mssparkutils.notebook.run('Deploy/DeployDeltaTable', arguments={'table_name': table_name, 'database_name': self.database_name, 'column_info': self.column_info, 'storage_account': storage_account})\r\n",
					"\r\n",
					"            \r\n",
					"    #     # Starting Table deployment notebook for silver.delta_mock_container\r\n",
					"    #     # ['simon_test', 'powershell_test', 'powershell_test_2', 'csv_integration', 'json_integration', 'delta_test', 'delta_mock_container']\r\n",
					"    #     # Table silver.delta_mock_container does exist, but not inside of the silver container.\r\n",
					"    #     # Removing silver.delta_mock_container from silver db, and creating a new table &amp; link\r\n",
					"    #     # Creating table schema\r\n",
					"    #     # Creating delta table\r\n",
					"            \r\n",
					"    #     self.assertIn(\"Starting Table deployment notebook for silver.delta_mock_container\",mock_stdout.getvalue())\r\n",
					"        \r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeployDeltaTables)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					""
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_DeployDeltaTable_v3.json">
{
	"name": "Test_DeployDeltaTable_v3",
	"properties": {
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "6adce960-33c8-4e04-bf10-d6238ca62188"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Deploy v3\n",
					"**Purpose**: This Notebook will use the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the Spark Notebooks in the folder *Deploy*.\n",
					"\n",
					"Unit tests: To test if a method does what we expect it to do, a untit test is written. A unit test isolates a method from the overall environment and executes the method. The aim is to be able to define the expected outcome of the method and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the method successfully\n",
					"\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import necessary packages"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import unittest\n",
					"from unittest.mock import patch, MagicMock, Mock, ANY, call\n",
					"from notebookutils import mssparkutils\n",
					"from io import StringIO\n",
					"from pyspark.sql import SparkSession\n",
					"import delta.tables as dt\n",
					"import pyspark.sql as sql\n",
					""
				],
				"execution_count": 41
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. When testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Parameters that need to be given to create the delta table\n",
					"environment_code = 'dev'"
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\n",
					"storage_account = f'{env_code}dapstdala1' # The storage account where the delta table is expected to be stored\n",
					"container_name = 'silver'\n",
					"\n",
					"\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 42
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the methods that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Deploy/DeployDeltaTable_v3"
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 45
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define the test classes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: InitialisationDeltaTable\n",
					"# This class will test the __init__ function of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_InitialisationDeltaTable(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # Patch python functions: The clean_containers function needs to be run in isolation. \n",
					"        # Other function-calls need to be mocked to avoid getting errors on things that do not need to be tested.\n",
					"        # Set the parameters that will be used to initialise a DeltaTable-object\n",
					"        self.table_name = 'unittest_table'\n",
					"        self.container_name = 'unittest_database'\n",
					"        self.storage_account = 'unittest'\n",
					"        self.table_description = 'Unittest created table'\n",
					"        self.column_info = \"[{}]\" #\"[{ 'column_name': 'column_1', 'data_type': 'string', 'dimenion': 'BK' }, { 'column_name': 'column_2','data_type': 'integer', 'dimenion': 'SCD2' }, { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimenion': 'BK' }]\"\n",
					"        self.target_options = {}\n",
					"\n",
					"\n",
					"        # Patch the functions that are called by the init-function of the DeltaTable-class\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\n",
					"\n",
					"        ## Function in the DeployDeltaTable notebook        \n",
					"        self.mock_process_column_info_string_patcher = patch('__main__.process_column_info_string')\n",
					"        self.mock_process_column_info_string = self.mock_process_column_info_string_patcher.start()\n",
					"        \n",
					"        ## Method in the DeployDeltaTable notebook        \n",
					"        self.mock_configure_datatypes_patcher = patch('__main__.configure_datatypes')\n",
					"        self.mock_configure_datatypes = self.mock_configure_datatypes_patcher.start()\n",
					"\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher-functions \n",
					"        self.mock_process_column_info_string_patcher.stop()\n",
					"        self.mock_configure_datatypes_patcher.stop()\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the __init__ function\n",
					"\n",
					"    # Success: Validate that the parameters full_name, silver_path, and column_dict all take on the expected values\n",
					"    def test_delta_table_init(self):\n",
					"        # PREPROCESS\n",
					"        # Based on the function call in the EXECUTION-section, the following parameter values are expected to be initialised\n",
					"        # expected_full_name = 'unittest_database.unittest_table'\n",
					"        expected_silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.container_name, self.storage_account, self.table_name)\n",
					"        expected_column_dict = { \"column_1\": \"string\", \"column_2\": \"integer\", \"column_3\": \"decimal(12,5)\", \"column_4\": \"varchar(4)\" }\n",
					"        expected_column_dimensions = { \"column_1\": \"BK\", \"column_2\": \"SCD2\", \"column_3\": \"BK\", \"column_4\": \"SCD2\" }\n",
					"\n",
					"\n",
					"        # Since the output from the process_column_info_string function is called and used from the __init__ function, the output (= return_value) is mocked\n",
					"        # When assuming the process_column_info_string function works, the following output would be expected to be returned:\n",
					"        self.mock_process_column_info_string.return_value = { \"column_name\": \"column_1\", \"data_type\": \"string\", \"dimension\": \"BK\" }, { \"column_name\": \"column_2\",\"data_type\": \"integer\", \"dimension\": \"SCD2\" }, { \"column_name\": \"column_3\",  \"data_type\": \"decimal(12,5)\", \"dimension\": \"BK\" }, { \"column_name\": \"column_4\",  \"data_type\": \"varchar(4)\", \"dimension\": \"SCD2\" }\n",
					"\n",
					"        # EXECUTE\n",
					"        # Initialize a DeltaTable-object, thereby calling the to-be tested __init__ function\n",
					"        delta_table = DeltaTableClass( self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info, self.target_options )\n",
					"\n",
					"        # EVALUATE\n",
					"        # After initialization, the following object-parameters should have been created\n",
					"        # actual_full_name = delta_table.full_name\n",
					"        actual_silver_path = delta_table.silver_path\n",
					"        actual_column_dict = delta_table.column_dict\n",
					"        actual_column_dimensions = delta_table.column_dimensions\n",
					"\n",
					"        # Validate that the expected object-parameters are equal to the actual object-parameters\n",
					"        # self.assertEqual(expected_full_name, actual_full_name,      'Something went wrong during initialisation of full_name')\n",
					"        self.assertEqual(expected_silver_path, actual_silver_path,  'Something went wrong during initialisation of silver_path')\n",
					"        self.assertEqual(expected_column_dict, actual_column_dict,  'Something went wrong during initialisation of column_dict')\n",
					"        self.assertEqual(expected_column_dimensions, actual_column_dimensions,  'Something went wrong during initialisation of column_dimensions')\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_InitialisationDeltaTable)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: DeployDeltaTable_v3\n",
					"# This class will test the deploy_delta_table() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_DeployDeltaTable_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # Create a DeltaTable-object using the below parameters\n",
					"        self.table_name = 'unittest_table'\n",
					"        self.container_name = 'unittest_database'\n",
					"        self.storage_account = 'unittest'\n",
					"        self.table_description = 'Unittest created table'\n",
					"        self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string', 'dimension':'BK' }, { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }, { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimension':'BK' }, { 'column_name': 'column_4',  'data_type': 'varchar(4)', 'dimension':'SCD2' }]\"\n",
					"        \n",
					"        self.delta_table_object = DeltaTableClass(self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info)\n",
					"        \n",
					"        # Patch the functions that are called by the deploy_delta_table-method of the DeltaTable-class\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\n",
					"\n",
					"        ## Method in the DeltaTable class\n",
					"        self.mock_validate_table_patcher = patch.object(DeltaTableClass, 'validate_table')\n",
					"        self.mock_validate_table = self.mock_validate_table_patcher.start()\n",
					"        \n",
					"        ## Method in the DeltaTable class        \n",
					"        self.mock_create_delta_table_patcher = patch.object(DeltaTableClass, 'create_delta_table')\n",
					"        self.mock_create_delta_table = self.mock_create_delta_table_patcher.start()\n",
					"\n",
					"        ## Function in the CleanWorkspace notebook\n",
					"        self.mock_clean_table_patcher = patch('__main__.clean_table')\n",
					"        self.mock_clean_table = self.mock_clean_table_patcher.start()\n",
					"\n",
					"        ## Method coming from library notebookutils.mssparkutils\n",
					"        self.mock_mssparkutils_fs_patcher = patch.object(mssparkutils, 'fs')\n",
					"        self.mock_mssparkutils_fs = self.mock_mssparkutils_fs_patcher.start()\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        self.mock_create_delta_table_patcher.stop()\n",
					"        self.mock_validate_table_patcher.stop()\n",
					"        self.mock_clean_table_patcher.stop()\n",
					"        self.mock_mssparkutils_fs_patcher.stop()\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the deploy_delta_table method\n",
					"\n",
					"    # Success: If table exists, call validate_table() function\n",
					"    def test_deploy_delta_table_call_validate(self):\n",
					"        # PREPROCESS\n",
					"        # Table does exist -&gt; call validate_table()\n",
					"        self.mock_mssparkutils_fs.exists.return_value = True\n",
					"\n",
					"        # EXECUTE\n",
					"        self.delta_table_object.deploy_delta_table()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Assert whether validate_table has been called exactly once\n",
					"        self.mock_validate_table.assert_called_once()\n",
					"\n",
					"    # Success: If tbale does not exist, call create_table() function\n",
					"    def test_deploy_delta_table_call_create(self):\n",
					"        # PREPROCESS\n",
					"        # Table does not exist -&gt; call validate_table()\n",
					"        self.mock_mssparkutils_fs.exists.return_value = False\n",
					"\n",
					"        # EXECUTE\n",
					"        self.delta_table_object.deploy_delta_table()\n",
					"\n",
					"        # EVLUATE\n",
					"        # Assert whether create_delta_table has been called exactly once\n",
					"        self.mock_create_delta_table.assert_called_once()\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_DeployDeltaTable_v3)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)"
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CreateDeltaTable_v3\n",
					"# This class will test the deploy_delta_table() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_CreateDeltaTable_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # Create a DeltaTable-object using the below parameters\n",
					"        self.table_name = 'unittest_create_table'\n",
					"        self.container_name = 'unittest'\n",
					"        self.storage_account = storage_account\n",
					"        self.table_description = 'Unittest created table'\n",
					"        self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string', 'dimension':'BK' }, { 'column_name': 'column_2','data_type': 'integer', 'dimension':'SCD2' }, { 'column_name': 'column_3',  'data_type': 'decimal(12,5)', 'dimension':'BK' }, { 'column_name': 'column_4',  'data_type': 'varchar(4)', 'dimension':'SCD2' }]\"\n",
					"        self.partitioning = {}\n",
					"        self.delta_table_object = DeltaTableClass(self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info, self.partitioning)\n",
					"\n",
					"        ## Method in the IngestionFunctions class\n",
					"        self.mock_create_partioning_list_patcher = patch('__main__.create_partioning_list')\n",
					"        self.mock_create_partioning_list = self.mock_create_partioning_list_patcher.start()\n",
					"\n",
					"        ## Method in the IngestionFunctions class\n",
					"        self.mock_add_notnull_constraints_patcher = patch.object( DeltaTableClass, 'add_notnull_constraints')\n",
					"        self.mock_add_notnull_constraints = self.mock_add_notnull_constraints_patcher.start()\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        self.mock_create_partioning_list_patcher.stop()\n",
					"        self.mock_add_notnull_constraints_patcher.stop()\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the deploy_delta_table method\n",
					"\n",
					"    # Success: Create a table with a certain schema and having a varchar(4) datatype\n",
					"    # The varchar(4) datatype is expected to be part of the metadata of the column \n",
					"    def test_create_delta_table_no_partitioning(self):\n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(self.delta_table_object.silver_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(env_code, self.container_name, self.table_name)\n",
					"\n",
					"        # PREPROCESS:\n",
					"        # Define the expected schema and metadata values\n",
					"        # The following schema is expected to be returned\n",
					"        mock_schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"column_2\", sql.types.IntegerType(), True),\n",
					"            sql.types.StructField(\"column_3\", sql.types.DecimalType(12,5), True),\n",
					"            sql.types.StructField(\"column_4\", sql.types.StringType(), True)\n",
					"        ])\n",
					"        df = spark.createDataFrame(data=[], schema=mock_schema)\n",
					"\n",
					"        # Define the exepcted schema and metadata definitions\n",
					"        expected_schema = df.printSchema()\n",
					"        # For column_4 especially, the expectation is that there is a metadata-field that limits the total number of characters that can be added\n",
					"        expected_metadata_column4 = {'column_4': 'metadata about the column column_4', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(4)'}\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        self.delta_table_object.create_delta_table()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the schema and metadata from the table that has just been created\n",
					"        deltaTable = dt.DeltaTable.forPath(spark, self.delta_table_object.silver_path)\n",
					"        actual_schema = deltaTable.toDF().printSchema()\n",
					"        actual_metadata = deltaTable.toDF().select(\"column_4\").schema[0].metadata\n",
					"\n",
					"        # Validate that these match the expected values\n",
					"        self.assertEqual(expected_schema, actual_schema)\n",
					"        self.assertEqual(expected_metadata_column4, actual_metadata)\n",
					"        self.mock_create_partioning_list.assert_not_called()\n",
					"        self.mock_add_notnull_constraints.assert_called_once()\n",
					"\n",
					"    def test_create_delta_table_partitioning(self):\n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(self.delta_table_object.silver_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(env_code, self.container_name, self.table_name)\n",
					"\n",
					"        # PREPROCESS:\n",
					"        # Define the expected schema and metadata values\n",
					"        # The following schema is expected to be returned\n",
					"        mock_schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"column_2\", sql.types.IntegerType(), True),\n",
					"            sql.types.StructField(\"column_3\", sql.types.DecimalType(12,5), True),\n",
					"            sql.types.StructField(\"column_4\", sql.types.StringType(), True, metadata={'column_4': 'metadata about the column column_4', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(4)'})\n",
					"        ])\n",
					"        df = spark.createDataFrame(data=[], schema=mock_schema)\n",
					"\n",
					"        # Define the exepcted schema and metadata definitions\n",
					"        expected_schema = df.printSchema()\n",
					"        # For column_4 especially, the expectation is that there is a metadata-field that limits the total number of characters that can be added\n",
					"        expected_metadata_column4 = {'column_4': 'metadata about the column column_4', '__CHAR_VARCHAR_TYPE_STRING': 'varchar(4)'}\n",
					"        self.delta_table_object.partitioning = [{\"name\": \"column_1\", \"sequence\": 1}, {\"name\": \"column_2\", \"sequence\": 1}]\n",
					"        self.mock_create_partioning_list.return_value = df, [\"column_1\", \"column_2\"]\n",
					"\n",
					"        # EXECUTE\n",
					"        self.delta_table_object.create_delta_table()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the schema and metadata from the table that has just been created\n",
					"        deltaTable = dt.DeltaTable.forPath(spark, self.delta_table_object.silver_path)\n",
					"        actual_schema = deltaTable.toDF().printSchema()\n",
					"        actual_metadata = deltaTable.toDF().select(\"column_4\").schema[0].metadata\n",
					"\n",
					"        # Validate that these match the expected values\n",
					"        self.assertEqual(expected_schema, actual_schema)\n",
					"        self.assertEqual(expected_metadata_column4, actual_metadata)\n",
					"        self.mock_create_partioning_list.assert_called_once()\n",
					"        self.mock_add_notnull_constraints.assert_called_once()\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_CreateDeltaTable_v3)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)\n",
					""
				],
				"execution_count": 20
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: AddNotNullConstraint_v3\n",
					"# This class will test the add_notnull_constraint() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_AddNotNullConstraint_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"\n",
					"        # PREPROCESS:\n",
					"        # Define the expected schema and metadata values\n",
					"        # The following schema is expected to be returned\n",
					"        mock_schema = sql.types.StructType([\n",
					"            sql.types.StructField(\"column_1\", sql.types.StringType(), True),\n",
					"            sql.types.StructField(\"column_2\", sql.types.IntegerType(), True),\n",
					"            sql.types.StructField(\"column_3\", sql.types.DecimalType(12,5), True),\n",
					"            sql.types.StructField(\"column_4\", sql.types.StringType(), True)\n",
					"        ])\n",
					"        df = spark.createDataFrame(data=[], schema=mock_schema)\n",
					"\n",
					"        table_name = 'add_notnull_constraints'\n",
					"        container_name = 'unittest'\n",
					"        storage_account = f'{env_code}dapstdala1'\n",
					"        table_description = ''\n",
					"        column_info = \"[{ 'column_name': 'mock', 'data_type': 'string', 'dimension':'BK' }]\"\n",
					"        partitioning = dict()\n",
					"\n",
					"        self.silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (container_name, storage_account, table_name)\n",
					"        # CATCH: Make sure that the table that is trying to be created does not exist already\n",
					"        if mssparkutils.fs.exists(self.silver_path):\n",
					"            # raise Exception(f\"Table definition already exists. Table {self.table_name} should be removed before testing creation\")\n",
					"            clean_delta_table(env_code, container_name, table_name)\n",
					"        df.write.format('delta').save(self.silver_path)\n",
					"\n",
					"        self.delta_table_object = DeltaTableClass(table_name, container_name, storage_account, table_description, column_info, partitioning)\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the add_notnull_constraint method\n",
					"\n",
					"    # Success: Create a table with a certain schema and having a varchar(4) datatype\n",
					"    # The varchar(4) datatype is expected to be part of the metadata of the column \n",
					"    def test_addnotnullconstraint_deploydeltatable_success(self):\n",
					"        # SCD2\n",
					"        self.delta_table_object.column_dimensions = {\"column_1\": \"PK\", \"column_2\": \"SCD2\", \"column_3\": \"PK\", \"column_4\": \"SCD2\"}\n",
					"        expected_constraint_names = ['delta.constraints.pk__notnull_column_1', 'delta.constraints.pk__notnull_column_3']\n",
					"        expected_constraint_values = ['column_1 IS NOT NULL', 'column_3 IS NOT NULL']\n",
					"        # EXECUTE\n",
					"        self.delta_table_object.add_notnull_constraints()\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the list of constraints that have been added to the table\n",
					"        constraints = spark.sql(f\"DESCRIBE DETAIL  delta.`{self.silver_path}`\").select('properties').collect()[0][0]\n",
					"        actual_constraint_names = list(constraints.keys())\n",
					"        actual_constraint_values = list(constraints.values())\n",
					"\n",
					"        # Compare the actuals with the expectations\n",
					"        self.assertEqual(expected_constraint_names, actual_constraint_names,   f\"[DeployDeltaTables:AddNotNullConstraints] Constraints were not initialised as expected: actual = {actual_constraint_names} versus expected = {expected_constraint_names}\")\n",
					"        self.assertEqual(expected_constraint_values, actual_constraint_values, f\"[DeployDeltaTables:AddNotNullConstraints] Constraints were not initialised as expected: actual = {actual_constraint_values} versus expected = {expected_constraint_values}\")\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_AddNotNullConstraint_v3)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)\n",
					""
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CreateDeltaTable_v3\n",
					"# This class will test the add_structfield_logging_columns() method of the DeltaTable-class defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_AddLoggingColumns_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the add_structfield_logging_columns() method\n",
					"\n",
					"    # Success: Pass an empty list and expect all logging-columns to be returned in a StructField format\n",
					"    def test_add_structfield_logging_columns_empty_list(self):\n",
					"        # PREPROCESS\n",
					"        expected_structfield_list = [\n",
					"            sql.types.StructField('t_load_date_raw', sql.types.TimestampType(), True),\n",
					"            sql.types.StructField('t_load_date_silver', sql.types.TimestampType(), True),\n",
					"            sql.types.StructField('t_file_name', sql.types.StringType(), True),\n",
					"            sql.types.StructField('t_plan_id', sql.types.IntegerType(), False),\n",
					"            sql.types.StructField('t_task_id', sql.types.IntegerType(), False),\n",
					"            sql.types.StructField('t_file_id', sql.types.IntegerType(), False)\n",
					"        ]\n",
					"\n",
					"        # EXECUTE\n",
					"        actual_structfield_list = add_structfield_logging_columns(structfield_list=[])\n",
					"\n",
					"        # EVALUATE\n",
					"        # Check if the list contains all the necessary objects\n",
					"        self.assertEqual(len(actual_structfield_list), len(expected_structfield_list))\n",
					"\n",
					"        # # Check if the list contains the correct values for each field\n",
					"        # # Dev-Info: A direct list-compare (using assertEqual) returns an error. It is unclear why this is, but assuming that comparing object definitions is not as straightfoward\n",
					"        for field, expected_field in zip(actual_structfield_list, expected_structfield_list):\n",
					"            with self.subTest(field=field):\n",
					"                self.assertEqual(field.name, expected_field.name)\n",
					"                self.assertEqual(field.dataType, expected_field.dataType)\n",
					"                self.assertEqual(field.nullable, expected_field.nullable)\n",
					"\n",
					"    # Success: Pass a non-empty list and expect all logging-columns to be returned in a StructField format\n",
					"    def test_add_structfield_logging_columns_nonempty_list(self):\n",
					"        # PREPROCESS\n",
					"        expected_structfield_list = [\n",
					"            sql.types.StructField('test', sql.types.StringType(), False),\n",
					"            sql.types.StructField('t_load_date_raw', sql.types.TimestampType(), True),\n",
					"            sql.types.StructField('t_load_date_silver', sql.types.TimestampType(), True),\n",
					"            sql.types.StructField('t_file_name', sql.types.StringType(), True),\n",
					"            sql.types.StructField('t_plan_id', sql.types.IntegerType(), False),\n",
					"            sql.types.StructField('t_task_id', sql.types.IntegerType(), False),\n",
					"            sql.types.StructField('t_file_id', sql.types.IntegerType(), False)\n",
					"        ]        \n",
					"\n",
					"        # EXECUTE\n",
					"        structfield_list = [sql.types.StructField('test', sql.types.StringType(), False)]\n",
					"        actual_structfield_list = add_structfield_logging_columns(structfield_list=structfield_list)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Check if the list contains all the necessary objects\n",
					"        self.assertEqual(len(actual_structfield_list), len(expected_structfield_list), \n",
					"            f\"The expected structfield list does not match the actual list: {expected_structfield_list} versus {actual_structfield_list}\")\n",
					"\n",
					"        # # Check if the list contains the correct values for each field\n",
					"        # # Dev-Info: A direct list-compare (using assertEqual) returns an error. It is unclear why this is, but assuming that comparing object definitions is not as straightfoward\n",
					"        for field, expected_field in zip(actual_structfield_list, expected_structfield_list):\n",
					"            with self.subTest(field=field):\n",
					"                self.assertEqual(field.name, expected_field.name, f\"Field names do not match for {field} and {expected_field}\")\n",
					"                self.assertEqual(field.dataType, expected_field.dataType, f\"Datatypes do not match for {field} and {expected_field}\")\n",
					"                self.assertEqual(field.nullable, expected_field.nullable, f\"Nullables argument do not match for {field} and {expected_field}\")\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_AddLoggingColumns_v3)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in create delta table tests, something went wrong!\")\n",
					"#     print(test_resuts)"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: ProcessColumnInfoString_v3\n",
					"# This class will test the process_column_info_string() method defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_ProcessColumnInfoString_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # Test the process_column_info_string function from the DeployDeltaTable_v2 notebook\n",
					"    # The process_column_info_string method converts a string to a json-object\n",
					"    def setUp(self):\n",
					"        # No set-up needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # No tear-down needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the deploy_delta_table method\n",
					"\n",
					"    # Success: Pass a valid (simple) dictionary string and expect the conversion to happen correctly\n",
					"    def test_process_column_info_string_2(self):\n",
					"        # PREPROCESS\n",
					"        # Define the expected return object\n",
					"        expected_object = {\"key\": \"value\"}\n",
					"\n",
					"        # EXECUTE\n",
					"        # Invoke the function with the below argument\n",
					"        string_object = '{\"key\":\"value\"}'\n",
					"        actual_object = process_column_info_string(string_object)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the actual return value is equal to the expectated return value\n",
					"        self.assertEqual(expected_object, actual_object, f\"The column info string has not been processed correctly. Expected was {expected_object}, but got {actual_object}\")\n",
					"\n",
					"\n",
					"    # Success: Pass a valid list of dictionaries as a string and expect the conversion to happen correctly\n",
					"    def test_process_column_info_string(self):\n",
					"        # PREPROCESS\n",
					"        # Define the expected return object\n",
					"        expected_object =  [{'column_name':'column_1', 'data_type':'string'}, {'column_name':'column_2', 'data_type':'integer'}, {'column_name':'column_3', 'data_type':'decimal(12,5)'}, {'column_name':'column_4', 'data_type':'varchar(4)'}]\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the function with the below argument\n",
					"        string_object = '[{\"column_name\":\"column_1\", \"data_type\":\"string\"}, {\"column_name\":\"column_2\", \"data_type\":\"integer\"}, {\"column_name\":\"column_3\", \"data_type\":\"decimal(12,5)\"}, {\"column_name\":\"column_4\", \"data_type\":\"varchar(4)\"}]'\n",
					"        actual_object = process_column_info_string(string_object=string_object)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the actual return value is equal to the expectated return value\n",
					"        self.assertEqual(expected_object, actual_object, f\"The column info string has not been processed correctly. Expected was {expected_object}, but got {actual_object}\")\n",
					"\n",
					"    # Failure: Test that if the string object is not a valid object, an error gets thrown\n",
					"    def test_process_column_info_string_invalid_json(self):\n",
					"        # PREPROCESS\n",
					"        # Define the string_object argument\n",
					"        string_object = '[invalid_json_object]'\n",
					"        # Expect the following error to be returned\n",
					"        expected_error = f\"The string_object that was passed cannot be converted to a json object. Passed string: {string_object}\"\n",
					"\n",
					"        # EXECUTE\n",
					"        with self.assertRaises(TypeError) as error:\n",
					"            process_column_info_string(string_object)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the returned exception string\n",
					"        actual_error = str(error.exception)\n",
					"        # Validate that the returned error is equal to the expectated error\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the process_column_info_string was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: ConfigureDataTypes_v3\n",
					"# This class will test the configure_datatypes() method defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_ConfigureDataTypes_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # Test the configurate_datatypes function from the DeployDeltaTable_v3 notebook\n",
					"    # The configurate_datatypes method converts a string to a json-object\n",
					"\n",
					"    def setUp(self):\n",
					"\n",
					"        # Patch the functions that are called by the validate_table-method of the DeltaTable-class\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\n",
					"\n",
					"        ## Function in the DeployDeltaTable_v3 notebook\n",
					"        self.mock_add_decimal_datatype_patcher = patch('__main__.add_decimal_datatype')\n",
					"        self.mock_add_decimal_datatype = self.mock_add_decimal_datatype_patcher.start()\n",
					"\n",
					"        self.mock_add_varchar_datatype_patcher = patch('__main__.add_varchar_datatype')\n",
					"        self.mock_add_varchar_datatype = self.mock_add_varchar_datatype_patcher.start()\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        self.mock_add_decimal_datatype_patcher.stop()\n",
					"        self.mock_add_varchar_datatype_patcher.stop()\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the configure_datatypes function\n",
					"\n",
					"\n",
					"    # Success: Pass an empty dictionary and return a dictionary of predefined datatypes\n",
					"    def test_configure_datatypes_return_generic_datatypes(self):\n",
					"        # PREPROCESS\n",
					"        # Expect the below dictionary to be returned, containing all the generic datatypes\n",
					"        expected_dictionary = {\"array\": sql.types.ArrayType(sql.types.StringType()), \"binary\": sql.types.BinaryType(), \n",
					"            \"boolean\": sql.types.BooleanType(), \"date\": sql.types.DateType(), \"string\": sql.types.StringType(), \n",
					"            \"timestamp\": sql.types.TimestampType(), \"decimal\": sql.types.DecimalType(38,4), \"float\": sql.types.FloatType(), \n",
					"            \"byte\": sql.types.ByteType(), \"integer\": sql.types.IntegerType(), \"long_integer\":  sql.types.LongType(), \n",
					"            \"varchar(max)\": sql.types.StringType(), \"int\": sql.types.IntegerType()\n",
					"        }\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with an empty dictionary argument\n",
					"        empty_dictionary = dict()\n",
					"        actual_dictionary = configure_datatypes(empty_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the returned dictionary matches the expected dictionary\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\n",
					"    \n",
					"    # Success: Validate that the datatype-specific conversion functions (add_decimal_datatype and add_varchar_datatype) are not invoked when there are no value-matches in the dictionary\n",
					"    def test_configure_datatypes_dont_call_datatype_specific_functions(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing no values referencing 'decimal' or 'varchar'\n",
					"        decimal_dictionary = {'key': 'string'}\n",
					"        configure_datatypes(decimal_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the datatype-specific functions have not been called\n",
					"        self.mock_add_decimal_datatype.assert_not_called()\n",
					"        self.mock_add_varchar_datatype.assert_not_called()\n",
					"    \n",
					"    # Success: Validate that when a dictionary is passed that contains a value 'decimal', the function add_decimal_datatype is called\n",
					"    def test_configure_datatypes_decimal(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing a generic value 'decimal'\n",
					"        decimal_dictionary = {'key': 'decimal'}\n",
					"        configure_datatypes(decimal_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the function has been called exactly once\n",
					"        self.mock_add_decimal_datatype.assert_called_once()\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a value 'decimal(&lt;xx&gt;,&lt;yy&gt;', the function add_decimal_datatype is called\n",
					"    def test_configure_datatypes_decimal_2(self):        \n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing a specific value 'decimal(12,5)'\n",
					"        decimal_dictionary = {'key': 'decimal(12,5)'}\n",
					"        configure_datatypes(decimal_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the function has been called exactly once\n",
					"        self.mock_add_decimal_datatype.assert_called_with('decimal(12,5)', ANY)\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a generic value 'decimal' and specific 'decimal(&lt;xx&gt;,&lt;yy&gt;), \n",
					"    # the function add_decimal_datatype is called for each reference\n",
					"    def test_configure_datatypes_multiple_decimals(self):\n",
					"        # PREPROCESS\n",
					"        # Define the list of expected function-calls to add_decimal_datatype\n",
					"        expected_calls = [call('decimal', ANY), call('decimal(12,5)', ANY)]\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing a generic value 'decimal' and a specific value 'decimal(12,5)'\n",
					"        decimal_dictionary = {'key': 'decimal', 'other_key': 'decimal(12,5)'}\n",
					"        configure_datatypes(decimal_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the function has been called for each instance of 'decimal' \n",
					"        self.mock_add_decimal_datatype.assert_has_calls(expected_calls, any_order=True)\n",
					"\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a generic value 'varchar(max)', the function add_varchar_datatype is called\n",
					"    def test_configure_datatypes_varchar(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing a generic value referencing 'varchar'\n",
					"        varchar_dictionary = {'key': 'varchar(max)'}\n",
					"        configure_datatypes(varchar_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the function add_varchar_datatype() has been called exactly once\n",
					"        self.mock_add_varchar_datatype.assert_called_once()\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a specific value 'varchar(4)', the function add_varchar_datatype is called\n",
					"    def test_configure_datatypes_varchar_2(self):\n",
					"        # PREPROCESS\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing a specific value referencing 'varchar(4)'\n",
					"        varchar_dictionary = {'key': 'varchar(4)'}\n",
					"        configure_datatypes(varchar_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the function has been called exactly once\n",
					"        self.mock_add_varchar_datatype.assert_called_with('varchar(4)', ANY)\n",
					"\n",
					"    # Success: Validate that when a dictionary is passed that contains a generic value 'varchar(max)' and a specific value 'varchar(4)', \n",
					"    # the function add_varchar_datatype is called for each reference\n",
					"    def test_configure_datatypes_multiple_varchars(self):\n",
					"        # PREPROCESS\n",
					"        # Define the list of expected function-calls to add_varchar_datatype\n",
					"        expected_calls = [call('varchar(max)', ANY), call('varchar(4)', ANY)]\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with a non-empty dictionary argument containing a generic value 'varchar(max)' and a specific value 'varchar(4)'\n",
					"        varchar_dictionary = {'key': 'varchar(max)', 'other_key': 'varchar(4)'}\n",
					"        configure_datatypes(varchar_dictionary)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the function has been called for each instance of 'varchar'\n",
					"        self.mock_add_varchar_datatype.assert_has_calls(expected_calls, any_order=True)\n",
					"\n",
					"\n",
					"    # Failure: Test that if an invalid (or non-configured) datatype is passed, an error is thrown\n",
					"    def test_configure_datatypes_invalid_datatypes(self):\n",
					"        # PREPROCESS\n",
					"        # Expect the following error to be thrown:\n",
					"        expected_error = f\"The value 'invalid_datatype' has not been configured as a datatype in the current set-up.\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with an invalid datatype\n",
					"        decimal_dictionary = {'key': 'string', 'other_key': 'invalid_datatype'}\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            configure_datatypes(decimal_dictionary)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the thrown error is equal to the expected thrown error\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_decimal_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: AddDecimalDatatype_v3\n",
					"# This class will test the configure_datatypes() method defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_AddDecimalDatatype_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # Test the add_decimal_datatype function from the DeployDeltaTable_v2 notebook\n",
					"    # The add_decimal_datatype method adds specific sql.type.DecimalType references with limited characters before and after the comma\n",
					"\n",
					"    def setUp(self):\n",
					"        # No set-up needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # No tear-down needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"        \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the add_decimal_datatype() function\n",
					"\n",
					"    # Success: Add a decimal-datatype reference to the types-dictionary\n",
					"    def test_add_decimal_datatype(self):\n",
					"        # PREPROCESS\n",
					"        # Expect the following dictionary to be returned:\n",
					"        expected_dictionary = {\"decimal(12,5)\": sql.types.DecimalType(12,5)}\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with the following arguments\n",
					"        types = dict()\n",
					"        value = 'decimal(12,5)'\n",
					"        actual_dictionary = add_decimal_datatype(value, types)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Assert equality between the dictionaries\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\n",
					"\n",
					"    # Failure: Deny the additional reference of an invalid decimal configuration to the types-dictionary \n",
					"    def test_invalid_value(self):\n",
					"        # PREPROCESS\n",
					"        # Expect the following dictionary to be returned:\n",
					"        expected_error = f\"The decimal value decimal(12) is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the function with the following arguments\n",
					"        types = dict()\n",
					"        value = 'decimal(12)'\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            add_decimal_datatype(value, types)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the expected error is actually returned\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_decimal_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: AddVarcharDatatype_v3\n",
					"# This class will test the configure_datatypes() method defined in the DeployDeltaTable_v3 notebook\n",
					"class Test_AddVarcharDatatype_v3(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # Test the add_varchar_datatype function from the DeployDeltaTable_v2 notebook\n",
					"    # The add_varchar_datatype method adds specific sql.type.varcharType references with limited characters before and after the comma\n",
					"\n",
					"    def setUp(self):\n",
					"        # No set-up needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # No tear-down needed. Keeping function for potential future use\n",
					"        return\n",
					"\n",
					"        \n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the add_varchar_datatype() function\n",
					"\n",
					"    # Success: Add a varchar-datatype reference to the types-dictionary\n",
					"    def test_add_varchar_datatype(self):\n",
					"        # PREPROCESS\n",
					"        # Expect the following dictionary to be returned:\n",
					"        expected_dictionary = {\"varchar(12)\": sql.types.StringType()}\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the function with the following arguments\n",
					"        types = dict()\n",
					"        value = 'varchar(12)'\n",
					"        actual_dictionary = add_varchar_datatype(value, types)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Assert equality between the dictionaries\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\n",
					"\n",
					"    # Failure: Deny the additional reference of an invalid varchar configuration to the types-dictionary \n",
					"    def test_invalid_value(self):\n",
					"        # PREPROCESS\n",
					"        # Expect the following dictionary to be returned:\n",
					"        expected_error = f\"The varchar value varchar is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...\"\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the function with the following arguments\n",
					"        types = dict()\n",
					"        value = 'varchar'\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            add_varchar_datatype(value, types)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the expected error is actually returned\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_varchar_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					""
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# class Test_ValidateTable(unittest.TestCase):\n",
					"#     # Test the validate_table method from the DeltaTable class (See notebook: DeployDeltaTable_v2)\n",
					"#     # The validate_table method validates some features of the delta table\n",
					"#     def setUp(self):\n",
					"#         # Create a DeltaTable-object using the below parameters\n",
					"#         self.table_name = 'unittest_tablename'\n",
					"#         self.container_name = 'unittest_database'\n",
					"#         self.storage_account = 'unittest_storageaccount'\n",
					"#         self.table_description = 'Unittest created table'\n",
					"#         self.column_info = \"[{ 'column_name': 'unittest', 'data_type': 'string' }]\"\n",
					"\n",
					"#         self.delta_table_object = DeltaTableClass(self.table_name, self.container_name, self.storage_account, self.table_description, self.column_info)\n",
					"\n",
					"        \n",
					"#         # Patch the functions that are called by the validate_table-method of the DeltaTable-class\n",
					"#         # The patched functions are self-made functions or functions coming from external libaries.\n",
					"#         # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\n",
					"\n",
					"#         ## Method coming from library pyspark.sql\n",
					"#         self.mock_SparkSession_sql_patcher = patch.object(SparkSession, 'sql')\n",
					"#         self.mock_SparkSession_sql = self.mock_SparkSession_sql_patcher.start()\n",
					"\n",
					"#         ## Method coming from library delta.table\n",
					"#         self.mock_deltatable_patcher = patch.object(dt, 'DeltaTable')\n",
					"#         self.mock_deltatable = self.mock_deltatable_patcher.start()\n",
					"\n",
					"#         # The validate_table method queries the details of the 'to be deployed' delta table.\n",
					"#         # The details that are checked are: format, name, and location\n",
					"#         # In the method, spark.sql returns a dataframe with these values\n",
					"#         # Here, the schema of the relevant parameters is created. \n",
					"#         # The schema will be used in the testing-methods (below) to create spark dataframes with the relevant table details to be tested\n",
					"#         self.schema = dt.StructType([\n",
					"#             dt.StructField(name='format', dataType=sql.types.StringType(), nullable=False),\n",
					"#             dt.StructField(name='name', dataType=sql.types.StringType(), nullable=False),\n",
					"#             dt.StructField(name='location', dataType=sql.types.StringType(), nullable=False)\n",
					"#         ])\n",
					"\n",
					"#     def tearDown(self):\n",
					"#         # Stop all the patcher functions\n",
					"#         self.mock_SparkSession_sql_patcher.stop()\n",
					"#         self.mock_deltatable_patcher.stop()\n",
					"\n",
					"#     def test_validate_table(self):\n",
					"#         # Create a dataframe with values that will be passed by the validate_table method\n",
					"#         valid_data = [\n",
					"#             ('delta', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         valid_df = spark.createDataFrame(valid_data, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = valid_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = True\n",
					"\n",
					"#         # Expected returned value: True -&gt; The delta table that is being validated has passed all the checks\n",
					"#         expected_return = True\n",
					"\n",
					"#         # Run the function: validate_table for delta-table object 'self.delta_table_object'\n",
					"#         actual_return = self.delta_table_object.validate_table()\n",
					"\n",
					"#         # Assert that the actual return value is equal to the expectated return value\n",
					"#         self.assertEqual(expected_return, actual_return,  f'The return value from the validate_delta method was \"{actual_return}\", while the expected return was \"{expected_return}\".')\n",
					"\n",
					"#     def test_validate_table_format(self):\n",
					"#         # Create a dataframe with where the format-value is invalid (csv instead of delta)\n",
					"#         invalid_format = [\n",
					"#             ('csv', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         invalid_format_df = spark.createDataFrame(invalid_format, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = invalid_format_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = True\n",
					"\n",
					"#         # Expected returned value: Expect the below error to be thrown\n",
					"#         expected_error = \"The format of the table is not a delta table\"\n",
					"        \n",
					"#         # Run the function: validate_table for delta-table object 'self.delta_table_object'\n",
					"#         with self.assertRaises(TypeError) as error:\n",
					"#             self.delta_table_object.validate_table()\n",
					"#         actual_error = str(error.exception)\n",
					"    \n",
					"#         # Assert that the actual return value is equal to the expectated return value\n",
					"#         self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"#     def test_validate_full_table_name_1(self):\n",
					"#         # Create a dataframe with where the database-value is invalid in the name-parameter (invalid_database instead of unittest_database)\n",
					"#         invalid_database = [\n",
					"#             ('delta', f'invalid_database.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         invalid_database_df = spark.createDataFrame(invalid_database, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = invalid_database_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = True\n",
					"\n",
					"#         # Expected returned value: Expect the below error to be thrown\n",
					"#         expected_error = f\"The name of the table {self.table_name} or the namespace {self.container_name} does not match with the name of the table invalid_database.{self.table_name}\"\n",
					"#         with self.assertRaises(TypeError) as error:\n",
					"#             self.delta_table_object.validate_table()\n",
					"        \n",
					"#         actual_error = str(error.exception)\n",
					"#         self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"#     def test_validate_full_table_name_2(self):\n",
					"#         # Create a dataframe with where the tablename-value is invalid in the name-parameter (invalid_tablename instead of unittest_table)\n",
					"#         invalid_tablename = [\n",
					"#             ('delta', f'{self.container_name}.invalid_tablename', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         invalid_tablename_df = spark.createDataFrame(invalid_tablename, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = invalid_tablename_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = True\n",
					"\n",
					"#         # Expected returned value: Expect the below error to be thrown\n",
					"#         expected_error = f\"The name of the table {self.table_name} or the namespace {self.container_name} does not match with the name of the table {self.container_name}.invalid_tablename\"\n",
					"        \n",
					"#         # Run the function: validate_table for delta-table object 'self.delta_table_object'\n",
					"#         with self.assertRaises(TypeError) as error:\n",
					"#             self.delta_table_object.validate_table()\n",
					"#         actual_error = str(error.exception)\n",
					"\n",
					"#         # Assert that the actual return value is equal to the expectated return value\n",
					"#         self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"\n",
					"#     def test_validate_filelocation_1(self):\n",
					"#         # Create a dataframe with where the storage_account-value is invalid in the name-parameter (invalid_storageaccount instead of unittest_storageaccount)\n",
					"#         invalid_storageaccount = [\n",
					"#             ('delta', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         invalid_storageaccount_df = spark.createDataFrame(invalid_storageaccount, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = invalid_storageaccount_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = True\n",
					"\n",
					"#         # Expected returned value: Expect the below error to be thrown\n",
					"#         expected_error = f\"The table is not stored on the given storage account {self.storage_account}. The storage location is abfss://{self.container_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}\"\n",
					"\n",
					"#         # Run the function: validate_table for delta-table object 'self.delta_table_object'\n",
					"#         with self.assertRaises(ValueError) as error:\n",
					"#             self.delta_table_object.validate_table()\n",
					"#         actual_error = str(error.exception)\n",
					"\n",
					"#         # Assert that the actual return value is equal to the expectated return value\n",
					"#         self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"#     def test_validate_filelocation_2(self):\n",
					"#         # Create a dataframe with where the database-value is invalid in the name-parameter (invalid_database instead of unittest_database)\n",
					"#         invalid_database = [\n",
					"#             ('delta', f'{self.container_name}.{self.table_name}', f'abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         invalid_database_df = spark.createDataFrame(invalid_database, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = invalid_database_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = True\n",
					"\n",
					"#         # Expected returned value: Expect the below error to be thrown\n",
					"#         expected_error = f\"The table is not stored on the expected container {self.container_name}. The storage location is abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}\"\n",
					"        \n",
					"#         # Run the function: validate_table for delta-table object 'self.delta_table_object'\n",
					"#         with self.assertRaises(ValueError) as error:\n",
					"#             self.delta_table_object.validate_table()\n",
					"#         actual_error = str(error.exception)\n",
					"\n",
					"#         # Assert that the actual return value is equal to the expectated return value\n",
					"#         self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"\n",
					"#     def test_validate_isdeltatable(self):\n",
					"#         # Create a dataframe with where the table is not an actual DeltaTable (isDeltaTable = False)\n",
					"#         valid_data = [\n",
					"#             ('delta', f'{self.container_name}.{self.table_name}', f'abfss://{self.container_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\n",
					"#         ]\n",
					"#         valid_df = spark.createDataFrame(valid_data, self.schema)\n",
					"\n",
					"#         # Configure the return_value of the spar.sql query to be equal to the above created dataframe\n",
					"#         self.mock_SparkSession_sql.return_value = valid_df\n",
					"#         # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = False \n",
					"#         self.mock_deltatable.isDeltaTable.return_value = False\n",
					"\n",
					"#         # Expected returned value: Expect the below error to be thrown\n",
					"#         expected_error = f\"The files stored on path {self.delta_table_object.silver_path} for table {self.table_name} are not a delta table\"\n",
					"\n",
					"#         # Run the function: validate_table for delta-table object 'self.delta_table_object'\n",
					"#         with self.assertRaises(TypeError) as error:\n",
					"#             self.delta_table_object.validate_table()\n",
					"#         actual_error = str(error.exception)\n",
					"\n",
					"#         # Assert that the actual return value is equal to the expectated return value\n",
					"#         self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					""
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_Deploy_v2.json">
{
	"name": "Test_Deploy_v2",
	"properties": {
		"folder": {
			"name": "ScrapNotebooks"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "1",
				"spark.autotune.trackingId": "d5574afc-4903-47b0-a5e2-96ec07197f2b"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.3",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Deploy\r\n",
					"**Purpose**: This Notebook will use the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the Spark Notebooks in the folder *Deploy*.\r\n",
					"\r\n",
					"Unit tests: To test if a method does what we expect it to do, a untit test is written. A unit test isolates a method from the overall environment and executes the method. The aim is to be able to define the expected outcome of the method and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the method successfully\r\n",
					"\r\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\r\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\r\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import necessary packages"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"import unittest\r\n",
					"from unittest.mock import patch, MagicMock, Mock, ANY, call\r\n",
					"from notebookutils import mssparkutils\r\n",
					"from io import StringIO\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"import delta.tables as dt\r\n",
					"import pyspark.sql as sql\r\n",
					""
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters\r\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. When testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"# Parameters that need to be given to create the delta table\r\n",
					"environment_code = 'dev'"
				],
				"execution_count": 17
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\r\n",
					"storage_account = f'{env_code}dapstdala1' # The storage account where the delta table is expected to be stored\r\n",
					"database_name = 'silver'"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the methods that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/CleanWorkspace"
				],
				"execution_count": 19
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Deploy/DeployDeltaTable_v2"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define the test classes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_InitialisationDeltaTable(unittest.TestCase):\r\n",
					"    # Test the __init__ function from the DeltaTable class (See notebook: DeployDeltaTable_v2)\r\n",
					"    # The tests will attempt to create a DeltaTable-object\r\n",
					"    def setUp(self):\r\n",
					"        # Set the parameters that will be used to initialise a DeltaTable-object\r\n",
					"        self.table_name = 'unittest_table'\r\n",
					"        self.database_name = 'unittest_database'\r\n",
					"        self.storage_account = 'unittest'\r\n",
					"        self.table_description = 'Unittest created table'\r\n",
					"        self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string' }, { 'column_name': 'column_2','data_type': 'integer' }, { 'column_name': 'column_3',  'data_type': 'decimal(12,5)' }]\"\r\n",
					"\r\n",
					"\r\n",
					"        # Patch the functions that are called by the init-function of the DeltaTable-class\r\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\r\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\r\n",
					"\r\n",
					"        ## Function in the DeployDeltaTable notebook        \r\n",
					"        self.mock_process_column_info_string_patcher = patch('__main__.process_column_info_string')\r\n",
					"        self.mock_process_column_info_string = self.mock_process_column_info_string_patcher.start()\r\n",
					"        \r\n",
					"        ## Method in the DeployDeltaTable notebook        \r\n",
					"        self.mock_configure_datatypes_patcher = patch('__main__.configure_datatypes')\r\n",
					"        self.mock_configure_datatypes = self.mock_configure_datatypes_patcher.start()\r\n",
					"\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher-functions \r\n",
					"        self.mock_process_column_info_string_patcher.stop()\r\n",
					"        self.mock_configure_datatypes_patcher.stop()\r\n",
					"\r\n",
					"\r\n",
					"    def test_delta_table_init(self):\r\n",
					"        # Based on the parameters defined above, the following object-parameters are expected to be created by the __init__ function of the DeltaTable class\r\n",
					"        expected_full_name = 'unittest_database.unittest_table'\r\n",
					"        expected_silver_path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % (self.database_name, self.storage_account, self.table_name)\r\n",
					"        expected_column_dict = { \"column_1\": \"string\", \"column_2\": \"integer\", \"column_3\": \"decimal(12,5)\" }\r\n",
					"\r\n",
					"        # Since the output from the process_column_info_string function is called and used from the __init__ function, the output (= return_value) is mocked\r\n",
					"        # When assuming the process_column_info_string function works, the following output would be expected to be returned:\r\n",
					"        self.mock_process_column_info_string.return_value = { \"column_name\": \"column_1\", \"data_type\": \"string\" }, { \"column_name\": \"column_2\",\"data_type\": \"integer\" }, { \"column_name\": \"column_3\",  \"data_type\": \"decimal(12,5)\" }\r\n",
					"\r\n",
					"        # Initialize a DeltaTable-object, thereby calling the to-be tested __init__ function\r\n",
					"        delta_table = DeltaTableClass( self.table_name, self.database_name, self.storage_account, self.table_description, self.column_info )\r\n",
					"\r\n",
					"        # After initialization, the following object-parameters should have been created\r\n",
					"        actual_full_name = delta_table.full_name\r\n",
					"        actual_silver_path = delta_table.silver_path\r\n",
					"        actual_column_dict = delta_table.column_dict\r\n",
					"\r\n",
					"        # Validate that the expected object-parameters are equal to the actual object-parameters\r\n",
					"        self.assertEqual(expected_full_name, actual_full_name,      'Something went wrong during initialisation of full_name')\r\n",
					"        self.assertEqual(expected_silver_path, actual_silver_path,  'Something went wrong during initialisation of silver_path')\r\n",
					"        self.assertEqual(expected_column_dict, actual_column_dict,  'Something went wrong during initialisation of column_dict')\r\n",
					""
				],
				"execution_count": 21
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_DeployDeltaTable(unittest.TestCase):\r\n",
					"    # Test the deploy_delta_table method from the DeltaTable class (See notebook: DeployDeltaTable_v2)\r\n",
					"    # The deploy_delta_table method invokes other functions, depending on certain parameters\r\n",
					"    # The tests will validate that the correct functions are invoked using different parameters\r\n",
					"    def setUp(self):\r\n",
					"\r\n",
					"        # Create a DeltaTable-object using the below parameters\r\n",
					"        self.table_name = 'unittest_table'\r\n",
					"        self.database_name = 'unittest_database'\r\n",
					"        self.storage_account = 'unittest'\r\n",
					"        self.table_description = 'Unittest created table'\r\n",
					"        self.column_info = \"[{ 'column_name': 'column_1', 'data_type': 'string' }, { 'column_name': 'column_2','data_type': 'integer' }, { 'column_name': 'column_3',  'data_type': 'decimal(12,5)' }]\"\r\n",
					"\r\n",
					"        self.delta_table_object = DeltaTableClass(self.table_name, self.database_name, self.storage_account, self.table_description, self.column_info)\r\n",
					"        \r\n",
					"        # Patch the functions that are called by the deploy_delta_table-method of the DeltaTable-class\r\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\r\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\r\n",
					"\r\n",
					"        ## Method in the DeltaTable class\r\n",
					"        self.mock_validate_table_patcher = patch.object(DeltaTableClass, 'validate_table')\r\n",
					"        self.mock_validate_table = self.mock_validate_table_patcher.start()\r\n",
					"        \r\n",
					"        ## Method in the DeltaTable class        \r\n",
					"        self.mock_create_table_patcher = patch.object(DeltaTableClass, 'create_table')\r\n",
					"        self.mock_create_table = self.mock_create_table_patcher.start()\r\n",
					"\r\n",
					"        ## Function in the CleanWorkspace notebook\r\n",
					"        self.mock_clean_table_patcher = patch('__main__.clean_table')\r\n",
					"        self.mock_clean_table = self.mock_clean_table_patcher.start()\r\n",
					"\r\n",
					"        ## Method coming from library pyspark.sql\r\n",
					"        self.mock_SparkSession_sql_patcher = patch.object(SparkSession, 'sql')\r\n",
					"        self.mock_SparkSession_sql = self.mock_SparkSession_sql_patcher.start()\r\n",
					"\r\n",
					"        ## Method coming from library notebookutils.mssparkutils\r\n",
					"        self.mock_mssparkutils_fs_patcher = patch.object(mssparkutils, 'fs')\r\n",
					"        self.mock_mssparkutils_fs = self.mock_mssparkutils_fs_patcher.start()\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        self.mock_create_table_patcher.stop()\r\n",
					"        self.mock_validate_table_patcher.stop()\r\n",
					"        self.mock_clean_table_patcher.stop()\r\n",
					"        self.mock_SparkSession_sql_patcher.stop()\r\n",
					"        self.mock_mssparkutils_fs_patcher.stop()\r\n",
					"\r\n",
					"\r\n",
					"    def test_deploy_delta_table_call_validate(self):\r\n",
					"        # Since the output from the spark.sql.collect() function is called and used from the deploy_delta_table method, the output (= return_value) is mocked\r\n",
					"        # Test 1:\r\n",
					"        #   spark.sql.collect(): Returns a list of tables present in the database. One of the tables is the one that needs to be 'deployed'\r\n",
					"        #   mssparkutils.fs.exists: Returns 'True', indicating that the table also exists in the storage account\r\n",
					"        # Expected result: Since the table exists in the database and the storage account, the validate_tabe needs to be called\r\n",
					"\r\n",
					"        self.mock_SparkSession_sql.return_value.collect.return_value = [[Mock(), self.table_name], [Mock(), 'non_existing_table']]\r\n",
					"        self.mock_mssparkutils_fs.exists.return_value = True\r\n",
					"\r\n",
					"        # Execute function\r\n",
					"        self.delta_table_object.deploy_delta_table()\r\n",
					"\r\n",
					"        # Assert whether validate_table has been called exactly once\r\n",
					"        self.mock_validate_table.assert_called_once()\r\n",
					"\r\n",
					"    def test_deploy_delta_table_call_clean_and_create(self):\r\n",
					"        # Since the output from the spark.sql.collect() function is called and used from the deploy_delta_table method, the output (= return_value) is mocked\r\n",
					"        # Test 2:\r\n",
					"        #   spark.sql.collect(): Returns a list of tables present in the database. One of the tables is the one that needs to be 'deployed'\r\n",
					"        #   mssparkutils.fs.exists: Returns 'False', indicating that the table does not exists in the storage account\r\n",
					"        # Expected result: Since the table exists in the database, but not in the storage account, the table is \"cleaned\" from the database and created from scratch using clean_table and create_table\r\n",
					"        \r\n",
					"        self.mock_SparkSession_sql.return_value.collect.return_value = [[Mock(), self.table_name], [Mock(), 'non_existing_table']]\r\n",
					"        self.mock_mssparkutils_fs.exists.return_value = False\r\n",
					"\r\n",
					"        # Execute function: deploy_delta_table\r\n",
					"        self.delta_table_object.deploy_delta_table()\r\n",
					"\r\n",
					"        # Assert whether clean_table and create_table have been called exactly once\r\n",
					"        self.mock_clean_table.assert_called_once()\r\n",
					"        self.mock_create_table.assert_called_once()\r\n",
					"\r\n",
					"\r\n",
					"    def test_deploy_delta_table_call_create(self):\r\n",
					"        # Since the output from the spark.sql.collect() function is called and used from the deploy_delta_table method, the output (= return_value) is mocked\r\n",
					"        # Test 3:\r\n",
					"        #   spark.sql.collect(): Returns a list of tables present in the database. None of the tables is the one that needs to be 'deployed'\r\n",
					"        # Expected result: Since the table does not exist in the database, it is created from scratch using create_table\r\n",
					"        self.mock_SparkSession_sql.return_value.collect.return_value = [[Mock(), 'non_existing_table'], [Mock(), 'other_non_existing_table']]\r\n",
					"\r\n",
					"        # Execute function\r\n",
					"        self.delta_table_object.deploy_delta_table()\r\n",
					"\r\n",
					"        # Assert whether create_table has been called exactly once\r\n",
					"        self.mock_create_table.assert_called_once()"
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_ValidateTable(unittest.TestCase):\r\n",
					"    # Test the validate_table method from the DeltaTable class (See notebook: DeployDeltaTable_v2)\r\n",
					"    # The validate_table method validates some features of the delta table\r\n",
					"    def setUp(self):\r\n",
					"        # Create a DeltaTable-object using the below parameters\r\n",
					"        self.table_name = 'unittest_tablename'\r\n",
					"        self.database_name = 'unittest_database'\r\n",
					"        self.storage_account = 'unittest_storageaccount'\r\n",
					"        self.table_description = 'Unittest created table'\r\n",
					"        self.column_info = \"[{ 'column_name': 'unittest', 'data_type': 'string' }]\"\r\n",
					"\r\n",
					"        self.delta_table_object = DeltaTableClass(self.table_name, self.database_name, self.storage_account, self.table_description, self.column_info)\r\n",
					"\r\n",
					"        \r\n",
					"        # Patch the functions that are called by the validate_table-method of the DeltaTable-class\r\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\r\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\r\n",
					"\r\n",
					"        ## Method coming from library pyspark.sql\r\n",
					"        self.mock_SparkSession_sql_patcher = patch.object(SparkSession, 'sql')\r\n",
					"        self.mock_SparkSession_sql = self.mock_SparkSession_sql_patcher.start()\r\n",
					"\r\n",
					"        ## Method coming from library delta.table\r\n",
					"        self.mock_deltatable_patcher = patch.object(dt, 'DeltaTable')\r\n",
					"        self.mock_deltatable = self.mock_deltatable_patcher.start()\r\n",
					"\r\n",
					"        # The validate_table method queries the details of the 'to be deployed' delta table.\r\n",
					"        # The details that are checked are: format, name, and location\r\n",
					"        # In the method, spark.sql returns a dataframe with these values\r\n",
					"        # Here, the schema of the relevant parameters is created. \r\n",
					"        # The schema will be used in the testing-methods (below) to create spark dataframes with the relevant table details to be tested\r\n",
					"        self.schema = dt.StructType([\r\n",
					"            dt.StructField(name='format', dataType=sql.types.StringType(), nullable=False),\r\n",
					"            dt.StructField(name='name', dataType=sql.types.StringType(), nullable=False),\r\n",
					"            dt.StructField(name='location', dataType=sql.types.StringType(), nullable=False)\r\n",
					"        ])\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        self.mock_SparkSession_sql_patcher.stop()\r\n",
					"        self.mock_deltatable_patcher.stop()\r\n",
					"\r\n",
					"    def test_validate_table(self):\r\n",
					"        # Create a dataframe with values that will be passed by the validate_table method\r\n",
					"        valid_data = [\r\n",
					"            ('delta', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        valid_df = spark.createDataFrame(valid_data, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = valid_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = True\r\n",
					"\r\n",
					"        # Expected returned value: True -&gt; The delta table that is being validated has passed all the checks\r\n",
					"        expected_return = True\r\n",
					"\r\n",
					"        # Run the function: validate_table for delta-table object 'self.delta_table_object'\r\n",
					"        actual_return = self.delta_table_object.validate_table()\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_return, actual_return,  f'The return value from the validate_delta method was \"{actual_return}\", while the expected return was \"{expected_return}\".')\r\n",
					"\r\n",
					"    def test_validate_table_format(self):\r\n",
					"        # Create a dataframe with where the format-value is invalid (csv instead of delta)\r\n",
					"        invalid_format = [\r\n",
					"            ('csv', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        invalid_format_df = spark.createDataFrame(invalid_format, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = invalid_format_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = True\r\n",
					"\r\n",
					"        # Expected returned value: Expect the below error to be thrown\r\n",
					"        expected_error = \"The format of the table is not a delta table\"\r\n",
					"        \r\n",
					"        # Run the function: validate_table for delta-table object 'self.delta_table_object'\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            self.delta_table_object.validate_table()\r\n",
					"        actual_error = str(error.exception)\r\n",
					"    \r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					"\r\n",
					"    def test_validate_full_table_name_1(self):\r\n",
					"        # Create a dataframe with where the database-value is invalid in the name-parameter (invalid_database instead of unittest_database)\r\n",
					"        invalid_database = [\r\n",
					"            ('delta', f'invalid_database.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        invalid_database_df = spark.createDataFrame(invalid_database, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = invalid_database_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = True\r\n",
					"\r\n",
					"        # Expected returned value: Expect the below error to be thrown\r\n",
					"        expected_error = f\"The name of the table {self.table_name} or the namespace {self.database_name} does not match with the name of the table invalid_database.{self.table_name}\"\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            self.delta_table_object.validate_table()\r\n",
					"        \r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					"\r\n",
					"    def test_validate_full_table_name_2(self):\r\n",
					"        # Create a dataframe with where the tablename-value is invalid in the name-parameter (invalid_tablename instead of unittest_table)\r\n",
					"        invalid_tablename = [\r\n",
					"            ('delta', f'{self.database_name}.invalid_tablename', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        invalid_tablename_df = spark.createDataFrame(invalid_tablename, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = invalid_tablename_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = True\r\n",
					"\r\n",
					"        # Expected returned value: Expect the below error to be thrown\r\n",
					"        expected_error = f\"The name of the table {self.table_name} or the namespace {self.database_name} does not match with the name of the table {self.database_name}.invalid_tablename\"\r\n",
					"        \r\n",
					"        # Run the function: validate_table for delta-table object 'self.delta_table_object'\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            self.delta_table_object.validate_table()\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					"\r\n",
					"\r\n",
					"    def test_validate_filelocation_1(self):\r\n",
					"        # Create a dataframe with where the storage_account-value is invalid in the name-parameter (invalid_storageaccount instead of unittest_storageaccount)\r\n",
					"        invalid_storageaccount = [\r\n",
					"            ('delta', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        invalid_storageaccount_df = spark.createDataFrame(invalid_storageaccount, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = invalid_storageaccount_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = True\r\n",
					"\r\n",
					"        # Expected returned value: Expect the below error to be thrown\r\n",
					"        expected_error = f\"The table is not stored on the given storage account {self.storage_account}. The storage location is abfss://{self.database_name}@invalid_storageaccount.dfs.core.windows.net/{self.table_name}\"\r\n",
					"\r\n",
					"        # Run the function: validate_table for delta-table object 'self.delta_table_object'\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            self.delta_table_object.validate_table()\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					"\r\n",
					"    def test_validate_filelocation_2(self):\r\n",
					"        # Create a dataframe with where the database-value is invalid in the name-parameter (invalid_database instead of unittest_database)\r\n",
					"        invalid_database = [\r\n",
					"            ('delta', f'{self.database_name}.{self.table_name}', f'abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        invalid_database_df = spark.createDataFrame(invalid_database, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = invalid_database_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = True \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = True\r\n",
					"\r\n",
					"        # Expected returned value: Expect the below error to be thrown\r\n",
					"        expected_error = f\"The table is not stored on the expected container {self.database_name}. The storage location is abfss://invalid_database@{self.storage_account}.dfs.core.windows.net/{self.table_name}\"\r\n",
					"        \r\n",
					"        # Run the function: validate_table for delta-table object 'self.delta_table_object'\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            self.delta_table_object.validate_table()\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					"\r\n",
					"\r\n",
					"    def test_validate_isdeltatable(self):\r\n",
					"        # Create a dataframe with where the table is not an actual DeltaTable (isDeltaTable = False)\r\n",
					"        valid_data = [\r\n",
					"            ('delta', f'{self.database_name}.{self.table_name}', f'abfss://{self.database_name}@{self.storage_account}.dfs.core.windows.net/{self.table_name}')\r\n",
					"        ]\r\n",
					"        valid_df = spark.createDataFrame(valid_data, self.schema)\r\n",
					"\r\n",
					"        # Configure the return_value of the spar.sql query to be equal to the above created dataframe\r\n",
					"        self.mock_SparkSession_sql.return_value = valid_df\r\n",
					"        # Set the return_value of the delta.table.DeltaTable.isDeltaTable method = False \r\n",
					"        self.mock_deltatable.isDeltaTable.return_value = False\r\n",
					"\r\n",
					"        # Expected returned value: Expect the below error to be thrown\r\n",
					"        expected_error = f\"The files stored on path {self.delta_table_object.silver_path} for table {self.table_name} are not a delta table\"\r\n",
					"\r\n",
					"        # Run the function: validate_table for delta-table object 'self.delta_table_object'\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            self.delta_table_object.validate_table()\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the validate_delta was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					""
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_ProcessColumnInfoString(unittest.TestCase):\r\n",
					"    # Test the process_column_info_string function from the DeployDeltaTable_v2 notebook\r\n",
					"    # The process_column_info_string method converts a string to a json-object\r\n",
					"    def setUp(self):\r\n",
					"        # No set-up needed. Keeping function for potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # No tear-down needed. Keeping function for potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"    def test_process_column_info_string(self):\r\n",
					"        # Define parameters:\r\n",
					"        #   Push the following string object to the function\r\n",
					"        self.string_object = '[{\"column_name\":\"column_1\", \"data_type\":\"string\"}, {\"column_name\":\"column_2\", \"data_type\":\"integer\"}, {\"column_name\":\"column_3\", \"data_type\":\"decimal(12,5)\"}]'\r\n",
					"\r\n",
					"        # The expected returned object is the following: \r\n",
					"        expected_object =  [{'column_name':'column_1', 'data_type':'string'}, {'column_name':'column_2', 'data_type':'integer'}, {'column_name':'column_3', 'data_type':'decimal(12,5)'}]\r\n",
					"        \r\n",
					"        # Invoke the function process_column_info_string with the self.string_object as its parameter\r\n",
					"        actual_object = process_column_info_string(self.string_object)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_object, actual_object, f\"The column info string has not been processed correctly. Expected was {expected_object}, but got {actual_object}\")\r\n",
					"\r\n",
					"    def test_process_column_info_string_2(self):\r\n",
					"        # Define parameters:\r\n",
					"        #   Push the following string object to the function\r\n",
					"        self.string_object = '{\"key\":\"value\"}'\r\n",
					"\r\n",
					"        # The expected returned object is the following: \r\n",
					"        expected_object = {\"key\": \"value\"}\r\n",
					"\r\n",
					"        # Invoke the function process_column_info_string with the self.string_object as its parameter\r\n",
					"        actual_object = process_column_info_string(self.string_object)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_object, actual_object, f\"The column info string has not been processed correctly. Expected was {expected_object}, but got {actual_object}\")\r\n",
					"\r\n",
					"    def test_process_column_info_string_invalid_json(self):\r\n",
					"        # Define parameters:\r\n",
					"        #   Push the following string object to the function\r\n",
					"        self.string_object = '[invalid_json_object]'\r\n",
					"\r\n",
					"        # Expect the following dictionary to be returned:\r\n",
					"        expected_error = f\"The string_object that was passed cannot be converted to a json object. Passed string: {self.string_object}\"\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            process_column_info_string(self.string_object)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the process_column_info_string was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_ConfigureDataTypes(unittest.TestCase):\r\n",
					"    # Test the configure_data_types function from the DeployDeltaTable_v2 notebook\r\n",
					"    # The configure_data_types method creates a dictionary of datatypes and their respective sql.types-method\r\n",
					"    # \r\n",
					"    def setUp(self):\r\n",
					"\r\n",
					"        # Patch the functions that are called by the validate_table-method of the DeltaTable-class\r\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\r\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\r\n",
					"\r\n",
					"        ## Function in the DeployDeltaTable_v2 notebook\r\n",
					"        self.mock_add_decimal_datatype_patcher = patch('__main__.add_decimal_datatype')\r\n",
					"        self.mock_add_decimal_datatype = self.mock_add_decimal_datatype_patcher.start()\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        self.mock_add_decimal_datatype_patcher.stop()\r\n",
					"\r\n",
					"    def test_configure_datatypes(self):\r\n",
					"        # Pass an empty dictionary to the configure_datatypes function\r\n",
					"        empty_dictionary = dict()\r\n",
					"\r\n",
					"        # Expectation: A dictionary is returned with all default data types\r\n",
					"        expected_dictionary = {\"array\": sql.types.ArrayType(sql.types.StringType()), \"binary\": sql.types.BinaryType(), \r\n",
					"            \"boolean\": sql.types.BooleanType(), \"date\": sql.types.DateType(), \"string\": sql.types.StringType(), \r\n",
					"            \"timestamp\": sql.types.TimestampType(), \"decimal\": sql.types.DecimalType(38,4), \"float\": sql.types.FloatType(), \r\n",
					"            \"byte\": sql.types.ByteType(), \"integer\": sql.types.IntegerType(), \"long_integer\":  sql.types.LongType()\r\n",
					"        }\r\n",
					"\r\n",
					"        # Run the function to get the actual returned dictionary\r\n",
					"        actual_dictionary = configure_datatypes(empty_dictionary)\r\n",
					"\r\n",
					"        # Assert that the expectations match the actuals\r\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\r\n",
					"\r\n",
					"    def test_configure_datatypes_decimal(self):\r\n",
					"        # If the dictionary contains a value 'decimal', the add_decimal_datatype should be called\r\n",
					"        decimal_dictionary = {'key': 'decimal'}\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        configure_datatypes(decimal_dictionary)\r\n",
					"\r\n",
					"        # Validate that the function has been called exactly once\r\n",
					"        self.mock_add_decimal_datatype.assert_called_once()\r\n",
					"\r\n",
					"    def test_configure_datatypes_decimal_2(self):\r\n",
					"        # If the dictionary contains a value 'decimal', the add_decimal_datatype should be called\r\n",
					"        decimal_dictionary = {'key': 'decimal(12,5)'}\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        configure_datatypes(decimal_dictionary)\r\n",
					"\r\n",
					"        # Validate that the function has been called exactly once\r\n",
					"        self.mock_add_decimal_datatype.assert_called_with('decimal(12,5)', ANY)\r\n",
					"\r\n",
					"    def test_configure_datatypes_non_decimal(self):\r\n",
					"        # If the dictionary does not contain a value 'decimal', the add_decimal_datatype should not be called\r\n",
					"        decimal_dictionary = {'key': 'string'}\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        configure_datatypes(decimal_dictionary)\r\n",
					"\r\n",
					"        # Validate that the function has not been called\r\n",
					"        self.mock_add_decimal_datatype.assert_not_called()\r\n",
					"\r\n",
					"    def test_configure_datatypes_multiple_decimals(self):\r\n",
					"        # If the dictionary contains multiple values 'decimal', the add_decimal_datatype should be called multiple times\r\n",
					"        decimal_dictionary = {'key': 'decimal', 'other_key': 'decimal(12,5)'}\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        configure_datatypes(decimal_dictionary)\r\n",
					"\r\n",
					"        # Validate that the function has been called for each instance of 'decimal'\r\n",
					"        # Expected calls:\r\n",
					"        expected_calls = [call('decimal', ANY), call('decimal(12,5)', ANY)]\r\n",
					"        self.mock_add_decimal_datatype.assert_has_calls(expected_calls, any_order=True)\r\n",
					"\r\n",
					"    def test_configure_datatypes_invalid_datatypes(self):\r\n",
					"        # If the dictionary contains an invalid_datatype, an error should be thrown\r\n",
					"        decimal_dictionary = {'key': 'string', 'other_key': 'invalid_datatype'}\r\n",
					"\r\n",
					"        # Expect the following dictionary to be returned:\r\n",
					"        expected_error = f\"The value 'invalid_datatype' has not been configured as a datatype in the current set-up.\"\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            configure_datatypes(decimal_dictionary)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_decimal_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					""
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_AddDecimalDatatype(unittest.TestCase):\r\n",
					"    # Test the process_column_info_string function from the DeployDeltaTable_v2 notebook\r\n",
					"    # The process_column_info_string method converts a string to a json-object\r\n",
					"    def setUp(self):\r\n",
					"        # No set-up needed. Keeping function for potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # No tear-down needed. Keeping function for potential future use\r\n",
					"        return\r\n",
					"\r\n",
					"    def test_add_decimal_datatype(self):\r\n",
					"        # Define the parameters to be passed to the add_decimal_datatype function\r\n",
					"        types = dict()\r\n",
					"        value = 'decimal(12,5)'\r\n",
					"\r\n",
					"        # Expect the following dictionary to be returned:\r\n",
					"        expected_dictionary = {\"decimal(12,5)\": sql.types.DecimalType(12,5)}\r\n",
					"\r\n",
					"        # Get the actual dictionary that is returned when running the function\r\n",
					"        actual_dictionary = add_decimal_datatype(value, types)\r\n",
					"\r\n",
					"        # Assert equality between the dictionaries\r\n",
					"        self.assertEqual(expected_dictionary, actual_dictionary, f\"The returned datatypes dictionary does not match expectations. Actual: {actual_dictionary} ; \\n Expected=: {expected_dictionary}\")\r\n",
					"\r\n",
					"    def test_invalid_value(self):\r\n",
					"        # Define the parameters to be passed to the add_decimal_datatype function\r\n",
					"        types = dict()\r\n",
					"        value = 'decimal(12)'\r\n",
					"\r\n",
					"        # Expect the following dictionary to be returned:\r\n",
					"        expected_error = f\"The decimal value {value} is not a valid datatype value and cannot be configured correctly. Aborting delta table deployment...\"\r\n",
					"\r\n",
					"        # Run the function\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            add_decimal_datatype(value, types)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # Assert that the actual return value is equal to the expectated return value\r\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the add_decimal_datatype was \"{actual_error}\", while the expected error was \"{expected_error}\".')\r\n",
					""
				],
				"execution_count": 26
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# # First, create a file and then write every line to it\r\n",
					"# log_file = 'output.txt'\r\n",
					"# with open(log_file, \"w\") as f:\r\n",
					"#     runner = unittest.TextTestRunner(f, verbosity=2)\r\n",
					"#     findings = unittest.main(argv=[''], testRunner=runner, verbosity=2, exit=False, warnings='ignore')\r\n",
					"\r\n",
					"# # Then, read what's in the list and write it to the designated container\r\n",
					"# with open(log_file, \"r\") as f:\r\n",
					"#         test = f.read()\r\n",
					"#         print(test)"
				],
				"execution_count": 27
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_ErrorHandling.json">
{
	"name": "Test_ErrorHandling",
	"properties": {
		"folder": {
			"name": "Modules/Test_Modules"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore2",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "677a0b75-d53d-4491-82a6-f7233a0966a7"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore2",
				"name": "devdapsspcore2",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore2",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Classes\r\n",
					"**Purpose**: This Notebook will using the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the Spark Notebooks in the folder *Modules*.\r\n",
					"\r\n",
					"Unit tests: To test if a method does what we expect it to do, a untit test is written. A unit test isolates a method from the overall environment and executes the method. The aim is to be able to define the expected outcome of the method and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the method successfully\r\n",
					"\r\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\r\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\r\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import unittest\r\n",
					"from unittest.mock import patch, MagicMock, Mock, call, ANY\r\n",
					"\r\n",
					"import pyspark.testing.utils # Pyspark version 3.5 contains useful functions (assertDataFrameEqual, assertSchemaEqual...) but they do not exist in the current 3.3.1 pyspark version\r\n",
					"\r\n",
					"from pyspark.sql.types import StructType,StructField, StringType, IntegerType\r\n",
					"from delta.tables import DeltaTable\r\n",
					"from pyspark.sql.functions import col\r\n",
					"import pyspark\r\n",
					"\r\n",
					"import pandas as pd\r\n",
					"from pyspark.sql import SparkSession\r\n",
					"\r\n",
					"from io import StringIO\r\n",
					"import os\r\n",
					"from py4j.java_gateway import JavaObject\r\n",
					"\r\n",
					"import time\r\n",
					"import datetime\r\n",
					"from notebookutils import mssparkutils"
				],
				"execution_count": 47
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Parameters\r\n",
					"The parameters contain values that are used throughout the notebook, and can be changed externally when calling the notebook. When testing the notebook internally, defaults in the cell itself are used"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\r\n",
					"environment_code = 'dev'"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\r\n",
					"\r\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\r\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\r\n",
					"if env_code not in ['dev', 'int']:\r\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 49
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define global parameters to use over the entire notebook\r\n",
					"container_name = 'unittest'\r\n",
					"storage_account = f'{env_code}dapstdala1'"
				],
				"execution_count": 50
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the methods that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": 51
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define Test Cases"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Get_Exception_Information()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# This case will test the get_exception_information defined in the ErrorHandling notebook\r\n",
					"class Test_ErrorHandling_GetExceptionInformation(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown TestCass class\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls to function get_exception_information()\r\n",
					"    # Success: Throw a Py4JavaError (java) and expect the underlying error to be returned\r\n",
					"    def test_getexceptioninformation_errorhandling_success_javaerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Catch a java-native error: FileNotFoundError\r\n",
					"        try:\r\n",
					"            mssparkutils.fs.ls(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/nonexisting_foldername')\r\n",
					"        except Exception as thrown_error:\r\n",
					"            exception = thrown_error\r\n",
					"\r\n",
					"        expected_exception_class    = 'FileNotFoundException'\r\n",
					"        expected_exception_message  = 'Operation failed: \"The specified path does not exist.\"'\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_exception_class, actual_exception_message = get_exception_information(exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expectations match the actual results\r\n",
					"        self.assertEqual(expected_exception_class,      actual_exception_class,      f'[ErrorHandling:GetExceptionInformation] 1. Expectations do not match actuals: \\nEXPECTED: {expected_exception_class}   \\nACTUAL: {actual_exception_class}')\r\n",
					"        self.assertRegex(actual_exception_message,      expected_exception_message,  f'[ErrorHandling:GetExceptionInformation] 1. Expectations do not match actuals: \\nEXPECTED: {expected_exception_message} \\nACTUAL: {actual_exception_message}')\r\n",
					"\r\n",
					"    # Success: Throw a ValueError (python) and expect the underlying error to be returned\r\n",
					"    def test_getexceptioninformation_errorhandling_success_pythonerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Catch a java-native error: FileNotFoundError\r\n",
					"        exception = ValueError(\"This is a custom error message\")\r\n",
					"\r\n",
					"        expected_exception_class    = 'ValueError'\r\n",
					"        expected_exception_message  = 'This is a custom error message'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_exception_class, actual_exception_message = get_exception_information(exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expectations match the actual results\r\n",
					"        self.assertEqual(expected_exception_class,      actual_exception_class,      f'[ErrorHandling:GetExceptionInformation] 2. Expectations do not match actuals: \\nEXPECTED: {expected_exception_class}   \\nACTUAL: {actual_exception_class}')\r\n",
					"        self.assertEqual(expected_exception_message,    actual_exception_message,    f'[ErrorHandling:GetExceptionInformation] 2. Expectations do not match actuals: \\nEXPECTED: {expected_exception_message} \\nACTUAL: {actual_exception_message}')\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_ErrorHandling_GetExceptionInformation)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in ErrorHandling GetExceptionInformation tests, something went wrong!\")"
				],
				"execution_count": 52
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Is_Custom_Error()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# This case will test the is_custom_error defined in the ErrorHandling notebook\r\n",
					"class Test_ErrorHandling_IsCustomError(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown TestCass class\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls to function is_custom_error()\r\n",
					"\r\n",
					"    # Success: Initialize a ErrorHandling class-object with valid arguments \r\n",
					"    def test_iscustomerror_errorhandling_success_customerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        error_class = 'MiddleWareError'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run function under test\r\n",
					"        actual_result = is_custom_error(error_class)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertTrue(actual_result, f'[ErrorHandling:IsCustomError] Expected {error_class} to be a custom error, but is_custom_error returned False')\r\n",
					"\r\n",
					"    # Success: Initialize a ErrorHandling class-object with valid arguments \r\n",
					"    def test_iscustomerror_errorhandling_success_noncustomerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        error_class = 'ValueError'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run function under test\r\n",
					"        actual_result = is_custom_error(error_class)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertFalse(actual_result, f'[ErrorHandling:IsCustomError] Expected {error_class} not to be a custom error, but is_custom_error returned True')\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_ErrorHandling_IsCustomError)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in ErrorHandling IsCustomError tests, something went wrong!\")"
				],
				"execution_count": 53
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Handle_Exception()\r\n",
					"**Dev-Info**: The below Test Case implicitly tests each custom error class. Therefore, no explicit test case is written for these classes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# This case will test the handle_exception defined in the ErrorHandling notebook\r\n",
					"class Test_ErrorHandling_HandleException(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        # Define the env_code parameter to use throughout the class\r\n",
					"        self.env_code = env_code\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # tearDown TestCass class\r\n",
					"        return\r\n",
					"    \r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls to function handle_exception()\r\n",
					"\r\n",
					"    # Success: Initialize a ErrorHandling class-object with valid arguments \r\n",
					"    def test_handleexception_errorhandling_success_customerror_configurationerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        error_message   = \"Custom message for configuration error\"\r\n",
					"        notebook_name   = \"Test_ErrorHandling\"\r\n",
					"        class_name      = \"Test_ErrorHandling_HandleException\"\r\n",
					"        error_class     = \"ConfigurationError\"\r\n",
					"\r\n",
					"        error_object = ConfigurationError(custom_message=error_message, notebook_name=notebook_name, class_name=class_name)\r\n",
					"\r\n",
					"        expected_result = {\r\n",
					"            \"custom_message\": error_message,\r\n",
					"            \"custom_error_class\": error_class,\r\n",
					"            \"error_location_in_notebooks\": f\"[{notebook_name}:{class_name}:]\",\r\n",
					"            \"responsible_team\": \"DAP Engineers (add email)\"\r\n",
					"        }\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = handle_exceptions(exception_class=error_class, exception_message=error_message, exception_object=error_object)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_result, actual_result)\r\n",
					"\r\n",
					"    # Success: Initialize a ErrorHandling class-object with valid arguments \r\n",
					"    def test_handleexception_errorhandling_success_customerror_middlewareerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        error_message   = \"Custom message for middleware error\"\r\n",
					"        notebook_name   = \"Test_ErrorHandling\"\r\n",
					"        class_name      = \"Test_ErrorHandling_HandleException\"\r\n",
					"        error_class     = \"MiddleWareError\"\r\n",
					"\r\n",
					"        error_object = MiddleWareError(custom_message=error_message, notebook_name=notebook_name, class_name=class_name)\r\n",
					"\r\n",
					"        expected_result = {\r\n",
					"            \"custom_message\": error_message,\r\n",
					"            \"custom_error_class\": error_class,\r\n",
					"            \"error_location_in_notebooks\": f\"[{notebook_name}:{class_name}:]\",\r\n",
					"            \"responsible_team\": \"Middleware Team (add email)\"\r\n",
					"        }\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = handle_exceptions(exception_class=error_class, exception_message=error_message, exception_object=error_object)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_result, actual_result)\r\n",
					"\r\n",
					"    # Success: Initialize a ErrorHandling class-object with valid arguments \r\n",
					"    def test_handleexception_errorhandling_success_noncustomerror(self):\r\n",
					"        # PREPROCESS\r\n",
					"        error_message   = \"This is a value error message\"\r\n",
					"        error_class     = \"ValueError\"\r\n",
					"\r\n",
					"        error_object = ValueError(error_message)\r\n",
					"\r\n",
					"        expected_result = {\r\n",
					"            \"custom_message\": \"Exception: unknown exception. Rerun task %{task_name}% (task_id: %{task_id}% , plan_id: %{plan_id}%) in debug-mode to see where the error is coming from. After debugging, make sure the add the use case to the ErrorHandling notebook.\",\r\n",
					"            \"custom_error_class\": \"Exception\",\r\n",
					"            \"error_location_in_notebooks\": \"[MetaNotebook::]\",\r\n",
					"            \"responsible_team\": \"DAP Core Engineers\",\r\n",
					"            \"python_error_class\": error_class,\r\n",
					"            \"python_error_message\": error_message\r\n",
					"        }\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = handle_exceptions(exception_class=error_class, exception_message=error_message, exception_object=error_object)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_result, actual_result)\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_ErrorHandling_HandleException)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in ErrorHandling HandleException tests, something went wrong!\")"
				],
				"execution_count": 60
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_GenericFunctions.json">
{
	"name": "Test_GenericFunctions",
	"properties": {
		"folder": {
			"name": "Functions/Test_Functions"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"targetSparkConfiguration": {
			"referenceName": "core_configuration",
			"type": "SparkConfigurationReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "c99f2635-0b8c-4347-b2eb-15baa94c6dc4"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30,
			"targetSparkConfiguration": "core_configuration"
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test GenericFunctions\n",
					"**Purpose**: This Notebook will using the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the functions in Spark Notebook *GenericFunctions* in the folder *Functions*.\n",
					"\n",
					"Unit tests: To test if a function does what we expect it to do, a untit test is written. A unit test isolates a function from the overall environment and executes the function. The aim is to be able to define the expected outcome of the function and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the function successfully\n",
					"\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import unittest\n",
					"from unittest.mock import patch, MagicMock, ANY, call\n",
					"\n",
					"import pyspark.testing.utils # Pyspark version 3.5 contains useful functions (assertDataFrameEqual, assertSchemaEqual...) but they do not exist in the current 3.3.1 pyspark version\n",
					"import os\n",
					"import datetime\n",
					"import ast"
				],
				"execution_count": 1
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Initialize the environment_code parameter\r\n",
					"In almost all notebooks, a reference to this parameter will be found. This is because the CI/CD pipelines will not overwrite specific environment-references defined in notebooks. Since the notebooks will be deployed to several environments, the use of the environment_code parameter has been chosen. \r\n",
					"The environment_code will be used when the spark session has no environment_code-argument defined. This argument is linked to the core_configuration, which can be found under Manage -&gt; Apache Spark Configurations\r\n",
					"The use of these parameters and arguments is so that, during deployment, there is only 1 place where all references need to be overwritten."
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\n",
					"environment_code = 'dev'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\n",
					"\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define global parameters to use over the entire notebook\r\n",
					"container_name = 'unittest'\r\n",
					"storage_account = f'{env_code}dapstdala1'"
				],
				"execution_count": 4
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the functions that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/GenericFunctions"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Define Test Classes"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Validate_Argument()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_validate_argument(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: String argument is part of the allowed_list\r\n",
					"    def test_validateargument_genericfunctions_success_stringvalue(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_string\"\r\n",
					"        argument_value = \"success\"\r\n",
					"        allowed_list = [\"success\", \"foo\", \"bar\"]\r\n",
					"\r\n",
					"        \r\n",
					"        # EXECUTE &amp; EVALUTE\r\n",
					"        # Run the function under test\r\n",
					"        # Validate that no error is thrown for a valid string value\r\n",
					"        try:\r\n",
					"            validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n",
					"        except ValueError:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 1. validate_argument() should not have thrown a value-error\")\r\n",
					"        except Exception:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 1. validate_argument() should not have thrown an exception\")\r\n",
					"    \r\n",
					"    # Success: Integer argument is part of the allowed_list\r\n",
					"    def test_validateargument_genericfunctions_success_integervalue(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_integer\"\r\n",
					"        argument_value = 123\r\n",
					"        allowed_list = [456, 123, 789]\r\n",
					"\r\n",
					"        \r\n",
					"        # EXECUTE &amp; EVALUTE\r\n",
					"        # Run the function under test\r\n",
					"        # Validate that no error is thrown for a valid integer value\r\n",
					"        try:\r\n",
					"            validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n",
					"        except ValueError:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 2. validate_argument() should not have thrown a value-error\")\r\n",
					"        except Exception:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 2. validate_argument() should not have thrown an exception\")\r\n",
					"    \r\n",
					"    # Success: List argument is part of the allowed_list\r\n",
					"    def test_validateargument_genericfunctions_success_listvalue(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_integer\"\r\n",
					"        argument_value = [\"list\", \"of\", \"strings\"]\r\n",
					"        allowed_list = [[\"foo\", \"bar\"], [\"list\", \"of\", \"strings\"]]\r\n",
					"\r\n",
					"        \r\n",
					"        # EXECUTE &amp; EVALUTE\r\n",
					"        # Run the function under test\r\n",
					"        # Validate that no error is thrown for a valid list value\r\n",
					"        try:\r\n",
					"            validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n",
					"        except ValueError:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 3. validate_argument() should not have thrown a value-error\")\r\n",
					"        except Exception:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 3. validate_argument() should not have thrown an exception\")\r\n",
					"\r\n",
					"\r\n",
					"    # Failure: Argument is not part of the allowed_list\r\n",
					"    def test_validateargument_genericfunctions_failure_notinallowedlist(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_failtest\"\r\n",
					"        argument_value = \"failure\"\r\n",
					"        allowed_list = [\"success\", \"foo\", \"bar\"]\r\n",
					"        # Define expected return values\r\n",
					"        expected_error = f\"The {argument_name}-argument '{str(argument_value)}' is not listed in the allowed {argument_name} list: {allowed_list}\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            validate_argument(argument_name=argument_name, argument_value=argument_value, allowed_list=allowed_list)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Expect an error to be returned\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[GenericFunctions:ValidateArgument] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_validate_argument)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in validate_argument() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Validate_ArgumentList()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_ValidateArgumentList(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: String argument is part of the allowed_list\r\n",
					"    def test_validateargumentlist_genericfunctions_success_singleitem(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_string\"\r\n",
					"        argument_list = [\"success\"]\r\n",
					"        allowed_list = [\"success\", \"foo\", \"bar\"]\r\n",
					"\r\n",
					"        \r\n",
					"        # EXECUTE &amp; EVALUTE\r\n",
					"        # Run the function under test\r\n",
					"        # Validate that no error is thrown for a valid list of one item\r\n",
					"        try:\r\n",
					"            validate_argument_list(argument_name=argument_name, argument_list=argument_list, allowed_list=allowed_list)\r\n",
					"        except ValueError:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 1. validate_argument_list() should not have thrown a value-error\")\r\n",
					"        except Exception:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 1. validate_argument_list() should not have thrown an exception\")\r\n",
					"\r\n",
					"    # Success: Integer argument is part of the allowed_list\r\n",
					"    def test_validateargumentlist_genericfunctions_success_multipleitems(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_integer\"\r\n",
					"        argument_list = [\"success\", \"foo\", \"success\", \"bar\"]\r\n",
					"        allowed_list = [\"success\", \"foo\", \"bar\"]\r\n",
					"\r\n",
					"        \r\n",
					"        # EXECUTE &amp; EVALUTE\r\n",
					"        # Run the function under test\r\n",
					"        # Validate that no error is thrown for a valid list of multiple items\r\n",
					"        try:\r\n",
					"            validate_argument_list(argument_name=argument_name, argument_list=argument_list, allowed_list=allowed_list)\r\n",
					"        except ValueError:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 2. validate_argument_list() should not have thrown a value-error\")\r\n",
					"        except Exception:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 2. validate_argument_list() should not have thrown an exception\")\r\n",
					"    \r\n",
					"\r\n",
					"    # Failure: Argument is not part of the allowed_list\r\n",
					"    def test_validateargumentlist_genericfunctions_failure_notinallowedlist(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        argument_name = \"unittest_failtest\"\r\n",
					"        argument_list = [\"success\", \"failure\", \"error\"]\r\n",
					"        allowed_list = [\"success\", \"foo\", \"bar\"]\r\n",
					"\r\n",
					"        # Define expected return values\r\n",
					"        not_allowed_list = [\"failure\", \"error\"]\r\n",
					"        expected_error = f\"[GenericFunctions] The {argument_name}-argument does not take the following list of give parameters: '{', '.join(not_allowed_list)}'; Allowed values: {', '.join(allowed_list)}\"\r\n",
					"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            validate_argument_list(argument_name=argument_name, argument_list=argument_list, allowed_list=allowed_list)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the thrown error matches expectations\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[GenericFunctions:ValidateArgument] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_ValidateArgumentList)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in validate_argumentlist() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Validate_LinkedService()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_ValidateLinkedService(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: String argument is part of the allowed_list\r\n",
					"    def test_validatelinkedservice_genericfunctions_success_exists(self):\r\n",
					"        # PREPROCESS\r\n",
					"        linked_service = \"ls_dap_sql_meta\"\r\n",
					"\r\n",
					"        # EXECUTE &amp; EVALUTE\r\n",
					"        # Run the function under test\r\n",
					"        # Validate that no error is thrown for a valid (existing) linked service\r\n",
					"        try:\r\n",
					"            validate_linked_service(linked_service=linked_service)\r\n",
					"        except Exception:\r\n",
					"            self.fail(f\"[GenericFunctions:ValidateArgument] 1. validate_argument_list() should not have thrown an exception\")\r\n",
					"\r\n",
					"\r\n",
					"    # TESTS:\r\n",
					"    # Success: Call mssparkutils.credentials.getPropertiesAll()\r\n",
					"    def test_validatelinkedservice_genericfunctions_success_functioncalls(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        linked_service = \"ls_dap_sql_meta\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        with patch.object(mssparkutils.credentials, \"getPropertiesAll\") as mock_get_properties_all:\r\n",
					"            validate_linked_service(linked_service=linked_service)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the mocked functions have been called with the expected arguments\r\n",
					"        mock_get_properties_all.assert_called_once_with(linked_service)\r\n",
					"\r\n",
					"\r\n",
					"    # Failure: Linked Service does not exist\r\n",
					"    def test_validatelinkedservice_genericfunctions_success_doesnotexist(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments that will be used to call the function\r\n",
					"        linked_service = \"does_not_exist\"\r\n",
					"        # Define expected return values\r\n",
					"        expected_error = f\"Linked service {linked_service} does not exist or has not been published\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            validate_linked_service(linked_service=linked_service)\r\n",
					"\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the thrown error matches expectations\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[GenericFunctions:ValidateArgument] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_ValidateLinkedService)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in validate_linked_service() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Filter_List()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_FilterList(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        # Create a list of temporary file objects\r\n",
					"        self.files = []\r\n",
					"        path= f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/filter_list/'\r\n",
					"        path_contents = mssparkutils.fs.ls(path)\r\n",
					"        for item in path_contents:\r\n",
					"            self.files.append(item)\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Return two files using regex filter\r\n",
					"    def test_filterlist_genericfunctions_success_with_regex(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        pattern = '.*(MMDAI).*'\r\n",
					"        # Define expected return values\r\n",
					"        # Dev-Note: the classes take the entire object, so we need to extract the name to compare it with the expected_filtered_list\r\n",
					"        expected_filtered_list=[\"MMDAI\",\"MMDAILY5365\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_filtered_list=filter_list(self.files,pattern)\r\n",
					"\r\n",
					"        # EVALUTE\r\n",
					"        # Validate that the returned filtered list matches with the expectations\r\n",
					"        actual_filtered_list_name=[]\r\n",
					"        for item in actual_filtered_list:\r\n",
					"            actual_filtered_list_name.append(item.name)\r\n",
					"        self.assertEqual(expected_filtered_list,actual_filtered_list_name, f\"[GenericFunctions:FilterList] 1.The lists are not equal. \\nExpected {expected_filtered_list} \\nActual{actual_filtered_list_name}\")\r\n",
					"    \r\n",
					"    # Success: Return one file using non-regex\r\n",
					"    def test_filterlist_genericfunctions_success_without_regex(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        pattern = '^MMDAI$'\r\n",
					"        # Define expected return values\r\n",
					"        # Dev-Note: the classes take the entire object, so we need to extract the name to compare it with the expected_filtered_list\r\n",
					"        expected_filtered_list=[\"MMDAI\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_filtered_list=filter_list(self.files,pattern)\r\n",
					"\r\n",
					"        # EVALUTE\r\n",
					"        # Validate that the returned filtered list matches with the expectations\r\n",
					"        actual_filtered_list_name=[]\r\n",
					"        for item in actual_filtered_list:\r\n",
					"            actual_filtered_list_name.append(item.name)\r\n",
					"        self.assertEqual(expected_filtered_list,actual_filtered_list_name, f\"[GenericFunctions:FilterList] 2. The lists are not equal. \\nExpected {expected_filtered_list} \\nActual{actual_filtered_list_name}\")\r\n",
					"    \r\n",
					"    # Success: Return all files using pattern='*'\r\n",
					"    def test_filterlist_genericfunctions_success_all(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        pattern = '.*'\r\n",
					"        # Define expected return values\r\n",
					"        # Dev-Note: the classes take the entire object, so we need to extract the name to compare it with the expected_filtered_list\r\n",
					"        expected_filtered_list=[\"MMDAI\",\"MMDAILY5365\",\"reqgrg3\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_filtered_list=filter_list(self.files,pattern)\r\n",
					"\r\n",
					"        # EVALUTE\r\n",
					"        # Validate that the returned filtered list matches with the expectations\r\n",
					"        actual_filtered_list_name=[]\r\n",
					"        for item in actual_filtered_list:\r\n",
					"            actual_filtered_list_name.append(item.name)\r\n",
					"        self.assertEqual(expected_filtered_list,actual_filtered_list_name, f\"[GenericFunctions:FilterList] 3. The lists are not equal. \\nExpected {expected_filtered_list} \\nActual{actual_filtered_list_name}\")\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_FilterList)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in filter_list() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## contains_regex_special_chars()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_ContainsRegexSpecialChars(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: return True if the value contains a regex character\r\n",
					"    def test_ContainsRegexSpecialChars_genericfunctions_success_true(self):\r\n",
					"        # PREPARE\r\n",
					"        # Define function arguments\r\n",
					"        value = \"this is a string with .* regex characters\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result:bool = contains_regex_special_chars(value=value)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned result is True\r\n",
					"        self.assertTrue(actual_result, f\"[GenericFunctions:ContainsRegexSpecialChars] Expected to return True for string with regex, but got {actual_result}\")\r\n",
					"\r\n",
					"    # Success: return False if the value does not contain a regex character\r\n",
					"    def test_ContainsRegexSpecialChars_genericfunctions_success_false(self):\r\n",
					"        # PREPARE\r\n",
					"        # Define function arguments\r\n",
					"        value = \"this is a string without regex characters\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result:bool = contains_regex_special_chars(value=value)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned result is False\r\n",
					"        self.assertFalse(actual_result, f\"[GenericFunctions:ContainsRegexSpecialChars] Expected to return False for string with regex, but got {actual_result}\")\r\n",
					"\r\n",
					"    # Success: return False if the value contains a slash\r\n",
					"    def test_ContainsRegexSpecialChars_genericfunctions_success_slash(self):\r\n",
					"        # PREPARE\r\n",
					"        # Define function arguments\r\n",
					"        value = \"pass/a/path/like/string\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result:bool = contains_regex_special_chars(value=value)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned result is False\r\n",
					"        self.assertFalse(actual_result, f\"[GenericFunctions:ContainsRegexSpecialChars] Expected to return False for string with regex, but got {actual_result}\")\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_ContainsRegexSpecialChars)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in list_directory_content() tests, something went wrong!\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## split_string_at_regex()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_SplitStringAtRegex(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Basic split with a simple regex\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_basic_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \"this/is/a/$w+/regex/path\"\r\n",
					"        split_character = \"/\"\r\n",
					"        expected_path = \"this/is/a\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(actual_path, expected_path)\r\n",
					"\r\n",
					"\r\n",
					"    # Success: Split on space with regex\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_space_split_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \"this is *w+ a regex .* sentence\"\r\n",
					"        split_character = \" \"\r\n",
					"        expected_path = \"this is\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(actual_path, expected_path)\r\n",
					"\r\n",
					"\r\n",
					"    # Success: No regex present, return full string\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_no_regex_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \"this/is/a/normal/path\"\r\n",
					"        split_character = \"/\"\r\n",
					"        expected_path = \"this/is/a/normal/path\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(actual_path, expected_path)\r\n",
					"\r\n",
					"\r\n",
					"    # Success: Multiple split characters\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_multiple_split_characters_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \"part1;part2;part3/$w+/part4\"\r\n",
					"        split_character = \";\"\r\n",
					"        expected_path = \"part1;part2\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(actual_path, expected_path)\r\n",
					"\r\n",
					"\r\n",
					"    # Edge Case: Empty string input\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_empty_string(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \"\"\r\n",
					"        split_character = \"/\"\r\n",
					"        \r\n",
					"        # EXECUTE &amp; EVALUATE\r\n",
					"        with self.assertRaises(ConfigurationError):\r\n",
					"            split_string_at_regex(value=value, split_character=split_character)\r\n",
					"\r\n",
					"\r\n",
					"    # Edge Case: String with only regex characters\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_only_regex_characters(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \".*$w+\"\r\n",
					"        split_character = \"/\"\r\n",
					"        expected_path = \".*$w+\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(actual_path, expected_path)\r\n",
					"\r\n",
					"\r\n",
					"    # Edge Case: No split character present\r\n",
					"    def test_SplitStringAtRegex_genericfunctions_no_split_character(self):\r\n",
					"        # PREPROCESS\r\n",
					"        value = \"thisisaregex$w+\"\r\n",
					"        split_character = \",\"\r\n",
					"        expected_path = \"thisisaregex$w+\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        actual_path = split_string_at_regex(value=value, split_character=split_character)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(actual_path, expected_path)\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_SplitStringAtRegex)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in list_directory_content() tests, something went wrong!\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## locate_static_path()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Implicitly test the locate_static_path function by calling list_directory_contents\r\n",
					"class Test_GenericFunctions_LocateStaticPath(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: List a set of files with a path that has a regex-expression and has a part after the regex \r\n",
					"    def test_LocateStaticPath_genericfunctions_regexpath(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/.*/filtered'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        self.assertEqual(len(actual_files), 2)\r\n",
					"        file_paths = [file.path for file in actual_files]\r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/filtered/included.txt', file_paths            ,f\"[GenericFunctions:LocateStaticPath] 1. Could not find file 'subfolder/filtered/included.txt' in returned file_list: {file_paths}\")\r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths      ,f\"[GenericFunctions:LocateStaticPath] 1. Could not find file 'other_subfolder/filtered/included.txt' in returned file_list: {file_paths}\")        \r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths           ,f\"[GenericFunctions:LocateStaticPath] 1. Unexpectedly found file 'subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")\r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths     ,f\"[GenericFunctions:LocateStaticPath] 1. Unexpectedly found file 'other_subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")\r\n",
					"\r\n",
					"\r\n",
					"    # Success: List a set of files with a path that ends in a regex expression and uses a subfolder\r\n",
					"    def test_LocateStaticPath_genericfunctions_regexpath_1(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/.*'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        self.assertEqual(len(actual_files), 2)\r\n",
					"        file_paths = [file.path for file in actual_files]\r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/filtered/included.txt', file_paths             ,f\"[GenericFunctions:LocateStaticPath] 2. Could not find file 'subfolder/filtered/included.txt' in returned file_list: {file_paths}\")\r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths               ,f\"[GenericFunctions:LocateStaticPath] 2. Unexpectedly found file 'other_subfolder/filtered/included.txt' in returned file_list: {file_paths}\")   \r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths    ,f\"[GenericFunctions:LocateStaticPath] 2. Could not find 'subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")        \r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths      ,f\"[GenericFunctions:LocateStaticPath] 2. Unexpectedly found file 'other_subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")\r\n",
					"\r\n",
					"\r\n",
					"\r\n",
					"    # Success: List a set of files with a path that ends in a regex expression and lists everything from the parent folder\r\n",
					"    def test_LocateStaticPath_genericfunctions_regexpath_2(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/.*'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        self.assertEqual(len(actual_files), 4)\r\n",
					"        file_paths = [file.path for file in actual_files]\r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/filtered/included.txt', file_paths        ,f\"[GenericFunctions:LocateStaticPath] 3. Could not find file 'subfolder/filtered/included.txt' in returned file_list: {file_paths}\")\r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths  ,f\"[GenericFunctions:LocateStaticPath] 3. Could not find file 'other_subfolder/filtered/included.txt' in returned file_list: {file_paths}\")   \r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths          ,f\"[GenericFunctions:LocateStaticPath] 3. Could not find file 'subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")        \r\n",
					"        self.assertIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths    ,f\"[GenericFunctions:LocateStaticPath] 3. Could not find file 'other_subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")\r\n",
					"\r\n",
					"\r\n",
					"    # Success: List a set of files with a path that has a regex expression and filters on a non-existing path\r\n",
					"    def test_LocateStaticPath_genericfunctions_regexpath_3(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        regex_path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/.*/nonexisting_subfolder'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_files, actual_folders = list_directory_content(regex_path,[],[])\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        self.assertEqual(len(actual_files), 0)\r\n",
					"        file_paths = [file.path for file in actual_files]\r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/filtered/included.txt', file_paths         ,f\"[GenericFunctions:LocateStaticPath] 4. Unexpectedly found file 'subfolder/filtered/included.txt' in returned file_list: {file_paths}\")\r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/filtered/included.txt', file_paths   ,f\"[GenericFunctions:LocateStaticPath] 4. Unexpectedly found file 'other_subfolder/filtered/included.txt' in returned file_list: {file_paths}\")   \r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/subfolder/ignored/skipped.txt', file_paths           ,f\"[GenericFunctions:LocateStaticPath] 4. Unexpectedly found file 'subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")        \r\n",
					"        self.assertNotIn(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_static_path/other_subfolder/ignored/skipped.txt', file_paths     ,f\"[GenericFunctions:LocateStaticPath] 4. Unexpectedly found file 'other_subfolder/ignored/skipped.txt' in returned file_list: {file_paths}\")\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_LocateStaticPath)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in list_directory_content() tests, something went wrong!\")"
				],
				"execution_count": 19
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## List_Directory_Content()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_ListDirectoryContent(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Return the full directory and check if the expected files are in the directory\r\n",
					"    def test_listdirectorycontent_genericfunctions_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path = f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/list_directory_content'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_files, actual_folders = list_directory_content(path, list(), list())\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        self.assertEqual(len(actual_files), 3)\r\n",
					"        self.assertEqual(len(actual_folders), 2)\r\n",
					"        file_paths = [file.path for file in actual_files]\r\n",
					"        folder_paths = [folder.path for folder in actual_folders]\r\n",
					"        self.assertIn(f'{path}/root.txt', file_paths)\r\n",
					"        self.assertIn(f'{path}/parent/parent.csv', file_paths)\r\n",
					"        self.assertIn(f'{path}/parent/child/child.json', file_paths)\r\n",
					"        self.assertIn(f'{path}/parent', folder_paths)\r\n",
					"        self.assertIn(f'{path}/parent/child', folder_paths)\r\n",
					"    \r\n",
					"    def test_listdirectorycontent_folder_does_not_exist_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/list_directory_content/folderdoesnotexist'\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        files,folders=list_directory_content(path,list(),list())\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        self.assertTrue(len(files) ==0, f'[GenericFunctions:List_Directory_Content] Expected no files to be returned but found {files}.')\r\n",
					"        self.assertTrue(len(folders) ==0,f'[GenericFunctions:List_Directory_Content] Expected no folders to be returned but found {folders}.')\r\n",
					"\r\n",
					"    def test_listdirectorycontent_folder_failed(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path_failed=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunction/list_directory_content/folderdoesnotexist'\r\n",
					"        list_files=[]\r\n",
					"        list_folders=[\"test\"]\r\n",
					"        expected_message=\"There is an issue with the parent folder [test]\"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        with self.assertRaises(Exception) as error:\r\n",
					"            list_directory_content(path_failed,list_files,list_folders)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the returned list of contains the expected values\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        actual_message = ast.literal_eval(actual_error)['custom_message']\r\n",
					"        self.assertEqual(expected_message, actual_message, f\"[GenericFunctions:List_Directory_Content] Expected error does not match actual: \\nExpected: {expected_message} \\nActual: {actual_message}\")\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_ListDirectoryContent)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in list_directory_content() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Remove_Extension()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_RemoveExtension(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Successfully remove the extension\r\n",
					"    def test_removeextension_genericfunctions_success_exists(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        string_with_extension = 'string.txt'\r\n",
					"        extension_value = 'txt'\r\n",
					"        # Define expected return values\r\n",
					"        expected_result = 'string'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = remove_extension(string_with_extension = string_with_extension, extension_value=extension_value)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the result matches expectations\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:RemoveExtension] Expected result does not match actual: \\nExpected: {expected_result} \\nActual: {actual_result}\")\r\n",
					"\r\n",
					"\r\n",
					"    # Failure: Throw error since no '.' in string value\r\n",
					"    def test_removeextension_genericfunctions_failure(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        string_with_extension = 'string'\r\n",
					"        extension_value = 'txt'\r\n",
					"        # Define expected return values\r\n",
					"        expected_message = f\"Trying to remove extension {extension_value} from {string_with_extension}, but extension is not at the end of the string\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"\r\n",
					"        with self.assertRaises(ConfigurationError) as error:\r\n",
					"            remove_extension(string_with_extension = string_with_extension, extension_value=extension_value)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # # Validate that the returned error is the expected error\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        actual_error = ast.literal_eval(actual_error)\r\n",
					"        actual_message = actual_error[\"custom_message\"]\r\n",
					"        self.assertEqual(expected_message, actual_message, f\"[GenericFunctions:RemoveExtension] Expected error message does not match actual: \\nExpected: {expected_message} \\nActual: {actual_message}\")\r\n",
					"\r\n",
					"    # Success: Successfully remove the extension\r\n",
					"    def test_removeextension_genericfunctions_success_noextension(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        string_with_extension = 'string'\r\n",
					"        extension_value = None\r\n",
					"        # Define expected return values\r\n",
					"        expected_result = 'string'\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = remove_extension(string_with_extension = string_with_extension, extension_value=extension_value)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the result matches expectations\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:RemoveExtension] Expected result does not match actual: \\nExpected: {expected_result} \\nActual: {actual_result}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_RemoveExtension)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in remove_extension() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Process_String_To_Json()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_ProcessStringToJson(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Return single json object\r\n",
					"    def test_processstringtojson_genericfunctions_success_simpleobject(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        string_object = \"{'key': 'value', 'foo': 'bar'}\"\r\n",
					"        # Define expected return values\r\n",
					"        expected_result = {'key': 'value', 'foo': 'bar'}\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = process_string_to_json(string_object=string_object)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the result matches expectations\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:ProcessStringToJson] Expected result does not match actual: \\nExpected: {expected_result} \\nActual: {actual_result}\")\r\n",
					"\r\n",
					"     # Success: Return list of json objects\r\n",
					"    def test_processstringtojson_genericfunctions_success_complexobject(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        string_object = \"[{'key': 'value', 'foo': 'bar'}, {'key2': 'value2', 'bar': 'foo'}]\"\r\n",
					"        # Define expected return values\r\n",
					"        expected_result = [{'key': 'value', 'foo': 'bar'}, {'key2': 'value2', 'bar': 'foo'}]\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = process_string_to_json(string_object=string_object)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the result matches expectations\r\n",
					"        self.assertEqual(expected_result, actual_result,  f\"[GenericFunctions:ProcessStringToJson] Expected result does not match actual: \\nExpected: {expected_result} \\nActual: {actual_result}\")  \r\n",
					"\r\n",
					"     # Failure: Invalid json object\r\n",
					"    def test_processstringtojson_genericfunctions_failure(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        string_object = \"not a json\"\r\n",
					"        # Define expected return values\r\n",
					"        expected_error = f\"The string_object that was passed cannot be converted to a json object. Passed string: {string_object}\"\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        with self.assertRaises(TypeError) as error:\r\n",
					"            process_string_to_json(string_object=string_object)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[GenericFunctions:ProcessStringToJson] Expected error message does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")   \r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_ProcessStringToJson)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in process_string_to_json() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Remove_drop_folder()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_RemoveDropFolder(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        #files to delete to have empty folders\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Return a list of non-empty folder\r\n",
					"    def test_removedropfolder_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        self.path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/remove_parent_folder/'\r\n",
					"        path_contents = mssparkutils.fs.ls(self.path)\r\n",
					"        expected_folder_names=['20240802_112048','20240802_112049','20240802_112052','test2','20240802_112053','test3']\r\n",
					"        \r\n",
					"        mssparkutils.fs.rm(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/remove_parent_folder/20240802_112047/to_delete')\r\n",
					"        mssparkutils.fs.rm(f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/remove_parent_folder/20240802_112051/test1/to_delete')\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        for folder in path_contents:\r\n",
					"            remove_dropfolder(str(folder.path))\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"        actual_folder_names=[]\r\n",
					"        actual_files,actual_folders=list_directory_content(self.path, list(), list())\r\n",
					"        for folder in actual_folders :\r\n",
					"            actual_folder_names.append(folder.name)\r\n",
					"        self.assertEqual(actual_folder_names,expected_folder_names, f\"[GenericFunctions:FilterList] 3. The lists are not equal. \\nExpected {expected_folder_names} \\nActual{actual_folder_names}\")\r\n",
					"        \r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_RemoveDropFolder)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in process_string_to_json() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## locate_dropfolder()"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_locate_DropFolder(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Success: Returns a path where the final folder is a timestamp\r\n",
					"    def test_locate_dropfolder_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path_success=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_dropfolder/Test_drop_folder_success/20240802_112047/subfolder/subsubfolder'\r\n",
					"        Expected_path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_dropfolder/Test_drop_folder_success/20240802_112047'\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        Actual_path=locate_dropfolder(path_success)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"        self.assertEqual(Actual_path,Expected_path, f\"[GenericFunctions:locate_dropfolder] 3. The lists are not equal. \\nExpected {Expected_path} \\nActual{Actual_path}\")\r\n",
					"    \r\n",
					"    # Failure: there is no timestamp folder in the path\r\n",
					"    def test_locate_dropfolder_failed(self):  \r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path_failed=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/locate_dropfolder/Test_drop_folder_failed/subfolder/subsubfolder'\r\n",
					"        Expected_error='You reached the parent folder without finding the drop folder'\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        with self.assertRaises(Exception) as error:\r\n",
					"            locate_dropfolder(path_failed)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        self.assertEqual(actual_error,Expected_error, f\"[GenericFunctions:locate_dropfolder] 3. The lists are not equal. \\nExpected {Expected_error} \\nActual{actual_error}\")\r\n",
					"\r\n",
					"    def test_locate_dropfolder_timestamp_file_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        path_success=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/list_directory_content/test/integration_tests/data/dv360/PRD/media-spending/20241025_112048/DV360_NALO_20240916_011416_1262814714_4763705405.csv'\r\n",
					"        Expected_path=f'abfss://{container_name}@{storage_account}.dfs.core.windows.net/GenericFunctions/list_directory_content/test/integration_tests/data/dv360/PRD/media-spending/20241025_112048'\r\n",
					"        \r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        Actual_path=locate_dropfolder(path_success)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the error matches expectations\r\n",
					"        self.assertEqual(Actual_path,Expected_path, f\"[GenericFunctions:locate_dropfolder] 3. The lists are not equal. \\nExpected {Expected_path} \\nActual{Actual_path}\")\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_locate_DropFolder)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in process_string_to_json() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: Test_ConvertListsToListOfDicts\r\n",
					"# This class will test the convert_lists_to_list_of_dicts() function defined in the IngestionFunction notebook\r\n",
					"class Test_ConvertListsToListOfDicts(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the convert_lists_to_list_of_dicts function\r\n",
					"\r\n",
					"    # Success: \r\n",
					"    def test_ConvertListsToListOfDicts_succes(self):\r\n",
					"        # PREPROCESS\r\n",
					"        list_of_lists = [\r\n",
					"            [\"value11\", \"value12\"], \r\n",
					"            [\"value21\", \"value22\"]\r\n",
					"        ]\r\n",
					"        list_of_keys  = [\"key1\", \"key2\"]\r\n",
					"\r\n",
					"        expected_result = [\r\n",
					"            {\r\n",
					"                \"key1\": \"value11\", \r\n",
					"                \"key2\": \"value21\"\r\n",
					"            },\r\n",
					"            {\r\n",
					"                \"key1\": \"value12\", \r\n",
					"                \"key2\": \"value22\"\r\n",
					"            }\r\n",
					"        ]\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        actual_result:list = convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[IngestionFunctions:ConvertListsToListsOfDicts] Expected result does not match actuals: \\nExpected: {expected_result} \\nActual: {actual_result}\")\r\n",
					"\r\n",
					"    def test_ConvertListsToListOfDicts_failure_raiseerror_too_many_keys(self):\r\n",
					"        # PREPROCESS\r\n",
					"        list_of_lists = [\r\n",
					"            [\"value11\", \"value12\"], \r\n",
					"            [\"value21\", \"value22\"]\r\n",
					"        ]\r\n",
					"        list_of_keys  = [\"key1\", \"key2\", \"key3\"]\r\n",
					"\r\n",
					"        expected_error = f\"Number of keys does not match with number of lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[IngestionFunctions:ConvertListsToListsOfDicts] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"    def test_ConvertListsToListOfDicts_failure_raiseerror_too_many_lists(self):\r\n",
					"        # PREPROCESS\r\n",
					"        list_of_lists = [\r\n",
					"            [\"value11\", \"value12\"], \r\n",
					"            [\"value21\", \"value22\"],\r\n",
					"            [\"value31\", \"value32\"]\r\n",
					"        ]\r\n",
					"        list_of_keys  = [\"key1\", \"key2\"]\r\n",
					"\r\n",
					"        expected_error = f\"Number of keys does not match with number of lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[IngestionFunctions:ConvertListsToListsOfDicts] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_ConvertListsToListOfDicts)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_ReturnFilteredKeys(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"\r\n",
					"    # Success: Return the list of keys that do not contain value \"test\"\r\n",
					"    def test_genericfunctions_returnfilteredkeys_success_reverse(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define function arguments\r\n",
					"        obj = {\r\n",
					"            \"column1\": None,\r\n",
					"            \"column2\": {\"test\": False},\r\n",
					"            \"column3\": {\"test\": True},\r\n",
					"            \"column4\": {\"anothertest\": False},\r\n",
					"            \"column5\": {\"test\": False, \"anothertest\": False},\r\n",
					"            \"column6\": {\"test\": True, \"anothertest\": False},\r\n",
					"        }\r\n",
					"        value = \"test\"\r\n",
					"\r\n",
					"        # Define the expected values\r\n",
					"        expected_result =  [\"column1\", \"column2\", \"column4\", \"column5\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = return_filtered_keys(obj=obj, prop=value, reverse=True)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that expectations match actuals\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:ReturnFilteredKeys] 1. Expectations do not match actuals: \\nEXPECTED: {expected_result} \\nACTUALS:{actual_result}\")\r\n",
					"\r\n",
					"    # Success: Return the list of keys that contain value \"test\"\r\n",
					"    def test_genericfunctions_returnfilteredkeys_success_noreverse(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define function arguments\r\n",
					"        obj = {\r\n",
					"            \"column1\": None,\r\n",
					"            \"column2\": {\"test\": False},\r\n",
					"            \"column3\": {\"test\": True},\r\n",
					"            \"column4\": {\"anothertest\": False},\r\n",
					"            \"column5\": {\"test\": False, \"anothertest\": True},\r\n",
					"            \"column6\": {\"test\": True, \"anothertest\": False},\r\n",
					"        }\r\n",
					"        value = \"test\"\r\n",
					"\r\n",
					"        # Define the expected values\r\n",
					"        expected_result =  [\"column3\", \"column6\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = return_filtered_keys(obj=obj, prop=value, reverse=False)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that expectations match actuals\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:ReturnFilteredKeys] 2. Expectations do not match actuals: \\nEXPECTED: {expected_result} \\nACTUALS:{actual_result}\")\r\n",
					"\r\n",
					"    # Success: Return the list of keys that contain value \"test\"\r\n",
					"    def test_genericfunctions_returnfilteredkeys_success_zeroone(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define function arguments\r\n",
					"        obj = {\r\n",
					"            \"column1\": None,\r\n",
					"            \"column2\": {\"test\": 0},\r\n",
					"            \"column3\": {\"test\": 1},\r\n",
					"            \"column4\": {\"anothertest\": 0},\r\n",
					"            \"column5\": {\"test\": 0, \"anothertest\": 1},\r\n",
					"            \"column6\": {\"test\": 1, \"anothertest\": 0},\r\n",
					"        }\r\n",
					"        value = \"test\"\r\n",
					"\r\n",
					"        # Define the expected values\r\n",
					"        expected_result =  [\"column3\", \"column6\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result = return_filtered_keys(obj=obj, prop=value, reverse=False)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that expectations match actuals\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:ReturnFilteredKeys] 2. Expectations do not match actuals: \\nEXPECTED: {expected_result} \\nACTUALS:{actual_result}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_ReturnFilteredKeys)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in process_string_to_json() tests, something went wrong!\")"
				],
				"execution_count": null
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GenericFunctions_CreateJsonObject(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    \r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    def test_genericfunctions_CreateJsonObject_sucess( self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define function arguments\r\n",
					"        string='This is a test'\r\n",
					"        value=3\r\n",
					"        date='2024-12-19'\r\n",
					"        # Define the expected values\r\n",
					"        expected_result=\"\"\"{\"string\": \"This is a test\", \"value\": 3, \"date\": \"2024-12-19\"}\"\"\"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the method under test\r\n",
					"        actual_result=create_json_object(string=string,value=value,date=date)\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that expectations match actuals\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[GenericFunctions:CreateJsonObject] Expectations do not match actuals: \\nEXPECTED: {expected_result} \\nACTUALS:{actual_result}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GenericFunctions_CreateJsonObject)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in process_string_to_json() tests, something went wrong!\")"
				],
				"execution_count": null
			}
		]
	}
}
</file>
<file name="src\synapse\studio\notebook\Test_IngestionFunctions.json">
{
	"name": "Test_IngestionFunctions",
	"properties": {
		"folder": {
			"name": "Functions/Test_Functions"
		},
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "devdapsspcore",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": true,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "b61b5575-3fbb-4b1f-9847-29724ce8e9ea"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": true,
			"kernelspec": {
				"name": "synapse_pyspark",
				"display_name": "Synapse PySpark"
			},
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/07f1b2e8-52dc-4020-967e-3eacb668d07a/resourceGroups/dev-dap-rg-core/providers/Microsoft.Synapse/workspaces/dev-dap-syn-core/bigDataPools/devdapsspcore",
				"name": "devdapsspcore",
				"type": "Spark",
				"endpoint": "https://dev-dap-syn-core.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/devdapsspcore",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net"
				},
				"sparkVersion": "3.4",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"automaticScaleJobs": true
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Functions\n",
					"**Purpose**: This Notebook will using the unittest library and pyspark.testing.utils library of Python and PySpark to execute unit tests on the Spark Notebooks in the folder *Functions*.\n",
					"\n",
					"Unit tests: To test if a function does what we expect it to do, a untit test is written. A unit test isolates a function from the overall environment and executes the function. The aim is to be able to define the expected outcome of the function and see if this aligns with the actual result. For this, it might be necessary to prepare some files, patch some functions/methods, or mock variables needed to execute the function successfully\n",
					"\n",
					"- Prepare files: Some functions will try to move or open files, or generate a list of all the files within a certain directory. To test this properly, a set of files should be uploaded to the test-container on the delta lake. This is done automatically by a PowerShell script.\n",
					"- Patch functions: Some functions are dependent on other functions or methods. These functions/methods execute certain logic and potentially return a set of results. However, this is not part of our unit test as we aim to execute our function in isolation. Because of this, patching can be used. Here, it is possible to say 'When function x is called, return this result set' or 'When function x is called, skip its execution entirely'\n",
					"- Mock variables: When defining a class-instance or when a function requires a set of parameters, it is possible to mock these variables. Simply put, this means that you do not define an explicit value for the parameters/variables, but you also do not leave them empty. The system will interpret the mocked parameter and still execute successfully. This can, for example, be used when the parameter is not directly used in the function but is passed through to another function. Or when a class-parameter is not necessary needing to be defined to execute a specific method of that class. "
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Import the necessary libraries"
				]
			},
			{
				"cell_type": "code",
				"source": [
					"import unittest\n",
					"from unittest.mock import patch, MagicMock, ANY, call\n",
					"\n",
					"import pyspark.testing.utils # Pyspark version 3.5 contains useful functions (assertDataFrameEqual, assertSchemaEqual...) but they do not exist in the current 3.3.1 pyspark version\n",
					"\n",
					"from pyspark.sql.types import StructType,StructField, StringType, IntegerType, TimestampType\n",
					"from delta.tables import DeltaTable\n",
					"\n",
					"from types import SimpleNamespace # Used to mimic JSON-object as a returned object\n",
					"\n",
					"import pandas as pd\n",
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import udf, lit, col\n",
					"import datetime"
				],
				"execution_count": 1
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"tags": [
						"parameters"
					]
				},
				"source": [
					"## Environment base is the phase the notebook is currently in (dev/acc/prod)\n",
					"environment_code = 'dev'"
				],
				"execution_count": 2
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"env_code = spark.conf.get('spark.environment_code', environment_code)\n",
					"\n",
					"# Consistency check: The TestNotebook may only be ran in the development and integration stages, NEVER in test, acceptance or production\n",
					"# According to the CI/CD, this notebook should never even be deployed there, but just in case ;)\n",
					"if env_code not in ['dev', 'int']:\n",
					"    raise ValueError(f\"TestNotebook is not allowed to run outside of dev and int environment. Run invoked for {env_code}. Canceling...\")"
				],
				"execution_count": 3
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## \"Import\" the functions that need to be tested"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Functions/IngestionFunctions"
				],
				"execution_count": 4
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/Classes"
				],
				"execution_count": 5
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"%run Modules/ErrorHandling"
				],
				"execution_count": 6
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Define Test Classes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test case for convert_lists_to_list_of_dicts -&gt; Move to GenericFunctions when possible and work out the Test case\r\n",
					"# lists = [\r\n",
					"#     [\"string\", \"integer\", \"timestamp\"],\r\n",
					"#     [\"varchar(max)\", \"varchar(4)\", \"date\"]\r\n",
					"# ]\r\n",
					"\r\n",
					"# keys = [\"column_name\", \"data_type\"]\r\n",
					"\r\n",
					"# result = convert_lists_to_list_of_dicts(lists=lists, keys=keys)\r\n",
					"# print(result)\r\n",
					"# # Expected: [{column_name: string, data_type:varchar(max)}, {column_name: integer, data_type:varchar(4)}, {column_name: timestamp, data_type:date}]"
				],
				"execution_count": 7
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: Test_ConvertListsToListOfDicts\r\n",
					"# This class will test the convert_lists_to_list_of_dicts() function defined in the IngestionFunction notebook\r\n",
					"class Test_ConvertListsToListOfDicts(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the convert_lists_to_list_of_dicts function\r\n",
					"\r\n",
					"    # Success: \r\n",
					"    def test_ConvertListsToListOfDicts_succes(self):\r\n",
					"        # PREPROCESS\r\n",
					"        list_of_lists = [\r\n",
					"            [\"value11\", \"value12\"], \r\n",
					"            [\"value21\", \"value22\"]\r\n",
					"        ]\r\n",
					"        list_of_keys  = [\"key1\", \"key2\"]\r\n",
					"\r\n",
					"        expected_result = [\r\n",
					"            {\r\n",
					"                \"key1\": \"value11\", \r\n",
					"                \"key2\": \"value21\"\r\n",
					"            },\r\n",
					"            {\r\n",
					"                \"key1\": \"value12\", \r\n",
					"                \"key2\": \"value22\"\r\n",
					"            }\r\n",
					"        ]\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        actual_result:list = convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[IngestionFunctions:ConvertListsToListsOfDicts] Expected result does not match actuals: \\nExpected: {expected_result} \\nActual: {actual_result}\")\r\n",
					"\r\n",
					"    def test_ConvertListsToListOfDicts_failure_raiseerror_too_many_keys(self):\r\n",
					"        # PREPROCESS\r\n",
					"        list_of_lists = [\r\n",
					"            [\"value11\", \"value12\"], \r\n",
					"            [\"value21\", \"value22\"]\r\n",
					"        ]\r\n",
					"        list_of_keys  = [\"key1\", \"key2\", \"key3\"]\r\n",
					"\r\n",
					"        expected_error = f\"Number of keys does not match with number of lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[IngestionFunctions:ConvertListsToListsOfDicts] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"    def test_ConvertListsToListOfDicts_failure_raiseerror_too_many_lists(self):\r\n",
					"        # PREPROCESS\r\n",
					"        list_of_lists = [\r\n",
					"            [\"value11\", \"value12\"], \r\n",
					"            [\"value21\", \"value22\"],\r\n",
					"            [\"value31\", \"value32\"]\r\n",
					"        ]\r\n",
					"        list_of_keys  = [\"key1\", \"key2\"]\r\n",
					"\r\n",
					"        expected_error = f\"Number of keys does not match with number of lists: \\n{list_of_keys}\\n{list_of_lists}\"\r\n",
					"       \r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            convert_lists_to_list_of_dicts(list_of_lists, list_of_keys)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[IngestionFunctions:ConvertListsToListsOfDicts] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_ConvertListsToListOfDicts)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 8
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: Test_ReplaceListItemsByKeyValues\r\n",
					"# This class will test the replace_list_items_by_key_values() function defined in the IngestionFunction notebook\r\n",
					"class Test_ReplaceListItemsByKeyValues(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    def setUp(self):\r\n",
					"        # Define parameters and arguments to be used in test-functions\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher functions\r\n",
					"        return\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the replace_list_items_by_key_values function\r\n",
					"\r\n",
					"    # Success: \r\n",
					"    def test_replacelistitemsbykeyvalues_success(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments to pass to the function create_partitioning_list()\r\n",
					"        list_items = [\"foo\", \"bar\"]\r\n",
					"        key_value_pairs = {\"test\": \"TEST\", \"foo\":\"FOO\", \"bar\": \"BAR\"}\r\n",
					"        expected_result = [\"FOO\", \"BAR\"]\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the create_partitioning_list() and define the actual returned values\r\n",
					"        actual_result = replace_listitems_by_keyvalues(list_items, key_value_pairs)\r\n",
					"        \r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the actuals match the expectations\r\n",
					"        self.assertEqual(expected_result, actual_result, f\"[IngestionFunctions:ReplaceListItemsByKeyValues] Expected result does not match actual: \\nExpected: {expected_result} \\nActual: {actual_result}\")\r\n",
					"\r\n",
					"        # Success: \r\n",
					"    def test_replacelistitemsbykeyvalues_failure_listitem_notin_keyvaluepairs(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments to pass to the function create_partitioning_list()\r\n",
					"        list_items = [\"foo\", \"string\", \"bar\"]\r\n",
					"        key_value_pairs = {\"test\": \"TEST\", \"foo\":\"FOO\", \"bar\": \"BAR\"}\r\n",
					"        expected_error = \"Key string cannot be replaced by a value. Process is stopped for safety reasons...\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the convert_lists_to_list_of_dicts() and define the actual returned values\r\n",
					"        with self.assertRaises(ValueError) as error:\r\n",
					"            replace_listitems_by_keyvalues(list_items, key_value_pairs)\r\n",
					"        actual_error = str(error.exception)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        self.assertEqual(expected_error, actual_error, f\"[IngestionFunctions:ReplaceListItemsByKeyValues] Expected error does not match actual: \\nExpected: {expected_error} \\nActual: {actual_error}\")\r\n",
					"\r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_ReplaceListItemsByKeyValues)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 9
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: Test_CreatePartitioningList\n",
					"# This class will test the create_partioning_list() function defined in the IngestionFunction notebook\n",
					"class Test_CreatePartitioningList(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"        # Define parameters and arguments to be used in test-functions\n",
					"        self.dataframe = spark.createDataFrame(data=[\n",
					"                ('2024-07-17', 'test'),\n",
					"                ('2023-04-03', 'function'),\n",
					"                ('2024-02-28', 'call')\n",
					"            ],\n",
					"            schema = ['date', 'string']\n",
					"        )\n",
					"\n",
					"        # Mock function cast_partitioning_timestamp() of notebook IngestionFunction\n",
					"        self.mock_cast_partitioning_timestamp_patcher = patch('__main__.cast_partitioning_timestamp')\n",
					"        self.mock_cast_partitioning_timestamp = self.mock_cast_partitioning_timestamp_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        self.mock_cast_partitioning_timestamp_patcher.stop()\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the create_partioning_list function\n",
					"\n",
					"    # Success: \n",
					"    def test_CreatePartitioningList_succes_notimestamp(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the function create_partitioning_list()\n",
					"        partitioning_objects = [\n",
					"            {\"name\": \"string\", \"sequence\": 1}\n",
					"        ]\n",
					"\n",
					"        # Define the expected returned values\n",
					"        ### The expected partition_list returned by the function\n",
					"        expected_partition_list = ['string']\n",
					"        ### The expected dataframe returned by the function\n",
					"        expected_dataframe = self.dataframe\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the create_partitioning_list() and define the actual returned values\n",
					"        actual_df, actual_partition_list = create_partioning_list(self.dataframe, partitioning_objects)\n",
					"        \n",
					"        # EVALUATE\n",
					"        # Validate that the actuals match the expectations\n",
					"        self.assertEqual(expected_partition_list, actual_partition_list, f\"Returned partitioning list does not match with expectations: {actual_partition_list} versus {expected_partition_list}\")\n",
					"        self.assertEqual(expected_dataframe, actual_df, f\"Returned dataframe does not match with expectations: {actual_df.show()} versus {expected_dataframe.show}\")\n",
					"        # Validate that the cast_partitioning_timestamp was not called\n",
					"        self.mock_cast_partitioning_timestamp.assert_not_called()\n",
					"\n",
					"    # Success\n",
					"    def test_CreatePartitioningList_succes_withtimestamp(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the function create_partitioning_list()\n",
					"        partitioning_objects = [\n",
					"            {\"name\": \"date\", \"sequence\": 1, \"datePart\": \"year\"},\n",
					"            {\"name\": \"date\", \"sequence\": 1, \"datePart\": \"day\"}\n",
					"        ]\n",
					"        \n",
					"        # Define the expected returned values\n",
					"        ### The expected partition_list returned by the function\n",
					"        expected_partition_list = ['p_year', 'p_day']\n",
					"        ### The expected dataframe column names returned by the function\n",
					"        expected_dataframe_columns = ['date', 'string', 'p_year', 'p_day']\n",
					"        ### The expected function calls done to the mocked cast_partitioning_timestamp function\n",
					"        expected_function_calls = [call(ANY, 'date'), call(ANY, 'date')]\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the create_partitioning_list() and expect an error to be thrown\n",
					"        actual_df, actual_partition_list = create_partioning_list(self.dataframe, partitioning_objects)\n",
					"        actual_dataframe_columns = actual_df.columns\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the actual returned values match expectations\n",
					"        self.assertEqual(expected_partition_list, actual_partition_list, f\"Returned partitioning list does not match with expectations: {actual_partition_list} versus {expected_partition_list}\")\n",
					"        self.assertEqual(expected_dataframe_columns, actual_dataframe_columns, f\"Returned dataframe columns do not match with expectations: {actual_dataframe_columns} versus {expected_dataframe_columns}\")\n",
					"        self.mock_cast_partitioning_timestamp.assert_has_calls(expected_function_calls, any_order=True)\n",
					"\n",
					"\n",
					"    # Failure: Invalid value for the datePart value should throw an error\n",
					"    def test_CreatePartitioningList_fail_withtimestamp(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the function create_partitioning_list()\n",
					"        invalid_datepart = 'invalid'\n",
					"        partitioning_objects = [\n",
					"            {\"name\": \"date\", \"sequence\": 1, \"datePart\": \"year\"},\n",
					"            {\"name\": \"date\", \"sequence\": 1, \"datePart\": invalid_datepart}\n",
					"        ]\n",
					"\n",
					"        # Expect the following error to be thrown\n",
					"        expected_error = f'Invalid key value for datePart: {invalid_datepart}. Only values year, month, and day are allowed.'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the create_partitioning_list() and expect an error to be thrown\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            create_partioning_list(self.dataframe, partitioning_objects)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the thrown error matches expectations\n",
					"        self.assertEqual(expected_error, actual_error, f\"Returned error by create_partioning_list() does not match with expectations: {actual_error} versus {expected_error}\")\n",
					"        \n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_CreatePartitioningList)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CreatePartitioningTimestamp\n",
					"# This class will test the cast_partitioning_timestamp() function defined in the IngestionFunctions notebook\n",
					"class Test_CreatePartitioningTimestamp(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"        # Define parameters and arguments to be used in test-functions\n",
					"        self.dataframe = spark.createDataFrame(data=[\n",
					"                ('2024-07-17', 'test'),\n",
					"                ('2023-04-03', 'function'),\n",
					"                ('2024-02-28', 'call')\n",
					"            ],\n",
					"            schema = ['date', 'string']\n",
					"        )\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the cast_partitioning_timestamp function\n",
					"\n",
					"    # Success: The date-column is actually a timestamp and no errors are expected\n",
					"    def test_CreatePartitioningTimestamp_succes(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the function cast_partitioning_timestamp()\n",
					"        partitioning_key = 'date'\n",
					"        # Define the expected return values\n",
					"        expected_date = datetime.datetime(2024, 7, 17, 0, 0)\n",
					"        expected_mismatch = 0\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the cast_partitioning_timestamp() function\n",
					"        returned_df = cast_partitioning_timestamp(self.dataframe, partitioning_key)\n",
					"        \n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the retunrned\n",
					"        actual_date = returned_df.collect()[0][0]\n",
					"        \n",
					"        self.assertEqual(expected_date, actual_date, f\"Dates are not equal: {expected_date} versus {actual_date}\")\n",
					"\n",
					"        # Validate that the casted_date and date values have the same result\n",
					"        # Dev-Note: This is a preventive check as it catches potential logic-changes to the casted_values method of the Checks-class\n",
					"        #   Not having the reverse test (check if casted is NULL and original not) could cause for logic issues in the future\n",
					"        actual_mismatch = returned_df.filter(\n",
					"                col('casted_date').isNull() &amp;\n",
					"                col('date').isNotNull()\n",
					"        ).count()\n",
					"\n",
					"        # Validate that there is no mismatch and throw an error otherwise\n",
					"        self.assertEqual(expected_mismatch, actual_mismatch, \n",
					"            \"The NULL values in the casted columns do not match the NULL values in the original columns. A conversion issue occured in the Checks function casted_values\")\n",
					"\n",
					"    # Failure: The date-column is not a timestamp and a conversion error is expected\n",
					"    def test_CreatePartitioningTimestamp_fail_on_conversion(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the udf-function cast_partitioning_timestamp()\n",
					"        partitioning_key = 'string'\n",
					"        # Define the expected error to be returned\n",
					"        expected_error = f'Casting error: NULL values in column {partitioning_key} after casting: Could not convert {self.dataframe.count()} columns'\n",
					"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the cast_partitioning_timestamp() function and expect an error to be thrown\n",
					"        with self.assertRaises(AssertionError) as error:\n",
					"            cast_partitioning_timestamp(self.dataframe, partitioning_key)\n",
					"        actual_error = str(error.exception)\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the thrown error matches expectations\n",
					"        self.assertEqual(expected_error, actual_error, f\"Returned error does not match with expectations: {actual_error} versus {expected_error}\")\n",
					"        \n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_CreatePartitioningTimestamp)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: SkipFirstLines\n",
					"# This class will test the skip_first_lines() function defined in the IngestionFunctions notebook\n",
					"class Test_Function_SkipFirstLines(unittest.TestCase):\n",
					"\n",
					"    def setUp(self):\n",
					"        self.skip_lines = 4\n",
					"        self.separator = ';'\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        return\n",
					"\n",
					"    def test_skipFirstLines_headerless_return(self):\n",
					"        landing_path_headerless = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/IngestionFunctions/skip_first_lines_headerless.csv'\n",
					"        header = False\n",
					"\n",
					"        expected_count = 3\n",
					"        expected_headers = ['_c0', '_c1', '_c2']\n",
					"\n",
					"        df = skip_first_lines(landing_path_headerless, self.separator, header, self.skip_lines)\n",
					"        \n",
					"        actual_count = df.count()\n",
					"        actual_headers = df.columns\n",
					"\n",
					"        self.assertEqual(expected_headers, actual_headers, f\"Headerless: The returned header-line {actual_headers} does not match expectations {expected_headers}\")\n",
					"        self.assertEqual(expected_count, actual_count, f\"Headerless: The number of rows in the df {actual_count} does not match expectations {expected_count}\")\n",
					"\n",
					"    def test_skipFirstLines_header_return(self):\n",
					"        landing_path_headerless = f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/IngestionFunctions/skip_first_lines_headers.csv'\n",
					"        header = True\n",
					"\n",
					"        expected_count = 3\n",
					"        expected_headers = ['Column_1', 'Column_2', 'Column_3']\n",
					"\n",
					"        df = skip_first_lines(landing_path_headerless, self.separator, header, self.skip_lines)\n",
					"        \n",
					"        actual_count = df.count()\n",
					"        actual_headers = df.columns\n",
					"\n",
					"        self.assertEqual(expected_headers, actual_headers, f\"Headers: The returned header-line {actual_headers} does not match expectations {expected_headers}\")\n",
					"        self.assertEqual(expected_count, actual_count, f\"Headers: The number of rows in the df {actual_count} does not match expectations {expected_count}\")\n",
					"\n",
					"\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_SkipFirstLines)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: CreateDeltaTable_v3\n",
					"# This class will test the try_cast_dataframe() function of the Checks-class defined in the Classes notebook\n",
					"class Test_EnforceVarcharConstraint(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"        # Define parameters and arguments to be used in test-functions\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher functions\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # TESTS:\n",
					"    # Define a set of functions to test successful and unsuccesful calls of the try_cast_dataframe function\n",
					"\n",
					"    # Failure: If the __CHAR_VARCHAR_TYPE_STRING key does not contain a varchar-value, thrown a ValueError\n",
					"    def test_enforcevarcharconstraint_empty_varchar(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the udf-function enforce_varchar_constraint()\n",
					"        metadata = {'__CHAR_VARCHAR_TYPE_STRING': ''}\n",
					"        expected_error = f\"Metadata does not contain correct varchar-definition: {metadata}. Expected something like {{'__CHAR_VARCHAR_TYPE_STRING': 'varchar(xx)'}}\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the enforce_varchar_constraint() udf and expect an error to be thrown\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            enforce_varchar_constraint(metadata)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the thrown error matches expectations\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"Expected error '{expected_error}' was not thrown. Got '{actual_error}' instead\")\n",
					"\n",
					"    def test_enforcevarcharconstraint_invalid_metadata(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the udf-function enforce_varchar_constraint()\n",
					"        metadata = {'column_description': 'this is a set'}\n",
					"        expected_error = f\"Metadata does not contain correct varchar-definition: {metadata}. Expected something like {{'__CHAR_VARCHAR_TYPE_STRING': 'varchar(xx)'}}\"\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the enforce_varchar_constraint() udf and expect an error to be thrown\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            enforce_varchar_constraint(metadata)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that the thrown error matches expectations\n",
					"        actual_error = str(error.exception)\n",
					"        self.assertEqual(expected_error, actual_error, f\"Expected error '{expected_error}' was not thrown. Got '{actual_error}' instead\")\n",
					"\n",
					"\n",
					"    def test_max_length_invalid(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the udf-function enforce_varchar_constraint()\n",
					"        metadata = {'__CHAR_VARCHAR_TYPE_STRING': 'varchar(10)'}\n",
					"        df = spark.createDataFrame([(\"this is a very long string that exceeds the max length\",)], [\"col\"])\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the enforce_varchar_constraint() udf on the metadata\n",
					"        varchar_udf = enforce_varchar_constraint(metadata)\n",
					"        # Enforce the udf on the dataframe\n",
					"        result_df = df.withColumn(\"col\", varchar_udf(df[\"col\"]))\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_result = result_df.collect()[0][0]\n",
					"        # Validate that the too long string has been set to None\n",
					"        self.assertIsNone(actual_result)\n",
					"\n",
					"\n",
					"    def test_max_length_valid(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the udf-function enforce_varchar_constraint()\n",
					"        metadata = {'__CHAR_VARCHAR_TYPE_STRING': 'varchar(10)'}\n",
					"        expected_result = 'short'\n",
					"        df = spark.createDataFrame([('short',)], [\"col\"])\n",
					"\n",
					"        # EXECUTE\n",
					"        # Run the enforce_varchar_constraint() udf on the metadata\n",
					"        varchar_udf = enforce_varchar_constraint(metadata)\n",
					"        result_df = df.withColumn(\"col\", varchar_udf(df[\"col\"]))\n",
					"\n",
					"        # EVALUATE\n",
					"        actual_result = result_df.collect()[0][0]\n",
					"        # Validate that the too long string has been set to None\n",
					"        self.assertEqual(actual_result, 'short')\n",
					"\n",
					"    def test_udf_type(self):\n",
					"        # PREPROCESS\n",
					"        # Define the arguments to pass to the udf-function enforce_varchar_constraint()\n",
					"        metadata = {'__CHAR_VARCHAR_TYPE_STRING': 'varchar(10)'}\n",
					"        \n",
					"        # EXECUTE\n",
					"        # Run the enforce_varchar_constraint() udf on the metadata\n",
					"        udf_func = enforce_varchar_constraint(metadata)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Validate that we are dealing with an object-type variable\n",
					"        self.assertIsInstance(udf_func, object)\n",
					"        # Validate the return-type of the udf\n",
					"        self.assertEqual(udf_func.returnType, StringType())\n",
					"       \n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_EnforceVarcharConstraint)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 13
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: List_Directory_Content()\n",
					"Function Purpose: Given a directory-path, return a recursive list of all the folders and files that are contained in the path.\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Function_List_Directory_Content(unittest.TestCase):\n",
					"\n",
					"    def test1_list_directory_content(self):\n",
					"        # Test_1: Analyze lowest level directory\n",
					"        #   The directory unittest/env/timestamp/ is expected to contain 1 file (\"test1_timestamp.txt\") and no folders\n",
					"\n",
					"        # Define variables\n",
					"        path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"unittest/env/timestamp/\")\n",
					"\n",
					"        # Execute function\n",
					"        test1_files, test1_folders = list_directory_content(path, list(), list())\n",
					"\n",
					"        # Unit tests\n",
					"        # Test if the file name that we expect is actually returned\n",
					"        expected_file = [\"test1_timestamp.txt\"]\n",
					"        files = [file.name for file in test1_files]\n",
					"        self.assertEqual(files, expected_file)\n",
					"\n",
					"        # Test if the folder name that we expect is actually returned\n",
					"        expected_folder = []\n",
					"        folder = [folder.name for folder in test1_folders]\n",
					"        self.assertEqual (folder, expected_folder)\n",
					"\n",
					"    def test2_list_directory_content(self):\n",
					"        # Test_2: Analyze hihgest level directory\n",
					"        #   The directory unittest/ is expected to contain 3 files (\"test1_unittest.txt\", \"test1_env.txt\", \"test1_timestamp.txt\") and 2 folders (env and timestamp)\n",
					"\n",
					"        # Define variables\n",
					"        path = \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"unittest\")\n",
					"\n",
					"        # Execute function\n",
					"        test2_files, test2_folders = list_directory_content(path, list(), list())\n",
					"\n",
					"        # Unit tests:\n",
					"        # Test if the number of files we expect is actually the same as what is returned\n",
					"        expected_files = [\"test1_unittest.txt\", \"test1_env.txt\", \"test1_timestamp.txt\"]\n",
					"        files = [file.name for file in test2_files]\n",
					"        self.assertCountEqual(files, expected_files)\n",
					"\n",
					"        # Test if the number of folders we expect is actually the same as what is returned\n",
					"        expected_folders = [\"env\", \"timestamp\"]\n",
					"        folders = [folder.name for folder in test2_folders]\n",
					"        self.assertCountEqual (folders, expected_folders)\n",
					"\n",
					"        # TO DO: Compare the lists -&gt; Not possible with the library (assertItemsEqual has been expired from Python v3)"
				],
				"execution_count": 14
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: filter_directory_content\n",
					"Function Purpose: Get items based on a certain path, and then filter on a certain regex match / file pattern"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_Filter_Directory_Content(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        self.mock_list_directory_content_patcher = patch('__main__.list_directory_content')\n",
					"        self.mock_list_directory_content = self.mock_list_directory_content_patcher.start()\n",
					"\n",
					"    def tearDown(self):\n",
					"        self.mock_list_directory_content_patcher.stop()\n",
					"\n",
					"    def test_filter_directory_content_pass(self):\n",
					"        # Test: See if 'mock_item' is filtered out of the list and that only the right items remain (i.e. only those who have 'list' inside of them)\n",
					"        self.mock_list_directory_content.return_value = [\n",
					"            SimpleNamespace(name=\"list_item1\"),\n",
					"            SimpleNamespace(name=\"list_item2\"),\n",
					"            SimpleNamespace(name=\"mock_item\")], []\n",
					"\n",
					"        # This loop is just for the sake of the unittest, since you need a .name attribute\n",
					"        # You can't really set a JSON-object that easily as a return value inside of a unit test\n",
					"        filtered_list = [folder.name for folder in filter_directory_content(\"mock_path/child_path\", \"list\")]\n",
					"\n",
					"        self.assertEqual([\"list_item1\", \"list_item2\"], filtered_list)\n",
					"\n",
					"    def test_filter_directory_content_pass2(self):\n",
					"        # Test: See if everything gets returned because '*' wildcard operator is used\n",
					"        self.mock_list_directory_content.return_value = [\n",
					"            SimpleNamespace(name=\"list_item1\"),\n",
					"            SimpleNamespace(name=\"list_item2\"),\n",
					"            SimpleNamespace(name=\"mock_item\")], []\n",
					"\n",
					"        # This loop is just for the sake of the unittest, since you need a .name attribute\n",
					"        # You can't really set a JSON-object that easily as a return value inside of a unit test\n",
					"        filtered_list = [folder.name for folder in filter_directory_content(\"mock_path/child_path\", \"*\")]\n",
					"\n",
					"        self.assertEqual([\"list_item1\", \"list_item2\", \"mock_item\"], filtered_list)\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Filter_Directory_Content)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 15
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: removeRowsDF\n",
					"Function Purpose: Remove N-rows from a PySpark dataframe"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_removeRowsDF(unittest.TestCase):\n",
					"    def setUp(self):\n",
					"        self.df_path = f\"abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/IngestionFunctions/remove_rows_dataframe.csv\"\n",
					"\n",
					"    def test_removeRowsDF_pass(self):\n",
					"        # Test: See if you are able to remove rows from a dataframe\n",
					"        df = spark.read.load(self.df_path, header=\"True\", sep=\",\", format=\"csv\")\n",
					"\n",
					"        # Save the necessary rows to check them later\n",
					"        row_three = list(df.collect()[2])\n",
					"        row_four = list(df.collect()[3])\n",
					"\n",
					"        df_new = removeRowsDF(df, [0, 1])\n",
					"        row_one_new = list(df_new.collect()[0])\n",
					"        row_two_new = list(df_new.collect()[1])\n",
					"\n",
					"        # Row three of the original dataframe should now be the first row in the new dataframe\n",
					"        self.assertEqual(row_three, row_one_new)\n",
					"\n",
					"        # Row four of the original dataframe should now be the second row in the new dataframe\n",
					"        self.assertEqual(row_four, row_two_new)\n",
					"\n",
					"        # In the new dataframe, there should only be 8 rows right now\n",
					"        self.assertEqual(8, df_new.rdd.count())\n",
					"\n",
					"    def test_removeRowsDF_outofbounds_error(self):\n",
					"        # Test: See if an error is thrown when your index &gt; the last index in the dataframe\n",
					"        df = spark.read.load(self.df_path, header=\"True\", sep=\",\", format=\"csv\")\n",
					"        \n",
					"        with self.assertRaisesRegex(Exception, \"^(removeRowsDF).*(9).*(10).*(OutOfBounds)$\"):\n",
					"            df_new = removeRowsDF(df, [0, 10])\n",
					"\n",
					"    def test_removeRowsDF_noint_error(self):\n",
					"        # Test: See if an error is thrown when an item in your list isn't an int\n",
					"        df = spark.read.load(self.df_path, header=\"True\", sep=\",\", format=\"csv\")\n",
					"        \n",
					"        with self.assertRaisesRegex(Exception, \"^(removeRowsDF).*('test' inside rows_to_remove).*(no).*(int).*([oO]nly).*(int).*(allowed)$\"):\n",
					"            df_new = removeRowsDF(df, [0, \"test\"])\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_removeRowsDF)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 16
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: PK_Match()\n",
					"Function Purpose: Given the primary column names of the delta table and raw data, define a statement that matches the names.\n",
					"\n",
					"    Example: delta_table.primary_column_name = raw_data.primary_column_name AND delta_table.primary_column_name_2 = raw_data.primary_column_name_2"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# Test Class: IngestionFunctions\n",
					"# This class will test the pk_match function defined in the IngestionFunctions notebook\n",
					"class Test_Function_PK_Match(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def setUp(self):\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher-functions \n",
					"        return\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    def test1_pk_match(self):\n",
					"        # Test_1: Test result for one primary column\n",
					"        #   Given delta table primary column 'id' and raw data primary column 'ID', return statement 'testAliasOld.id = testAliasNew.ID'\n",
					"\n",
					"        # Variables\n",
					"        old_alias = 'testAliasOld'\n",
					"        new_alias = 'testAliasNew'\n",
					"        pk_columns = dict({ 'id': 'ID' })\n",
					"        \n",
					"        # Execute function\n",
					"        actual_statement = pk_match(old_alias, new_alias, pk_columns)\n",
					"\n",
					"        # Define expected return value\n",
					"        expected_statement = 'testAliasOld.id = testAliasNew.`ID`'\n",
					"\n",
					"        # Compare expected with actual\n",
					"        self.assertEqual(expected_statement, actual_statement)\n",
					"\n",
					"    def test2_pk_match(self):\n",
					"        # Test_2: Test result for two primary columns\n",
					"        #   Given delta table primary column 'id' and raw data primary column 'ID', return statement 'testAliasOld.id = testAliasNew.ID AND testAliasOld.date = testAliasNew.DATE'\n",
					"        \n",
					"        # Variables\n",
					"        old_alias = 'testAliasOld'\n",
					"        new_alias = 'testAliasNew'\n",
					"        pk_columns = dict({ 'id': 'ID', 'date': 'DATE' })\n",
					"        \n",
					"        # Execute function\n",
					"        actual_statement = pk_match(old_alias, new_alias, pk_columns)\n",
					"\n",
					"        # Define expected return value\n",
					"        expected_statement = 'testAliasOld.id = testAliasNew.`ID` AND testAliasOld.date = testAliasNew.`DATE`'\n",
					"\n",
					"        # Compare expected with actual\n",
					"        self.assertEqual(expected_statement, actual_statement)\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_PK_Match)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 17
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"## Test: Fill_Extraction_Date\r\n",
					"Function Purpose: Determine the extraction date of a source file based on a configuration option in target_options"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Test Class: IngestionFunctions\r\n",
					"# This class will test the Fill_Extraction_Date function defined in the IngestionFunctions notebook\r\n",
					"class Test_Function_FillExtractionDate(unittest.TestCase):\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # SET-UP: \r\n",
					"    # Define the setUp and tearDown functions of the class\r\n",
					"    # Used to patch objects that will be used over several function-calls\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"    def setUp(self):\r\n",
					"        # Initialize a reusable dataframe\r\n",
					"        self.dataframe = spark.createDataFrame(\r\n",
					"            data=[\r\n",
					"                ('20241025_150230', '20241026', '20241032_150230', '20241026', '25102024_150230', '26102024', None, None, None),\r\n",
					"                ('20241025_150230', '20241026', '20241032_150230', 'invalid',  '25102024_150230', '26102024', None, None, None),\r\n",
					"                ('20241025_150230', '20241026', '20241032_150230', '20241026', '25102024_150230', '26102024', None, None, None)\r\n",
					"            ],\r\n",
					"            schema='valid_timestamp string, valid_date string, invalid_timestamp string, invalid_date string, european_timestamp string, european_date string, t_extract_date timestamp, t_update_date timestamp, t_insert_date timestamp '\r\n",
					"        )\r\n",
					"        return\r\n",
					"\r\n",
					"    def tearDown(self):\r\n",
					"        # Stop all the patcher-functions \r\n",
					"        return\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"\r\n",
					"\r\n",
					"    # ------------------------------------------------------------------------------------------------------------------\r\n",
					"    # TESTS:\r\n",
					"    # Define a set of functions to test successful and unsuccesful calls to the fill_extraction_date() function\r\n",
					"\r\n",
					"    # Success: Return the dataframe with the t_extract_date being the current time\r\n",
					"    def test_fillextractiondate_ingestionfunctions_success_notargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {}\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Get a list of all values in column t_extract_date\r\n",
					"        extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatMap(lambda x: x).collect()\r\n",
					"        # Validate that the values in t_extract_date are not None and all values are of datatype 'datetime'\r\n",
					"        self.assertIsNotNone(extract_date)\r\n",
					"        self.assertTrue(all(item is not None for item in extract_date),                     f\"[IngestionFunctions:FillExtractionDate] 1. Expected extract date to not be empty: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(isinstance(item, datetime.datetime) for item in extract_date),  f\"[IngestionFunctions:FillExtractionDate] 1. Expected extract date to contain only datetime objects: \\nACTUAL: {extract_date}\")\r\n",
					"\r\n",
					"\r\n",
					"    # Success: Return the dataframe with the t_extract_date being a specific datetime\r\n",
					"    def test_fillextractiondate_ingestionfunctions_success_timestamptargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"valid_timestamp\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n",
					"                \"extract_date_format\": \"yyyyMMdd_HHmmss\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        # Define the expected datetime value for the values in t_extract_date\r\n",
					"        expected_datetime = datetime.datetime(2024, 10, 25, 15, 2, 30)\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Get a list of all values in column t_extract_date\r\n",
					"        extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatMap(lambda x: x).collect()\r\n",
					"        # Validate that the values in t_extract_date are not None and all values are of datatype 'datetime' and value '2024:10:25 15:02:30'\r\n",
					"        self.assertIsNotNone(extract_date)\r\n",
					"        self.assertTrue(all(item is not None for item in extract_date),                                 f\"[IngestionFunctions:FillExtractionDate] 2. Expected extract date to not be empty: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(isinstance(item, datetime.datetime) for item in extract_date),              f\"[IngestionFunctions:FillExtractionDate] 2. Expected extract date to only contain datetime datatpes: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(actual_datetime == expected_datetime for actual_datetime in extract_date),  f\"[IngestionFunctions:FillExtractionDate] 2. Expected extract date to contain timestamp 2024:10:25 15:02:30: \\nACTUAL: {extract_date}\")\r\n",
					"\r\n",
					"\r\n",
					"    # Success: Return the dataframe with the t_extract_date being a specific date extracted from a datetime\r\n",
					"    def test_fillextractiondate_ingestionfunctions_success_timestamptargetoptions_capturinggroup(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"valid_timestamp\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8})_\\\\d{6}\",\r\n",
					"                \"extract_date_format\": \"yyyyMMdd\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"        expected_datetime = datetime.datetime(2024, 10, 25, 0, 0)\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Get a list of all values in column t_extract_date\r\n",
					"        extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatMap(lambda x: x).collect()\r\n",
					"        # Validate that the values in t_extract_date are not None and all values are of datatype 'datetime' and value '2024:10:25 00:00'\r\n",
					"        \r\n",
					"        self.assertIsNotNone(extract_date)\r\n",
					"        self.assertTrue(all(item is not None for item in extract_date),                                 f\"[IngestionFunctions:FillExtractionDate] 3. Expected extract date to not be empty: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(isinstance(item, datetime.datetime) for item in extract_date),              f\"[IngestionFunctions:FillExtractionDate] 3. Expected extract date to only contain datetime datatpes: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(actual_datetime == expected_datetime for actual_datetime in extract_date),  f\"[IngestionFunctions:FillExtractionDate] 3. Expected extract date to contain timestamp 2024:10:25 00:00:00: \\nACTUAL: {extract_date}\")\r\n",
					"  \r\n",
					"        \r\n",
					"\r\n",
					"    # Success: Return the dataframe with the t_extract_date being a specific date\r\n",
					"    def test_fillextractiondate_ingestionfunctions_success_datetargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"valid_date\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8})\",\r\n",
					"                \"extract_date_format\": \"yyyyMMdd\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        expected_datetime = datetime.datetime(2024, 10, 26, 0, 0)\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE        \r\n",
					"        # Get a list of all values in column t_extract_date\r\n",
					"        extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatMap(lambda x: x).collect()\r\n",
					"        # Validate that the values in t_extract_date are not None and all values are of datatype 'datetime' and value '2024:10:26 00:00'\r\n",
					"        self.assertIsNotNone(extract_date)\r\n",
					"        self.assertTrue(all(item is not None for item in extract_date),                                 f\"[IngestionFunctions:FillExtractionDate] 4. Expected extract date to not be empty: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(isinstance(item, datetime.datetime) for item in extract_date),              f\"[IngestionFunctions:FillExtractionDate] 4. Expected extract date to only contain datetime datatpes: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(actual_datetime == expected_datetime for actual_datetime in extract_date),  f\"[IngestionFunctions:FillExtractionDate] 4. Expected extract date to contain timestamp 2024:10:26 00:00:00: \\nACTUAL: {extract_date}\")\r\n",
					"  \r\n",
					"\r\n",
					"    # Success: Return the dataframe with the t_extract_date being converted to a european timestamp\r\n",
					"    def test_fillextractiondate_ingestionfunctions_success_europeantimestamptargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"european_timestamp\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n",
					"                \"extract_date_format\": \"ddMMyyyy_HHmmss\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        expected_datetime = datetime.datetime(2024, 10, 25, 15, 2, 30)\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Get a list of all values in column t_extract_date\r\n",
					"        extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatMap(lambda x: x).collect()\r\n",
					"        # Validate that the values in t_extract_date are not None and all values are of datatype 'datetime' and value '2024:10:26 15:02:30'\r\n",
					"        \r\n",
					"        self.assertIsNotNone(extract_date)  # Ensure the result is not None\r\n",
					"        self.assertTrue(all(item is not None for item in extract_date),                                 f\"[IngestionFunctions:FillExtractionDate] 5. Expected extract date to not be empty: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(isinstance(item, datetime.datetime) for item in extract_date),              f\"[IngestionFunctions:FillExtractionDate] 5. Expected extract date to only contain datetime datatpes: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(actual_datetime == expected_datetime for actual_datetime in extract_date),  f\"[IngestionFunctions:FillExtractionDate] 5. Expected extract date to contain timestamp 2024:10:26 15:02:30: \\nACTUAL: {extract_date}\")\r\n",
					"      \r\n",
					"      \r\n",
					"    # Success: Return the dataframe with the t_extract_date being a european date\r\n",
					"    def test_fillextractiondate_ingestionfunctions_success_europeandatetargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"european_date\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8})\",\r\n",
					"                \"extract_date_format\": \"ddMMyyyy\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        expected_datetime = datetime.datetime(2024, 10, 26, 0, 0)\r\n",
					"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test\r\n",
					"        actual_dataframe = fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Get a list of all values in column t_extract_date\r\n",
					"        extract_date:list = actual_dataframe.select('t_extract_date').rdd.flatMap(lambda x: x).collect()\r\n",
					"        # Validate that the values in t_extract_date are not None and all values are of datatype 'datetime' and value '2024:10:26 00:00'\r\n",
					"        \r\n",
					"        self.assertIsNotNone(extract_date)  # Ensure the result is not None\r\n",
					"        self.assertTrue(all(item is not None for item in extract_date),                                 f\"[IngestionFunctions:FillExtractionDate] 6. Expected extract date to not be empty: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(isinstance(item, datetime.datetime) for item in extract_date),              f\"[IngestionFunctions:FillExtractionDate] 6. Expected extract date to only contain datetime datatpes: \\nACTUAL: {extract_date}\")\r\n",
					"        self.assertTrue(all(actual_datetime == expected_datetime for actual_datetime in extract_date),  f\"[IngestionFunctions:FillExtractionDate] 6. Expected extract date to contain timestamp 2024:10:26 00:00:00: \\nACTUAL: {extract_date}\")\r\n",
					"  \r\n",
					"\r\n",
					"    # Failure: When column contains a date-like value that cannot be converted to a datetime (eg. 31 February xxxx), throw an error\r\n",
					"    def test_fillextractiondate_ingestionfunctions_failure_invalidtimestamptargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"invalid_timestamp\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n",
					"                \"extract_date_format\": \"yyyyMMdd_HHmmss\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        expected_message = \"Extract date could not be set using extract_column invalid_timestamp\"\r\n",
					"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test and expect ConfigurationError\r\n",
					"        with self.assertRaises(ConfigurationError) as error:\r\n",
					"            fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expectations match the actuals\r\n",
					"        # Convert the string-error to a json object and get the value for \"custom_message\"\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        json_string = actual_error.replace(\"'\", '\"')\r\n",
					"        actual_message = json.loads(json_string)[\"custom_message\"]\r\n",
					"        self.assertEqual(actual_message, expected_message, f\"[IngestionFunctions:FillExtractionDate] 7. Expected error does not match actual: \\nEXPECTED: {expected_message} \\nACTUAL: {actual_message}\")\r\n",
					"        \r\n",
					"        \r\n",
					"        \r\n",
					"    # Failure: When column contains a value that is not a date (eg. string), throw an error\r\n",
					"    def test_fillextractiondate_ingestionfunctions_failure_invaliddatetargetoptions(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"invalid_date\",\r\n",
					"                \"regex_expression\": \"(\\\\d{8}_\\\\d{6})\",\r\n",
					"                \"extract_date_format\": \"yyyyMMdd_HHmmss\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        expected_message = \"Extract date could not be set using extract_column invalid_date\"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test and expect ConfigurationError\r\n",
					"        with self.assertRaises(ConfigurationError) as error:\r\n",
					"            fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expectations match the actuals\r\n",
					"        # Convert the string-error to a json object and get the value for \"custom_message\"\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        json_string = actual_error.replace(\"'\", '\"')\r\n",
					"        actual_message = json.loads(json_string)[\"custom_message\"]\r\n",
					"        self.assertEqual(actual_message, expected_message, f\"[IngestionFunctions:FillExtractionDate] 8. Expected error does not match actual: \\nEXPECTED: {expected_message} \\nACTUAL: {actual_message}\")\r\n",
					"        \r\n",
					"    # Failure: When column does not exist in dataframe, throw an error\r\n",
					"    def test_fillextractiondate_ingestionfunctions_failure_invalidcolumnname(self):\r\n",
					"        # PREPROCESS\r\n",
					"        # Define the arguments used to call the function\r\n",
					"        target_options = {\r\n",
					"            \"extract_date\": {\r\n",
					"                \"column_name\": \"does_not_exist\"\r\n",
					"            }\r\n",
					"        }\r\n",
					"\r\n",
					"        expected_message = \"Column does_not_exist cannot be found in the given dataframe\"\r\n",
					"        # EXECUTE\r\n",
					"        # Run the function under test and expect ConfigurationError\r\n",
					"        with self.assertRaises(ConfigurationError) as error:\r\n",
					"            fill_extraction_date(self.dataframe, target_options)\r\n",
					"\r\n",
					"        # EVALUATE\r\n",
					"        # Validate that the expectations match the actuals\r\n",
					"        # Convert the string-error to a json object and get the value for \"custom_message\"\r\n",
					"        actual_error = str(error.exception)\r\n",
					"        json_string = actual_error.replace(\"'\", '\"')\r\n",
					"        actual_message = json.loads(json_string)[\"custom_message\"]\r\n",
					"        self.assertEqual(actual_message, expected_message, f\"[IngestionFunctions:FillExtractionDate] 9. Expected error does not match actual: \\nEXPECTED: {expected_message} \\nACTUAL: {actual_message}\")\r\n",
					"        \r\n",
					"\r\n",
					"# test_loader = unittest.TestLoader()\r\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_FillExtractionDate)\r\n",
					"\r\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\r\n",
					"\r\n",
					"# if len(test_results.errors) != 0:\r\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 18
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: Merge_File\n",
					"\n",
					"Function Purpose: Given a delta table and a raw data dataframe, insert the raw data into the delta table\n",
					""
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					},
					"collapsed": false
				},
				"source": [
					"# Test Class: InitialisationDeltaTable\n",
					"# This class will test the MergeFile function defined in the IngestionFunctions notebook\n",
					"class Test_Function_Merge_File(unittest.TestCase):\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    # SET-UP: \n",
					"    # Define the setUp and tearDown functions of the class\n",
					"    # Used to patch objects that will be used over several function-calls\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"    def setUp(self):\n",
					"        # Create class-variables to use during the test functions\n",
					"        self.delta_schema = StructType([StructField('id', StringType(),True), StructField('t_load_date_silver', TimestampType(), False), StructField('t_extract_date', TimestampType(), False), StructField('t_update_date', TimestampType(), False), StructField('t_insert_date', TimestampType(), False)])\n",
					"\n",
					"        # Define the primary columns dictionary and column_names dictionary needed to initiate the functions\n",
					"        self.pk_columns_dict = { 'id': 'ID' }\n",
					"        self.column_names_dict = { 'id': 'ID', 't_load_date_silver': 't_load_date_silver', 't_extract_date':'t_extract_date', 't_update_date':'t_update_date', 't_insert_date':'t_insert_date'}\n",
					"\n",
					"        # Patch the functions that are called by the init-function of the DeltaTable-class\n",
					"        # The patched functions are self-made functions or functions coming from external libaries.\n",
					"        # Self-made functions are tested separately, and are \"expected\" to work when being called from other functions\n",
					"\n",
					"        ## Function in the IngestionFunctions notebook  \n",
					"        self.mock_pk_match_patcher = patch('__main__.pk_match')\n",
					"        self.mock_pk_match = self.mock_pk_match_patcher.start()\n",
					"\n",
					"        ## Function in the IngestionFunctions notebook  \n",
					"        self.mock_columnmatch_patcher = patch('__main__.ColumnMatch')\n",
					"        self.mock_columnmatch = self.mock_columnmatch_patcher.start()\n",
					"\n",
					"        ## Function in the IngestionFunctions notebook  \n",
					"        self.mock_nomatch_patcher = patch('__main__.NoMatch')\n",
					"        self.mock_nomatch = self.mock_nomatch_patcher.start()\n",
					"\n",
					"        ## Function in the IngestionFunctions notebook  \n",
					"        self.mock_create_partioning_list_patcher = patch('__main__.create_partioning_list')\n",
					"        self.mock_create_partioning_list = self.mock_create_partioning_list_patcher.start()\n",
					"\n",
					"        return\n",
					"\n",
					"    def tearDown(self):\n",
					"        # Stop all the patcher-functions \n",
					"        self.mock_pk_match_patcher.stop()\n",
					"        self.mock_columnmatch_patcher.stop()\n",
					"        self.mock_create_partioning_list_patcher.stop()\n",
					"        return\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"\n",
					"    # ------------------------------------------------------------------------------------------------------------------\n",
					"\n",
					"    # Success: Given an empty delta table and a raw data dataframe with one row, insert the row in the delta table\n",
					"    def test_mergefile_success(self):\n",
					"        # PREPROCESS\n",
					"        # Remove the merge_test folder if it exists\n",
					"        try:\n",
					"            mssparkutils.fs.rm(f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/merge_test', True)\n",
					"        except:\n",
					"            pass\n",
					"\n",
					"        # Create an empty spark dataframe with the delta_schema and save the dataframe as a delta table\n",
					"        deltaTable = spark.createDataFrame([], self.delta_schema)\n",
					"        deltaTable.write.mode('overwrite').format(\"delta\").save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n",
					"        deltaTable = DeltaTable.forPath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n",
					"\n",
					"        # Create a spark dataframe with the raw schema containing one row with value '0123456789' for the ID column and an empty timestamp for t_load_date_silver\n",
					"        raw_schema =  StructType([StructField('ID', StringType(),True), StructField('t_load_date_silver', TimestampType(), True), StructField('t_extract_date', TimestampType(), True), StructField('t_update_date', TimestampType(), True), StructField('t_insert_date', TimestampType(), True)])\n",
					"        rawData = spark.createDataFrame(data=[('0123456789', None, None, None, None)], schema=raw_schema)\n",
					"\n",
					"        # Set return values for mocked functions\n",
					"        self.mock_pk_match.return_value = 'oldData.id = newData.ID'\n",
					"        self.mock_columnmatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date',}\n",
					"        self.mock_nomatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date',}\n",
					"\n",
					"        # Expected results: We expect the id from raw data to be inserted into the delta table\n",
					"        expected_id = '0123456789'\n",
					"        not_expected_loaddate = 'null'\n",
					"        \n",
					"        # RUN\n",
					"        # Execute function\n",
					"        MergeFile(deltaTable, rawData, self.pk_columns_dict, self.column_names_dict, target_options={})\n",
					"\n",
					"\n",
					"        # EVALUATE\n",
					"        # Set variables with the actual results after function-execution\n",
					"        actual_df = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n",
					"        actual_id = actual_df.collect()[0].id\n",
					"        actual_loaddate = actual_df.collect()[0].t_load_date_silver\n",
					"\n",
					"        # Validate that the expected results match with the actual results\n",
					"        self.assertEqual(expected_id, actual_id)\n",
					"        # Validate that the t_load_date_silver is no longer a 'null' value\n",
					"        self.assertNotEqual(not_expected_loaddate, actual_loaddate, 'The actual t_load_date_silver is still \"null\" and has not been changed')\n",
					"        # Validate that the t_load_date_silver is of type 'datetime'\n",
					"        self.assertIs(type(actual_loaddate), datetime.datetime)\n",
					"\n",
					"    # Failure: The rawData dataframe does not have the t_load_date_silver column and an error should be thrown\n",
					"    def test_mergefile_fail_on_missing_column(self):\n",
					"        # PREPROCESS\n",
					"        # Create an empty spark dataframe\n",
					"        deltaTable = spark.createDataFrame([], self.delta_schema)\n",
					"        # Create a spark dataframe with the raw schema containing one row with value '0123456789' for the ID column and an empty timestamp for load_date_silver\n",
					"        raw_schema =  StructType([StructField('ID', StringType(),True)])\n",
					"        rawData = spark.createDataFrame([('0123456789', )], raw_schema)\n",
					"        \n",
					"        # Define expected error\n",
					"        expected_error = \"Column 't_load_date_silver' does not exist in the raw dataframe. Something must have gone wrong when writing from landing to raw...\"\n",
					"\n",
					"        # RUN\n",
					"        # Execute function\n",
					"        with self.assertRaises(ValueError) as error:\n",
					"            MergeFile(deltaTable, rawData, self.pk_columns_dict, self.column_names_dict, target_options={})\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the returned exception string\n",
					"        actual_error = str(error.exception)\n",
					"        # Validate that the returned error is equal to the expectated error\n",
					"        self.assertEqual(expected_error, actual_error,  f'The returned error from the process_column_info_string was \"{actual_error}\", while the expected error was \"{expected_error}\".')\n",
					"\n",
					"    def test_mergefile_call_partitioning_function(self):\n",
					"        # PREPROCESS\n",
					"        # Create an empty spark dataframe\n",
					"        partition_schema = StructType([StructField('id', StringType(),True), StructField('t_load_date_silver', TimestampType(), True), StructField('t_extract_date', TimestampType(), True), StructField('t_update_date', TimestampType(), True), StructField('t_insert_date', TimestampType(), True)])\n",
					"\n",
					"        deltaTable = spark.createDataFrame([], partition_schema)\n",
					"        deltaTable.write.mode('overwrite').format(\"delta\").partitionBy('id').save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_test\"))\n",
					"        deltaTable = DeltaTable.forPath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_test\"))\n",
					"        # Create a spark dataframe with the raw schema containing one row with value '0123456789' for the ID column and an empty timestamp for load_date_silver\n",
					"        raw_schema =  StructType([StructField('ID', StringType(),True), StructField('t_load_date_silver', TimestampType(), True), StructField('t_extract_date', TimestampType(), True), StructField('t_update_date', TimestampType(), True), StructField('t_insert_date', TimestampType(), True)])\n",
					"        rawData = spark.createDataFrame([('0123456789', None, None, None, None)], raw_schema)\n",
					"        target_options = {'partitioning': [{'name': 'id', 'sequence': 1}]}\n",
					"        source_partitions = [{'name': 'ID', 'sequence': 1}]\n",
					"\n",
					"        self.mock_create_partioning_list.return_value = rawData, [\"ID\"]\n",
					"        self.mock_pk_match.return_value = 'oldData.id = newData.ID'\n",
					"        self.mock_columnmatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date'}\n",
					"        self.mock_nomatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date'}\n",
					"        \n",
					"        with patch('__main__.fill_extraction_date') as mock_fill_extraction_date:\n",
					"            mock_fill_extraction_date.return_value = rawData\n",
					"            MergeFile(deltaTable, rawData, self.pk_columns_dict, self.column_names_dict, target_options=target_options)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Dev-Note: ANY because the 'rawData' object used in the expected function and the actual function are not the same and an AssertionError will be thrown\n",
					"        #   While they may actually be the same, the t_load_date_silver has been added to the object and therefore, the assert-method does not consider them the same object\n",
					"        self.mock_create_partioning_list.assert_called_once_with(ANY, source_partitions)\n",
					"        mock_fill_extraction_date.assert_called_once_with(dataframe=ANY, target_options=target_options)\n",
					"\n",
					"\n",
					"    def test_mergefile_test_timestamp_partitioning(self):\n",
					"        # PREPROCESS\n",
					"        # Create an empty spark dataframe\n",
					"        partition_schema = StructType([StructField('id', StringType(),True), StructField('t_load_date_silver', TimestampType(), False), StructField('t_extract_date', TimestampType(), True), StructField('t_update_date', TimestampType(), True), StructField('t_insert_date', TimestampType(), True), StructField('p_year', IntegerType(), False), StructField('p_month', IntegerType(), False)])\n",
					"\n",
					"        deltaTable = spark.createDataFrame([], partition_schema)\n",
					"        deltaTable.write.mode('overwrite').format(\"delta\").partitionBy('p_year', 'p_month').save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_timestamp_test\"))\n",
					"        deltaTable = DeltaTable.forPath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"partition_timestamp_test\"))\n",
					"\n",
					"        # Create a spark dataframe with the raw schema containing one row with value '0123456789' for the ID column and an empty timestamp for load_date_silver\n",
					"        raw_schema =  StructType([StructField('ID', StringType(),True), StructField('t_load_date_silver', TimestampType(), True),  StructField('t_extract_date', TimestampType(), True), StructField('t_update_date', TimestampType(), True), StructField('t_insert_date', TimestampType(), True)])\n",
					"        rawData = spark.createDataFrame([('0123456789', None, None, None, None)], raw_schema)\n",
					"        target_options = {'partitioning': [{'name': 't_load_date_silver', 'sequence': 1, 'datePart': 'year'}, {'name': 't_load_date_silver', 'sequence': 2, 'datePart': 'month'}]}\n",
					"        \n",
					"\n",
					"        partitioned_rawData = rawData.withColumn('p_year', lit(2024)).withColumn('p_month', lit(5))\n",
					"        self.mock_create_partioning_list.return_value =  partitioned_rawData, ['p_year', 'p_month']\n",
					"        self.mock_pk_match.return_value = 'oldData.id = newData.ID'\n",
					"        self.mock_columnmatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date', 'p_year': 'newData.p_year', 'p_month': 'newData.p_month'}\n",
					"        self.mock_nomatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date', 'p_year': 'newData.p_year', 'p_month': 'newData.p_month'}\n",
					"        expected_columns_dict = {**self.column_names_dict, **{'p_year': 'p_year', 'p_month': 'p_month'}}\n",
					"\n",
					"        # RUN\n",
					"        MergeFile(deltaTable, rawData, self.pk_columns_dict, self.column_names_dict, target_options=target_options)\n",
					"\n",
					"        # EVALUATE\n",
					"        # Get the returned exception string\n",
					"        self.mock_create_partioning_list.assert_called_once_with(ANY, target_options['partitioning'])\n",
					"        self.mock_columnmatch.assert_called_once_with(expected_columns_dict, 'newData')\n",
					"\n",
					"    # Success: Add 2 raw dataframes (sequentially) to a delta table and check the timestamps of t_update_date and t_insert_date\n",
					"    def test_mergefile_updateexistingrecord(self):\n",
					"        # Drop the table if it exists already\n",
					"        try:\n",
					"            mssparkutils.fs.rm(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"), True)\n",
					"        except:\n",
					"            pass\n",
					"        \n",
					"        # PREPROCESS\n",
					"        # Create an empty delta table\n",
					"        deltaTable = spark.createDataFrame([], self.delta_schema)\n",
					"        deltaTable.write.mode('overwrite').format(\"delta\").save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"))\n",
					"        deltaTable = DeltaTable.forPath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"))\n",
					"        \n",
					"        # Create 2 spark dataframes:\n",
					"        # rawData_one: Contains 2 unique values for ID\n",
					"        # rawData_two: Contains 1 unique value for ID and one value that is already in rawData_one\n",
					"        raw_schema =  StructType([StructField('ID', StringType(),True), StructField('t_load_date_silver', TimestampType(), True),  StructField('t_extract_date', TimestampType(), True), StructField('t_update_date', TimestampType(), True), StructField('t_insert_date', TimestampType(), True)])\n",
					"        rawData_one = spark.createDataFrame(\n",
					"            data=[\n",
					"                ('0123456789', None, None, None, None), \n",
					"                ('9876543210', None, None, None, None)\n",
					"            ], \n",
					"            schema=raw_schema\n",
					"        )\n",
					"        rawData_two = spark.createDataFrame(\n",
					"            data=[\n",
					"                ('0123456789', None, None, None, None), \n",
					"                ('5432109876', None, None, None, None)\n",
					"            ], \n",
					"            schema=raw_schema\n",
					"        )\n",
					"\n",
					"        # Mock the outputs of the functions called by MergeFile()\n",
					"        self.mock_pk_match.return_value = 'oldData.id = newData.ID'\n",
					"        self.mock_columnmatch.return_value = {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date'}\n",
					"        self.mock_nomatch.return_value =     {'id': 'newData.ID', 't_load_date_silver': 'newData.t_load_date_silver', 't_extract_date': 'newData.t_extract_date', 't_update_date': 'newData.t_update_date', 't_insert_date': 'newData.t_insert_date'}\n",
					"        \n",
					"\n",
					"        # RUN\n",
					"        # 1. Ingest rawData_one into the deltaTable and get the values for t_update_date and t_insert_date for both inserted ID\n",
					"        # NOTE: It is very important to cache the dataframes as they will change over the different steps and we \"lose\" the initial status of the dataframe\n",
					"        MergeFile(deltaTable, rawData_one, self.pk_columns_dict, self.column_names_dict, target_options={})\n",
					"        \n",
					"        df_one = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"), format=\"delta\").cache()\n",
					"        df_one_updatedate_id1 = df_one.filter(df_one.id == '0123456789').select(\"t_update_date\").collect()\n",
					"        df_one_insertdate_id1 = df_one.filter(df_one.id == '0123456789').select(\"t_insert_date\").collect()\n",
					"        df_one_updatedate_id2 = df_one.filter(df_one.id == '9876543210').select(\"t_update_date\").collect()\n",
					"        df_one_insertdate_id2 = df_one.filter(df_one.id == '9876543210').select(\"t_insert_date\").collect()\n",
					"\n",
					"        \n",
					"        # 2. Ingest rawData_two into the deltaTabel and get the values for t_update_date and t_insert_date for all existing IDs in the table\n",
					"        deltaTable_two = DeltaTable.forPath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"))\n",
					"        MergeFile(deltaTable_two, rawData_two, self.pk_columns_dict, self.column_names_dict, target_options={})\n",
					"        \n",
					"        df_two = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"update_existing_record\"), format=\"delta\").cache()\n",
					"        df_two_updatedate_id1 = df_two.filter(df_two.id == '0123456789').select(\"t_update_date\").collect()\n",
					"        df_two_insertdate_id1 = df_two.filter(df_two.id == '0123456789').select(\"t_insert_date\").collect()\n",
					"        df_two_updatedate_id2 = df_two.filter(df_two.id == '9876543210').select(\"t_update_date\").collect()\n",
					"        df_two_insertdate_id2 = df_two.filter(df_two.id == '9876543210').select(\"t_insert_date\").collect()\n",
					"        df_two_updatedate_id3 = df_two.filter(df_two.id == '5432109876').select(\"t_update_date\").collect()\n",
					"        df_two_insertdate_id3 = df_two.filter(df_two.id == '5432109876').select(\"t_insert_date\").collect()\n",
					"\n",
					"        # EVALUATE\n",
					"        # 1. id1 (0123456789): \n",
					"        #   Initially, in df_one, both t_update_date and t_insert_date are expected to be the same\n",
					"        #   After ingesting rawdata_two, expect t_update_date to be different in df_two compared to the previous version of df_one\n",
					"        #   However, t_insert_date is not expected to have changed over time\n",
					"        self.assertEqual(df_one_updatedate_id1, df_one_insertdate_id1, f'[IngestionFunctions:MergeFile] 1. Expectations do not match actuals: \\nEXPECTED {df_one_updatedate_id1}\\n ACTUALS: {df_one_insertdate_id1}')\n",
					"        self.assertNotEqual(df_two_updatedate_id1, df_two_insertdate_id1, f'[IngestionFunctions:MergeFile] 2. Expectations do not match actuals: \\nEXPECTED {df_two_updatedate_id1}\\n ACTUALS: {df_two_insertdate_id1}')\n",
					"        self.assertEqual(df_two_insertdate_id1, df_one_insertdate_id1, f'[IngestionFunctions:MergeFile] 3. Expectations do not match actuals: \\nEXPECTED {df_two_insertdate_id1}\\n ACTUALS: {df_one_insertdate_id1}')\n",
					"\n",
					"        # 2. id2 (9876543210): \n",
					"        #   Initially, in df_one, both t_update_date and t_insert_date are expected to be the same\n",
					"        #   After ingesting rawdata_two, no changes are expected to have happened to either column as they should not have changed over time\n",
					"        self.assertEqual(df_one_updatedate_id2, df_one_insertdate_id2, f'[IngestionFunctions:MergeFile] 4. Expectations do not match actuals: \\nEXPECTED {df_one_updatedate_id2}\\n ACTUALS: {df_one_insertdate_id2}')\n",
					"        self.assertEqual(df_two_updatedate_id2, df_two_insertdate_id2, f'[IngestionFunctions:MergeFile] 5. Expectations do not match actuals: \\nEXPECTED {df_two_updatedate_id2}\\n ACTUALS: {df_two_insertdate_id2}')\n",
					"        self.assertEqual(df_one_insertdate_id2, df_two_insertdate_id2, f'[IngestionFunctions:MergeFile] 6. Expectations do not match actuals: \\nEXPECTED {df_one_insertdate_id2}\\n ACTUALS: {df_two_insertdate_id2}')\n",
					"        self.assertEqual(df_one_updatedate_id2, df_two_updatedate_id2, f'[IngestionFunctions:MergeFile] 7. Expectations do not match actuals: \\nEXPECTED {df_one_updatedate_id2}\\n ACTUALS: {df_two_updatedate_id2}')\n",
					"        \n",
					"\n",
					"        # 2. id2 (5432109876): \n",
					"        #   Initially, rawData_one does not contain this row so nothing is expected\n",
					"        #   After ingesting rawdata_two, expect t_update_date and t_insert_date to be the same\n",
					"        #   As a safety check: They should be the same the values for id1 (0123456789) as the timestamps are staticly created\n",
					"        self.assertEqual(df_two_updatedate_id3, df_two_insertdate_id3, f'[IngestionFunctions:MergeFile] 8. Expectations do not match actuals: \\nEXPECTED {df_two_updatedate_id3}\\n ACTUALS: {df_two_insertdate_id3}')\n",
					"        self.assertEqual(df_two_updatedate_id1, df_two_insertdate_id3, f'[IngestionFunctions:MergeFile] 9. Expectations do not match actuals: \\nEXPECTED {df_two_updatedate_id1}\\n ACTUALS: {df_two_insertdate_id3}')\n",
					"        self.assertEqual(df_two_updatedate_id1, df_two_insertdate_id3, f'[IngestionFunctions:MergeFile] 10. Expectations do not match actuals: \\nEXPECTED {df_two_updatedate_id1}\\n ACTUALS: {df_two_insertdate_id3}')\n",
					"\n",
					"        # Remove the dataframes from memory (remove cache)\n",
					"        df_two.unpersist()\n",
					"        df_one.unpersist()\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_Function_Merge_File)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")\n",
					""
				],
				"execution_count": 24
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: remove_ext\n",
					"\n",
					"Function Purpose: Remove a file / path extension from a given string"
				]
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: get_parent_folder\n",
					"\n",
					"Function Purpose: Retrieve the parent folder of the file"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_GetParentFolder(unittest.TestCase):\n",
					"    def test_get_parent_folder_pass(self):\n",
					"        # Test: Get the parent folder of a child_file with DEPTH = 1\n",
					"        parent_folder = get_parent_folder(\"mock_parent_folder/mock_child_file.csv\")\n",
					"\n",
					"        self.assertEqual(parent_folder, \"mock_parent_folder\")\n",
					"\n",
					"    def test_get_parent_folder_pass2(self):\n",
					"        # Test: Get the parent folder of a child_file with DEPTH = 2\n",
					"        parent_folder = get_parent_folder(\"mock_grandparent_folder/mock_parent_folder/mock_child_file.csv\")\n",
					"\n",
					"        self.assertEqual(parent_folder, \"mock_parent_folder\")\n",
					"\n",
					"    def test_get_parent_folder_pass3(self):\n",
					"        # Test: Get the parent folder of a child_file with DEPTH = 5\n",
					"        parent_folder = get_parent_folder(\"mock_ggggrandparent_folder/mock_gggrandparent_folder/mock_ggrandparent_folder/mock_grandparent_folder/mock_parent_folder/mock_child_file.csv\")\n",
					"\n",
					"        self.assertEqual(parent_folder, \"mock_parent_folder\")\n",
					"    \n",
					"    def test_get_parent_folder_error(self):\n",
					"        # Test: Error is expected when it doesn't look like a path / there is no parent folder\n",
					"        \n",
					"        with self.assertRaisesRegex(Exception, \"^(get_parent_folder).*(mock_child_file\\.csv).*(doesn't qualify as a path).*(doesn't have a parent folder)$\"):\n",
					"            parent_folder = get_parent_folder(\"mock_child_file.csv\")\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_GetParentFolder)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 20
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test: jsons_to_dict\n",
					"\n",
					"Function Purpose: Turn a list of flat-json objects, stored as strings, into a dictionary"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"class Test_JsonsToDict(unittest.TestCase):\n",
					"    def test_jsons_to_dict_pass(self):\n",
					"        # Test to see if everything works properly\n",
					"        input_dict = ['{\"mock_key1\": 1}', '{\"mock_key2\": 2}']\n",
					"\n",
					"        output_dict = jsons_to_dict(input_dict)\n",
					"        self.assertEqual({'mock_key1': 1, 'mock_key2': 2}, output_dict)\n",
					"\n",
					"    def test_jsons_to_dict_pass2(self):\n",
					"        # Test to see if everything works properly when you have multiple json objects in one string\n",
					"        input_dict = ['{\"mock_key1\": 1}, {\"mock_key2\": 2}']\n",
					"\n",
					"        output_dict = jsons_to_dict(input_dict)\n",
					"        self.assertEqual({'mock_key1': 1, 'mock_key2': 2}, output_dict)\n",
					"\n",
					"    def test_jsons_to_dict_parse_error(self):\n",
					"        # Test to see if parsing of multiple json objects in one string fails when there's a typo\n",
					"        input_dict = ['{\"mock_k 1}, {\"mock_key2\": 2}']\n",
					"\n",
					"        with self.assertRaises(Exception) as parse_error:\n",
					"            output_dict = jsons_to_dict(input_dict)\n",
					"\n",
					"        self.assertEqual(str(parse_error.exception), 'Unable to parse string {\"mock_k1} to JSON')\n",
					"\n",
					"    def test_jsons_to_dict_parse_error2(self):\n",
					"        # Test to see if an error is thrown when you get a JSON with a typo inside\n",
					"        input_dict = ['{\"mock_k 1}', '{\"mock_key2\": 2}']\n",
					"\n",
					"        with self.assertRaises(Exception) as parse_error:\n",
					"            output_dict = jsons_to_dict(input_dict)\n",
					"        \n",
					"        self.assertEqual(str(parse_error.exception), 'Unable to parse string {\"mock_k 1} to JSON')\n",
					"\n",
					"# test_loader = unittest.TestLoader()\n",
					"# test_suite = test_loader.loadTestsFromTestCase(Test_JsonsToDict)\n",
					"\n",
					"# test_results = unittest.TextTestRunner(verbosity=2).run(test_suite)\n",
					"\n",
					"# if len(test_results.errors) != 0:\n",
					"#     raise Exception(\"Error in header tests, something went wrong!\")"
				],
				"execution_count": 21
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Execute the test classes"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# unittest.main(argv=[''], verbosity=2,exit=False, warnings='ignore')"
				],
				"execution_count": 22
			},
			{
				"cell_type": "markdown",
				"metadata": {
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"### Test class to be explored: Check if it is possible to compare/assert different pandas dataframes instead of having to compare its individual values"
				]
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"source_hidden": false,
						"outputs_hidden": false
					},
					"nteract": {
						"transient": {
							"deleting": false
						}
					}
				},
				"source": [
					"# TO TRY: Improved unit testing cases\n",
					"\n",
					"# from pyspark.conf import SparkConf\n",
					"# from pyspark.testing.pandasutils import PandasOnSparkTestCase\n",
					"# help(pyspark.testing.pandasutils)\n",
					"\n",
					"# class PySparkTestCase(unittest.TestCase):\n",
					"#     @classmethod\n",
					"#     def setUpClass(cls):\n",
					"#         config = {\n",
					"#                     \"name\": \"runtime_datasource\",\n",
					"#                     \"class_name\": \"Datasource\",\n",
					"#                     \"module_name\": \"great_expectations.datasource\",\n",
					"#                     \"execution_engine\": {\n",
					"#                         \"module_name\": \"great_expectations.execution_engine\",\n",
					"#                         \"class_name\": \"SparkDFExecutionEngine\",\n",
					"#                         \"spark_config\":{\n",
					"#                             \"spark.app.name\": spark.sparkContext.getConf().get(\"spark.app.name\")\n",
					"#                         }\n",
					"#                     },\n",
					"#                     \"data_connectors\": {\n",
					"#                         \"default_runtime_data_connector_name\": {\n",
					"#                             \"class_name\": \"RuntimeDataConnector\",\n",
					"#                             \"batch_identifiers\": [\"default_identifier_name\"],\n",
					"#                         },\n",
					"#                     },\n",
					"#                 }\n",
					"#         cls.spark = SparkSession.builder.appName(\"Sample PySpark ETL\").getOrCreate()\n",
					"#         # SparkSession.builder.master(\"local\").appName(\"Word Count\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
					"\n",
					"#     @classmethod\n",
					"#     def tearDownClass(cls):\n",
					"#         cls.spark.stop()\n",
					"\n",
					"\n",
					"# class Test_Function_Merge_File(unittest.TestCase):\n",
					"\n",
					"#     # TO DO: Fix function -&gt; Insert a row from raw into delta. Set expected_id = *inserted value* and actual_id is extracted from the delta table itself\n",
					"#     def test1_merge_file(self):\n",
					"#         # mssparkutils.fs.rm(f'abfss://unittest@{env_code}dapstdala1.dfs.core.windows.net/merge_test', True)\n",
					"\n",
					"#         # # Variables\n",
					"#         # delta_schema = StructType([StructField('id', StringType(),True)])\n",
					"#         # raw_schema =   StructType([StructField('ID', StringType(),True)])\n",
					"\n",
					"#         # deltaTable = spark.createDataFrame([], delta_schema)\n",
					"#         # deltaTable.write.mode('overwrite').format(\"delta\").save(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n",
					"\n",
					"#         # deltaTable = DeltaTable.forPath(spark, \"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n",
					"#         # rawData= spark.createDataFrame([('0123456789', )], raw_schema)\n",
					"#         # pk_columns_dict = { 'id': 'ID' }\n",
					"#         # column_names_dict = { 'id': 'ID' }\n",
					"\n",
					"#         # # display(rawData)\n",
					"#         # # display(deltaTable)\n",
					"\n",
					"#         # # Mock functions\n",
					"#         # with (patch(\"__main__.pk_match\") as mock_pk_match,\n",
					"#         #         patch(\"__main__.ColumnMatch\") as mock_column_match,\n",
					"#         #         patch(\"__main__.NoMatch\") as mock_no_match):\n",
					"#         #     # Set return values for called functions\n",
					"#         #     mock_pk_match.return_value = 'oldData.id = newData.ID'\n",
					"#         #     mock_column_match.return_value = {'id': 'newData.ID'}\n",
					"#         #     mock_no_match.return_value = {'id': 'newData.ID'}\n",
					"        \n",
					"#         # # Execute function\n",
					"#         # MergeFile(deltaTable, rawData, pk_columns_dict, column_names_dict)\n",
					"\n",
					"#         # Expected results\n",
					"#         expected_id = '0123456789'\n",
					"\n",
					"#         # Actual results\n",
					"#         actual_df = spark.read.load(\"abfss://%s@%s.dfs.core.windows.net/%s\" % ('unittest', f\"{env_code}dapstdala1\", \"merge_test\"))\n",
					"#         actual_id = actual_df.collect()[0].id\n",
					"\n",
					"#         # \n",
					"#         # # Compare actual with expected\n",
					"#         self.assertEqual(expected_id, actual_id)\n",
					"\n",
					"#     # def tearDown(self):\n",
					"#     #     mock_pk_match.close()\n",
					"#     #     mock_column_match.close()\n",
					"#     #     mock_no_match.close()\n",
					"\n",
					"# unittest.main(argv=[''], verbosity=2,exit=False)"
				],
				"execution_count": 23
			}
		]
	}
}
</file>
<file name="src\synapse\studio\pipeline\obsolete_pl_start_ingestion.json">
{
	"name": "obsolete_pl_start_ingestion",
	"properties": {
		"activities": [
			{
				"name": "Get Metadata1",
				"type": "GetMetadata",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"dataset": {
						"referenceName": "ds_binary_file",
						"type": "DatasetReference",
						"parameters": {
							"container_name": "ziptest",
							"folder_directory": "data_source/*/*",
							"file_name": "PDM_extracts_20220613.gz"
						}
					},
					"fieldList": [
						"itemName",
						"itemType"
					],
					"storeSettings": {
						"type": "AzureBlobFSReadSettings",
						"enablePartitionDiscovery": false
					},
					"formatSettings": {
						"type": "BinaryReadSettings"
					}
				}
			}
		],
		"annotations": [],
		"lastPublishTime": "2023-09-29T12:09:56Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}
</file>
<file name="src\synapse\studio\pipeline\pl_dummy_worker.json">
{
	"name": "pl_dummy_worker",
	"properties": {
		"activities": [
			{
				"name": "start_task",
				"description": "Executes the stored procedure start_task in the meta db",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "[meta].[usp_start_task]",
					"storedProcedureParameters": {
						"Pipeline_ID": {
							"value": {
								"value": "@pipeline().RunId",
								"type": "Expression"
							},
							"type": "String"
						},
						"Plan_ID": {
							"value": {
								"value": "@pipeline().parameters.Plan_ID",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"Task_ID": {
							"value": {
								"value": "@pipeline().parameters.Task.Task_ID",
								"type": "Expression"
							},
							"type": "Int32"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "dummy_task",
				"description": "Insert wait to mimic a small task being executed",
				"type": "Wait",
				"dependsOn": [
					{
						"activity": "start_task",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"waitTimeInSeconds": 1
				}
			},
			{
				"name": "end_task",
				"description": "Executed stored procedure meta.usp_end_task",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "dummy_task",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "[meta].[usp_end_task]",
					"storedProcedureParameters": {
						"comment": {
							"value": "SUCCESS",
							"type": "String"
						},
						"plan_id": {
							"value": {
								"value": "@pipeline().parameters.Plan_ID",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"success_flag": {
							"value": "true",
							"type": "Boolean"
						},
						"task_id": {
							"value": {
								"value": "@pipeline().parameters.Task.Task_ID",
								"type": "Expression"
							},
							"type": "Int32"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			}
		],
		"parameters": {
			"Plan_ID": {
				"type": "int"
			},
			"Task": {
				"type": "object"
			}
		},
		"folder": {
			"name": "Workers"
		},
		"annotations": [],
		"lastPublishTime": "2023-09-19T09:05:43Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}
</file>
<file name="src\synapse\studio\pipeline\pl_meta_pipeline.json">
{
	"name": "pl_meta_pipeline",
	"properties": {
		"activities": [
			{
				"name": "For each task in task group",
				"type": "ForEach",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@json(pipeline().parameters.Tasks)",
						"type": "Expression"
					},
					"activities": [
						{
							"name": "Run pipeline",
							"type": "Switch",
							"dependsOn": [],
							"userProperties": [],
							"typeProperties": {
								"on": {
									"value": "@item().Worker_Name",
									"type": "Expression"
								},
								"cases": [
									{
										"value": "pl_unzip_worker",
										"activities": [
											{
												"name": "Execute pl_start_unzip",
												"description": "Execute pl_start_unzip",
												"type": "ExecutePipeline",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"pipeline": {
														"referenceName": "pl_unzip_worker",
														"type": "PipelineReference"
													},
													"waitOnCompletion": true,
													"parameters": {
														"TaskID": {
															"value": "@item().Task_ID",
															"type": "Expression"
														},
														"PlanID": {
															"value": "@pipeline().parameters.Plan_ID",
															"type": "Expression"
														}
													}
												}
											}
										]
									},
									{
										"value": "pl_dummy_worker",
										"activities": [
											{
												"name": "Execute pl_dummy_worker",
												"description": "Execute pl_dummy_worker",
												"type": "ExecutePipeline",
												"dependsOn": [],
												"userProperties": [],
												"typeProperties": {
													"pipeline": {
														"referenceName": "pl_dummy_worker",
														"type": "PipelineReference"
													},
													"waitOnCompletion": true,
													"parameters": {
														"Plan_ID": {
															"value": "@pipeline().parameters.Plan_ID",
															"type": "Expression"
														},
														"Task": {
															"value": "@item()",
															"type": "Expression"
														}
													}
												}
											}
										]
									}
								]
							}
						}
					]
				}
			}
		],
		"parameters": {
			"Tasks": {
				"type": "string"
			},
			"Plan_ID": {
				"type": "int",
				"defaultValue": 1
			}
		},
		"variables": {
			"Task_array": {
				"type": "Array"
			}
		},
		"annotations": []
	}
}
</file>
<file name="src\synapse\studio\pipeline\pl_start_plan.json">
{
	"name": "pl_start_plan",
	"properties": {
		"activities": [
			{
				"name": "Run INGEST tasks",
				"description": "Invoke the pipeline pl_start_task_group for ingest tasks",
				"type": "ExecutePipeline",
				"dependsOn": [
					{
						"activity": "Run PREPROCESS tasks",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "pl_start_task_group",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"Task_Group": "INGEST",
						"Plan_ID": {
							"value": "@pipeline().parameters.Plan_ID",
							"type": "Expression"
						},
						"Run_ID": {
							"value": "@pipeline().parameters.Run_ID",
							"type": "Expression"
						}
					}
				}
			},
			{
				"name": "Run DUMMY tasks",
				"description": "Invoke the pipeline pl_start_task_group for Dummy tasks",
				"type": "ExecutePipeline",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "pl_start_task_group",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"Task_Group": "DUMMY",
						"Plan_ID": {
							"value": "@pipeline().parameters.Plan_ID",
							"type": "Expression"
						},
						"Run_ID": {
							"value": "@pipeline().parameters.Run_ID",
							"type": "Expression"
						}
					}
				}
			},
			{
				"name": "Run PREPROCESS tasks",
				"description": "Invoke the pipeline pl_start_task_group for Preprocessing tasks",
				"type": "ExecutePipeline",
				"dependsOn": [
					{
						"activity": "Run DUMMY tasks",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"pipeline": {
						"referenceName": "pl_start_task_group",
						"type": "PipelineReference"
					},
					"waitOnCompletion": true,
					"parameters": {
						"Task_Group": "PREPROCESS",
						"Plan_ID": {
							"value": "@pipeline().parameters.Plan_ID",
							"type": "Expression"
						},
						"Run_ID": {
							"value": "@pipeline().parameters.Run_ID",
							"type": "Expression"
						}
					}
				}
			}
		],
		"parameters": {
			"Plan_ID": {
				"type": "int"
			},
			"Run_ID": {
				"type": "int"
			}
		},
		"folder": {
			"name": "Meta"
		},
		"annotations": [],
		"lastPublishTime": "2023-09-18T08:56:15Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}
</file>
<file name="src\synapse\studio\pipeline\pl_start_run.json">
{
	"name": "pl_start_run",
	"properties": {
		"activities": [
			{
				"name": "New Run",
				"description": "Check log_plans &amp; start new run",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "AzureSqlSource",
						"sqlReaderStoredProcedureName": "[meta].[usp_new_run]",
						"storedProcedureParameters": {
							"debug": {
								"type": "Boolean",
								"value": "false"
							},
							"new_plan": {
								"type": "Boolean",
								"value": {
									"value": "@pipeline().parameters.New_Plan",
									"type": "Expression"
								}
							},
							"no_select": {
								"type": "Boolean",
								"value": "false"
							},
							"pipeline_id": {
								"type": "String",
								"value": {
									"value": "@pipeline().RunId",
									"type": "Expression"
								}
							},
							"plan_name": {
								"type": "String",
								"value": {
									"value": "@pipeline().parameters.Plan_Name",
									"type": "Expression"
								}
							}
						},
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "ds_dap_sql_meta",
						"type": "DatasetReference",
						"parameters": {
							"schema": "meta",
							"target": "log_plans"
						}
					},
					"firstRowOnly": true
				}
			},
			{
				"name": "Set Run_ID",
				"description": "Sets a run_id based on the output of \"Lookup - New run\"",
				"type": "SetVariable",
				"dependsOn": [
					{
						"activity": "New Run",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"variableName": "Run_ID",
					"value": {
						"value": "@activity('New Run').output.firstRow.run_id",
						"type": "Expression"
					}
				}
			},
			{
				"name": "Start Run",
				"description": "Execute the stored procedure 'meta.usp_start_run'",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "Set Run_ID",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "[meta].[usp_start_run]",
					"storedProcedureParameters": {
						"run_id": {
							"value": {
								"value": "@variables('Run_ID')",
								"type": "Expression"
							},
							"type": "Int32"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Get Plans",
				"description": "Run the stored procedure 'Get plans'",
				"type": "Lookup",
				"dependsOn": [
					{
						"activity": "Start Run",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "AzureSqlSource",
						"sqlReaderStoredProcedureName": "[meta].[usp_get_plans]",
						"storedProcedureParameters": {
							"plan_name": {
								"type": "String",
								"value": {
									"value": "@pipeline().parameters.Plan_Name",
									"type": "Expression"
								}
							},
							"run_id": {
								"type": "Int32",
								"value": {
									"value": "@variables('Run_ID')",
									"type": "Expression"
								}
							}
						},
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "ds_dap_sql_meta",
						"type": "DatasetReference",
						"parameters": {
							"schema": "meta",
							"target": "log_plans"
						}
					},
					"firstRowOnly": false
				}
			},
			{
				"name": "For Each Plan",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "Get Plans",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('Get Plans').output.value",
						"type": "Expression"
					},
					"isSequential": true,
					"activities": [
						{
							"name": "Start Plan",
							"description": "Execute stored procedure start_plan in meta db",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "[meta].[usp_start_plan]",
								"storedProcedureParameters": {
									"pipeline_id": {
										"value": {
											"value": "@pipeline().RunId",
											"type": "Expression"
										},
										"type": "String"
									},
									"plan_id": {
										"value": {
											"value": "@item().Plan_ID",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"run_id": {
										"value": {
											"value": "@variables('Run_ID')",
											"type": "Expression"
										},
										"type": "Int32"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "End Plan",
							"description": "Execute the stored procedure end_plan in the meta db, but set Comment to SUCCESS",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [
								{
									"activity": "Execute Plan",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "[meta].[usp_end_plan]",
								"storedProcedureParameters": {
									"comment": {
										"value": "SUCCESS",
										"type": "String"
									},
									"pipeline_id": {
										"value": {
											"value": "@pipeline().RunId",
											"type": "Expression"
										},
										"type": "String"
									},
									"plan_id": {
										"value": {
											"value": "@item().plan_id",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"run_id": {
										"value": {
											"value": "@variables('Run_ID')",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"success_flag": {
										"value": "true",
										"type": "Boolean"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "Execute Plan",
							"description": "Invoke the pipeline pl_start_plan",
							"type": "ExecutePipeline",
							"dependsOn": [
								{
									"activity": "Start Plan",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "pl_start_plan",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"Plan_ID": {
										"value": "@item().plan_id",
										"type": "Expression"
									},
									"Run_ID": {
										"value": "@variables('Run_ID')",
										"type": "Expression"
									}
								}
							}
						},
						{
							"name": "End Plan Failure",
							"description": "Execute the stored procedure end_plan in the meta db, but set Comment to FAILED",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [
								{
									"activity": "Execute Plan",
									"dependencyConditions": [
										"Failed"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "[meta].[usp_end_plan]",
								"storedProcedureParameters": {
									"comment": {
										"value": "FAILED",
										"type": "String"
									},
									"pipeline_id": {
										"value": {
											"value": "@pipeline().RunId",
											"type": "Expression"
										},
										"type": "String"
									},
									"plan_id": {
										"value": {
											"value": "@item().plan_id",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"run_id": {
										"value": {
											"value": "@variables('Run_ID')",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"success_flag": {
										"value": "false",
										"type": "Boolean"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						}
					]
				}
			},
			{
				"name": "End Run Success",
				"description": "Execute stored procedure end_run in the meta db, setting it to SUCCESS",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "For Each Plan",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "[meta].[usp_end_run]",
					"storedProcedureParameters": {
						"comment": {
							"value": "SUCCESS",
							"type": "String"
						},
						"pipeline_id": {
							"value": {
								"value": "@pipeline().RunId",
								"type": "Expression"
							},
							"type": "String"
						},
						"run_id": {
							"value": {
								"value": "@variables('Run_ID')",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"success_flag": {
							"value": "true",
							"type": "Boolean"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "End Run Failure Foreach",
				"description": "Execute stored procedure end_run in the meta db, setting it to FAILED",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "For Each Plan",
						"dependencyConditions": [
							"Failed"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "[meta].[usp_end_run]",
					"storedProcedureParameters": {
						"comment": {
							"value": "FAILED",
							"type": "String"
						},
						"pipeline_id": {
							"value": {
								"value": "@pipeline().RunId",
								"type": "Expression"
							},
							"type": "String"
						},
						"run_id": {
							"value": {
								"value": "@variables('Run_ID')",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"success_flag": {
							"value": "false",
							"type": "Boolean"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "End Run Failure LookUp",
				"description": "Execute stored procedure end_run in the meta db, setting the run to FAILED",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "Get Plans",
						"dependencyConditions": [
							"Failed"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "[meta].[usp_end_run]",
					"storedProcedureParameters": {
						"comment": {
							"value": "FAILED",
							"type": "String"
						},
						"pipeline_id": {
							"value": {
								"value": "@pipeline().RunId",
								"type": "Expression"
							},
							"type": "String"
						},
						"run_id": {
							"value": {
								"value": "@variables('Run_ID')",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"success_flag": {
							"value": "false",
							"type": "Boolean"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			}
		],
		"parameters": {
			"New_Plan": {
				"type": "bool",
				"defaultValue": true
			},
			"Plan_Name": {
				"type": "string",
				"defaultValue": "qualifio_plan"
			}
		},
		"variables": {
			"Run_ID": {
				"type": "Integer"
			}
		},
		"folder": {
			"name": "Meta"
		},
		"annotations": [],
		"lastPublishTime": "2023-10-23T07:37:01Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}
</file>
<file name="src\synapse\studio\pipeline\pl_start_task_group.json">
{
	"name": "pl_start_task_group",
	"properties": {
		"activities": [
			{
				"name": "Get tasks for Notebooks",
				"description": "Run the stored procedure get_plan_metadata in the meta db",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "AzureSqlSource",
						"sqlReaderStoredProcedureName": "[meta].[usp_get_plan_metadata]",
						"storedProcedureParameters": {
							"num_tasks": {
								"type": "Int32",
								"value": null
							},
							"plan_id": {
								"type": "Int32",
								"value": {
									"value": "@pipeline().parameters.Plan_ID",
									"type": "Expression"
								}
							},
							"results_as_select": {
								"type": "Boolean",
								"value": "true"
							},
							"run_id": {
								"type": "Int32",
								"value": {
									"value": "@pipeline().parameters.Run_ID",
									"type": "Expression"
								}
							},
							"task_group": {
								"type": "String",
								"value": {
									"value": "@pipeline().parameters.Task_Group",
									"type": "Expression"
								}
							},
							"task_type": {
								"value": "SPARK_NOTEBOOK"
							}
						},
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "ds_dap_sql_meta",
						"type": "DatasetReference",
						"parameters": {
							"schema": "meta",
							"target": "get_plan_metadata"
						}
					},
					"firstRowOnly": true
				}
			},
			{
				"name": "If notebook tasks to run",
				"description": "Only activates when statement is true, doesn't do anything at false",
				"type": "IfCondition",
				"dependsOn": [
					{
						"activity": "Get tasks for Notebooks",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"expression": {
						"value": "@not(equals(activity('Get tasks for Notebooks').output.firstRow.num_tasks, 0))",
						"type": "Expression"
					},
					"ifTrueActivities": [
						{
							"name": "Run ingestion notebook",
							"description": "Run notebook 'MetaNotebook'",
							"type": "SynapseNotebook",
							"dependsOn": [],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"notebook": {
									"referenceName": "MetaNotebook",
									"type": "NotebookReference"
								},
								"parameters": {
									"task_list": {
										"value": {
											"value": "@string(activity('Get tasks for Notebooks').output.firstRow.tasks)",
											"type": "Expression"
										},
										"type": "string"
									},
									"plan_id": {
										"value": {
											"value": "@pipeline().parameters.Plan_ID",
											"type": "Expression"
										},
										"type": "string"
									},
									"env_code": {
										"value": {
											"value": "@variables('ENV_CODE')",
											"type": "Expression"
										},
										"type": "string"
									},
									"debug": {
										"value": "False",
										"type": "bool"
									}
								},
								"snapshot": true,
								"sparkPool": {
									"referenceName": {
										"value": "@variables('spark_pool')",
										"type": "Expression"
									},
									"type": "BigDataPoolReference"
								},
								"executorSize": "Small",
								"conf": {
									"spark.dynamicAllocation.enabled": false
								},
								"driverSize": "Small"
							}
						}
					]
				}
			},
			{
				"name": "Get tasks for Pipelines",
				"description": "Run the stored procedure get_plan_metadata in the meta db",
				"type": "Lookup",
				"dependsOn": [],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"source": {
						"type": "AzureSqlSource",
						"sqlReaderStoredProcedureName": "[meta].[usp_get_plan_metadata]",
						"storedProcedureParameters": {
							"num_tasks": {
								"type": "Int32",
								"value": null
							},
							"plan_id": {
								"type": "Int32",
								"value": {
									"value": "@pipeline().parameters.Plan_ID",
									"type": "Expression"
								}
							},
							"results_as_select": {
								"type": "Boolean",
								"value": "true"
							},
							"run_id": {
								"type": "Int32",
								"value": {
									"value": "@pipeline().parameters.Run_ID",
									"type": "Expression"
								}
							},
							"task_group": {
								"type": "String",
								"value": {
									"value": "@pipeline().parameters.Task_Group",
									"type": "Expression"
								}
							},
							"task_type": {
								"value": "SYNAPSE_PIPELINE"
							}
						},
						"partitionOption": "None"
					},
					"dataset": {
						"referenceName": "ds_dap_sql_meta",
						"type": "DatasetReference",
						"parameters": {
							"schema": "meta",
							"target": "get_plan_metadata"
						}
					},
					"firstRowOnly": true
				}
			},
			{
				"name": "If pipeline tasks to run",
				"description": "Only activates when statement is true, doesn't do anything at false",
				"type": "IfCondition",
				"dependsOn": [
					{
						"activity": "Get tasks for Pipelines",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"expression": {
						"value": "@not(equals(activity('Get tasks for Pipelines').output.firstRow.num_tasks, 0))",
						"type": "Expression"
					},
					"ifTrueActivities": [
						{
							"name": "Execute meta pipeline",
							"description": "Execute pl_meta_pipeline",
							"type": "ExecutePipeline",
							"dependsOn": [],
							"policy": {
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"pipeline": {
									"referenceName": "pl_meta_pipeline",
									"type": "PipelineReference"
								},
								"waitOnCompletion": true,
								"parameters": {
									"Tasks": {
										"value": "@activity('Get tasks for Pipelines').output.firstRow.tasks",
										"type": "Expression"
									},
									"Plan_ID": {
										"value": "@pipeline().parameters.Plan_ID",
										"type": "Expression"
									}
								}
							}
						}
					]
				}
			}
		],
		"parameters": {
			"Task_Group": {
				"type": "string",
				"defaultValue": "DUMMY"
			},
			"Plan_ID": {
				"type": "int",
				"defaultValue": 1
			},
			"Run_ID": {
				"type": "int",
				"defaultValue": 1
			}
		},
		"variables": {
			"ENV_CODE": {
				"type": "String",
				"defaultValue": "dev"
			},
			"spark_pool": {
				"type": "String",
				"defaultValue": "devdapsspcore"
			}
		},
		"folder": {
			"name": "Meta"
		},
		"annotations": [],
		"lastPublishTime": "2023-10-30T15:55:50Z"
	},
	"type": "Microsoft.Synapse/workspaces/pipelines"
}
</file>
<file name="src\synapse\studio\pipeline\pl_unzip_worker.json">
{
	"name": "pl_unzip_worker",
	"properties": {
		"activities": [
			{
				"name": "Set worker variables",
				"type": "IfCondition",
				"dependsOn": [],
				"userProperties": [],
				"typeProperties": {
					"expression": {
						"value": "@equals(1, 1)",
						"type": "Expression"
					},
					"ifTrueActivities": [
						{
							"name": "Set task to IN PROGRESS",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "meta.usp_start_task",
								"storedProcedureParameters": {
									"task_id": {
										"value": {
											"value": "@pipeline().parameters.TaskID",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"plan_id": {
										"value": {
											"value": "@pipeline().parameters.PlanID",
											"type": "Expression"
										},
										"type": "Int32"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "Get Task Metadata",
							"type": "Lookup",
							"dependsOn": [
								{
									"activity": "Set task to IN PROGRESS",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "AzureSqlSource",
									"sqlReaderStoredProcedureName": "meta.usp_get_task_metadata",
									"storedProcedureParameters": {
										"task_id": {
											"type": "Int32",
											"value": {
												"value": "@pipeline().parameters.TaskID",
												"type": "Expression"
											}
										}
									},
									"queryTimeout": "02:00:00",
									"partitionOption": "None"
								},
								"dataset": {
									"referenceName": "ds_dap_sql_meta",
									"type": "DatasetReference",
									"parameters": {
										"schema": "meta",
										"target": "get_task_metadata"
									}
								}
							}
						},
						{
							"name": "Set sink folder variable",
							"type": "SetVariable",
							"dependsOn": [
								{
									"activity": "Get Task Metadata",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"variableName": "sinkFolderName",
								"value": {
									"value": "@json(activity('Get Task Metadata').output.firstRow.task_config)[0].table_name",
									"type": "Expression"
								}
							}
						},
						{
							"name": "Set source folder variable",
							"type": "SetVariable",
							"dependsOn": [
								{
									"activity": "Get Task Metadata",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"variableName": "sourceFolderName",
								"value": {
									"value": "@json(activity('Get Task Metadata').output.firstRow.task_config)[0].source_folder",
									"type": "Expression"
								}
							}
						},
						{
							"name": "Set source container variable",
							"type": "SetVariable",
							"dependsOn": [
								{
									"activity": "Get Task Metadata",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"variableName": "containerName",
								"value": {
									"value": "@json(activity('Get Task Metadata').output.firstRow.task_config)[0].container_name",
									"type": "Expression"
								}
							}
						},
						{
							"name": "Set source file extension",
							"type": "SetVariable",
							"dependsOn": [
								{
									"activity": "Get Task Metadata",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"variableName": "sourceExtension",
								"value": {
									"value": "@json(activity('Get Task Metadata').output.firstRow.source_config)[0].file_extension",
									"type": "Expression"
								}
							}
						},
						{
							"name": "Set source file pattern",
							"type": "SetVariable",
							"dependsOn": [
								{
									"activity": "Get Task Metadata",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"variableName": "sourceFilePattern",
								"value": {
									"value": "@json(activity('Get Task Metadata').output.firstRow.source_config)[0].file_pattern",
									"type": "Expression"
								}
							}
						}
					]
				}
			},
			{
				"name": "Get Content List",
				"type": "IfCondition",
				"dependsOn": [
					{
						"activity": "Set worker variables",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"expression": {
						"value": "@equals(1,1)",
						"type": "Expression"
					},
					"ifTrueActivities": [
						{
							"name": "Get zipped content from container",
							"type": "GetMetadata",
							"dependsOn": [],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"dataset": {
									"referenceName": "ds_binary_file",
									"type": "DatasetReference",
									"parameters": {
										"container_name": {
											"value": "@variables('containerName')",
											"type": "Expression"
										},
										"folder_directory": {
											"value": "@variables('sourceFolderName')",
											"type": "Expression"
										},
										"file_name": {
											"value": "@''",
											"type": "Expression"
										}
									}
								},
								"fieldList": [
									"childItems"
								],
								"storeSettings": {
									"type": "AzureBlobFSReadSettings",
									"recursive": true,
									"enablePartitionDiscovery": false
								},
								"formatSettings": {
									"type": "BinaryReadSettings"
								}
							}
						},
						{
							"name": "Set childItemsList",
							"type": "SetVariable",
							"dependsOn": [
								{
									"activity": "Get zipped content from container",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"variableName": "childItemsList",
								"value": {
									"value": "@activity('Get zipped content from container').output.childItems",
									"type": "Expression"
								}
							}
						},
						{
							"name": "Filter on source file pattern",
							"type": "Filter",
							"dependsOn": [
								{
									"activity": "Filter on extension",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"items": {
									"value": "@activity('Filter on extension').output.value",
									"type": "Expression"
								},
								"condition": {
									"value": "@contains(item().name, variables('sourceFilePattern'))",
									"type": "Expression"
								}
							}
						},
						{
							"name": "Filter on extension",
							"type": "Filter",
							"dependsOn": [
								{
									"activity": "Set childItemsList",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"userProperties": [],
							"typeProperties": {
								"items": {
									"value": "@variables('childItemsList')",
									"type": "Expression"
								},
								"condition": {
									"value": "@endswith(item().name, variables('sourceExtension'))",
									"type": "Expression"
								}
							}
						}
					]
				}
			},
			{
				"name": "For each item in childItemsList",
				"type": "ForEach",
				"dependsOn": [
					{
						"activity": "Get Content List",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"userProperties": [],
				"typeProperties": {
					"items": {
						"value": "@activity('Filter on source file pattern').output.value",
						"type": "Expression"
					},
					"activities": [
						{
							"name": "unzip_folder",
							"type": "Copy",
							"dependsOn": [
								{
									"activity": "Log New File",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "BinarySource",
									"storeSettings": {
										"type": "AzureBlobFSReadSettings",
										"recursive": false,
										"wildcardFolderPath": {
											"value": "@variables('sourceFolderName')",
											"type": "Expression"
										},
										"wildcardFileName": {
											"value": "@item().name",
											"type": "Expression"
										},
										"deleteFilesAfterCompletion": false
									},
									"formatSettings": {
										"type": "BinaryReadSettings",
										"compressionProperties": {
											"type": "ZipDeflateReadSettings",
											"preserveZipFileNameAsFolder": true
										}
									}
								},
								"sink": {
									"type": "BinarySink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings",
										"copyBehavior": "PreserveHierarchy"
									}
								},
								"enableStaging": false
							},
							"inputs": [
								{
									"referenceName": "ds_binary_zip_folder",
									"type": "DatasetReference",
									"parameters": {
										"container_name": {
											"value": "@variables('containerName')",
											"type": "Expression"
										},
										"folder_directory": {
											"value": "@variables('sourceFolderName')",
											"type": "Expression"
										},
										"file_name": {
											"value": "@item().name",
											"type": "Expression"
										}
									}
								}
							],
							"outputs": [
								{
									"referenceName": "ds_binary_file",
									"type": "DatasetReference",
									"parameters": {
										"container_name": {
											"value": "@variables('containerName')",
											"type": "Expression"
										},
										"folder_directory": {
											"value": "@variables('sinkFolderName')",
											"type": "Expression"
										},
										"file_name": {
											"value": "@''",
											"type": "Expression"
										}
									}
								}
							]
						},
						{
							"name": "Log New File",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "meta.usp_new_file",
								"storedProcedureParameters": {
									"task_id": {
										"value": {
											"value": "@pipeline().parameters.TaskID",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"plan_id": {
										"value": {
											"value": "@pipeline().parameters.PlanID",
											"type": "Expression"
										},
										"type": "Int32"
									},
									"filename": {
										"value": {
											"value": "@item().name",
											"type": "Expression"
										},
										"type": "String"
									},
									"extended_filename": {
										"value": {
											"value": "@item().name",
											"type": "Expression"
										},
										"type": "String"
									},
									"info_message": {
										"value": {
											"value": "@concat('abfss://',\n        variables('containerName'),\n        '@',\n        variables('ENV_CODE'),\n        'dapstdala1.dfs.core.windows.net/',\n        variables('sourceFolderName'),\n        item().name)",
											"type": "Expression"
										},
										"type": "String"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "Log SUCCESS movement to ARCHIVE",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [
								{
									"activity": "Copy zipfolder to archive",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "meta.usp_update_file_activity",
								"storedProcedureParameters": {
									"extended_filename": {
										"value": {
											"value": "@item().name",
											"type": "Expression"
										},
										"type": "String"
									},
									"activity": {
										"value": "ARCHIVE",
										"type": "String"
									},
									"success": {
										"value": "true",
										"type": "Boolean"
									},
									"info_message": {
										"value": {
											"value": "@concat('abfss://',\n        'archive',\n        '@',\n        variables('ENV_CODE'),\n        'dapstdala1.dfs.core.windows.net/',\n        variables('sinkFolderName'),\n        item().name)",
											"type": "Expression"
										},
										"type": "String"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "Log FAILED movement to ARCHIVE_copy1",
							"type": "SqlServerStoredProcedure",
							"dependsOn": [
								{
									"activity": "unzip_folder",
									"dependencyConditions": [
										"Failed"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"storedProcedureName": "meta.usp_update_file_activity",
								"storedProcedureParameters": {
									"extended_filename": {
										"value": {
											"value": "@item().name",
											"type": "Expression"
										},
										"type": "String"
									},
									"activity": {
										"value": "ARCHIVE",
										"type": "String"
									},
									"success": {
										"value": "false",
										"type": "Boolean"
									},
									"info_message": {
										"value": {
											"value": "@concat('abfss://',\n        variables('containerName'),\n        '',\n        variables('ENV_CODE'),\n        'dapstdala1.dfs.core.windows.net/',\n        variables('sinkFolderName'),\n        item().name)",
											"type": "Expression"
										},
										"type": "String"
									}
								}
							},
							"linkedServiceName": {
								"referenceName": "ls_dap_sql_meta",
								"type": "LinkedServiceReference"
							}
						},
						{
							"name": "Copy zipfolder to archive",
							"type": "Copy",
							"dependsOn": [
								{
									"activity": "unzip_folder",
									"dependencyConditions": [
										"Succeeded"
									]
								}
							],
							"policy": {
								"timeout": "0.12:00:00",
								"retry": 0,
								"retryIntervalInSeconds": 30,
								"secureOutput": false,
								"secureInput": false
							},
							"userProperties": [],
							"typeProperties": {
								"source": {
									"type": "BinarySource",
									"storeSettings": {
										"type": "AzureBlobFSReadSettings",
										"recursive": true,
										"wildcardFolderPath": {
											"value": "@variables('sourceFolderName')",
											"type": "Expression"
										},
										"deleteFilesAfterCompletion": true
									},
									"formatSettings": {
										"type": "BinaryReadSettings"
									}
								},
								"sink": {
									"type": "BinarySink",
									"storeSettings": {
										"type": "AzureBlobFSWriteSettings",
										"copyBehavior": "PreserveHierarchy"
									}
								},
								"enableStaging": false
							},
							"inputs": [
								{
									"referenceName": "ds_binary_file",
									"type": "DatasetReference",
									"parameters": {
										"container_name": {
											"value": "@variables('containerName')",
											"type": "Expression"
										},
										"folder_directory": {
											"value": "@''",
											"type": "Expression"
										},
										"file_name": {
											"value": "@item().name",
											"type": "Expression"
										}
									}
								}
							],
							"outputs": [
								{
									"referenceName": "ds_binary_file",
									"type": "DatasetReference",
									"parameters": {
										"container_name": "archive",
										"folder_directory": {
											"value": "@variables('sourceFolderName')",
											"type": "Expression"
										},
										"file_name": {
											"value": "@''",
											"type": "Expression"
										}
									}
								}
							]
						}
					]
				}
			},
			{
				"name": "Set task to SUCCESS",
				"description": "Set the status of the batch in 'meta.batch_tracking' to \"success\"",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "For each item in childItemsList",
						"dependencyConditions": [
							"Succeeded"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "meta.usp_end_task",
					"storedProcedureParameters": {
						"task_id": {
							"value": {
								"value": "@pipeline().parameters.TaskID",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"plan_id": {
							"value": {
								"value": "@pipeline().parameters.PlanID",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"success_flag": {
							"value": "true",
							"type": "Boolean"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			},
			{
				"name": "Set task to failed",
				"description": "Set the status of the batch in 'meta.batch_tracking' to \"failed\"",
				"type": "SqlServerStoredProcedure",
				"dependsOn": [
					{
						"activity": "For each item in childItemsList",
						"dependencyConditions": [
							"Failed"
						]
					}
				],
				"policy": {
					"timeout": "0.12:00:00",
					"retry": 0,
					"retryIntervalInSeconds": 30,
					"secureOutput": false,
					"secureInput": false
				},
				"userProperties": [],
				"typeProperties": {
					"storedProcedureName": "meta.usp_end_task",
					"storedProcedureParameters": {
						"task_id": {
							"value": {
								"value": "@pipeline().parameters.TaskID",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"plan_id": {
							"value": {
								"value": "@pipeline().parameters.PlanID",
								"type": "Expression"
							},
							"type": "Int32"
						},
						"success_flag": {
							"value": "false",
							"type": "Boolean"
						}
					}
				},
				"linkedServiceName": {
					"referenceName": "ls_dap_sql_meta",
					"type": "LinkedServiceReference"
				}
			}
		],
		"parameters": {
			"TaskID": {
				"type": "string",
				"defaultValue": "4"
			},
			"PlanID": {
				"type": "string",
				"defaultValue": "2"
			}
		},
		"variables": {
			"containerName": {
				"type": "String"
			},
			"sourceFolderName": {
				"type": "String"
			},
			"sinkFolderName": {
				"type": "String"
			},
			"sourceExtension": {
				"type": "String"
			},
			"sourceFilePattern": {
				"type": "String"
			},
			"childItemsList": {
				"type": "Array"
			},
			"ENV_CODE": {
				"type": "String",
				"defaultValue": "dev"
			}
		},
		"folder": {
			"name": "Workers"
		},
		"annotations": []
	}
}
</file>
<file name="src\synapse\studio\sparkConfiguration\core_configuration.json">
{
	"name": "core_configuration",
	"properties": {
		"description": "Configuration file with environment variables that will be used across the notebooks used in the ingestion process of the Synapse core workspace",
		"configs": {
			"spark.environment_code": "dev",
			"spark.sql.legacy.timeParserPolicy": "LEGACY"
		},
		"created": "2024-04-11T13:55:30.6290000+02:00",
		"createdBy": "PLA917@nationale-loterij.be",
		"annotations": [],
		"configMergeRule": {
			"artifact.currentOperation.spark.environment_code": "replace"
		}
	}
}
</file>
<file name="src\synapse\studio\sparkConfiguration\core_configuration.md">
**Understanding spark.sql.legacy.timeParserPolicy**

The spark.sql.legacy.timeParserPolicy setting in Spark determines how to parse timestamps in SQL queries. This setting was introduced in Spark 2.3.0 to provide more flexibility and control over timestamp parsing.

**Options for spark.sql.legacy.timeParserPolicy**

There are three options for this setting:

**1. LEGACY**

**Behavior**: This is the default behavior in Spark 2.2 and earlier versions. It uses the java.sql.Timestamp class to parse timestamps, which can lead to incorrect results for certain timestamp formats.

**Advantages**: 
- None significant.

**Drawbacks**:
- Can lead to incorrect results for certain timestamp formats.
- Does not support parsing timestamps with timezone information.

**2. CORRECTED**

**Behavior**: This option uses the java.time.Instant class to parse timestamps, which provides more accurate and correct results.

**Advantages**:
- Provides more accurate and correct results for timestamp parsing.
- Supports parsing timestamps with timezone information.

**Drawbacks**:
- Can break existing queries that rely on the legacy behavior.
- May require changes to existing code or queries.

**2. EXCEPTION**

**Behavior**: This option throws an exception when it encounters a timestamp that cannot be parsed using the java.time.Instant class.

**Advantages**:
- Provides more accurate and correct results for timestamp parsing.
- Supports parsing timestamps with timezone information.
- Helps to identify and fix issues with timestamp parsing in queries.

**Drawbacks**:
- Can break existing queries that rely on the legacy behavior.
- May require changes to existing code or queries.

**Decision**
Whilst the desciption above would seem to favor EXCEPTION and CORRECTED, the LEGACY option has been chosen. This is because the jave.time.Instant class converts 4-digit integers to timestamps by adding default values. That is, if value '2024' is being passed into a timestamp column, spark will format is as '2024-01-01 00:00:00.0000' instead of throwing an error. Whilst this is still a feature/bug, the option for LEGACY remains the most appropriate.
</file>
<file name="src\synapse\_test\metadata\datasets\filworker_pivot.json">
{
  "datasets": [
    {
      "name": "filworker_pivot",
      "description": "Test and see if a file with .fil gets properly turned into a csv",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "FilWorker",
      "ingestion": {
        "target_table": "preprocess_test/filworker_test/preprocess",
        "source_folder": "preprocess_test/filworker_test",
        "container": "landing",
        "match_pattern": "firstrow_to_headers.fil",
        "extension": "fil",
        "encoding": "UTF-8",
        "column_delimiter": "\t",
        "row_delimiter": "",
        "escape_character": "",
        "quote_character": "",
        "header_line": false
      },
      "columns": [
        {
          "sequence": 1,
          "name": "FK_CONNECTION",
          "dimension": "PK",
          "data_type": "integer"
        },
        {
          "sequence": 2,
          "name": "FK_GAME",
          "dimension": "PK",
          "data_type": "integer"
        },
        {
          "sequence": 3,
          "name": "CAT_CLASS",
          "dimension": "PK",
          "data_type": "integer"
        },
        {
          "sequence": 4,
          "name": "NB_PAYMENT",
          "data_type": "integer"
        },
        {
          "sequence": 5,
          "name": "AMT_PAYMENT",
          "data_type": "decimal"
        },
        {
          "sequence": 6,
          "name": "X_DT_EXTRACT",
          "data_type": "integer"
        },
        {
          "sequence": 7,
          "name": "X_DT_TRANSACTION",
          "data_type": "integer"
        },
        {
          "sequence": 8,
          "name": "DRAWNUMBER",
          "data_type": "integer"
        }
      ]
    }
  ]
}

</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_checks_headerless.json">
{
  "datasets": [
    {
      "name": "ingestionworker_checks_headerless",
      "description": "Test and see if everything works as it should for a headerless dataset",
      "kind": "csv",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "IngestionWorker",

      "ingestion": {
        "target_table": "unittest_spark_checks_headerless",
        "source_folder": "ingestion_test",
        "container": "landing",
        "match_pattern": "headerless_header_check.csv",
        "extension": "csv",
        "encoding": "UTF-8",
        "column_delimiter": ",",
        "row_delimiter": "",
        "escape_character": "",
        "quote_character": "",
        "header_line": false
      },
      "checks": [
        {
          "name": "header"
        }
      ],
      "columns": [
        {
          "sequence": 1,
          "name": "character_name",
          "dimension": "PK"
        },
        {
          "sequence": 2,
          "name": "level",
          "data_type": "integer"
        },
        {
          "sequence": 3,
          "name": "race"
        },
        {
          "sequence": 4,
          "name": "class"
        },
        {
          "sequence": 5,
          "name": "guild"
        }
      ]
    }
  ]
}

</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_defaultreplace.json">
{

    "datasets": [
    {
          "name": "ingestionworker_csv_defaultreplace",
          "description": "Execute csv integration test file",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "csv_defaultreplace",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "csv_defaultreplace.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "checks": [
            {
              "name": "header"
            },
            {
              "name": "data_type"
            },
            {
              "name": "default_replace"
            }
          ],
          "columns": [
              {   "sequence":  1, "name": "Column_1" ,     "sink_name": "column_1",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_2" ,     "sink_name": "column_2",   "data_type": "timestamp"  },
              {   "sequence":  3, "name": "Column_3",      "sink_name": "column_3",   "data_type": "date"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_recurse_search.json">
{

    "datasets": [
    {
          "name": "ingestionworker_csv_recurse_search",
          "description": "Execute csv integration test file and test if the structure is checked recursively",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "csv_recurse_search",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "recurse.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "columns": [
              {   "sequence":  1, "name": "Column_A" ,     "sink_name": "column_a",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_B" ,     "sink_name": "column_b",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column_C",     "sink_name": "column_c",    "data_type": "string"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_specialstring.json">
{

    "datasets": [
    {
          "name": "ingestionworker_csv_specialstring",
          "description": "Execute csv integration test file",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "csv_specialstring",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "csv_specialstring.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ",",
            "row_delimiter": "",
            "escape_character": "\"",
            "quote_character": "\"",
            "header_line": true
          },
          "columns": [
              {   "sequence":  1, "name": "column_1" ,    "sink_name": "column_1",   "data_type": "integer"},
              {   "sequence":  2, "name": "column_2" ,    "sink_name": "column_2",   "data_type": "string"  },
              {   "sequence":  3, "name": "column_3",     "sink_name": "column_3",   "data_type": "string", "dimension": "PK" }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_task.json">
{

    "datasets": [
    {
          "name": "ingestionworker_csv_task",
          "description": "Execute csv integration test file",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "csv_integration",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": ".*(csv_test).*",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "checks": [
            {
              "name": "landing_rows",
              "config_params": "{\"landing_rows_expected\": 3}"
            },
            {
              "name": "header"
            },
            {
              "name": "data_type"
            }
          ],
          "columns": [
              {   "sequence":  1, "name": "Column_1" ,     "sink_name": "column_1",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_2" ,     "sink_name": "column_2",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column_3",     "sink_name": "column_3",    "data_type": "string"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_task_replace_null_pk.json">
{

    "datasets": [
    {
          "name": "ingestionworker_csv_task_replace_null_pk",
          "description": "Execute csv integration test file",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "csv_integration_replace_value",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": ".*(csv_replace_value).*",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "checks": [
            {
              "name": "replace_primary_key"
            }
          ],
          "columns": [
              {   "sequence":  1, "name": "Column_1" ,     "sink_name": "column_1",   "data_type": "string", "dimension": "PK" , "column_info":{"replace_value":"ERROR"}},
              {   "sequence":  2, "name": "Column_2" ,     "sink_name": "column_2",   "data_type": "integer" },
              {   "sequence":  3, "name": "Column_3",     "sink_name": "column_3",    "data_type": "string", "dimension": "PK" }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_csv_task_vacuum.json">
{

    "datasets": [
    {
          "name": "ingestionworker_csv_task_vacuum",
          "description": "Execute csv integration test file",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "csv_integration_vacuum",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": ".*(csv_test_vacuum).*",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true,
            "target_options": {
              "retention_time":170
            }
          }
          ,
          "columns": [
              {   "sequence":  1, "name": "Column_1" ,     "sink_name": "column_1",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_2" ,     "sink_name": "column_2",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column_3",     "sink_name": "column_3",    "data_type": "string"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_datatypes.json">
{

    "datasets": [
    {
          "name": "ingestionworker_datatypes",
          "description": "Deploy a delta table with all data types",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "datatype_table",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "datatype_test.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "checks": [
            {
            "name": "data_type"
            }
          ],
          "columns": [
              {   "sequence":  1, "name": "StringColumn" ,     "sink_name": "string_column",    "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "IntegerColumn" ,     "sink_name": "integer_column",  "data_type": "integer"  },
              {   "sequence":  3, "name": "DecimalColumn",     "sink_name": "decimal_column",   "data_type": "decimal(12,5)"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_formatting.json">
{

  "datasets": [
  {
        "name": "ingestionworker_formatting_task",
        "description": "Execute formatting integration test file",
        "kind": "csv",
        "task_type": "SPARK_NOTEBOOK",
        "worker" : "IngestionWorker",
        
        "ingestion": {
          "target_table": "formatting_integration",
          "source_folder": "ingestion_test",
          "container" : "landing",
          "match_pattern": "formatting_test.csv",
          "extension": "csv",
          "encoding": "UTF-8",
          "column_delimiter": ";",
          "row_delimiter": "",
          "escape_character": "",
          "quote_character": "",
          "header_line": true
        },
        "checks": [
          {
            "name": "header"
          },
          {
            "name": "data_type"
          }
        ],
        "columns": [
            {   
              "sequence":  1, 
              "name": "primary_key",  
              "sink_name": "primary_key", 
              "data_type": "integer", 
              "dimension": "PK"
            },
            {   
              "sequence":  2, 
              "name": "decimal_comma",  
              "sink_name": "decimal_comma", 
              "data_type": "decimal(4,2)", 
              "dimension": "SCD2" ,
              "column_info":{
                "decimal_separator": ","
              }
            },
            {   
              "sequence":  3, 
              "name": "decimal_dot",    
              "sink_name": "decimal_dot",   
              "data_type": "decimal(4,2)", 
              "dimension": "SCD2"
            },
            {   
              "sequence":  4, 
              "name": "timestamp_us",  
              "sink_name": "timestamp_us", 
              "data_type": "timestamp", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "yyyy-MM-dd HH:mm:ss"
              }
            },
            {   
              "sequence":  5, 
              "name": "timestamp_eu",  
              "sink_name": "timestamp_eu", 
              "data_type": "timestamp", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "dd-MM-yyyy HH:mm:ss"
              }
            },
            {   
              "sequence":  6, 
              "name": "date_us",  
              "sink_name": "date_us", 
              "data_type": "date", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "yyyy-MM-dd"
              }
            },
            {   
              "sequence":  7, 
              "name": "date_eu",  
              "sink_name": "date_eu", 
              "data_type": "date", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "dd-MM-yyyy"
              }
            },
            {   
              "sequence":  8, 
              "name": "date_nosep",  
              "sink_name": "date_nosep", 
              "data_type": "date", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "ddMMyyyy"
              }
            },
            {   
              "sequence":  9, 
              "name": "date_us_slash",  
              "sink_name": "date_us_slash", 
              "data_type": "date", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "yyyy/MM/dd"
              }
            },
            {   
              "sequence":  10, 
              "name": "date_eu_slash",  
              "sink_name": "date_eu_slash", 
              "data_type": "date", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "dd/MM/yyyy"
              }
            },
            {   
              "sequence":  11, 
              "name": "unix_format",  
              "sink_name": "unix_format", 
              "data_type": "timestamp", 
              "dimension": "SCD2" ,
              "column_info":{
                "format": "epoch"
              }
            }
        ]
    }
]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_json_task.json">
{
  "datasets": [
    {
      "name": "ingestionworker_json_task",
      "description": "Execute json integration test file",
      "kind": "json",
      "task_type": "SPARK_NOTEBOOK",
      "worker": "IngestionWorker",

      "ingestion": {
        "target_table": "json_integration",
        "source_folder": "ingestion_test/",
        "container": "landing",
        "match_pattern": ".*(json_test).*",
        "extension": "json",
        "encoding": "UTF-8",
        "column_delimiter": ",",
        "row_delimiter": "",
        "escape_character": "",
        "quote_character": "",
        "header_line": true
      },
      "columns": [
        {
          "sequence": 1,
          "name": "id",
          "dimension":"PK",
          "data_type": "integer"
        },
        {
          "sequence": 2,
          "name": "data",
          "sink_name": "employees",
          "date_type": "string"
        },
        {
          "sequence": 3,
          "name": "company",
          "data_type": "string"
        }
      ]
    }
  ]
}

</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_optional_columns.json">
{

    "datasets": [
    {
          "name": "ingestionworker_optional_columns",
          "description": "Execute optional_columns test",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "options_optional_columns",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "csv_optional_columns.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "columns": [
              {   
                "sequence":  1, 
                "name": "Column_1" ,    
                "sink_name": "column_1",   
                "data_type": "string", 
                "dimension": "PK" 
              },
              {   
                "sequence":  2, 
                "name": "Column_2" ,    
                "sink_name": "column_2",   
                "data_type": "string"
              },
              {   
                "sequence":  3, 
                "name": "Column_3",     
                "sink_name": "column_3",    
                "data_type": "string",
                "column_info": {
                  "optional": true
                }
              }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_options_partitioning.json">
{

    "datasets": [
    {
          "name": "ingestionworker_options_partitioning",
          "description": "Execute csv partitioning to validate the workings of the partitioning-functionality in integration-mode",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "options_partitioning",
            "target_options": {
              "partitioning": [
                {
                  "name": "column_c",
                  "sequence": 1,
                  "datePart": "year"
                },
                {
                  "name": "column_c",
                  "sequence": 2,
                  "datePart": "month"
                }
              ]
            },
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "csv_partitioning.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "columns": [
              {   "sequence":  1, "name": "Column A" ,     "sink_name": "column_a",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column B" ,     "sink_name": "column_b",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column C",     "sink_name": "column_c",    "data_type": "date"     }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_options_partitioning_on_filename.json">
{

    "datasets": [
    {
          "name": "ingestionworker_options_partitioning_on_filename",
          "description": "Execute csv partitioning to validate the workings of the partitioning-functionality in integration-mode",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "options_filename_partitioning",
            "target_options": {
              "partitioning": [
                {
                  "name": "t_extract_date",
                  "sequence": 1,
                  "datePart": "year"
                },
                {
                  "name": "t_extract_date",
                  "sequence": 2,
                  "datePart": "month"
                },
                {
                  "name": "t_extract_date",
                  "sequence": 3,
                  "datePart": "day"
                }
              ],
              "extract_date": {
                "column_name": "t_file_name",
                "regex_expression": "_(\\d{8})",
                "extract_date_format": "yyyyMMdd"
              }
            },
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "(partition_on_filename_\\d{8}).csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "columns": [
              {   "sequence":  1, "name": "Column_A" ,     "sink_name": "column_a",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_B" ,     "sink_name": "column_b",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column_C",     "sink_name": "column_c",    "data_type": "string"     }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_parquet_task.json">
{

    "datasets": [
    {
          "name": "ingestionworker_parquet_task",
          "description": "Execute parquet integration test file",
          "kind": "parquet",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "parquet_integration",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "parquet_test.parquet",
            "extension": "parquet",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true
          },
          "checks": [
            {
              "name": "landing_rows",
              "config_params": "{\"landing_rows_expected\": 3}"
            },
            {
              "name": "header"
            },
            {
              "name": "data_type"
            }
          ],
          "columns": [
              {   "sequence":  1, "name": "Column_1" ,     "sink_name": "column_1",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_2" ,     "sink_name": "column_2",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column_3",     "sink_name": "column_3",    "data_type": "string"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\ingestionworker_skiplines.json">
{

    "datasets": [
    {
          "name": "ingestionworker_skiplines",
          "description": "Execute csv skiplines to validate the skip_first_lines parameter functionality in integration-mode",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "IngestionWorker",
          
          "ingestion": {
            "target_table": "skiplines",
            "source_folder": "ingestion_test",
            "container" : "landing",
            "match_pattern": "csv_skiplines.csv",
            "extension": "csv",
            "encoding": "UTF-8",
            "column_delimiter": ";",
            "row_delimiter": "",
            "escape_character": "",
            "quote_character": "",
            "header_line": true,
            "skip_first_lines": 4
          },
          "checks": [
            {
              "name": "header"
            }
          ],
          "columns": [
              {   "sequence":  1, "name": "Column_A" ,     "sink_name": "column_a",   "data_type": "string", "dimension": "PK" },
              {   "sequence":  2, "name": "Column_B" ,     "sink_name": "column_b",   "data_type": "integer"  },
              {   "sequence":  3, "name": "Column_C",     "sink_name": "column_c",    "data_type": "string"  }
          ]
      }
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\preprocess_test.json">
{
  "preprocess": [
    {
      "name": "preprocess_test",
      "description": "Test the flow of the preprocess synapse workspace",
      "task_type": "SYNAPSE_PIPELINE",
      "worker_pipeline": "pl_preprocess_template",
      "source_pattern": "preprocessing"
    }
  ]
}

</file>
<file name="src\synapse\_test\metadata\datasets\unittest_spark_dummy_task.json">
{
    "datasets": [
    {
          "name": "unittest_spark_dummy_task",
          "description": "",
          "kind": "csv",
          "task_type": "SPARK_NOTEBOOK",
          "worker" : "DummyWorker",

          "ingestion": {
            "target_table": "mandatory_field",
            "source_folder": "mandatory_field",
            "match_pattern": "mandatory_field",
            "extension": "csv"
          }
      }
  
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\unittest_synapse_dummy_task.json">
{
    "datasets": [
    {
        "name": "unittest_synapse_dummy_task",
        "description": "Test dummy invocation",
        "kind": "csv",
        "task_type": "SYNAPSE_PIPELINE",
        "worker": "pl_dummy_worker",

        "ingestion": {
          "target_table": "mandatory_field",
          "source_folder": "mandatory_field",
          "match_pattern": "mandatory_field",
          "extension": "csv"
        }
      }
  
  ]
}
</file>
<file name="src\synapse\_test\metadata\datasets\unittest_synapse_unzip_gzfolder.json">
{
  "datasets": [
    {
      "name": "unittest_synapse_unzip_gzfolder",
      "description": "Execute unzip of a zipped unittest folder with .gz extension",
      "kind": "zip",
      "task_type": "SYNAPSE_PIPELINE",
      "worker": "pl_unzip_worker",

      "ingestion": {
        "target_table": "preprocess_test/unzipped_test_gz",
        "source_folder": "preprocess_test/zipped_content_test",
        "container": "landing",
        "match_pattern": "gzipped_landing",
        "extension": "gz"
      }
    }
  ]
}

</file>
<file name="src\synapse\_test\metadata\datasets\unittest_synapse_unzip_zipfolder.json">
{
  "datasets": [
    {
      "name": "unittest_synapse_unzip_zipfolder",
      "description": "Execute unzip of a zipped unittest folder with .zip extension",
      "kind": "zip",
      "task_type": "SYNAPSE_PIPELINE",
      "worker": "pl_unzip_worker",

      "ingestion": {
        "target_table": "preprocess_test/unzipped_test_zip",
        "source_folder": "preprocess_test/zipped_content_test",
        "container": "landing",
        "match_pattern": "zipped_landing",
        "extension": "zip"
      }
    }
  ]
}

</file>
<file name="src\synapse\_test\metadata\plantasks\preprocessing.json">
{
    "plans": [

        {
            "name": "preprocessing",
            "description": "Test preprocessing in dev and int",
            "environments" : "dev|int",
            "enabled": true,
            "task_groups": [
                {
                    "name": "PREPROCESS",
                    "tasks": [
                        {"name": "preprocess_test", "sequence": 1, "enabled": true}
                    ]
                }
            ]
        }
    ]
}

</file>
<file name="src\synapse\_test\metadata\plantasks\unittest_filworker.json">
{
    "plans": [

        {
            "name": "unittest_filworker",
            "description": "Validate FilWorker",
            "environments" : "dev|int",
            "enabled": true,
            "task_groups": [
                {
                    "name": "PREPROCESS",
                    "tasks": [
                        {"name": "filworker_pivot", "sequence": 1, "enabled": true}
                    ]
                }
            ]
        }
    ]
}

</file>
<file name="src\synapse\_test\metadata\plantasks\unittest_ingestionworker.json">
{
    "plans": [

        {
            "name": "unittest_ingestionworker",
            "description": "Validate IngestionWorker",
            "environments" : "dev|int",
            "enabled": true,
            "task_groups": [
                {
                    "name": "INGEST",
                    "tasks": [
                        { "name": "ingestionworker_csv_task",                           "sequence": 1,  "enabled": true },
                        { "name": "ingestionworker_json_task",                          "sequence": 2,  "enabled": true },
                        { "name": "ingestionworker_csv_recurse_search",                 "sequence": 3,  "enabled": true },
                        { "name": "ingestionworker_checks_headerless",                  "sequence": 4,  "enabled": true },
                        { "name": "ingestionworker_parquet_task",                       "sequence": 5,  "enabled": true },
                        { "name": "ingestionworker_datatypes",                          "sequence": 6,  "enabled": true },
                        { "name": "ingestionworker_options_partitioning",               "sequence": 7,  "enabled": true },
                        { "name": "ingestionworker_skiplines",                          "sequence": 8,  "enabled": true },
                        { "name": "ingestionworker_csv_defaultreplace",                 "sequence": 9,  "enabled": true },
                        { "name": "ingestionworker_formatting_task",                    "sequence": 10, "enabled": true },
                        {"name": "ingestionworker_csv_specialstring",                   "sequence": 11, "enabled": true},
                        {"name": "ingestionworker_options_partitioning_on_filename",    "sequence": 12, "enabled": true},
                        {"name": "ingestionworker_optional_columns",                    "sequence": 13, "enabled": true},
                        {"name": "ingestionworker_csv_task_replace_null_pk",            "sequence": 14, "enabled": true},
                        {"name": "ingestionworker_csv_task_vacuum",                     "sequence": 15, "enabled": true}
                    ]
                }
            ]
        }
    ]
}

</file>
<file name="src\synapse\_test\metadata\plantasks\unittest_spark_dummy.json">
{
    "plans": [

        {
            "name": "unittest_spark_dummy",
            "description": "Check if spark meta notebook structure works",
            "environments" : "dev|int",
            "enabled": true,
            "task_groups": [
                {
                    "name": "DUMMY",
                    "tasks": [
                        { "name": "unittest_spark_dummy_task",  "sequence": 1,  "enabled": true }
                    ]
                }
            ]
        }
    ]
}


</file>
<file name="src\synapse\_test\metadata\plantasks\unittest_synapse_dummy.json">
{
    "plans": [

        {
            "name": "unittest_synapse_dummy",
            "description": "Check if synapse meta pipeline structure works",
            "environments" : "dev|int",
            "enabled": true,
            "task_groups": [
                {
                    "name": "DUMMY",
                    "tasks": [
                        { "name": "unittest_synapse_dummy_task",  "sequence": 1,  "enabled": true }
                    ]
                }
            ]
        }
    ]
}


</file>
<file name="src\synapse\_test\metadata\plantasks\unittest_synapse_unzip.json">
{
  "plans": [
    {
      "name": "unittest_synapse_unzip",
      "description": "Check if synapse pipeline pl_unzip_worker works",
      "environments": "dev|int",
      "enabled": true,
      "task_groups": [
        {
          "name": "PREPROCESS",
          "tasks": [
            { "name": "unittest_synapse_unzip_zipfolder", "sequence": 1, "enabled": true },
            { "name": "unittest_synapse_unzip_gzfolder", "sequence": 2, "enabled": true }
          ]
        }
      ]
    }
  ]
}

</file>
<file name="src\synapse\_test\test_files\fulltest_files\ingestion_test\20240802_112048\json_test.json">
{"id": 123, "data": [{"name": "XXX", "function": "External"}, {"name": "YYY", "function": "Internal"}], "company": "ZZZ"}
</file>
<file name="src\synapse\_test\test_files\fulltest_files\ingestion_test\20240802_112048\json_test_multiline.json">
{
    "id": 126,
    "data": [
        {
            "name": "XXX",
            "function": "External"
        },
        {
            "name": "YYY",
            "function": "Internal"
        }
    ],
    "company": "CCC"
}
</file>
<file name="src\synapse\_test\test_files\fulltest_files\ingestion_test\20240802_112048\json_test_multiobject.json">
[
    {
        "id": 125,
        "data": [
            {
                "name": "XXX",
                "function": "External"
            },
            {
                "name": "YYY",
                "function": "Internal"
            }
        ],
        "company": "BBB"
    },
    {
        "id": 124,
        "data": [
            {
                "name": "XXX",
                "function": "External"
            },
            {
                "name": "YYY",
                "function": "Internal"
            }
        ],
        "company": "AAA"
    }
]

</file>
<file name="src\synapse\_test\test_files\unittest_files\test1_root.txt">
This is test file
</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\archive\placeholder.txt">
Don't delete this file, it's to make sure that the folder is actually created with the PWS script.
</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\landing\move_to_raw\MOCK_MULTILINE_move.json">
{
    "id": 126,
    "data": [
        {
            "name": "XXX",
            "function": "External"
        },
        {
            "name": "YYY",
            "function": "Internal"
        }
    ],
    "company": "CCC"
}
</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\raw\placeholder.txt">
Don't delete this file, it's to make sure that the folder is actually created with the PWS script.

</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\ANewFile.txt.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\Test.txt.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\Treasure.txt.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\Classes\FileMovement\static\Worked.txt.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\list_directory_content\root.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\list_directory_content\parent\child\child.json">

</file>
<file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\other_subfolder\filtered\included.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\other_subfolder\ignored\skipped.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\subfolder\filtered\included.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\GenericFunctions\locate_static_path\subfolder\ignored\skipped.txt">

</file>
<file name="src\synapse\_test\test_files\unittest_files\SparkDataFrameClass\load_dataframe\load_json_test\json_test_multiobject.json">
[
    {
        "id": 125,
        "data": [
            {
                "name": "XXX",
                "function": "External"
            },
            {
                "name": "YYY",
                "function": "Internal"
            }
        ],
        "company": "BBB"
    },
    {
        "id": 124,
        "data": [
            {
                "name": "XXX",
                "function": "External"
            },
            {
                "name": "YYY",
                "function": "Internal"
            }
        ],
        "company": "AAA"
    },
    {
        "id": 127,
        "data": [
            {
                "name": "AAA",
                "function": "External"
            },
            {
                "name": "BBB",
                "function": "Internal"
            }
        ],
        "company": "CCC"
    }
]

</file>
<file name="src\synapse\_test\test_files\unittest_files\unittest\test1_unittest.txt">
This is a test file
</file>
<file name="src\synapse\_test\test_files\unittest_files\unittest\env\test1_env.txt">
This is a test file
</file>
<file name="src\synapse\_test\test_files\unittest_files\unittest\env\timestamp\test1_timestamp.txt">
This is a test file
</file>
<file name="src\synapse\_test\test_scripts\sql\filworker_preprocess_logcheck.sql">
-- Check for all files present in file logging tables (For preprocess task in FilWorker)
-- Dev note: Include failing tests
DECLARE @error_message NVARCHAR(max);

DECLARE @expected_count int;
DECLARE @expected_status NVARCHAR(max);

DECLARE @actual_count int;
DECLARE @actual_status NVARCHAR(max);

SET @expected_count = 1;
SET @expected_status = 'SUCCEEDED'

SET @actual_count = (SELECT count(*) FROM meta.log_files WHERE [filename] like 'firstrow_to_headers%')
SET @actual_status = (SELECT archive_status FROM meta.log_files where [filename] like 'firstrow_to_headers%')

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of files in the log files table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for preprocess task of FilWorker.');
    throw 50000, @error_message, 1;
end

if @expected_status &lt;&gt; @actual_status
begin
    set @error_message = concat('The expected ingestion status in the log files table (', isnull(@expected_status,'NULL'), ') does not equal the actual status (', isnull(@actual_status,'NULL'), ') for preprocess task of FilWorker.');
    throw 500000, @error_message, 1;
end 
</file>
<file name="src\synapse\_test\test_scripts\sql\ingestionworker_csv_logcheck.sql">
-- Check for all files present in file logging tables (for CSV only)
-- Dev note: Include failing tests
DECLARE @error_message NVARCHAR(max);

DECLARE @expected_count int;
DECLARE @expected_status NVARCHAR(max);

DECLARE @actual_count int;
DECLARE @actual_status NVARCHAR(max);

DECLARE @expected_updated_row int;
DECLARE @expected_inserted_row int;
DECLARE @expected_source_row int;

DECLARE @actual_updated_row int;
DECLARE @actual_inserted_row int;
DECLARE @actual_source_row int;

SET @expected_count = 1;
SET @expected_status = 'SUCCEEDED'

SET @expected_updated_row = 2;
SET @expected_inserted_row = 1;
SET @expected_source_row =3;

SET @actual_count = (SELECT count(*) FROM meta.log_files WHERE [filename] like 'csv_test')
SET @actual_status = (SELECT archive_status FROM meta.log_files where [filename] like 'csv_test')


SET @actual_updated_row = (SELECT  JSON_VALUE(silver_info,'$.row_updated') FROM meta.log_files WHERE [filename] = 'csv_test_t2')
SET @actual_inserted_row = (SELECT  JSON_VALUE(silver_info,'$.row_inserted') FROM meta.log_files WHERE [filename] = 'csv_test_t2')
SET @actual_source_row = (SELECT  JSON_VALUE(silver_info,'$.source_row') FROM meta.log_files WHERE [filename] = 'csv_test_t2')

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of files in the log files table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_task.');
    throw 50000, @error_message, 1;
end

if @expected_status &lt;&gt; @actual_status
begin
    set @error_message = concat('The expected ingestion status in the log files table (', isnull(@expected_status,'NULL'), ') does not equal the actual status (', isnull(@actual_status,'NULL'), ') for csv_task.');
    throw 500000, @error_message, 1;
end 

if @expected_updated_row &lt;&gt; @actual_updated_row
begin
    set @error_message = concat('The expected number of updated rows in the log files table (', isnull(@expected_updated_row,'NULL'), ') does not equal the actual count (', isnull(@actual_updated_row,'NULL'), ') for csv_test_t2.');
    throw 50000, @error_message, 1;
end

if @expected_inserted_row &lt;&gt; @actual_inserted_row
begin
    set @error_message = concat('The expected number of inserted rows in the log files table (', isnull(@expected_inserted_row,'NULL'), ') does not equal the actual count (', isnull(@actual_inserted_row,'NULL'), ') for csv_test_t2.');
    throw 50000, @error_message, 1;
end

if @expected_source_row &lt;&gt; @actual_source_row
begin
    set @error_message = concat('The expected number of rows in the log files table (', isnull(@expected_source_row,'NULL'), ') does not equal the actual count (', isnull(@actual_source_row,'NULL'), ') for csv_test_t2.');
    throw 50000, @error_message, 1;
end


</file>
<file name="src\synapse\_test\test_scripts\sql\ingestionworker_csv_recurse_logcheck.sql">
-- Check for all files present in file logging tables (for CSV_STRUCT only)
-- Dev note: Include failing tests
DECLARE @error_message NVARCHAR(max);

DECLARE @expected_count int;
DECLARE @expected_status NVARCHAR(max);

DECLARE @actual_count int;
DECLARE @actual_status NVARCHAR(max);

SET @expected_count = 1;
SET @expected_status = 'SUCCEEDED'

SET @actual_count = (SELECT count(*) FROM meta.log_files WHERE [filename] like 'recurse')
SET @actual_status = (SELECT archive_status FROM meta.log_files where [filename] like 'recurse')

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of files in the log files table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for recurse.');
    throw 50000, @error_message, 1;
end

if @expected_status &lt;&gt; @actual_status
begin
    set @error_message = concat('The expected ingestion status in the log files table (', isnull(@expected_status,'NULL'), ') does not equal the actual status (', isnull(@actual_status,'NULL'), ') for recurse.');
    throw 500000, @error_message, 1;
end 
</file>
<file name="src\synapse\_test\test_scripts\sql\ingestionworker_json_logcheck.sql">
-- Check for all files present in logging tables (for JSON only)
-- Dev note: Include failing tests
DECLARE @error_message NVARCHAR(max);

DECLARE @expected_count int;
DECLARE @expected_status NVARCHAR(max);

DECLARE @actual_count int;
DECLARE @actual_status NVARCHAR(max);

SET @expected_count = 1;
SET @expected_status = 'SUCCEEDED'

SET @actual_count = (SELECT count(*) FROM meta.log_files WHERE [filename] like 'json_test')
SET @actual_status = (SELECT archive_status FROM meta.log_files where [filename] like 'json_test')

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of files in the log files table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for json_task.');
    throw 50000, @error_message, 1;
end

if @expected_status &lt;&gt; @actual_status
begin
    set @error_message = concat('The expected ingestion status in the log files table (', isnull(@expected_status,'NULL'), ') does not equal the actual status (', isnull(@actual_status,'NULL'), ') for json_task.');
    throw 500000, @error_message, 1;
end 
</file>
<file name="src\synapse\_test\test_scripts\sql\ingestionworker_parquet_logcheck.sql">
-- Check for all files present in file logging tables (for PARQUET only)
-- Dev note: Include failing tests
DECLARE @error_message NVARCHAR(max);

DECLARE @expected_count int;
DECLARE @expected_status NVARCHAR(max);

DECLARE @actual_count int;
DECLARE @actual_status NVARCHAR(max);

SET @expected_count = 1;
SET @expected_status = 'SUCCEEDED'

SET @actual_count = (SELECT count(*) FROM meta.log_files WHERE [filename] like 'parquet_test')
SET @actual_status = (SELECT archive_status FROM meta.log_files where [filename] like 'parquet_test')

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of files in the log files table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for parquet_test.');
    throw 50000, @error_message, 1;
end

if @expected_status &lt;&gt; @actual_status
begin
    set @error_message = concat('The expected ingestion status in the log files table (', isnull(@expected_status,'NULL'), ') does not equal the actual status (', isnull(@actual_status,'NULL'), ') for parquet_test.');
    throw 500000, @error_message, 1;
end 
</file>
<file name="src\synapse\_test\test_scripts\sql\spark_dummy_notebook.sql">
-- Query which will return an error when the specific logs can't be found in the tables
-- Will check for execution of DummyWorker inside of MetaNotebook

DECLARE @error_message		NVARCHAR(MAX);
DECLARE @succeeded_count	INT;

SET @succeeded_count = (
	SELECT COUNT(*) as succeeded_count FROM meta.log_tasks 
	WHERE Task_Name = 'unittest_spark_dummy_task' 
	AND Current_Status = 'SUCCEEDED'
);

IF @succeeded_count = 0
BEGIN
	SET @error_message = 'Did not find the test in the logs. DummyWorker was not executed inside of MetaNotebook';
	THROW 50404, @error_message, 404;
END
</file>
<file name="src\synapse\_test\test_scripts\sql\synapse_dummy_pl.sql">
-- Query which will return an error when the specific logs can't be found in the tables
-- Will check for execution of pl_dummy_worker

DECLARE @error_message		NVARCHAR(MAX);
DECLARE @succeeded_count	INT;

SET @succeeded_count = (
	SELECT COUNT(*) as succeeded_count FROM meta.log_tasks 
	WHERE Task_Name = 'unittest_synapse_dummy_task' 
	AND Current_Status = 'SUCCEEDED'
);

IF @succeeded_count = 0
BEGIN
	SET @error_message = 'Did not find the test in the logs. pl_dummy_worker was not invoked';
	THROW 50404, @error_message, 404;
END
</file>
<file name="src\synapse\_test\test_scripts\sql\synapse_unzip_archive.sql">
-- Query which will return an error when the specific file logs can't be found in the tables
-- Will check for successfull execution of file movement to archive inside of unittest_unzip

DECLARE @expected_count		INT;
DECLARE @actual_count		INT;

DECLARE @error_message		NVARCHAR(MAX);


SET @expected_count = 2
SET @actual_count = (
	SELECT COUNT(*) FROM meta.log_files
	WHERE archive_status = 'SUCCEEDED'
	AND ([filename] LIKE 'gzipped_landing%' OR [filename] LIKE 'zipped_landing%')
);

if @expected_count &lt;&gt; @actual_count
BEGIN
	SET @error_message = CONCAT('Found (', @actual_count, ') succeeded movements to archive. Expected (', @expected_count, ') file movements for plan unittest_synapse_unzip');
	THROW 50404, @error_message, 404;
END
</file>
<file name="src\synapse\_test\test_scripts\sql\synapse_unzip_tasks.sql">
-- Query which will return an error when the specific tasks linked to the unzip plan can't be found in the tables
-- Will check for successfull execution of tasks inside of plan unittest_unzip

DECLARE @expected_count		INT;
DECLARE @actual_count		INT;

DECLARE @error_message		NVARCHAR(MAX);


SET @expected_count = 2
SET @actual_count = (
	SELECT COUNT(*) FROM meta.log_tasks
	WHERE Task_Name LIKE 'unittest_synapse_unzip%'
	AND Current_Status = 'SUCCEEDED'
);

if @expected_count &lt;&gt; @actual_count
BEGIN
	SET @error_message = CONCAT('Found (', @actual_count, ') succeeded tasks. Expected (', @expected_count, ') tasks for plan unittest_synapse_unzip');
	THROW 50404, @error_message, 404;
END
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 7;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_integration',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_task.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_defaultreplace.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 3;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_defaultreplace',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_defaultreplace.');
    throw 50000, @error_message, 1;
end

SET @expected_count = 1;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_defaultreplace',
     FORMAT = 'DELTA'
     ) as result
     where column_2 IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for column_2 in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_defaultreplace.');
    throw 50000, @error_message, 1;
end

SET @expected_count = 1;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_defaultreplace',
     FORMAT = 'DELTA'
     ) as result
     where column_3 IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for column_3 in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_defaultreplace.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_replace_null_pk.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;
DECLARE @expected_value NVARCHAR(80);
DECLARE @actual_value NVARCHAR(80);

SET @expected_count = 3;
SET @expected_value = 'ERROR'

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_integration_replace_value',
     FORMAT = 'DELTA'
     ) as result
) 

SET @actual_value = (
     select Column_1
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_integration_replace_value',
     FORMAT = 'DELTA'
     ) as result
     where Column_3='test_replace_value' )

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_test_replace_value.');
    throw 50000, @error_message, 1;
end

if @expected_value &lt;&gt; @actual_value
begin
    set @error_message = concat('The expected value in the delta table (', isnull(@expected_value,'NULL'), ') is not the same as the actual (', isnull(@actual_value,'NULL'), ') for csv_test_replace_value.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_skiplines.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 3;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/skiplines',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_task.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_specialstring.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 4;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_specialstring.');
    throw 50000, @error_message, 1;
end


------------------------------------------------------------------------------------

SET @expected_count = 4;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring',
     FORMAT = 'DELTA'
     ) as result
     where column_3 is not null
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_specialstring for column3.');
    throw 50000, @error_message, 1;
end


------------------------------------------------------------------------------------

SET @expected_count = 2;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring',
     FORMAT = 'DELTA'
     ) as result
     where column_2 = ', Belliard, 25'
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_specialstring for column2 (belliard).');
    throw 50000, @error_message, 1;
end


------------------------------------------------------------------------------------

SET @expected_count = 2;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_specialstring',
     FORMAT = 'DELTA'
     ) as result
     where column_2 = '""escaped""'
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_specialstring for column2 (escaped).');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_csv_struct_lines.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV RECURSE)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 3;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/csv_recurse_search',
     FORMAT = 'DELTA'
     ) as result
)

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for csv_task.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_formatting.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 3;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_task.');
    throw 50000, @error_message, 1;
end


----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where decimal_comma IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for decimal_comma in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end


----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where decimal_dot IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for decimal_comma in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end

----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where timestamp_us IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for timestamp_us in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end


----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where timestamp_eu IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for timestamp_eu in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end

----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where date_us IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for date_us in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end


----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where date_eu IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for date_eu in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end

----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where date_nosep IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for date_nosep in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end

----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where date_us_slash IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for date_us_slash in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end

----------------------------------------------------------

SET @expected_count = 0;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where date_eu_slash IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for date_eu_slash in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end



----------------------------------------------------------

SET @expected_count = 2;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/formatting_integration',
     FORMAT = 'DELTA'
     ) as result
     where unix_format = '2024-10-14 08:19:15.0000000'
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of NULL-values in for unix_format in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for formatting_integration.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_json.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (JSON)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 4;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/json_integration',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for json_task.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_optional_columns.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (CSV)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 3;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/options_optional_columns',
     FORMAT = 'DELTA'
     ) as result
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for options_optional_columns.');
    throw 50000, @error_message, 1;
end


-------------------------------------------------------------------------------------------------

SET @expected_count = 3;
SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/options_optional_columns',
     FORMAT = 'DELTA'
     ) as result
     where column_3 IS NULL
) 

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows for column_3 in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for options_optional_columns.');
    throw 50000, @error_message, 1;
end
</file>
<file name="src\synapse\_test\test_scripts\synapse\ingestionworker_parquet.sql">
-- CHECK: AMOUNT OF LINES IN TABLES (PARQUET)
DECLARE @error_message NVARCHAR(max)

DECLARE @expected_count int;
DECLARE @actual_count int;

SET @expected_count = 3;

SET @actual_count = (
     select count(*)
     from OPENROWSET(BULK 'https://$(storage_account).dfs.core.windows.net/silver/parquet_integration',
     FORMAT = 'DELTA'
     ) as result
)  

if @expected_count &lt;&gt; @actual_count
begin
    set @error_message = concat('The expected number of rows in the delta table (', isnull(@expected_count,'NULL'), ') does not equal the actual count (', isnull(@actual_count,'NULL'), ') for parquet_test.');
    throw 50000, @error_message, 1;
end
</file>
<file name="wiki\Architecture\dataconfig_module.md">
# DataConfig Module

Last Edit: 12/02/2024

- [DataConfig Module](#dataconfig-module)
  - [General Information](#general-information)
    - [Purpose](#purpose)
    - [Concepts](#concepts)
  - [Main Artefacts](#main-artefacts)
      - [DataConfig module overview](#dataconfig-module-overview)
  - [Test Artefacts](#test-artefacts)
  - [DevOps pipeline](#devops-pipeline)


## General Information
* Source code: src/powershell/modules/DataConfig
* Prerequisites: data_engineering_guide.md
* Test framework: Pester

### Purpose 
The purpose of this module it to easily deploy metadata describing data sources into the SQL Meta database. The module allows the user to only have configure 2 json files, one describing the plans and tasks and one describing the metadata of the source file. This should make the life of the user easier, as they do not have to work with *insert into* statements from SQL.
The module also prevents users from configuring metadata that is not allowed. For example, at the time of writing (12/02/2024), the ingestion framework is not build to deal with excel files. If the user configures a file type of 'excel', the system will reject the configuration immediately instead of having the user insert it into the database and later find out that it was not allowed. Or even worse, have the system ingest the data source that was not allowed to be ingested.
This also prevents users from configuring values that will mess up the system. For example, at the time of writing (12/02/2024), users are expected to define which worker in the Synapse workspace they want to use to execute the task they are configuring. The only existing worker is the *IngestionWorker*. If a user tries to define any other value, the configuration files will be rejected.

**Note:** While this module has no direct link to the ingestion process, it does help users to more efficiently deploy metadata to the SQL Meta database. And it allows developers to make sure that the data in the database is of sufficient quality.

### Concepts
* Orchestrator
* Plan-task configuration
* Dataset configuration


## Main Artefacts
The source code is mainly contained in the **public** folder, which contains a set of powershell files (.ps1). Each file contains a function of the DataConfig module that can be called upon after importing the module (see overview below). Next, two important files are **DataConfig.psm1** and **DataConfig.psd1**, which describe the DataConfig module and its data respectively. The .psm1-file defines which files/functions are part of the module by looking into the public and (*for now non-existent*) private folders. After collecting the files, the script imports the files into the environment running the module. The .psd1-file then contains all the functions that will be part of the module, which is configured by using the **.build.ps1** script.

Focusing a little more on the functions, the **Set-IngestionMetadataFromJsonFile** is the orchestrator of the process. This function expects the user to pass the path to a configuration file, and the relevant SQL parameters to where the configuration file needs to be deployed. First, the function calls **Compare-ngestionMetadataFromJsonFile**, which is a function that validates the configuration file. More specifically, the **schemas** folder contains a (set of) json file(s) that describes the expected schema of a configuration file. This schema also contains some of the "quality" checks discussed earlier to prevent users from inserting low quality metadata. The *enum-operator*, for example, lists the allowed values for a specific key. If the configured value is not in this list, the file is rejected. There is also an *additionalProperties-operator* which is set to false, which prevents users from creating their own parameter keys. 

After the schema-checks are passed, the file ingestion can start. The **Set-IngestionMetadataFromJsonFile** is split up into 2 sections, depending on whether we are dealing with a plan-task json file or a dataset json file. When dealing with a plan-task json file, functions **Set-IngestionPlanConfiguration** and **Set-IngestionPlanTaskConfiguration** are executed for the given metadata. When dealing with the dataset json file, functions **Set-IngestionTaskConfiguration**, **Set-IngestionSourceConfiguration**, **Set-IngestionSourceColumnConfiguration**, **Set-IngestionSourceCheckConfiguration** and **Set-IngestionDeltaTableConfiguration** are executed for the given metadata. Each of these configuration-functions constructs a query that uses one of the deploy-schema stored procedures. The query is passed to the **Get-SQLQueryResults** function, which interprets the query and returns some logging information about the number of rows in the configuration that were inserted, updated, and deleted.

**Note:** The build.ps1 script is only used locally to build the DataConfig module. When moving to DevOps, a NuGeT package artefact is created in the Artefact Feed which can be downloaded and used by other repositories and VMs. 

#### DataConfig module overview
| Function Name | Description | Parameters |
| -----------   | ----------- | ---------- |
| Set-IngestionMetadataFromJsonFile | Serves as an orchestrator function that takes a configuration file (plan or dataset). Depending on the type of configuration file, different functions are called and the metadata from the file is passed to the correct function. | Path to a json file, SQL login parameters, Target environment code (dev, int, tst, acc, prd) |
| Set-IngestionPlanConfiguration | Insert the metadata from a plan-task configuration file into the plan_configuration table. | SQL login parameters, plan name, plan description |
| Set-IngestionPlanTaskConfiguration | Insert the metadata from a plan-task configuration file into the plan_task_configuration table. | SQL login parameters, plan name, json with tasks that need to be in the plan |
| Set-IngestionTaskConfiguration | Insert the metadata from a dataset coniguration file into the task_configuration table. | SQL login parameters, TaskName, TaskDescription, TaskType, WorkerName, FileLayout, SourceFolder, ContainerName, TableName
| Set-IngestionSourceConfiguration | Insert the metadata from a dataset coniguration file into the source_configuration table. | SQL login parameters, FileLayout, MatchPattern, Extension, ColumnDelimiter, RowDelimiter, EscapeCharacter, QuoteCharacter, SheetName, DataRange, Encoding |
| Set-IngestionSourceColumnConfiguration | Insert the metadata from a dataset coniguration file into the source_column_configuration table. | SQL login parameters, a list with a set of json objects for each column, FileLayout |
| Set-IngestionSourceColumnConfiguration | Insert the metadata from a dataset coniguration file into the source_check_configuration table. | SQL login parameters, a list with a set of json objects for each check, FileLayout |
| Set-IngestionDeltaTableConfiguration | Check if the target table already exists in the silver database of the Synapse on-demand server. If not, start a spark session and create the delta table | Table name, target environment of where to deploy the delta tables, a description of the delta table (metadata), and a list of JSON objects describing the columns of the delta table. |
| Get-SQLQueryResults | Execute the deploy-schema stored procedures to get the metadata into the configuration tables. The function returns a table that stated how many rows were inserted, updated, deleted. | SQL login parameters, query to execute |



## Test Artefacts
The DataConfig module also needs to be properly tested. To do this, Powershell offers the *Pester* module. This module expects a set of **Tests.ps1** scripts in the public and private folders. Best practice requires the module to have one of these scripts for each function in that module. Each of these functions can contain multiple tests for the same function, each testing their own specific functionality. Therefore, every file in the public and private folder that has extension **Tests.ps1** will be considered to be part of the test artefact. 

Given the module that is constructed, some tests will require a configuration file. The configuration files used for testing the module can be found in the **.test_config** folder. The **unit_test.ps1** script is the script that will initiate all the test files. First, the context is set for Pester, indicating which files should be included in the tests and where the results need to be outputted to. This script can be seen as the orchestrator script for the unit tests for the DataConfig module

**Pester documentation:** https://pester.dev/docs/quick-start

## DevOps pipeline
The DevOps CI pipeline for the DataConfig module only contains one job: The build job. The reason for this, rather than also having a deploy and test job, is that there is no real deployment needed to any specific environment. The "deployment" is the creation of an Artefact that contains the necessary functions, which is already done in the build job. The pipeline does make a distinction between the development and integration environment, where the Artefact is published to a different artefact feed in DevOps.
The build job contains a test-section before actually building the artefact. This test-section will run all the unit tests configured in the test artefact discussed above. If all tests have passed successfully, the actual build-section of the pipeline can start. The **.build.ps1** script is initiated, and the results are published as a NuGeT package to a new directory. This package is later picked up, converted to an artefact, and published to a DevOps artefact feed where it can be picked up by any other pipeline in the future.

**Note:** The reason for building a NuGet package is because the current framework uses mulitple git repositories, namely the dap_appl_dala_code and the dap_appl_dala_config. Both repositories require the use of the DataConfig module. To facilitate this when using DevOps, an Artefact Feed has been configured to deploy a NuGeT package to. Both repositories will call upon this feed when needing to ingest metadata.
</file>
<file name="wiki\Architecture\sql_meta_database.md">
# SQL Meta Database

Last Edit: 13/02/2024

- [SQL Meta Database](#sql-meta-database)


## General Information
* Source code: src/sql
* Prerequisites: data_engineering_guide.md
* Test framework: tSQLt

### Purpose
The purpose of the SQL Meta Database is to contain all the configuration data about the data sources that need to be ingested using the ingestion framework, and contain the logs of all the Synapse pipeline runs that have been initiated during the lifetime of the ingestion framework. The SQL Meta database is a central point in the ingestion framework as it contains all the data to make the process tick. That is, the Synapse workspace calls upon the stored procedures stored in the database, which require data to be in the configuration tables.

### Concepts
* Metadata: This is a highly used concept in the metadata-driven ingestion framework. This concerns data describing the data sources (aka data about the data), such as the source location of where the source file is landed, the expected column names, or the target location of where to sink the data from the source file. Basically, the metadata describes how the ingestion framework should interpret the data coming from a specific data source, where to find the files on the landing zone, and where to move the files to in the silver layer.

## Main Artefacts
The main artefacts are stored in the **metadb** folder. Here, all schemas, tables, and stored procedures can be found, as well as a **.sqlproj** file. The sqlproj-file contains all the references to the artefacts that need to part of the meta database. The **db_build.ps1** script then transforms the sql-project file into a dacpac, which can later be deployed to a specific SQL database using the **db_deploy.ps1** script. 

The schemas in the project are **deploy** and **meta**. The deploy-schema is used for tables and stored procedures that are related to the deployment of data into the database. The meta-schema is used for tables and stored procedures that are related to the storage and usage of data in the database. 

### Main artefacts: Table overview
| Table name | Table schema | Description | Primary keys |
| ---------- | ------------ | ----------- | ------------ |
| log_runs | meta | This is an identity table that stores all the runs of the Synapse pipeline *pl_start_run*. You can find which plan was initiated by the run. It also stores to status of the run, allowing for quick monitoring of the SUCCEEDED, FAILED, aborted, and cancelled runs.| run_id |

| log_plans | meta | This is an identity table that stores all the plans that have been initiated over time, together with the plan id and the run id of the run that initiated the plan. It also stores to status of the plan, allowing for quick monitoring of the SUCCEEDED, FAILED, aborted, and cancelled plans.| plan_id |

| log_tasks | meta | This is an identity table that stores all the tasks that have been initiated over time, together with the task id and the plan id of the plan that initiated the task. It also stores to status of the task, allowing for quick monitoring of the SUCCEEDED, FAILED, aborted, and cancelled tasks. | task_id |

| log_files | meta | This is an identity table that stores all the files that have been processed by the ingestion framework over time. It also stores the current status of the ingestion. This monitoring capability allows a user to more easily check where a file is in the process and start debugging when an issue arises. | file_id; **Note:** extended_filename is expected to be unique |

| log_run_plans | meta | This table contains the IDs of the runs and plans that have been run together.| run_id, plan_id |

| log_plan_tasks | meta | This table contains the IDs of the plans and tasks that have been run together.| plan_id, task_id |

| plan_configuration | meta | The plan_configuration table contains all the metadata about the plans that are part of the ingestion framework. The metadata in this table concerns the name of the plan, a description, and an *enabled* boolean column indicating whether the plan can be used by the framework. | plan_name |

| task_configuration | meta | The task_configuration table contains all the metadata about the tasks that are configured for the ingestion framework. The metadata in this table concerns the name of task, a description, and an *enabled* boolean column. Depending on the type of task we are dealing with, different metadata columns need to be defined. For an ingestion task_type, the system needs to know where the source files will be stored (source_location) and in which table they need to be ingested (table_name). Furthermore, the name of the worker that will execute the task also needs to be given. *For more information on workers: synapse_workspace.md* | task_name |

| plan_task_configuration | meta | The plan_task_configuration table gives an overview of which tasks are part of which plan, and which task group the tasks belong to. Again, an *enabled* boolean column is configured as well. | plan_name, task_name, task_group |

| source_configuration | meta | The source_configuration table contains the metadata about how to interpret a source file from a specific data source. While the task_configuration considers the outer workings of the file (where is it located?, where does it need to know?, which worker will execute the task?...), the source_configuration table looks at the inner workings (what is the expected name of the file?, what are the column delimiter, row delimiter, etc.?,...). This source_configuration table focuses on 'How does the system need to interpret the provided file?'.  | file_layout |

| source_column_configuration | meta | Next to the initial configuration of each source, the expected column names and potential replacements also have to be configued. This is done in the source_column_configuration. Additional information, such as the data types and dimension (PK, SCD2, SCD3...) needs to be given. | file_layout, column_order |

| source_check_configuration | meta | Some source files require explicit checks to be executed before the source files can be ingested. For example, for PDM, the received zip-folder is expected to have 50 files. If this is not the case, an issue happened on the provider's side and the ingestion cannot happen. The checks for each data source are configured in the source_check_configuration. | file_layout, check_name |

### Main artefacts: Stored procedure overview
| Stored procedure name | Stored procedure schema | Description | Parameters |
| --------------------- | ----------------------- | ----------- | ---------- |
| end_plan | meta | Given the Plan ID, end a plan with a 'SUCCEEDED' or 'FAILED' status | plan_id, success_flag, run_id |

| end_run | meta | Given the Run ID, end a run with a 'SUCCEEDED' or 'FAILED' status | run_id, success_flag |

| end_task | meta | Given the Task ID, end a task with a 'SUCCEEDED' or 'FAILED' status | task_id, plan_id, success_flag |

| get_plan_metadata | meta | Given the plan ID and task group, return the task_type, list of tasks, and number of tasks to be executed for that plan. The list of tasks is returned by the meta.usp_get_task_metadata stored procedure. | run_id, plan_id, task_group |

| get_plans | meta | Given the Run ID and plan name, return the list of Plan IDs that need to be started. | run_id, plan_name |

| get_task_metadata | meta | Given the task id, return the metadata of the source file that the task is ingesting as a table of json objects. | task_id |

| get_tasks| meta | Given the plan_id and task group, return a list of json objects. Each object describes a task to be executed with its metadata | run_id, plan_id, task_group |

| new_file | meta | If the ingestion framework encounters a file in the landing zone, this stored procedure is called to create a new instance in the log_files table. | task_id, plan_id, filename, extended_filename, folder_path |

| new_run | meta | Start a new pipeline run for a given plan. Based on the name of the plan, the logging tables will initiate a new run for that plan. If the 'New_Plan' parameter is *true*, a new plan will be logged and, based on the plan_task_configuration table, all the associated tasks will also be initiated. Each new instance in the logging table will get a 'PENDING' status | pipeline_id, new_plan, plan_name, run_id, plan_name |

| start_plan | meta | Given the Plan ID, start a plan with the 'IN PROGRESS' status | plan_id, run_id |

| start_run | meta | Given the Run ID, start a plan with the 'IN PROGRESS' status | run_id |

| start_task | meta | Given the Task ID, start a task with the 'IN PROGRESS' status| task_id, plan_id |

| update_file_activity | meta | When a file is moved to a different layer of the medallion framework, the stored procedure updates the ingestion_status of the file that is being ingested. | extended_filename, activity, success, folder_path |

| set_plan_configuration | deploy | A stored procedure to deploy configuration data to the plan_configuration table | plan_name, plan_description, enabled |

| set_plan_task_configuration| deploy | A stored procedure to deploy configuration data to the plan_task_configuration table| plan_name, json object containing the task information (task group, name, sequence, enabled) |

| set_source_check_configuration | deploy | A stored procedure to deploy configuration data to the source_check_configuration table | file_layout, json object contianing the check information (name, enabled) |

| set_source_column_configuration | deploy | A stored procedure to deploy configuration data to the source_column_configuration table | file_layout, json object containing the column configuration (source name, sink name, data type, dimension, sequence) |

| set_source_configuration | deploy | A stored procedure to deploy configuration data to the source_configuration table | file_layout, file_pattern, file_extension, column_delimiter, row_delimiter, escape_character, quote_character, header, encoding |

| set_task_configuration | deploy | A stored procedure to deploy configuration data to the task_configuration table| task_name, task_type, worker_name, task_description, table_name, file_layout, source_folder, container_name, enabled |



## Test Artefacts
The test artefacts are stored in the **metadb_test** folder. Here, all schemas, dependencies, and stored procedures can be found, as well as a **.sqlproj** file. The sqlproj-file contains all the references to the artefacts that need to part of the test framework. The **db_build.ps1** script transforms the sql-project file into a dacpac, which can later be deployed to a specific SQL database using the **db_deploy.ps1** script. 

The framework used for testing is the **tSQLt** framework of SQL. This framework is contained in the **tSQLt_1.0.5873.27393.sql** script in the **_test** folder. The script contains all the stored procedures and tables used by the framework. This is a default script, similar to how you would import a module in Python or Powershell. All the tests for the test framework can be found in the **t_design_tests** and **t_unit_tests** folders. The design tests enforce the naming conventions for the table and column names. That is, they make sure that they are all lowercase. The unit tests are validating all the stored procedures of the main artefact. The t_unit_tests folder contains a set of folders, one for each stored procedure in the meta database. Each folder has a similar structure:
* There is one class-script that creates a schema for the test. This schema is added to the tSQLt.TestClass class, which is later interpreted by tSQLt as a test case.
* There is one setup-script that prepares the tests to be executed. It recreates some of the existing tables as FakeTables and fills it with test-data. In the end of the tSQLt procedure, these fake tables and their data is removed automatically by the framework.
* There are one or more tests files for each stored procedure. 

Both the **run_testclasses.ps1** and **tsqlt_with_xml_output.ps1** files can be used to run the test cases. The former file is meant to be used locally as it does not output the results, but the DevOps pipeline will use the latter as the output will be published by the pipeline.

The schema defined for the test project is **test** and both stored procedures for the project use this schema. Stored procedure **usp_reset_database** is used to completely clean all the data contained in the database. This is a procedure that should not be allowed in the production environment, but can be useful during development. Stored procedure **usp_run_tsqlt_tests** will run all the tests for a specific type (debug, unit...), and is called upon by the run_testclasses.ps1 script. This script can, again, be used during development but should not be part of the production environment. 

**Note:** Both the tSQLt base script and dacpac are included in the repository to make sure developers can access the source code while not having to worry about having to create a new dacpac.
**Note:** In the **metadb_test.sqlproj**, the tSQLt dacpac is included as a reference. However, building and deploying the project will NOT deploy the referenced dacpac. The reason for including the dacpac as a reference is do suppress the warnings given by the *dotnet build* in the build script. The story is the same for the AzueV12_master.dacpac. 


## DevOps pipeline
The DevOps CI pipeline for SQL, pipeline-ci-sqlmetadb, initiated 3 jobs: build, deploy and test. The build-job will build the dacpac for both the metadb and metadb_test sql project. These dacpacs will be converted into separate pipeline artefacts, together with the scripts needed for deployment and testing.

The deploy job will then, simply, deploy the metadb dacpac. The test job will also deploy the metadb_test dacpac, after deploying the tSQLt dacpac. After this, the tsqlt_with_xml_output.ps1 script is called to run the unit tests and publish the results from those tests.
</file>
<file name="wiki\Architecture\Synapse_workspace.md">
# Synapse Workspace

Last edit: 12/02/2024

- [Synapse Workspace](#synapse-workspace)
  - [General Information](#general-information)
    - [Purpose](#purpose)
    - [Concepts](#concepts)
  - [Main Artefacts](#main-artefacts)
    - [Pipelines](#pipelines)
      - [Pipeline overview](#pipeline-overview)
    - [Notebooks](#notebooks)
      - [Notebook overview](#notebook-overview)
  - [Test Artefacts](#test-artefacts)
    - [Pipelines](#pipelines-1)
    - [Notebooks](#notebooks-1)
  - [DevOps pipeline](#devops-pipeline)


## General Information
* Source code: src/synapse/studio/
* Prerequisites: data_engineering_guide.md
* Test framework: unittest module (Python)

### Purpose
The Synapse workspace serves as the orchestrator and executor of the ingestion process, and is an embodiment of the plan-task configuration. The workspace contains the main code to successfully move a file from landing to silver. Each activity (pipeline, function, method...) is driven by parameters, which refers to the metadata of the data source that needs to be ingested. The main resources are the pipelines and notebooks, each having their own orchestrator. The orchestrators are meant to interpret the metadata and invoke a set of activities depending on that metadata.

### Concepts
* Metadata: This is a highly used concept in the metadata-driven ingestion framework. This concerns data describing the data sources (aka data about the data), such as the source location of where the source file is landed, the expected column names, or the target location of where to sink the data from the source file. Basically, the metadata describes how the ingestion framework should interpret the data coming from a specific data source, where to find the files on the landing zone, and where to move the files to in the silver layer.
* Orchestrator: This concerns an artefact that will use the provided metadata to invoke a set of activities, functions, etc. necessary to complete a certain task.
* Worker: A worker invokes a set of functions, methods, activities, etc. in a specific order to successfully execute a task. 

*One can compare these concepts to a DevOps pipeline, where the pipeline invokes a set of jobs and each job has a set of tasks. The CI pipeline, being the orchestrator in this scenario, invokes a set of jobs. Before starting the jobs, it first checks whether we are in the development or the integration environment. Depending on the environment, a different variables-template needs to be called upon. This variables-template is the metadata. Based on the metadata, the CI pipeline invokes the build, deploy, and test jobs. These jobs, or workers, then call upon a set of scripts and tasks with the goal of completing a specific activity (e.g. building an artefact, deploying source code...).*


## Main Artefacts
The main artefacts are pipelines and notebooks. These contain all the logic, functions, methods, and activities of the ingestion framework.

### Pipelines
The **pl_start_run** pipeline is the orchestrator for the pipelines, as it will invoke a set of underlying pipelines. The pl_start_run pipeline requires two parameters: The name of the plan (Plan_Name) and whether or not this is a new plan or a rerun of a previously failed plan (New_Plan). This pipeline is also the starting point of the entire ingestion framework. Based on the Plan_Name, a stored procedure is invokes that will return a list of plans to be executed. This list contains a set of Plan IDs. That is, if a plan with the same name has been executed in the past and it did not complete successfully, the system will make sure that this plan (and its failed tasks) is executed again. This is important to control the ingestion flow and make sure that the files from each data source at ingested in a correct order. 

**Note:** Because one run can contain multiple plans to execute, the development team has opted to use runs and Run_IDs to logically group together the plans that are being executed during the same pipeline invocation. Runs are in theory not part of the plan-task configuration, but they have been assigned their individual log table to make pipeline debugging and maintenance easier. To simplify, every time the pl_start_run pipeline is started, a new run is started. A new run can create zero or one new plan in the logging tables, and can (re)start one or more plans. 

The pl_start_run pipeline has a ForEach activity that loops over the list of IDs, where for each ID the **pl_start_plan** pipeline is invoked. This pipeline invokes the **pl_start_task_group** pipeline multiple times, each time with a different Task_Group parameter. This step makes sure that the different task groups are executed in the correct order. That is, each task is part of a group, and each task in that group is expected to be independent from one another. If a task depends on another task, it should be part of another task group that will be executed at a later point in time.

The **pl_start_task_group** calls a stored procedure to collect the set of tasks that are part of the invoked task group and that need to be started for the specific plan that is being executed. The stored procedure also returns the type of worker that will execute the tasks at hand. That is, for some task groups, the logic is contained in a SYNAPSE_PIPELINE, while for other task groups the logic is contained in a SYNAPSE_NOTEBOOK. Depending on the worker type, a different activitiy is started. 

#### Pipeline overview
| Pipeline Name         | Description | Parameters |
| -----------           | ----------- | ---------- |
| pl_start_run          | Start the run of a plan.       | Plan_Name, New_Plan |
| pl_start_plan         | Execute a plan by invoking all task groups in a specific order        | Plan_ID, Run_ID |
| pl_start_task_group   | Collect the list of tasks to execute for a task group and plan and invoke the orchestrator notebook MetaNotebook      | Task_Group, Plan_ID, Run_ID |



### Notebooks
When the worker type (discussed above) is SYNAPSE_NOTEBOOK, the **MetaNotebook** is invoked. This is the orchestrator for the notebooks, as it will call upon functions and methods defined in other notebooks. The notebook takes a set of parameters: The environment_code (dev, int, tst, acc, or prd), the plan_id, and the list of tasks to execute. The orchestrator will interpret the list of tasks and execute the correct worker (notebook) to execute the each task individually. 

The notebook will loop over the list of tasks and, based on the metadata for each task, a worker will be called upon. This worker then contains the logic to successfully execute the task at hand.

**Worker notebooks**

The **IngestionWorker** is the worker that will execute the methods and functions to ingest a file from the landing zone into the delta lake layer.


**Functions and Methods**

The **Classes** notebook contains, as the name suggests, a set of classes. Each class has a set of methods (= functions of a class) that can be called upon by a class-object. The classes contained in the notebook are:
* SQLConnection: The SQLConnection class allows a user to set-up a connection object to a SQL Server and Database. The object can later be called upon to execute queries and stored procedures against this database, and interpret the returned results. 
* Plan: For the invoked plan, a plan-object is created on the Plan class. At the time of writing (12/02/2024), this class has no real purpose but it is a built-in precaution for potential future developments.
* Task: For each task in the task list, a task-object is created. The class contains the main methods needed to complete a task, where the main method is the collection of the metadata and creating file-objects (for the File class) for each file that is related to the task.
* File: For each file that requires processing (ingesting, etc.), a file object is created. The File-class contains the methods needed to move the file from the landig zone to the delta lake, as well as some logging capabilities for the SQL logging tables.
* Checks: Each task potentially requires some quality checks to be executed. These methods can be found in the checks-class, for which an object is created in the Task-class. The start_checks method serves at the orchestrator as it interprets the list of checks that need to be executed and invokes the correct method for each check.
* DeltaTable: *Redundant object, will be rewritten in Sprint 2*

The **IngestionFunctions** notebook contains a set of functions that are called during the execution of ingestion tasks. The main section here is the "Merge Functions" section, which groups together the functions that are meant for merging an existing delta table with a new source file. The functions first check whether there are any mathes between the primary keys between the delta table and source file. When there is no match, the system executes an insert-function (**NoMatch**). When there is a match, the system executes an update-function (**ColumnMatch**). The script also contains some standalone functions, such as the **list_directory_content** which returns a list of files and folders of a specific azure container.

#### Notebook overview
| Notebook Name        | Description | Parameters |
| -----------          | ----------- | ---------- |
| MetaNotebook         | Orchestration notebook for the Synapse Spark environment. This notebook "imports" all functions and loops over all tasks for a certain plan. Depending on the metadata, a worker is called.       | Plan ID, list of tasks, environment code |
| IngestionWorker      | The IngestionWorker notebook calls the functions and methods needed to ingest a file from the landing zone to the delta lake  | data lake storage account name, list of variable names needed to execute the ingestion function (= metadata) |
| Classes              | Notebook that contains a set of classes (SQLConnection, Plan, Tasks, File, Checks), each having their own methods. These methods can be called upon by a class-object  | |
| IngestionFunctions   | Set of functions that are being used during the ingestion process. The most important function here is **MergeFile** as it will merge the source file with the existing delta table. | |
| CleanWorkspace      | Set of functions that can be used to clean the workspace. That is, remove all the files from the data lake containers and remove the tables on a Synapse database        | environment code |

## Test Artefacts
The test artefacts are pipelines and notebooks. These contain all the logic, functions, methods, and activities to test the ingestion framework.

### Pipelines
**pl_dummy_worker** is a pipeline used to test whether the flow from pl_start_run to pl_start_task_group works and whether the stored procedures are properly integrated. The pipeline has no real purpose other than being invoked by other, overarching pipelines.


### Notebooks
**TestNotebook** orchestrates the unit testing of the Synapse notebooks. There are 2 other notebooks, **TestClassess** and **TestFunctions** that contain a set of classes and modules to properly test the modules and functions in the **Classess** and **IngestionFunctions** respectively. These 3 test-notebooks are the "Test framework" for the Synapse Notebooks set-up.

These notebooks all use the **unittest** module of Pyspark. The core of the module comes down to developing a class for each function/module that needs to be tested. This class should contain a set of modules that call the to-be-tested function/module and assert the actual results against the expected results. Whenever a developer creates a new function or module, a new test-class should be added to the correct notebook. 

The TestNotebook does not contain any classes, but serves as the orchestrator of the unit tests. That is, it "imports" all the classes from the other notebooks and using the unittest.main()-method, executes all methods in the classes. After execution, the results are written to a file on the logs-container in the data lake. Here, the results can be analyzed by the developer or by the DevOps pipeline that invoked the TestNotebook.

**More information:** https://docs.python.org/3/library/unittest.html

## DevOps pipeline
The DevOps CI pipeline for the synapse workspace is **pipeline-ci-synapse.yml** and can be found in the pipelines folder. Whenever a change is made to the src/synapse folder on the main branch, this pipeline is run to make sure no breaking changes happened. This pipeline also allows a developer to publish their changes to the publish branch (and Synapse Live Mode).

The CI pipeline executes 3 jobs, and depending on the branch, different variables are passed to the jobs.
1. Build job &amp;rarr; job-build-synapse.yml:
   * The build job will generate 4 artefacts, of which 2 will be used: synapse_main and synapse_test. The synapse_main artefact contains an ARM template that describes the main artefacts discussed above. This is done using a regex-approach during the execution of the DevOps task that creates the ARM template (Synapse workspace deployment@2). The synapse_test artefact contains an ARM template that describes the tet artefacts discussed above, using regex as well. An important note to make here is then: Follow the naming conventions for newly developed Synapse artifacts because this is important during the build of the ARM templates.
   * *FYI 1: The build job uses a task called "Synapse workspace deployment@2", which takes the json files from the git integrated synapse workspace and converts them into an ARM template. This step creates an artefact containing the ARM template and a parameter template that can be overwritten during deployment. This task, however, gives the issue that it overwrites existing ARM templates. Since our process builds 2 separate ARM templates, this messes up the system. This is why another task is used to copy the first ARM template into a separate artefact before generating the second ARM template*
   * *FYI 2: There is a file (src/synase/studio/template-parameter-definition.json) that determines which keys in the json architecture are parameterized. This is an important script as, for example, the name of the spark pool is tailored to the environment it is in (&lt;environment_code&gt;dapsspcore). If the Synapse workspace ARM template is deployed to a different environment, all references to the spark pool need to be overwritten with the correct name for that environment. Overriding the parameters does not happen during the build process, but the creation of the parameter template does.*
2. Deploy job &amp;rarr; job-deploy-synapse.yml: 
   * The deploy job deploys the ARM template of the synapse_main artefact to a Synapse workspace in a specific environment. Before deploying, the parameters of the ARM template are overwritten to match with the environment. After deployment, there is a task that checks whether the silver database already exists for the environment. If this is not the case, a script is initiated that creates the lake database.
3. Test job &amp;rarr; job-test-synapse.yml:
   * The test job deploy the synapse_test ARM template and executes the TestNotebook discussed above to test each individual function and method. After these initial tests, each worker is also tested individually to make sure that the system is well-integrated. 
</file>
<file name="wiki\DevOps\unit_tests\AddingTests.md">
# Adding unit tests to the YAML pipeline
## Purpose
This document displays the process and importance of adding unit tests to the DevOps pipelines.

## Importance of testing in DevOps pipelines
Adding unit tests or executors of unit tests to the DevOps pipeline provides an extra layer of protection to make sure that nothing was broken during development. Especially when implementing changes to previously created functions or features. It creates an opportunity to activate these tests with every pull request you make, as well as every deployment you do to other environments, ...

## Creating a new test
Creating a new test in the DevOps pipeline looks like quite the hassle, but is actually quite easy. This guide will take you through the process, without going too much into detail about how to actually create a unit test or creating a task inside of a DevOps pipeline. You'll have two parts: first developing locally, and then creating the task inside of the DevOps pipeline.

### Local creation
The first step in creating a test, is developing it locally in the repo. For that, you can take a look at the localdeploy [synapse](../../../development/localdeploy/localdeploy_synapse.ps1) script, or localdeploy [SQL](../../../development/localdeploy/localdeploy_sqldatabase.ps1) script. This is a spaced reserved to make sure you can test everything on your own pc, to see what steps the DevOps pipeline takes to test different aspects of the infrastructure. Inside of the localdeploy scripts, create your own logic, execute the external scripts and check if everything works. If it does, create a pull request, get it approved and then you can continue to create the test again inside the DevOps pipeline.

### DevOps pipeline
Adding a test to the DevOps pipeline is a bit more complex than testing something locally, but it isn't that difficult. You just know where to look.

#### Providing the resources
First of all, when creating a new test, you have to make sure that you provide all the necessary materials to your artifact. An artifact functions as a wrapper around all your resources that are needed to execute your tests inside of your DevOps pipeline.

To do this, go to right [jobs folder](../../../pipelines/templates/jobs/). You'll have to look at the job-build files to make resources available for your artifact. Look at the 'CopyFiles@2' tasks inside of those files, and add the right paths to it. 

&lt;blockquote style="
  display: block;
  background: #fbe9e9;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1;
  color: black;
  text-align: justify;
  border: 1px solid #cf1212;
  border-left: 15px solid #cf1212;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt; &amp;#128204; &lt;/span&gt; 
      &lt;b&gt;Important:&lt;/b&gt; 
      Make sure you don't include your repo name as the root!
  &lt;/p&gt;
&lt;/blockquote&gt;

#### Adding the tasks 
Once the resources are made available inside of your artifact, you can access that location inside of a task. 

&lt;blockquote style="
  display: block;
  background: #d9e8ff;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1;
  color: black;
  text-align: justify;
  border: 1px solid #2780e3;
  border-left: 15px solid #2780e3;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; 
      &lt;b&gt;Note:&lt;/b&gt;
      Note that if you have any errors during deployment where a certain script can't be found, it's either a typo or it isn't provided inside of your artifact.
  &lt;/p&gt;
&lt;/blockquote&gt;

Once again, go to the jobs folder and look at the job-test files. Depending on which part of the process you're testing, you have to put it in that file. Meaning that if you're testing something from Synapse, you're going to look at the job-test-synapse, and the same for the meta database. 

Take a look at the tasks inside of the files and reproduce the steps you took inside of the localdeploy scripts. The only thing that's different here, is the syntax. Besides that, it's not much of a difference compared to what you did inside of the local deploy scripts.

#### Testing your tests
Finally, when everything was configured, you can now start checking if your tests actually work. You do this by going to the [DevOps pipelines](https://dev.azure.com/lnlbe/Data_Platform/_build) inside of your workspace, and take select the right pipeline to start testing. After which it's just trial &amp; error to see what works and what doesn't.
</file>
<file name="wiki\Flows\Packages\DEP-External-package-in-Azure.md">
# DEP: External package in Azure

## Purpose

When [Data exfiltration protection (DEP)](https://learn.microsoft.com/en-us/azure/synapse-analytics/security/workspace-data-exfiltration-protection) is installed, you won't be able to install packages in your notebook via pip or %pip install. To handle the process of manually adding packages to your workspace, you can follow this guide.

## Visual

![VisualDEPPckages](./ExternalPackageDEP.png)

## Procedure

### Synapse workspace

First of all, select the desired synapse workspace you want to add the package to. You're going to open the Synapse studio of that workspace, and navigate to 'Manage' in the left bar. At the moment of writing, this is the little toolbox at the bottom of the navigation bar to the left.

#### Manage

##### Configuration &amp; Libraries

When you clicked on the "Manage" section, inside of the synapse workspace, you will see a menu with different options. Above the section "Source control", you'll find the "Configurations and libraries" section. In that section, click on "Workspace packages".

###### Workspace packages

In the workspace packages, you'll find a list of all the installed external packages. In most cases, this will be empty. Meaning that you have to add them via "Upload" at the top of the newly openend pop-up. This will open a window where you can upload the .whl file.

&lt;blockquote style="
  display: block;
  background: #fbe9e9;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1;
  color: black;
  text-align: justify;
  border: 1px solid #cf1212;
  border-left: 15px solid #cf1212;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt; &amp;#128204; &lt;/span&gt; 
      &lt;b&gt;Important:&lt;/b&gt; 
      Because DEP is enabled, you can only install .whl files. Other files will get stored in this section, but will cause issues later in the process. So look for the .whl extension only when installing external packages.
  &lt;/p&gt;
&lt;/blockquote&gt;

You can find the right file on [PyPi](https://pypi.org/). Just search for the package you need, and download the right version of that file.

&lt;blockquote style="
  display: block;
  background: #d9e8ff;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1;
  color: black;
  text-align: justify;
  border: 1px solid #2780e3;
  border-left: 15px solid #2780e3;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; 
      &lt;b&gt;Note:&lt;/b&gt;
      Azure Synapse Workspace doesn't frequently update their infrastructure, so things such as pip install are quite old. This means that it isn't advised to download the latest version of each package you'll install. So far the following external packages are known to be working:
      &lt;ul&gt;
        &lt;li&gt;Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&lt;/li&gt;
        &lt;li&gt;rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl&lt;/li&gt;
        &lt;li&gt;disposable_email_domains-0.0.90-py2.py3-none-any.whl&lt;/li&gt;
      &lt;/ul&gt;
      Make sure you use those exact file names. Since there are different files for a specific version of a specific package.
  &lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote style="
  display: block;
  background: #d9e8ff;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1;
  color: black;
  text-align: justify;
  border: 1px solid #2780e3;
  border-left: 15px solid #2780e3;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; 
      &lt;b&gt;Note:&lt;/b&gt;
      Some packages have dependencies based on other packages. Meaning that you have to download that package too, and install it first, before installing the actual package you want to use in your notebook. Known dependencies are:
      &lt;ul&gt;
        &lt;li&gt;Levenshtein has dependencies with RapidFuzz&lt;/li&gt;
      &lt;/ul&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

Once you have selected your desired package, select "Upload" at the bottom of your open window. This will close the window again, and your package will be added to your workspace. Before you can use those packages, you have to link them to a Apache Spark pool.

##### Analytics pools

To do this, go the "Analytics pools" under "Manage". Here, you'll find the spark pools that you need to configure.

###### Apache Spark pools

In the Apache Spark pools, you'll find every spark pools that you have configured for the workspace. Now, you have to link your external packages to your pool to be able to use them.

Hover over your desired sparkpool, which will show you the following three things:
![Options](https://imgur.com/1KQjmZM.png)

Click on the three dots, and this will open a little menu. In this menu, you'll see "Packages". Click on it, and it will open a window. You'll see different types of configurations. First of all, make sure "Allow session level packages" is set on Disabled. You won't be needing that.

Secondly, you have to look at "Workspace packages". If you don't see your package here, it means that you still have to add it to your spark pool. To do that, click on "+ Select from workspace packages". This will open the list of packages that you just added to your workspace. In here, you only have to select your workspace package and then press "Select" at the bottom of the screen. The process will now start to upload your package and try to attach it to your pool.

&lt;blockquote style="
  display: block;
  background: #d9e8ff;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1;
  color: black;
  text-align: justify;
  border: 1px solid #2780e3;
  border-left: 15px solid #2780e3;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt; &amp;#128172 &lt;/span&gt; 
      &lt;b&gt;Note:&lt;/b&gt;
      This process can take up to 20 minutes, so be patient and check the Activity log of your synapse workspace to see how things are going.
  &lt;/p&gt;
&lt;/blockquote&gt;

When there have been no errors in the process, everything should be working. To check if the packages are now linked to your pools, go back to "Workspace packages", and click on the package you just added.

![Related2](https://imgur.com/FjTdy05.png)

If you see a number in the related section of the package, you know it worked and it is attached to a pool in your Synapse Workspace. Now, just open a notebook and import the packages. Be sure to select the right Spark pool, and then you'll be good to go.

&lt;blockquote style="
  display: block;
  background: #e9fbe9;
  padding: 10px 5px 10px 10px;
  margin: 10px 2px 10px 2px;
  position: relative;
  font-family: Georgia, serif;
  font-size: 13px;
  line-height: 1.5 ;
  color: black;
  text-align: justify;
  border: 1px solid #04c10a;
  border-left: 15px solid #04c10a;
"&gt;
  &lt;p&gt; 
      &lt;span style='font-size:15px;'&gt;&amp;#128161;&lt;/span&gt; 
      &lt;b&gt;Tip:&lt;/b&gt; 
      Sometimes, you'll be having an error when trying to link your package to a pool. That's quite normal, and usually can easily be solved. Here are some common errors (found via the Activyt log):
      &lt;ul&gt;
      &lt;li&gt;&lt;b&gt;XXX.whl is not a supported wheel on this platform&lt;/b&gt;: This means that your file version isn't accepted by Azure Synapse. You'll have to find the right version on PyPi. Make sure to check the approved packages list a bit higher in this document. If they're not in there, you'll have to look for yourself.&lt;/li&gt;
      &lt;li&gt;&lt;b&gt;Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.&lt;/b&gt;: This means that the package you're trying to install, has a dependency with another package. To check this, install the package locally on your own computer and check for the dependencies over there. Then, download the extra package from PyPi and install it before you install your actual needed package.&lt;/li&gt;
      &lt;/ul&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

</file>
<file name="wiki\General doc\1.Cloning-and-setting-up.md">
# Cloning &amp; Setting up the repo

## Description

In this ReadMe-file, you can find everything related to cloning this repo and setting up the repo on your own PC.

## Prerequisites

1. Make sure that Visual Studio Code (or another code editor) is installed on your pc.
2. Make sure that Git is installed on your pc.
3. Basic Visual Studio Code knowledge.
4. Basic Windows Powershell or Powershell 7 knowledge

## Getting started

First of all, you have to clone your project in Visual Studio Code (VSC). To do that, start VSC and open a terminal in VSC itself. Check if you are in the right folderpath (C:\Users\NALO_INITIALS\\). You can choose your own, but this path is going to be important later on in your configuration files.

After that, go to [Azure DevOps repo's](https://dev.azure.com/lnlbe/Data_Platform/_git/dap_appl_dala_code) and select the right repo.

![Repo select](https://imgur.com/HqWmMuW.png)

Once you have selected the right repo, you can find everything regarding your repo. You'll see the file structure, as well as active branches, ...

![Repo overview](https://imgur.com/Bw49w0G.png)

In the right upper corner of the repo, you can see two buttons. "Set up build" and "Clone". You need to click on "Clone", which opens a menu where you can get the link that you need to paste in VSC.

![Clone click](https://imgur.com/Vk9VI2z.png)
![Select repo link](https://imgur.com/N1vE8Y5.png)

Once you have copied that link, open Powershell, clone the repo in the right location (C:\Users\XXXNNN, e.g. C:\Users\ver949) and open VSC again. That way, you can start working on the project.

![Paste repo link](https://imgur.com/CMg4d85.png)

That should have cloned the repo on your pc. If that didn't work, you need to make sure you are [logged into git](https://docs.gitlab.com/ee/gitlab-basics/start-using-git.html). If you are logged in, but you were unable to clone anything, you don't have the right permissions for that. Make sure to ask someone to check your permissions if that's the case.

After cloning the project, you have to create your own branch in the repo via VSC. You need to do this for every project you work on. That's because you are not allowed to push directly to main, which is for safety reasons ofcourse. If the cloning was succesful, you shouldn't have any issue with creating a new branch via VSC.

In the bottom left corner of VSC, you can see which branch your are currently in. That should be 'main', if you cloned the project for the first time. Click on it and create your own branch / project in the repo.

![Main branch](https://imgur.com/JzVVD6f.png)
![Create new branch](https://imgur.com/K0OkoRB.png)

When creating a branch, always do so starting with topic\_ followed by your initials (e.g. topic_dv for the user 'Dylan Verniers'). So every project you're working on, can be associated with you and that you're working in subfolders. This is a predetermined guideline.

![Click on branches](https://imgur.com/noYoxok.png)

Once you have done that, you are good to start with your first project.

</file>
<file name="wiki\General doc\2.Your-first-project.md">
# Your first project with VSC
## Description
Before you can actually start working on the code, the first project you need to complete is setting up your environment. This is also a nice first practice on how to work with Git and its versioning.

## Prerequisites
1. Having looked at "Cloning and setting up"
2. Make sure that you have, at least, powershell 7 installed on your pc

## Getting started
First of all, make sure that you are working in the main branch. To do that, look at the bottom left corner. If there's anything else standing there besides main, you need to change it. You can also not commit changes on the main branch anyway.

![Wrong branch](https://imgur.com/noYoxok.png)
![Select main](https://imgur.com/cRZzlDX.png)
![Main branch](https://imgur.com/JzVVD6f.png)

Now, in the terminal, do a git pull. This will make sure that every change that was committed to main, will be on your pc as well.

![git pull](https://imgur.com/9LN6T1b.png)

Once you have done that, create your (first) project with the branch name such as topic_dv/env_setup. You know how to do this already from a previous guide, so I won't be explaining that anymore.

Now, switch to your branch again (the one you just created), where you can write in the terminal ```git pull origin main```. This will pull everything that's in the main branch, to your own branch. So you know you are synced with the latest version of main. If you do this and make changes, they could get deleted. It's time to look at your first project.

In dap_appl_dala_code/scripts/local-env, you will find the environments for every user of the repo. These are used to make sure that you have the right credentials and folder paths to deploy your projects in the future.

![Open env folder](https://imgur.com/MOQqTwT.png)

Copy one of the files there, or use one of the examples in dap_appl_dala_code/scripts/local-env/, and start changing the credentials of the other user in your own file. The only things you have to change, are the things marked with red in the screenshot below. You have to change your mailadres at NALO and also your NALO credentials in the reporoot. If you have set the repo somewhere else, make sure to update the path accordingly.

![Open file](https://imgur.com/LSkLch3.png)

Now, run the file in your terminal. Make sure to save it, otherwise it won't work.

![Run file](https://imgur.com/yi3TikV.png)

If you have some errors after this step, it's usually because some modules couldn't be found and need to be installed. Run the script `development/help_scripts/start_up.ps1`. Normally, that should have worked, but if you're encountering some issues still; try executing the following commands in a powershell terminal (in VSC):

- ```Install-Module Az.Synapse -scope currentuser```
- ```Install-Module Az.Accounts -scope currentuser```
- ```Install-Module SQLServer -scope currentuser```

If there is still an issue, then you could try ```Connect-AzAccount``` to make sure that you are connected to azure. For this, don't forget that you have to be on the NALO network (via VPN or internal wifi).

You now need to commit the changes with git first. Everything can be found in the source control of VSC. Make sure you're not pushing to main, but to your own project / branch. Normally, pushing to main should not be allowed anyways.

PS. It's normal you won't see commit there at first, but it's possible you see a Publish button. This is used to publish your branch to DevOps, otherwise you won't see it there. You first need to publish your branch before you can commit to it.

![Source control](https://imgur.com/gupeqia.png)

Now, it's time to generate your first pull request.
</file>
<file name="wiki\General doc\3.First-pull-request.md">
# First pull request
## Description
In this ReadMe-file, you can find everything related to creating a pull request.

## Prerequisites
1. Make sure you have read 'Your first project', so you can actually create a pull request.

## Getting started
You have currently done a lot of things, but now you are going to create a request to merge it with the main branch. For this, you can go to the DevOps platform, Repo's and [pull requests](https://dev.azure.com/lnlbe/Data_Platform/_git/dap_appl_dala_code/pullrequests?_a=mine). If you don't have access to it or can't log in, be sure to check with your administrator(s)!

At the top of your screen, you will be seeing something which looks like a path (lnlbe/Data_Platform/Repos/Pull requests/REPO_NAME). That's normal, but make sure you have selected the correct repo to connect to. Otherwise, you won't be able to find your (git) branch.

![Search for repo](https://imgur.com/rOV7LNK.png)

If you have selected the repo, you usually have two options. In the first case, your pull request is displayed in a pop-up at the top of your page. You can select it an create the pull request based on your latest changes related to the repo.

![Pop up pull request](https://imgur.com/sXmQBd0.png)

If you don't see it or it's not the right branch, create a new pull request at the top.

![New pull request](https://imgur.com/qDJdynr.png)

In the new request, search for your desired branch you would like to commit. If you don't find it, then you're probably in the wrong repo. Or you haven't published your branch in VSC and didn't commit anything.

![Browse through branhces](https://imgur.com/WBhFNDg.png)

In the request itself, you can find several things you have to look at. Usually they are already pre-configured, but sometimes you need to change things too.

![Top part of request](https://imgur.com/EX2RCQ6.png)

At the top of the request, you can choose which branch you want to merge into. Usually that's main, but if you need something else, then you have to change it.

In files, you will find an overview of the changes you have made in contrast to the branch you're trying to merge into. So you see which changes are going to be made. Commits on the other hand is a collection of changes you have made in Visual Studio Code. If you have committed AND synced those changes, that is. You can click on them and see what changes you made during that commit.

As for the other parts of the request:

![Fields pull request](https://imgur.com/JC7v4Ve.png)
- The title: Doesn't need to be the same as the branch name. It's usually a quick summary of what you have changed. For example 'Made wiki' and that's it. Wouldn't write more than 5 words there.
- Description: Say something more about your PR if you feel like the title doesn't say plenty.
- Reviewers: If you work in a project, do add other reviewers so they get notified that they have a request waiting for them. They will then have to approve of your changes before you can complete your PR.
- Work items to link: That's based on the open tickets you have in the sprints. You can read more about that in the general README in the root of the repo. So make sure you create a ticket first you start to work on something.
- Tags: Not really necessary, but could come in handy for future look up. To link the right tags to your request, discuss this with your teamlead / administrator / manager / ...

And that's it! You can now submit a pull request, great! In the next documentation, we're going to look at checking a pull request and approving it.
</file>
<file name="wiki\General doc\4.Approve-and-check-PR.md">
# Approve &amp; check PR

## Description

In this ReadMe-file, you can find everything related to approving (your first/ a) pull request (PR).

## Prerequisites

1. Make sure you have checked "First pull request"

## Getting started

The pull requests are grouped by repo. So again, search for the right repo and you will find all the pull requests waiting to be approved and checked. Click on them to start.

![Search for repo](https://imgur.com/rOV7LNK.png)
![Top of PR](https://imgur.com/dsMAEt6.png)

At the top of your PR, you can see several things. The tabs will be discussed later in this documentation.

Regarding the other parts:

Under the title you can see a tag, to check the status.
![Tag](https://imgur.com/jbfkLlU.png)

At the right side of the top, you can approve the pull request yourself and set an auto-complete. The auto-complete can be set to make sure that, when the Pull request (PR) is approved, the merge is done automatically after approval.

![Show autocomplete](https://imgur.com/xV58bCq.png)

![Set auto-complete](https://imgur.com/QuZyeCU.png)
You can leave everything checked as is shown in the screenshot, besides maybe the required additional check. This is something you have to check first with your teamlead or someone who has requested this additional check. If it doesn't matter, you can uncheck it to make it not required.

PS. If you haven't set auto-complete, then you have to do the merge manually after the PR has been approved. So take my advice and turn it on.

On the right side of the PR, you can find additional information about the reviewers and if they are required or not. As well as about the tags and linked work items, that you had to configure in the initial PR-form.

![Right side PR](https://imgur.com/YG4Yb0B.png)

Now, the most important stuff in the pull request, is Files. The other parts are less important and 'Conflicts' is only used if you have conflicting merge issues. These should be resolved by the developer themselves and not by the reviewers of the PR.

![Files in PR](https://imgur.com/fCjPOcS.png)

In the files, you can check for changes and make comments for the person you are reviewing the PR for.

![Files example](https://imgur.com/k4TwzTj.png)

In Files, you will find several things:

- - after a file: There were changes made in this document. The file was recently added.
- - after a file: There were changes made in this document. Things were deleted from the file.
- Filenames ~~strikethrough~~: This means that a file was deleted completely.
- Filenames without anything behind them: There were changes made in this document. Something was changed in the document.

Please check those files to see what changes have been made. If you see some things that you would like to notify the requester about, or that they have to change something, it's best to leave them a comment.

![Comment](https://imgur.com/Ej75PFL.png)

If you want to be more precise about what part of the file you are having doubts about, select that part so your colleague know which specific part to look at.

![Specific selection](https://imgur.com/EkcoTPy.png)

That way, they know they have to change and look at something. Then they have to get back to the pull request, to overview, and check in the comments what changes have to be made / things need to be talked about.

![Comment overview](https://imgur.com/epYThsF.png)
Once they have looked at the comment (and changed something), they can click on resolve and then the pull request is ready to be approved.

That's all the things, in general, you need to know about when working with Azure DevOps. It's not that hard, and not everything was covered into extensive detail, but you'll get the picture. Now, just try out some other project and don't hesitate to ask for help to your colleagues!

</file>
<file name="wiki\synapse\unit_tests\functions.md">
[[__TOC__]]

# Functions

## Description
This markdown file contains the description of the unit tests executed on the functions contained in the Synapse Spark Notebooks.

## Unit tests

### list_directory_content()
* Purpose: Recursively list all the files and folders within a specific path in a container
* Unit tests:
&gt; - Valid unit tests:
&gt;&gt; 1. Given a full path to one specific file (not including the file), does the function return the file in the file-list and is the folder-list empty
&gt;&gt; 2. Given the path to a container, return the list of all the files and folders in the container

&gt; - Invalid unit tests:

### pk_match()
* Purpose: Create a string based on the primary columns 

### column_match()


### no_match()


### merge_file()



</file>
<file name="wiki\synapse\unit_tests\readme.md">
# Unit tests

## Description
This folder will describe all the unit test cases that are initiated during the testing of the Synapse architecture. Each markdown file will focus on one specific type of test:
1. Classes &amp; Methods
2. Functions
3. Workers

## Prerequisites
To be able to understand the unit tests, a deeper knowledge is required on the architecture of the Synapse Spark Notebook environment:
(TO DO: Add reference to Synapse Spark architecture markdown file)
</file>
<file name="wiki\user_guides\data_engineering_guide.md">
# Data Engineering Guide

Last edit: 09/02/2024

- [Data Engineering Guide](#data-engineering-guide)
  - [Introduction](#introduction)
  - [Contact details](#contact-details)
  - [Philosophy](#philosophy)
  - [General introduction: Frameworks and architectures](#general-introduction-frameworks-and-architectures)
    - [Data Lakehouse](#data-lakehouse)
    - [Medallion architecture](#medallion-architecture)
    - [Delta lake framework](#delta-lake-framework)
    - [Plan-Task configuration](#plan-task-configuration)
    - [Continuous integration, continuous deployment (CI/CD)](#continuous-integration-continuous-deployment-cicd)
    - [Bringing it all together](#bringing-it-all-together)
  - [Folder structure](#folder-structure)
    - [Configuration](#configuration)
    - [Development](#development)
    - [Pipelines](#pipelines)
    - [Src/source](#srcsource)
    - [Tests](#tests)
    - [Wiki](#wiki)


## Introduction
This guide is meant for people taking on the data engineering role in the DAP team. The role of a data engineer boils down to:
1. Maintaining the existing artefacts for SQL, PowerShell, and Synapse
2. Creating new functions and pipelines in Synapse
3. Adding new tables and stored procedures to the SQL project
4. Developing test scenarios for business cases

**Note:** This guide is meant to be timeless. This meant that any changes made to the repo should still be covered by the guide. As a result, the use of specific names of developed functions, tables, etc. is kept to a minimum.

## Contact details
For any questions or uncertainties, feel free to contact:
* joachim.baert@keyrus.com
* simon.plancke@keyrus.com
* dylan.verniers@keyrus.com

## Philosophy
Before diving into the tools and specifics of the repo, let's first shine a light on the philosophy of the repo and its orchestration. As will become clear further down this guide, there is a large focus on testing. The current development team believes that a good environment should always be properly tested before it is deployed to any production environment. Therefore, the team has a *test first, develop later* philosophy. This means that whenever an issue arises, a developer is expected to first write a failing test scenario that resembles the use case. After creating the test scenario, and validating that the expected error occurs, the developer can start on fixing the problem at hand. During the development, the test case can be used to check whether or not the new developments have solved the expected problem. If the test fails after the new developments have been implemented, chances are that the development was insufficient to solve the problem. 

The reason for taking this *test first* approach is so that a developer first needs to think about the problem they are trying to solve. This way, they will not be impacted by an already existing solution while doing so. This way, the test scenario often has a greater coverage instead of solving one specific problem.

## General introduction: Frameworks and architectures
The repository described in this user guide concerns the ingestion framework used at the National Lottery. The framework implements a set of well-known frameworks and architectures (see below). Overall, the ingestion framework uses a set of metadata tables to move files in between different containers on a data lake. Before a move from one container to another can happen, a set of validity checks is first executed (datatypes, expected column names, expected file format...). During this process, the aim is to keep the files as close to the original as possible, so calculated or derived columns are avoided as long as possible, as well as dropping columns. 

The aim of the ingestion process is to build a lakehouse using the delta lake framework, and move files along data lake containers using a medallion architecture.

### Data Lakehouse
A data lakehouse is an open data management architecture that combines the flexibility, cost-efficiency, and scale of data lakes with the data management and ACID transactions of data warehouses. The general idea is that a set of folders and files on a data lake container can be combined into one large database. A metadata layer sits on top of the folders and files to track which files are part of which table in the database.

More Information: https://www.databricks.com/glossary/data-lakehouse

### Medallion architecture
The medallion architecture is a data design pattern used to logically organize data in a lakehouse, with the goal of incrementally and progressively improving the structure and quality of data as it flows through each layer of the architecture. The structure implemented at the National Lottery uses 3 layers, next to the landing layer of where the source files are expected to be. Two of the three layers are made visible through a set of containers on the dedicated data lake on Azure [1], which are named raw and silver, whilst the third (gold) layer is resembled in the data marts [2].

The three layers, originally bronze, silver, and gold, each have their own quality checks. The idea is that a data source is moved from landing to raw, and gets converted to a parquet file. To move from raw to silver, an initial set of qualitiy checks are done. The main checks are making sure that the expected columns names are present and that the data in the columns is of the correct datatype (string, integer...). Moving from silver to gold, the source data gets split over different data marts necessary for the different business users.

**Note:** The data source movements from landing to silver are part of the self-service layer provided to the technical users of the National Lottery. Moving from silver to gold (for new sources) will most likely require human intervention and can therefore not be automated.

More information: https://www.databricks.com/glossary/medallion-architecture

[1]: See for example data lake account devdapstdala1.
[2]: At the time of writing (08/02/2024), the gold layer has not been developed yet.

### Delta lake framework
The delta lake framework is an open-source storage framework that can be built around a storage layer, providing the foundation for a lakehouse architecture. The delta lake, stored in the silver container, consists of a set of folders. Each folder represents a table in the lakehouse, and consists of a set of parquet and json files. When querying the table from the on-demand sql endpoint, the parquet and json files are combined to form a well-known structured SQL table. Further, the set-up allows a user to "timetravel". Less magical, this boils down to the fact that it is possible to query a previous version of the table. If it would, for example, be necessary to see the contents of the table from last quarter, this would theoretically be possible. One note to make here is that storing all files will become quite heavy over time. Policies are put in place for each data source to "vacuum" the table and reduce the number of versions that are being kept [3]

[3]: At the time of writing (08/02/2024), this feature has not been put into place yet.

### Plan-Task configuration
The plan-task configuration is a method used to execute a set of tasks in one batch (or plan). The idea here is that by invoking a specific plan, the system will figure out which tasks are part of this plan. Each task also has its own metadata assigned to it. For example, a data engineer wants to move a set of files from one container to another (not necessary all from and to the same container). The plan would be called 'move_files_plan'. The plan would contain a set of tasks, one for each file that you want to move. The metadata for each task would be the source and sink container that the file needs to moved to, and potentially some other metadata such as the file type (csv, json...) and the expected name of the file. If this set of files appears every day, instead of having to start an ETL pipeline for every file, the data engineer can just configure one pipeline to start the plan. Based on the tasks within that plan, and the metadata configured for each task, the files will be moved as desired. 

### Continuous integration, continuous deployment (CI/CD)
The process uses a CI/CD deployment framework with 5 environments: development, integration, test, acceptance, and production. Each environment has its own purpose, and their own settings. There are 3 building blocks used in this ingestion process: a Synapse Analytics workspace, self-made Powershell modules, and an Azure SQL Database. Each of the building blocks has its own CI/CD pipeline and testing framework. 

### Bringing it all together
The repo contains the files necessary to have a self-service, metadata-driven ingestion framework for a delta lake based lakehouse, using CI/CD practices for source code promotion. The Synapse workspace is the main orchestrator and executor of the framework. The notebooks of the workspace contain all the functions and methods needed to successfully ingest a data source into the delta lake. The pipelines contain the plan-task logic, where a plan is started and each task in that plan is executed. Since this plan-task framework is metadata-driven, there needs to be a strong connection to the Azure SQL database. This database contains all the metadata related to the plan-task configuration and the metadata for each task in a set of configuration tables. Next to that, the database also contains a set of stored procedures that query the configuration tables and return the necessary metadata to the metadata-driven Synapse pipeline.

Because ingesting the metadata into the SQL tables takes a lot of time when using *insert into* statements, the team has decided to build a Powershell module called **DataConfig**. This module allows a user to more easily insert metadata into the configuration tables as follows: The user configures two json files, one for the plan-tasks and one with the data source metadata (see templates in dap_appl_dala_config repo). When the user wants to deploy these files into the configruation tables, they can invoke Powershell function *Set-IngestionMetadataFromJsonFile* from the DataConifg module. The module will validate that the schema is correct, and ingest the relevant metadata into the correct configuration table. 


## Folder structure
Understanding the structure of the folders can already help a long way into understanding the overall ingestion process. It gives a first look into the actual code and allows you to get familiar with the thinking process of the team.

### Configuration
The idea of the configuration folder is that is can be closely linked to the deployment of different Azure services. That is, during the deployment of an Azure service using its ARM template, some parameters might need to be overwritten. The keys and values of these parameters can be stored in the configuration folder. This can for example be used when dealing with the deployment of a Synapse workspace using triggers. In the development environment, there is no need to have all triggers activated whilst they need to be activated for the production environment. Another example is key vault, where depending on the environment, different passwords will need to be used for a key

Example:
During the deployment of the Synapse workspace, the linked services will depend upon different services depending on the environment. In the development environment, for example, the SQL database containing the metadata is **dev**-dap-sqldb-core-meta whilst the environment database will be **int**-dap-sqldb-core-meta. To make sure the right links are used per environment, configuration file configuration/synapse/parameters.json was developed.

### Development
The development folder is there to help you locally develop the repo artefacts. Most important is the **myenv-template.ps1** script. As the name suggests, this is a template that you need to copy and paste in the same folder. Rename the copy to **myenv.ps1** and fill in the parameters depending on your local environment. Most of the parameters are defaults, pointing to the earlier discussed development environment.

**Note:** It is important to follow the naming conventions mentioned here. The myenv.ps1 script is part of the .gitignore, which prevents a file with local configurations to be part of the main branch when pushing changes.  

The use of these parameters becomes important when using the scripts in the **localdeploy** folder. The scripts in this folder are meant to mimic the DevOps CI pipelines (see folder **pipelines**). The CI pipelines, at their core, execute a set of scripts in a specific order. These same scripts are executed by the localdeploy-pipeline, and as such mimic the CI pipeline as well as possible. Conceptually, this boils down to the fact that one needs to first make sure that the developed scripts work locally before adding them to the DevOps pipeline. These localdeploy-pipelines are only allowed to be used in the development environment, and they use the environment parameters defined in the **myenv.ps1** script. 

**Note:** It is not always possible to completely match the DevOps CI pipelines. For Synapse, for example, there is not really a straight-forward way to build and deploy artefacts from the existing json files that can be repeated in the CI pipelines. In the CI pipeline, there is a specific task that allows to combine the Synapse architecture files into an ARM template, which is not a task that can be executed locally.

### Pipelines
The pipelines folder contains all the scripts that are executed by the CI and CD pipelines [4] in Azure DevOps. 

First, the orchestration of the pipeline jobs is done by the pipeline-ci-&lt;artefact&gt; yaml script. These scripts are the backbone of each pipeline, setting up the initial configuration and parameters, and invoking the correct pipeline jobs with the correct variables. The scripts for the jobs can be found in the **templates/jobs** folder. These scripts are created using the following orchestration:
1. Build job: The build job creates two pipeline artefacts that contains all the scripts and other files necessary to successfully deploy and test the artefact. One pipeline artefact (main artefact) contains all the scripts necessary to deploy the source code, and another (test artefact) contains all the scripts necessary to test the implementation of that source code.
2. Deploy job: The deploy job deploys the source code contained in the main artefact to the correct workspace/service. This is also the artefact that will later be deployed to the other, higher-level environments (tst, acc, prd).
3. Test job: The test job deploys the test artefact to the correct workspace/service. This is often source code that relates to (unit) testing the implementation of the environment, which is not needed in the higher-level environments.

In the pipeline-ci-&lt;artefact&gt; scripts, the deploy and test jobs are separated per environment, as they require different variables depending on the environment of where the deployment needs to happen. The variables are stored in the **variables** folder, where each environment has its dedicated script.

In the **templates/steps** folder, a set of reusable tasks can be found. These are tasks that are executed for different pipelines and act as "functions".

In the **scripts** folder, a set of powershell scripts can be found. These are the scripts that are called upon by the DevOps pipelines (and localdeploy pipelines), each having their own specific purpose in the CI process. The scripts are separated per artefact, and there is a **general** folder for scripts that can be called upon regardless of the artefact [5].

[4]: At the time of writing (09/02/2024), the CD pipelines still need to be developed.
[5]: At the time of writing  (09/02/2024), there are still some redundant scripts in the scripts folder


### Src/source
The src folder, also known as the source folder, contains the source code for the repo artefacts. This is the place were new developments should happen regarding the architecture of the ingetion process. At the time of writing (09/02/2024), the artifacts are:
* Powershell: Contains all the self-made modules for Powershell. Each module should have the same folder structure, where the main code is stored in a **private** and **public** folder. These folders contain a set of functions that build up the module. Each function is also expected to have a Tests.ps1 version that tests the inner workings of that function. If the tests need additional files, they should be stored in a **.test_config** folder. Next, each module should have a .psm1 and .psd1 file that describe the model and its data.


* SQL: Contains the sql metadata project, and its test project. The artefacts (tables, stored procedures, views...) that belong in the SQL metadata project should be stored in the **metadb** folder. The artefacts needed to test this metadata project should be stored in the **metadb_test** folder. Each one of these folders has a .sqlproj file that can be deployed to a SQL Server Instance, and will deploy all the artefacts contained in the project. The **_build/db_build.ps1** script will build these .sqlproj files using dotnet. The **_build/db_deploy.ps1** then deploys the file to a designated SQL Server Instance and SQL Database. For debugging purposes, this script also allows you to deploy the project as a script.

* Synapse: The Synapse workspace of the development environment is git-integrated, and the source code of this workspace can be found in the **studio** folder. Recommended would be to just use the Synpase UI when development needs to be done, but it is possible to make changes directly into the JSONs as well. The source code here will, during the CI/CD process be molded into an ARM template and deployed in the desired environment. The studio folder also contains a json file **template-parameter-definition.json** which is a great importance during this deployment process. It indicates which values of the ARM template should be parameterized during deployment. For example, underlying linked services need to changed depending on the environment. That is, when working in the development environment, a linked service is needed that points towards the development SQL Server. When deploying the ARM template to the integration environment, the same linked service should point to the integration SQL server. In the **_test** folder, the different test files for unittesting are stored.

**Note:** This will be discussed in another markdown file, but depending on the naming conventions of the files, they will be part of the main artefact, test artefact or no artefact at all. There is no possibility to organize the folder like the sql folder, clearly separating the main and test artefact. Therefore, the CI pipeline will build the artefacts based on naming conventions of the files.

### Tests
This folder contains all the files and scripts needed for testing. The **metadata** folder contains all the configuration files that need to be deployed to the SQL Meta DB. These files contain the metadata needed to successfully run some of the unit tests. That is, some test are focused on testing the interaction between different functions, pipelines, stored procedures, etc. This is mainly aimed towards the Synapse workspace, which is metadata oriented. This means that it is in constant communication with the SQL Meta DB and expects there to be metadata in the tables. To test, for example, whether a file can be successfully moved from the landing container to the silver container using the ingestion worker, metadata is needed to orchestrate the worker. Instead of having to write a bunch of *insert into* statements for the tests, the team has chosen to use the already validated DataConfig module to insert metadata into the already validated SQL Meta DB. 

The **test_scripts** folders then contains the scripts to validate whether or not the tests have executed successfully. The folder is separated into a sql and a synapse folder, since both services require different credentials to access their objects. The most basic SQL test validates whether the plans, tasks, and files were successfully logged. The Synapse tests validate the actual contents of the delta table that was creates/updated/deleted...

### Wiki
The wiki folder contains most of the documentation regarding the repo, including visual flows of which parts of the code impact one another.

</file>
</source>