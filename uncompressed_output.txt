<source type="github_repository" url="https://github.com/SimonPlancke/local_llm">
<file name="README.md">
# local_llm
</file>
<file name="answer.txt">
text='python

The SportCompetition program has the following potential shortcomings:

1. Lack of input validation: The program does not perform any input validation checks, which could lead to unexpected behavior or errors if the 
input data is not in the expected format or contains invalid values.

2. Magic numbers: The program contains several hard-coded "magic numbers" that are used in calculations and conditional statements. These 
numbers should be defined as constants or variables with descriptive names to improve readability and maintainability.

3. Code duplication: The program contains several instances of duplicate code, which could be refactored 
into reusable functions or methods to improve maintainability and reduce the risk of bugs.

4. Error handling: The program does not handle errors gracefully and could crash or produce unexpected results if an error occurs. Implementing proper 
error handling and logging mechanisms could help to identify and address errors more efficiently.

5. Scalability: The program is not designed to handle large datasets, which could impact its performance and efficiency. Implementing optimizations 
and best practices could improve its scalability and performance.

6. Testing: The program does not appear to have any testing implemented, which could lead to undetected bugs or errors. Implementing testing 
and validation checks could improve the reliability and robustness of the program.

7. Code documentation: The program lacks documentation, making it difficult to understand 
its purpose, functionality, and implementation. Adding comments and documentation could improve its readability and maintainability.

8. Code organization: The program could 
benefit from better organization and structure, such as breaking it down into smaller functions or modules, to make it easier to understand, maintain, 
and extend.'
</file>
<file name="debug.py">
content = '/tree/thisworks?'

if content.startswith(('/downloads', '/tree', '/uptodate')):
    pdf_url = 'https://sci-hub.se' + content
else:
    pdf_url = 'https:/' + content

print(pdf_url)
</file>
<file name="requirements.txt">
langchain==0.3.13
langchain_community
weaviate-client&gt;=3.26.7,&lt;4.0.0
protobuf==3.20
huggingface-hub
faiss-cpu
sentence-transformers
langchain_huggingface==0.1.2
</file>
<file name="src/python/create_text_file/create_text_file.py">
import sys
from urllib.parse import urlparse

# Import functions from library Rich (https://github.com/Textualize/rich)
from rich import print
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.prompt import Prompt
from rich.style import Style
from rich.syntax import Syntax
from rich.traceback import install
from rich.progress import Progress, TextColumn, BarColumn, TimeRemainingColumn

# Repository classes and scripts
from git_methods import GitMethods
import generic_functions as generic_functions
from file_processing import TextFileMethods, PDFFileMethods, FolderMethods, TranscriptionMethods

def safe_file_read(filepath, fallback_encoding:str='latin1') -&gt; str:
    """
    Safely reads the content of a file, attempting to handle encoding issues.

    This function first tries to read the file using UTF-8 encoding. If a UnicodeDecodeError occurs, 
    it attempts to read the file again using the specified fallback encoding.

    Parameters:
    ----------
    fallback_encoding : str, optional
        The encoding to use if the file cannot be read with UTF-8. 
        Default is 'latin1'.

    Returns:
    -------
    str
        The content of the file as a string.

    Raises:
    ------
    FileNotFoundError
        If the specified file does not exist.
    IOError
        If there is an error reading the file (e.g., permission issues).

    """
    try:
        with open(filepath, "r", encoding='utf-8') as file:
            return file.read()
    except UnicodeDecodeError:
        with open(filepath, "r", encoding=fallback_encoding) as file:
            return file.read()
        


def main():
    console = Console()

    intro_text = Text("\nInput Paths or URLs Processed:\n", style="dodger_blue1")
    input_types = [
        ("• Local folder path (flattens all files into text)", "bright_white"),
        ("• GitHub repository URL (flattens all files into text)", "bright_white"),
        ("• GitHub pull request URL (PR + Repo)", "bright_white"),
        ("• GitHub issue URL (Issue + Repo)", "bright_white"),
        ("• Documentation URL (base URL)", "bright_white"),
        ("• YouTube video URL (to fetch transcript)", "bright_white"),
        ("• ArXiv Paper URL", "bright_white"),
        ("• DOI or PMID to search on Sci-Hub", "bright_white"),
    ]

    for input_type, color in input_types:
        intro_text.append(f"\n{input_type}", style=color)

    intro_panel = Panel(
        intro_text,
        expand=False,
        border_style="bold",
        title="[bright_white]Copy to File and Clipboard[/bright_white]",
        title_align="center",
        padding=(1, 1),
    )
    console.print(intro_panel)

    # If an argument is passed, use it. Otherwise, prompt user to pass an argument
    if len(sys.argv) &gt; 1:
        input_path = sys.argv[1]
    else:
        input_path = Prompt.ask("\n[bold dodger_blue1]Enter the path or URL[/bold dodger_blue1]", console=console)
    
    console.print(f"\n[bold bright_green]You entered:[/bold bright_green] [bold bright_yellow]{input_path}[/bold bright_yellow]\n")

    output_file = "uncompressed_output.txt"
    processed_file = "compressed_output.txt"
    urls_list_file = "processed_urls.txt"

    with Progress(
        TextColumn("[bold bright_blue]{task.description}"),
        BarColumn(bar_width=None),
        TimeRemainingColumn(),
        console=console,
    ) as progress:

        task = progress.add_task("[bright_blue]Processing...", total=100)

        # Parse the input path to call the correct function
        try:
            # Git functions
            if "github.com" in input_path:
                gitmethods_object = GitMethods()
                final_output = gitmethods_object.handle_git_url()

            # URL functions
            elif urlparse(input_path).scheme in ["http", "https"]:
                if "youtube.com" in input_path or "youtu.be" in input_path:
                    final_output = TranscriptionMethods.fetch_youtube_transcript(input_path)
                elif "arxiv.org" in input_path:
                    final_output = PDFFileMethods.process_arxiv_pdf(input_path)
                # else:
                #     crawl_result = crawl_and_extract_text(input_path, max_depth=2, include_pdfs=True, ignore_epubs=True)
                #     final_output = crawl_result['content']
                #     with open(urls_list_file, 'w', encoding='utf-8') as urls_file:
                #         urls_file.write('\n'.join(crawl_result['processed_urls']))
            
            # Scientific papers from https://sci-hub.se/
            elif input_path.startswith("10.") and "/" in input_path or input_path.isdigit():
                final_output = PDFFileMethods.process_doi_or_pmid(input_path)
            
            # Local folder
            else:
                final_output = FolderMethods.process_local_folder(input_path)

            progress.update(task, advance=50)

            # Write the uncompressed output
            with open(output_file, "w", encoding="utf-8") as file:
                file.write(final_output)


            # Process the compressed output
            TextFileMethods.parse_text_as_xml(output_file, processed_file)

            progress.update(task, advance=50)

            compressed_text = safe_file_read(processed_file)
            compressed_token_count = get_token_count(compressed_text)
            console.print(f"\n[bright_green]Compressed Token Count:[/bright_green] [bold bright_cyan]{compressed_token_count}[/bold bright_cyan]")

            uncompressed_text = safe_file_read(output_file)
            uncompressed_token_count = get_token_count(uncompressed_text)
            console.print(f"[bright_green]Uncompressed Token Count:[/bright_green] [bold bright_cyan]{uncompressed_token_count}[/bold bright_cyan]")

            console.print(f"\n[bold bright_yellow]{processed_file}[/bold bright_yellow] and [bold bright_blue]{output_file}[/bold bright_blue] have been created in the working directory.")

            pyperclip.copy(uncompressed_text)
            console.print(f"\n[bright_white]The contents of [bold bright_blue]{output_file}[/bold bright_blue] have been copied to the clipboard.[/bright_white]")

        except Exception as e:
            console.print(f"\n[bold red]An error occurred:[/bold red] {str(e)}")
            console.print("\nPlease check your input and try again.")
            raise  # Re-raise the exception for debugging purposes


if __name__ == "__main__":
    main()

</file>
<file name="src/python/create_text_file/file_processing.py">
import re
import os
import requests
import wget

from generic_functions import get_stopword_list, escape_xml, is_allowed_filetype, process_ipynb_file, download_file

import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from PyPDF2 import PdfReader
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter


class TextFileMethods():
    def __init__(self, filepath:str):
        self.filepath = filepath

    @staticmethod
    def clean_text(text):
        # Remove new-line characters
        text = re.sub(r"[\n\r]+", "\n", text)
        # Remove unwanted characters
        text = re.sub(r"[^a-zA-Z0-9\s_.,!?:;@#$%^&amp;*()+\-=[\]{}|\\&lt;&gt;`~'\"/]+", "", text)
        # Normalize Whitespace
        text = re.sub(r"\s+", " ", text)
        # Convert to Lowercase
        text = text.lower()
        # Split into words
        words = text.split()

        # Drop stop words (the, is, in...)
        stop_words = get_stopword_list()
        words = [word for word in words if word not in stop_words]
        
        return " ".join(words)


    def parse_text_as_xml(self, input_file, output_file):
        with open(input_file, "r", encoding="utf-8") as input_file:
            input_text = input_file.read()

        try:
            # Try to parse the input as XML
            root = ET.fromstring(input_text)

            # Process text content while preserving XML structure
            for elem in root.iter():
                if elem.text:
                    elem.text = self.clean_text(elem.text)
                if elem.tail:
                    elem.tail = self.clean_text(elem.tail)

            # Write the processed XML to the output file
            tree = ET.ElementTree(root)
            tree.write(output_file, encoding="utf-8", xml_declaration=True)
            print("Text preprocessing completed with XML structure preserved.")
        except ET.ParseError:
            # If XML parsing fails, process the text without preserving XML structure
            processed_text = self.clean_text(input_text)
            with open(output_file, "w", encoding="utf-8") as out_file:
                out_file.write(processed_text)
            Warning("XML parsing failed. Text preprocessing completed without XML structure.")


class PDFFileMethods():

    def add_xml_tags_for_paper(self, text):
        paper_xml_formatted_text = '&lt;paper&gt;\n'
        paper_xml_formatted_text += escape_xml(' '.join(text))
        paper_xml_formatted_text += '\n&lt;/paper&gt;\n'

        return paper_xml_formatted_text

    def process_pdf(self, url):
        # Download PDF content
        response = requests.get(url)
        response.raise_for_status()
        content = response.content

        with open('temp.pdf', 'wb') as pdf_file:
            pdf_file.write(content)

        text = []
        with open('temp.pdf', 'rb') as pdf_file:
            pdf_reader = PdfReader(pdf_file)
            for page in range(len(pdf_reader.pages)):
                text.append(pdf_reader.pages[page].extract_text())

        os.remove('temp.pdf')
        return ' '.join(text)

    def process_arxiv_pdf(self, arxiv_abs_url):
        pdf_url = arxiv_abs_url.replace("/abs/", "/pdf/") + ".pdf"
        text = self.process_pdf(pdf_url)

        formatted_text = f'&lt;source type="arxiv_paper" url="{arxiv_abs_url}"&gt;\n'
        formatted_text += self.add_xml_tags_for_paper(text)
        formatted_text += '&lt;/source&gt;'

        os.remove('temp.pdf')
        print("ArXiv paper processed successfully.")

        return formatted_text

    def process_doi_or_pmid(self, identifier):
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 6.3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
            'Connection': 'keep-alive'
        }

        try:
            payload = {
                'sci-hub-plugin-check': '',
                'request': identifier
            }

            base_url = 'https://sci-hub.se/'
            response = requests.post(base_url, headers=headers, data=payload, timeout=60)
            soup = BeautifulSoup(response.content, 'html.parser')
            pdf_element = soup.find(id='pdf')

            if pdf_element is None:
                raise ValueError(f"No PDF found for identifier {identifier}. Sci-hub might be inaccessible or the document is not available.")

            content = pdf_element.get('src').replace('#navpanes=0&amp;view=FitH', '').replace('//', '/')

            if content.startswith(('/downloads', '/tree', '/uptodate')):
                pdf_url = 'https://sci-hub.se' + content
            else:
                pdf_url = 'https:/' + content

            pdf_filename = f"{identifier.replace('/', '-')}.pdf"
            wget.download(pdf_url, pdf_filename)

            with open(pdf_filename, 'rb') as pdf_file:
                pdf_reader = PdfReader(pdf_file)
                text = ""
                for page in range(len(pdf_reader.pages)):
                    text += pdf_reader.pages[page].extract_text()

            formatted_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n'
            formatted_text += self.add_xml_tags_for_paper(text)
            formatted_text += '&lt;/source&gt;'

            os.remove(pdf_filename)
            print(f"Identifier {identifier} processed successfully.")
            return formatted_text
        
        except (requests.RequestException, ValueError) as e:
            error_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n'
            error_text += f'&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n'
            error_text += '&lt;/source&gt;'
            print(f"Error processing identifier {identifier}: {str(e)}")
            print("Sci-hub appears to be inaccessible or the document was not found. Please try again later.")
            return error_text


# HERE!!!!
class FolderMethods():
    def __init__(self, filepath):
        self.filepath = filepath

    def process_local_directory(self):
        content = [f'&lt;source type="local_directory" path="{escape_xml(self.local_path)}"&gt;']
        for root, files in os.walk(self.local_path):
            for file in files:
                if is_allowed_filetype(file):
                    print(f"Processing {os.path.join(root, file)}...")

                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, self.local_path)
                    content.append(f'&lt;file name="{escape_xml(relative_path)}"&gt;')

                    if file.endswith(".ipynb"):
                        content.append(escape_xml(process_ipynb_file(file_path)))
                    else:
                        with open(file_path, "r", encoding='utf-8', errors='ignore') as f:
                            content.append(escape_xml(f.read()))

                    content.append('&lt;/file&gt;')

        content.append('&lt;/source&gt;')
        return '\n'.join(content)

    def process_local_folder(self, local_path):

        formatted_content = self.process_local_directory(local_path)
        print("All files processed.")
        return formatted_content
    
    def process_directory(self, url, output):
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        files = response.json()

        for file in files:
            if file["type"] == "file" and is_allowed_filetype(file["name"]):
                print(f"Processing {file['path']}...")

                temp_file = f"temp_{file['name']}"
                download_file(file["download_url"], temp_file)

                output.write(f"# {'-' * 3}\n")
                output.write(f"# Filename: {file['path']}\n")
                output.write(f"# {'-' * 3}\n\n")

                if file["name"].endswith(".ipynb"):
                    output.write(process_ipynb_file(temp_file))
                else:
                    with open(temp_file, "r", encoding='utf-8', errors='ignore') as f:
                        output.write(f.read())

                output.write("\n\n")
                os.remove(temp_file)
            elif file["type"] == "dir":
                self.process_directory(file["url"], output)

class TranscriptionMethods():

    def __init__(self, url):
        self.url = url

    def extract_video_id(self, url):
        pattern = r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&amp;]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})'
        match = re.search(pattern, url)
        if match:
            return match.group(1)
        return None


    def fetch_youtube_transcript(self):
        video_id = self.extract_video_id(self.url)
        if not video_id:
            return f'&lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&gt;\n&lt;error&gt;Could not extract video ID from URL.&lt;/error&gt;\n&lt;/source&gt;'

        try:
            transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
            formatter = TextFormatter()
            transcript = formatter.format_transcript(transcript_list)
            
            formatted_text = f'&lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&gt;\n'
            formatted_text += '&lt;transcript&gt;\n'
            formatted_text += escape_xml(transcript)
            formatted_text += '\n&lt;/transcript&gt;\n'
            formatted_text += '&lt;/source&gt;'
            
            return formatted_text
        except Exception as e:
            return f'&lt;source type="youtube_transcript" url="{escape_xml(self.url)}"&gt;\n&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n&lt;/source&gt;'

</file>
<file name="src/python/create_text_file/git_methods.py">
import requests
import os
import re

from dotenv import load_dotenv
from generic_functions import is_allowed_filetype, escape_xml, process_ipynb_file, download_file

class GitMethods():
    def __init__(self, url):
        self.url = url
        self.headers = self.get_github_token()


    @staticmethod
    def get_github_token():
        load_dotenv()
        github_token = os.getenv('GITHUB_TOKEN')
        headers = {"Authorization": f"token {github_token}"}
        return headers

    def process_github_main_branch(self, repo_owner, repo_name):
        repo_url = f"https://github.com/{repo_owner}/{repo_name}"
        repo_content = self.process_github_repo(repo_url)
        
        xml_formatted_repository = '&lt;repository&gt;\n'
        xml_formatted_repository += repo_content
        xml_formatted_repository += '&lt;/repository&gt;\n'
        xml_formatted_repository += '&lt;/source&gt;'

        return xml_formatted_repository

    def handle_git_url(self):
        if "/pull/" in self.url:
            final_output = self.process_github_pull_request()
        elif "/issues/" in self.url:
            final_output = self.process_github_issue()
        else:
            final_output = self.process_github_repo()

        return final_output
    
    def process_git_directory(self, repo_content):
        response = requests.get(self.url, headers=self.headers)
        response.raise_for_status()
        files = response.json()

        for file in files:
            if file["type"] == "file" and is_allowed_filetype(file["name"]):
                print(f"Processing {file['path']}...")

                temp_file = f"temp_{file['name']}"
                download_file(file["download_url"], temp_file, self.headers)

                repo_content.append(f'&lt;file name="{escape_xml(file["path"])}"&gt;') 
                if file["name"].endswith(".ipynb"):
                    repo_content.append(escape_xml(process_ipynb_file(temp_file)))
                else:
                    with open(temp_file, "r", encoding='utf-8', errors='ignore') as f:
                        repo_content.append(escape_xml(f.read()))
                repo_content.append('&lt;/file&gt;')
                os.remove(temp_file)

            elif file["type"] == "dir":
                self.process_git_directory(file["url"], repo_content)
            else:
                print(f"Skipped {file}")


    def process_github_repo(self, base_url=None):
        # Only possible for public repositories
        # Pull repository locally and use local path if private
        if not base_url:
            base_url = self.url

        url_parts = base_url.split("/")
        api_base_url = "https://api.github.com/repos"
        repo_url_parts = url_parts.split("https://github.com/")[-1].split("/")
        repo_name = "/".join(repo_url_parts[:2])

        # Detect if we have a branch or tag reference
        branch_or_tag = ""
        subdirectory = ""
        if len(repo_url_parts) &gt; 2 and repo_url_parts[2] == "tree":
            # The branch or tag name should be at index 3
            if len(repo_url_parts) &gt; 3:
                branch_or_tag = repo_url_parts[3]
            # Any remaining parts after the branch/tag name form the subdirectory
            if len(repo_url_parts) &gt; 4:
                subdirectory = "/".join(repo_url_parts[4:])
        
        contents_url = f"{api_base_url}/{repo_name}/contents"
        if subdirectory:
            contents_url = f"{contents_url}/{subdirectory}"
        if branch_or_tag:
            contents_url = f"{contents_url}?ref={branch_or_tag}"

        # Configure a variable with a XML-like structure
        # This variable will be exported as a .txt file
        repo_content = [f'&lt;source type="github_repository" url="{url_parts}"&gt;']

        self.process_git_directory(contents_url, repo_content, self.headers)
        repo_content.append('&lt;/source&gt;')
        print("All files processed.")

        return "\n".join(repo_content)


    def process_github_pull_request(self):
        url_parts = self.url.split("/")
        repo_owner = url_parts[3]
        repo_name = url_parts[4]
        pull_request_number = url_parts[-1]
        api_base_url = "https://api.github.com/repos"

        pull_request_url = f"{api_base_url}/{repo_owner}/{repo_name}/pulls/{pull_request_number}"
        
    #region PR: Get changes and comments
        # 1. Get all metadata related to the PR
        response = requests.get(pull_request_url, headers=self.headers)
        pull_request_data = response.json()

        # 2. Get all changes made to the repository
        diff_url = pull_request_data["diff_url"]
        diff_response = requests.get(diff_url, headers=self.headers)
        pull_request_diff = diff_response.text

        # 3. Get all PR-level comments (general comments left on the submitted PR)
        comments_url = pull_request_data["comments_url"]
        comments_response = requests.get(comments_url, headers=self.headers)
        comments_data = comments_response.json()

        # 4. Get all code-level comments (comments on the changed files)
        review_comments_url = pull_request_data["review_comments_url"]
        review_comments_response = requests.get(review_comments_url, headers=self.headers)
        review_comments_data = review_comments_response.json()

        # 5. Sort the comments on their position
        all_comments = comments_data + review_comments_data
        all_comments.sort(key=lambda comment: comment.get("position") or float("inf"))
    #endregion


    #region PR: Add XML elements
        # Add opening element: pull_request_info
        formatted_text = f'&lt;source type="github_pull_request" url="{self.url}"&gt;\n'
        formatted_text += '&lt;pull_request_info&gt;\n'

        # Add generic elements: title, description, merge_details
        formatted_text += f'&lt;title&gt;{generic_functions.escape_xml(pull_request_data["title"])}&lt;/title&gt;\n'
        formatted_text += f'&lt;description&gt;{generic_functions.escape_xml(pull_request_data["body"])}&lt;/description&gt;\n'
        formatted_text += '&lt;merge_details&gt;\n'
        formatted_text += f'{generic_functions.escape_xml(pull_request_data["user"]["login"])} wants to merge {pull_request_data["commits"]} commit into {repo_owner}:{pull_request_data["base"]["ref"]} from {pull_request_data["head"]["label"]}\n'
        formatted_text += '&lt;/merge_details&gt;\n'

        # Iteratively add PR changes and comments left on PR as text fields
        # Parent element: diff_and_comments
        formatted_text += '&lt;diff_and_comments&gt;\n'
        diff_lines = pull_request_diff.split("\n")
        comment_index = 0

        for line in diff_lines:
            formatted_text += f'{generic_functions.escape_xml(line)}\n'
            while comment_index &lt; len(all_comments) and all_comments[comment_index].get("position") == diff_lines.index(line):
                comment = all_comments[comment_index]
                # Comment element
                """
                &lt;review_comment&gt;
                    &lt;author&gt;xxx&lt;/author&gt;
                    &lt;content&gt;xxx&lt;/content&gt;
                    &lt;path&gt;xxx&lt;/path&gt;
                    &lt;line&gt;xxx&lt;/line&gt;
                &lt;/review_comment&gt;
                """
                formatted_text += f'&lt;review_comment&gt;\n'
                formatted_text += f'&lt;author&gt;{generic_functions.escape_xml(comment["user"]["login"])}&lt;/author&gt;\n'
                formatted_text += f'&lt;content&gt;{generic_functions.escape_xml(comment["body"])}&lt;/content&gt;\n'
                formatted_text += f'&lt;path&gt;{generic_functions.escape_xml(comment["path"])}&lt;/path&gt;\n'
                formatted_text += f'&lt;line&gt;{comment["original_line"]}&lt;/line&gt;\n'
                formatted_text += '&lt;/review_comment&gt;\n'
                comment_index += 1

        # Close open elements
        formatted_text += '&lt;/diff_and_comments&gt;\n'
        formatted_text += '&lt;/pull_request_info&gt;\n'

        # Regular processing of the repo
        formatted_text += self.process_github_main_branch(repo_owner, repo_name)
    #endregion

        print(f"Pull request {pull_request_number} and repository content processed successfully.")
        return formatted_text
        

    def process_github_issue(self):
        url_parts = self.issue_url.split("/")
        repo_owner = url_parts[3]
        repo_name = url_parts[4]
        issue_number = url_parts[-1]
        api_base_url = "https://api.github.com/repos"

        issue_url = f"{api_base_url}/{repo_owner}/{repo_name}/issues/{issue_number}"
    
    #region ISSUE: Get metadata
        response = requests.get(issue_url, headers=self.headers)
        issue_data = response.json()

        comments_url = issue_data["comments_url"]
        comments_response = requests.get(comments_url, headers=self.headers)
        comments_data = comments_response.json()
    #endregion


    #region ISSUE: Add XML elements
        # Add opening element: issue_info
        formatted_text = f'&lt;source type="github_issue" url="{self.issue_url}"&gt;\n'
        formatted_text += '&lt;issue_info&gt;\n'

        # Add generic elements: title, description
        formatted_text += f'&lt;title&gt;{generic_functions.escape_xml(issue_data["title"])}&lt;/title&gt;\n'
        formatted_text += f'&lt;description&gt;{generic_functions.escape_xml(issue_data["body"])}&lt;/description&gt;\n'
        
        # Iteratively add comments left on PR as text fields
        # Parent element: comments
        formatted_text += '&lt;comments&gt;\n'

        for comment in comments_data:
            # Comment element
            """
            &lt;comment&gt;
                &lt;author&gt;xxx&lt;/author&gt;
                &lt;content&gt;
                    &lt;code_snippet&gt;xxx&lt;/code_snippet&gt;
                &lt;/content&gt;
            &lt;/comment&gt;
            """
            formatted_text += '&lt;comment&gt;\n'
            formatted_text += f'&lt;author&gt;{generic_functions.escape_xml(comment["user"]["login"])}&lt;/author&gt;\n'
            formatted_text += f'&lt;content&gt;{generic_functions.escape_xml(comment["body"])}&lt;/content&gt;\n'

            # Extract code snippets from comments on a GitHub issue
            # Regex: Find all occurrences of URLs in the comment body that match the pattern for GitHub file links with line ranges
            code_snippets = re.findall(r'https://github.com/.*#L\d+-L\d+', comment['body'])
            for snippet_url in code_snippets:
                # Split URL into two parts: the base URL and the line range.
                url_parts = snippet_url.split("#")

                # Use the FILE URL to get the raw contents of the file
                try:
                    file_url = url_parts[0].replace("/blob/", "/raw/")
                    file_response = requests.get(file_url, headers=self.headers)
                    file_content = file_response.text

                    # Using the file content, get the relevant code snippets back using the start and end indices
                    line_range = url_parts[1]
                    start_line, end_line = map(int, line_range.split("-")[0][1:]), map(int, line_range.split("-")[1][1:])
                    code_lines = file_content.split("\n")[start_line-1:end_line]
                    code_snippet = "\n".join(code_lines)

                    # Format the final code snippet using XML
                    formatted_text += '&lt;code_snippet&gt;\n'
                    formatted_text += f'&lt;![CDATA[{code_snippet}]]&gt;\n'
                    formatted_text += '&lt;/code_snippet&gt;\n'
                except requests.exceptions.RequestException as e:
                    Warning(f"Error fetching file: {e}")
                    continue  # Skip to the next snippet if there's an error

            formatted_text += '&lt;/comment&gt;\n'

        # Close open elements
        formatted_text += '&lt;/comments&gt;\n'
        formatted_text += '&lt;/issue_info&gt;\n'

        # Regular processing of the repo
        formatted_text += self.process_github_main_branch(repo_owner, repo_name)
    #endregion

        print(f"Issue {issue_number} and repository content processed successfully.")

        return formatted_text
</file>
<file name="src/python/create_text_file/onefilellm.py">
import requests
from bs4 import BeautifulSoup, Comment
from urllib.parse import urljoin, urlparse
from PyPDF2 import PdfReader
import os
import sys
import tiktoken
import nltk
from nltk.corpus import stopwords
import re
from pathlib import Path
import nbformat
from nbconvert import PythonExporter
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api.formatters import TextFormatter
import pyperclip
import wget
from tqdm import tqdm
from time import sleep
from rich import print
from rich.console import Console
from rich.panel import Panel
from rich.text import Text
from rich.prompt import Prompt
from rich.style import Style
from rich.syntax import Syntax
from rich.traceback import install
from rich.progress import Progress, TextColumn, BarColumn, TimeRemainingColumn
import xml.etree.ElementTree as ET
from dotenv import load_dotenv

load_dotenv()
github_token = os.getenv('GITHUB_TOKEN')


def safe_file_read(filepath, fallback_encoding='latin1'):
    try:
        with open(filepath, "r", encoding='utf-8') as file:
            return file.read()
    except UnicodeDecodeError:
        with open(filepath, "r", encoding=fallback_encoding) as file:
            return file.read()

nltk.download("stopwords", quiet=True)
stop_words = set(stopwords.words("english"))

TOKEN = os.getenv('GITHUB_TOKEN', github_token)
if TOKEN == 'default_token_here':
    raise EnvironmentError("GITHUB_TOKEN environment variable not set.")

headers = {"Authorization": f"token {TOKEN}"}

def download_file(url, target_path):
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    with open(target_path, "wb") as f:
        f.write(response.content)

def is_allowed_filetype(filename):
    allowed_extensions = ['.py', '.txt', '.js', '.tsx', '.ts', '.md', '.cjs', '.html', '.json', '.ipynb', '.h', '.localhost', '.sh', '.yaml', '.example']
#    allowed_extensions = ['.md']
    return any(filename.endswith(ext) for ext in allowed_extensions)
    

def process_ipynb_file(temp_file):
    with open(temp_file, "r", encoding='utf-8', errors='ignore') as f:
        notebook_content = f.read()

    exporter = PythonExporter()
    python_code, _ = exporter.from_notebook_node(nbformat.reads(notebook_content, as_version=4))
    return python_code

def process_directory(url, output):
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    files = response.json()

    for file in files:
        if file["type"] == "file" and is_allowed_filetype(file["name"]):
            print(f"Processing {file['path']}...")

            temp_file = f"temp_{file['name']}"
            download_file(file["download_url"], temp_file)

            output.write(f"# {'-' * 3}\n")
            output.write(f"# Filename: {file['path']}\n")
            output.write(f"# {'-' * 3}\n\n")

            if file["name"].endswith(".ipynb"):
                output.write(process_ipynb_file(temp_file))
            else:
                with open(temp_file, "r", encoding='utf-8', errors='ignore') as f:
                    output.write(f.read())

            output.write("\n\n")
            os.remove(temp_file)
        elif file["type"] == "dir":
            process_directory(file["url"], output)

def process_local_directory(local_path, output):
    for root, dirs, files in os.walk(local_path):
        for file in files:
            if is_allowed_filetype(file):
                print(f"Processing {os.path.join(root, file)}...")

                output.write(f"# {'-' * 3}\n")
                output.write(f"# Filename: {os.path.join(root, file)}\n")
                output.write(f"# {'-' * 3}\n\n")

                file_path = os.path.join(root, file)

                if file.endswith(".ipynb"):
                    output.write(process_ipynb_file(file_path))
                else:
                    with open(file_path, "r", encoding='utf-8', errors='ignore') as f:
                        output.write(f.read())

                output.write("\n\n")

def process_github_repo(repo_url):
    api_base_url = "https://api.github.com/repos/"
    repo_url_parts = repo_url.split("https://github.com/")[-1].split("/")
    repo_name = "/".join(repo_url_parts[:2])

    # Detect if we have a branch or tag reference
    branch_or_tag = ""
    subdirectory = ""
    if len(repo_url_parts) &gt; 2 and repo_url_parts[2] == "tree":
        # The branch or tag name should be at index 3
        if len(repo_url_parts) &gt; 3:
            branch_or_tag = repo_url_parts[3]
        # Any remaining parts after the branch/tag name form the subdirectory
        if len(repo_url_parts) &gt; 4:
            subdirectory = "/".join(repo_url_parts[4:])
    
    contents_url = f"{api_base_url}{repo_name}/contents"
    if subdirectory:
        contents_url = f"{contents_url}/{subdirectory}"
    if branch_or_tag:
        contents_url = f"{contents_url}?ref={branch_or_tag}"

    repo_content = [f'&lt;source type="github_repository" url="{repo_url}"&gt;']

    def process_directory(url, repo_content):
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        files = response.json()

        for file in files:
            if file["type"] == "file" and is_allowed_filetype(file["name"]):
                print(f"Processing {file['path']}...")

                temp_file = f"temp_{file['name']}"
                download_file(file["download_url"], temp_file)

                repo_content.append(f'&lt;file name="{escape_xml(file["path"])}"&gt;') 
                if file["name"].endswith(".ipynb"):
                    repo_content.append(escape_xml(process_ipynb_file(temp_file)))
                else:
                    with open(temp_file, "r", encoding='utf-8', errors='ignore') as f:
                        repo_content.append(escape_xml(f.read()))
                repo_content.append('&lt;/file&gt;')
                os.remove(temp_file)

            elif file["type"] == "dir":
                process_directory(file["url"], repo_content)

    process_directory(contents_url, repo_content)
    repo_content.append('&lt;/source&gt;')
    print("All files processed.")

    return "\n".join(repo_content)

def process_local_folder(local_path):
    def process_local_directory(local_path):
        content = [f'&lt;source type="local_directory" path="{escape_xml(local_path)}"&gt;']
        for root, dirs, files in os.walk(local_path):
            for file in files:
                if is_allowed_filetype(file):
                    print(f"Processing {os.path.join(root, file)}...")

                    file_path = os.path.join(root, file)
                    relative_path = os.path.relpath(file_path, local_path)
                    content.append(f'&lt;file name="{escape_xml(relative_path)}"&gt;')

                    if file.endswith(".ipynb"):
                        content.append(escape_xml(process_ipynb_file(file_path)))
                    else:
                        with open(file_path, "r", encoding='utf-8', errors='ignore') as f:
                            content.append(escape_xml(f.read()))

                    content.append('&lt;/file&gt;')

        content.append('&lt;/source&gt;')
        return '\n'.join(content)

    formatted_content = process_local_directory(local_path)
    print("All files processed.")
    return formatted_content

def process_arxiv_pdf(arxiv_abs_url):
    pdf_url = arxiv_abs_url.replace("/abs/", "/pdf/") + ".pdf"
    response = requests.get(pdf_url)
    pdf_content = response.content

    with open('temp.pdf', 'wb') as pdf_file:
        pdf_file.write(pdf_content)

    text = []
    with open('temp.pdf', 'rb') as pdf_file:
        pdf_reader = PdfReader(pdf_file)
        for page in range(len(pdf_reader.pages)):
            text.append(pdf_reader.pages[page].extract_text())

    formatted_text = f'&lt;source type="arxiv_paper" url="{arxiv_abs_url}"&gt;\n'
    formatted_text += '&lt;paper&gt;\n'
    formatted_text += escape_xml(' '.join(text))
    formatted_text += '\n&lt;/paper&gt;\n'
    formatted_text += '&lt;/source&gt;'

    os.remove('temp.pdf')
    print("ArXiv paper processed successfully.")

    return formatted_text

def extract_links(input_file, output_file):
    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    
    with open(input_file, 'r', encoding='utf-8') as file:
        content = file.read()
        urls = re.findall(url_pattern, content)
    
    with open(output_file, 'w', encoding='utf-8') as output:
        for url in urls:
            output.write(url + '\n')

def fetch_youtube_transcript(url):
    def extract_video_id(url):
        pattern = r'(?:https?:\/\/)?(?:www\.)?(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&amp;]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})'
        match = re.search(pattern, url)
        if match:
            return match.group(1)
        return None

    video_id = extract_video_id(url)
    if not video_id:
        return f'&lt;source type="youtube_transcript" url="{escape_xml(url)}"&gt;\n&lt;error&gt;Could not extract video ID from URL.&lt;/error&gt;\n&lt;/source&gt;'

    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
        formatter = TextFormatter()
        transcript = formatter.format_transcript(transcript_list)
        
        formatted_text = f'&lt;source type="youtube_transcript" url="{escape_xml(url)}"&gt;\n'
        formatted_text += '&lt;transcript&gt;\n'
        formatted_text += escape_xml(transcript)
        formatted_text += '\n&lt;/transcript&gt;\n'
        formatted_text += '&lt;/source&gt;'
        
        return formatted_text
    except Exception as e:
        return f'&lt;source type="youtube_transcript" url="{escape_xml(url)}"&gt;\n&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n&lt;/source&gt;'

def preprocess_text(input_file, output_file):
    with open(input_file, "r", encoding="utf-8") as input_file:
        input_text = input_file.read()

    def process_text(text):
        text = re.sub(r"[\n\r]+", "\n", text)
        # Update the following line to include apostrophes and quotation marks
        text = re.sub(r"[^a-zA-Z0-9\s_.,!?:;@#$%^&amp;*()+\-=[\]{}|\\&lt;&gt;`~'\"/]+", "", text)
        text = re.sub(r"\s+", " ", text)
        text = text.lower()
        words = text.split()
        words = [word for word in words if word not in stop_words]
        return " ".join(words)

    try:
        # Try to parse the input as XML
        root = ET.fromstring(input_text)

        # Process text content while preserving XML structure
        for elem in root.iter():
            if elem.text:
                elem.text = process_text(elem.text)
            if elem.tail:
                elem.tail = process_text(elem.tail)

        # Write the processed XML to the output file
        tree = ET.ElementTree(root)
        tree.write(output_file, encoding="utf-8", xml_declaration=True)
        print("Text preprocessing completed with XML structure preserved.")
    except ET.ParseError:
        # If XML parsing fails, process the text without preserving XML structure
        processed_text = process_text(input_text)
        with open(output_file, "w", encoding="utf-8") as out_file:
            out_file.write(processed_text)
        print("XML parsing failed. Text preprocessing completed without XML structure.")

def get_token_count(text, disallowed_special=[], chunk_size=1000):
    enc = tiktoken.get_encoding("cl100k_base")

    # Remove XML tags
    text_without_tags = re.sub(r'&lt;[^&gt;]+&gt;', '', text)

    # Split the text into smaller chunks
    chunks = [text_without_tags[i:i+chunk_size] for i in range(0, len(text_without_tags), chunk_size)]
    total_tokens = 0

    for chunk in chunks:
        tokens = enc.encode(chunk, disallowed_special=disallowed_special)
        total_tokens += len(tokens)
    
    return total_tokens

def is_same_domain(base_url, new_url):
    return urlparse(base_url).netloc == urlparse(new_url).netloc

def is_within_depth(base_url, current_url, max_depth):
    base_parts = urlparse(base_url).path.rstrip('/').split('/')
    current_parts = urlparse(current_url).path.rstrip('/').split('/')

    if current_parts[:len(base_parts)] != base_parts:
        return False

    return len(current_parts) - len(base_parts) &lt;= max_depth

def process_pdf(url):
    response = requests.get(url)
    response.raise_for_status()

    with open('temp.pdf', 'wb') as pdf_file:
        pdf_file.write(response.content)

    text = []
    with open('temp.pdf', 'rb') as pdf_file:
        pdf_reader = PdfReader(pdf_file)
        for page in range(len(pdf_reader.pages)):
            text.append(pdf_reader.pages[page].extract_text())

    os.remove('temp.pdf')
    return ' '.join(text)

def crawl_and_extract_text(base_url, max_depth, include_pdfs, ignore_epubs):
    visited_urls = set()
    urls_to_visit = [(base_url, 0)]
    processed_urls = []
    all_text = [f'&lt;source type="web_documentation" url="{escape_xml(base_url)}"&gt;']

    while urls_to_visit:
        current_url, current_depth = urls_to_visit.pop(0)
        clean_url = current_url.split('#')[0]

        if clean_url not in visited_urls and is_same_domain(base_url, clean_url) and is_within_depth(base_url, clean_url, max_depth):
            if ignore_epubs and clean_url.endswith('.epub'):
                continue

            try:
                response = requests.get(current_url)
                soup = BeautifulSoup(response.content, 'html.parser')
                visited_urls.add(clean_url)

                if clean_url.endswith('.pdf') and include_pdfs:
                    text = process_pdf(clean_url)
                else:
                    for element in soup(['script', 'style', 'head', 'title', 'meta', '[document]']):
                        element.decompose()
                    comments = soup.find_all(string=lambda text: isinstance(text, Comment))
                    for comment in comments:
                        comment.extract()
                    text = soup.get_text(separator='\n', strip=True)

                all_text.append(f'&lt;page url="{escape_xml(clean_url)}"&gt;')
                all_text.append(escape_xml(text))
                all_text.append('&lt;/page&gt;')
                processed_urls.append(clean_url)
                print(f"Processed: {clean_url}")

                if current_depth &lt; max_depth:
                    for link in soup.find_all('a', href=True):
                        new_url = urljoin(current_url, link['href']).split('#')[0]
                        if new_url not in visited_urls and is_within_depth(base_url, new_url, max_depth) and (include_pdfs or not new_url.endswith('.pdf')) and not (ignore_epubs and new_url.endswith('.epub')):
                            urls_to_visit.append((new_url, current_depth + 1))

            except requests.RequestException as e:
                print(f"Failed to retrieve {clean_url}: {e}")

    all_text.append('&lt;/source&gt;')
    formatted_content = '\n'.join(all_text)

    return {
        'content': formatted_content,
        'processed_urls': processed_urls
    }

def process_doi_or_pmid(identifier):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36',
        'Connection': 'keep-alive'
    }

    try:
        payload = {
            'sci-hub-plugin-check': '',
            'request': identifier
        }

        base_url = 'https://sci-hub.se/'
        response = requests.post(base_url, headers=headers, data=payload, timeout=60)
        soup = BeautifulSoup(response.content, 'html.parser')
        pdf_element = soup.find(id='pdf')

        if pdf_element is None:
            raise ValueError(f"No PDF found for identifier {identifier}. Sci-hub might be inaccessible or the document is not available.")

        content = pdf_element.get('src').replace('#navpanes=0&amp;view=FitH', '').replace('//', '/')

        if content.startswith('/downloads'):
            pdf_url = 'https://sci-hub.se' + content
        elif content.startswith('/tree'):
            pdf_url = 'https://sci-hub.se' + content
        elif content.startswith('/uptodate'):
            pdf_url = 'https://sci-hub.se' + content
        else:
            pdf_url = 'https:/' + content

        pdf_filename = f"{identifier.replace('/', '-')}.pdf"
        wget.download(pdf_url, pdf_filename)

        with open(pdf_filename, 'rb') as pdf_file:
            pdf_reader = PdfReader(pdf_file)
            text = ""
            for page in range(len(pdf_reader.pages)):
                text += pdf_reader.pages[page].extract_text()

        formatted_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n'
        formatted_text += '&lt;paper&gt;\n'
        formatted_text += escape_xml(text)
        formatted_text += '\n&lt;/paper&gt;\n'
        formatted_text += '&lt;/source&gt;'

        os.remove(pdf_filename)
        print(f"Identifier {identifier} processed successfully.")
        return formatted_text
    except (requests.RequestException, ValueError) as e:
        error_text = f'&lt;source type="sci_hub_paper" identifier="{escape_xml(identifier)}"&gt;\n'
        error_text += f'&lt;error&gt;{escape_xml(str(e))}&lt;/error&gt;\n'
        error_text += '&lt;/source&gt;'
        print(f"Error processing identifier {identifier}: {str(e)}")
        print("Sci-hub appears to be inaccessible or the document was not found. Please try again later.")
        return error_text
        
def process_github_pull_request(pull_request_url):
    url_parts = pull_request_url.split("/")
    repo_owner = url_parts[3]
    repo_name = url_parts[4]
    pull_request_number = url_parts[-1]

    api_base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/pulls/{pull_request_number}"
    headers = {"Authorization": f"token {TOKEN}"}

    response = requests.get(api_base_url, headers=headers)
    pull_request_data = response.json()

    diff_url = pull_request_data["diff_url"]
    diff_response = requests.get(diff_url, headers=headers)
    pull_request_diff = diff_response.text

    comments_url = pull_request_data["comments_url"]
    review_comments_url = pull_request_data["review_comments_url"]
    comments_response = requests.get(comments_url, headers=headers)
    review_comments_response = requests.get(review_comments_url, headers=headers)
    comments_data = comments_response.json()
    review_comments_data = review_comments_response.json()

    all_comments = comments_data + review_comments_data
    all_comments.sort(key=lambda comment: comment.get("position") or float("inf"))

    formatted_text = f'&lt;source type="github_pull_request" url="{pull_request_url}"&gt;\n'
    formatted_text += '&lt;pull_request_info&gt;\n'
    formatted_text += f'&lt;title&gt;{escape_xml(pull_request_data["title"])}&lt;/title&gt;\n'
    formatted_text += f'&lt;description&gt;{escape_xml(pull_request_data["body"])}&lt;/description&gt;\n'
    formatted_text += '&lt;merge_details&gt;\n'
    formatted_text += f'{escape_xml(pull_request_data["user"]["login"])} wants to merge {pull_request_data["commits"]} commit into {repo_owner}:{pull_request_data["base"]["ref"]} from {pull_request_data["head"]["label"]}\n'
    formatted_text += '&lt;/merge_details&gt;\n'
    formatted_text += '&lt;diff_and_comments&gt;\n'

    diff_lines = pull_request_diff.split("\n")
    comment_index = 0
    for line in diff_lines:
        formatted_text += f'{escape_xml(line)}\n'
        while comment_index &lt; len(all_comments) and all_comments[comment_index].get("position") == diff_lines.index(line):
            comment = all_comments[comment_index]
            formatted_text += f'&lt;review_comment&gt;\n'
            formatted_text += f'&lt;author&gt;{escape_xml(comment["user"]["login"])}&lt;/author&gt;\n'
            formatted_text += f'&lt;content&gt;{escape_xml(comment["body"])}&lt;/content&gt;\n'
            formatted_text += f'&lt;path&gt;{escape_xml(comment["path"])}&lt;/path&gt;\n'
            formatted_text += f'&lt;line&gt;{comment["original_line"]}&lt;/line&gt;\n'
            formatted_text += '&lt;/review_comment&gt;\n'
            comment_index += 1

    formatted_text += '&lt;/diff_and_comments&gt;\n'
    formatted_text += '&lt;/pull_request_info&gt;\n'

    repo_url = f"https://github.com/{repo_owner}/{repo_name}"
    repo_content = process_github_repo(repo_url)
    
    formatted_text += '&lt;repository&gt;\n'
    formatted_text += repo_content
    formatted_text += '&lt;/repository&gt;\n'
    formatted_text += '&lt;/source&gt;'

    print(f"Pull request {pull_request_number} and repository content processed successfully.")

    return formatted_text
    
def escape_xml(text):
    return (
        str(text)
        .replace("&amp;", "&amp;amp;")
        .replace("&lt;", "&amp;lt;")
        .replace("&gt;", "&amp;gt;")
        # Remove the following lines to stop converting apostrophes and quotes
        # .replace("\"", "&amp;quot;")
        # .replace("'", "&amp;apos;")
    )

def process_github_issue(issue_url):
    url_parts = issue_url.split("/")
    repo_owner = url_parts[3]
    repo_name = url_parts[4]
    issue_number = url_parts[-1]

    api_base_url = f"https://api.github.com/repos/{repo_owner}/{repo_name}/issues/{issue_number}"
    headers = {"Authorization": f"token {TOKEN}"}

    response = requests.get(api_base_url, headers=headers)
    issue_data = response.json()

    comments_url = issue_data["comments_url"]
    comments_response = requests.get(comments_url, headers=headers)
    comments_data = comments_response.json()

    formatted_text = f'&lt;source type="github_issue" url="{issue_url}"&gt;\n'
    formatted_text += '&lt;issue_info&gt;\n'
    formatted_text += f'&lt;title&gt;{escape_xml(issue_data["title"])}&lt;/title&gt;\n'
    formatted_text += f'&lt;description&gt;{escape_xml(issue_data["body"])}&lt;/description&gt;\n'
    formatted_text += '&lt;comments&gt;\n'

    for comment in comments_data:
        formatted_text += '&lt;comment&gt;\n'
        formatted_text += f'&lt;author&gt;{escape_xml(comment["user"]["login"])}&lt;/author&gt;\n'
        formatted_text += f'&lt;content&gt;{escape_xml(comment["body"])}&lt;/content&gt;\n'

        code_snippets = re.findall(r'https://github.com/.*#L\d+-L\d+', comment['body'])
        for snippet_url in code_snippets:
            url_parts = snippet_url.split("#")
            file_url = url_parts[0].replace("/blob/", "/raw/")
            line_range = url_parts[1]
            start_line, end_line = map(int, line_range.split("-")[0][1:]), map(int, line_range.split("-")[1][1:])

            file_response = requests.get(file_url, headers=headers)
            file_content = file_response.text

            code_lines = file_content.split("\n")[start_line-1:end_line]
            code_snippet = "\n".join(code_lines)

            formatted_text += '&lt;code_snippet&gt;\n'
            formatted_text += f'&lt;![CDATA[{code_snippet}]]&gt;\n'
            formatted_text += '&lt;/code_snippet&gt;\n'

        formatted_text += '&lt;/comment&gt;\n'

    formatted_text += '&lt;/comments&gt;\n'
    formatted_text += '&lt;/issue_info&gt;\n'

    repo_url = f"https://github.com/{repo_owner}/{repo_name}"
    repo_content = process_github_repo(repo_url)
    
    formatted_text += '&lt;repository&gt;\n'
    formatted_text += repo_content
    formatted_text += '&lt;/repository&gt;\n'
    formatted_text += '&lt;/source&gt;'

    print(f"Issue {issue_number} and repository content processed successfully.")

    return formatted_text

def main():
    console = Console()

    intro_text = Text("\nInput Paths or URLs Processed:\n", style="dodger_blue1")
    input_types = [
        ("• Local folder path (flattens all files into text)", "bright_white"),
        ("• GitHub repository URL (flattens all files into text)", "bright_white"),
        ("• GitHub pull request URL (PR + Repo)", "bright_white"),
        ("• GitHub issue URL (Issue + Repo)", "bright_white"),
        ("• Documentation URL (base URL)", "bright_white"),
        ("• YouTube video URL (to fetch transcript)", "bright_white"),
        ("• ArXiv Paper URL", "bright_white"),
        ("• DOI or PMID to search on Sci-Hub", "bright_white"),
    ]

    for input_type, color in input_types:
        intro_text.append(f"\n{input_type}", style=color)

    intro_panel = Panel(
        intro_text,
        expand=False,
        border_style="bold",
        title="[bright_white]Copy to File and Clipboard[/bright_white]",
        title_align="center",
        padding=(1, 1),
    )
    console.print(intro_panel)

    if len(sys.argv) &gt; 1:
        input_path = sys.argv[1]
    else:
        input_path = Prompt.ask("\n[bold dodger_blue1]Enter the path or URL[/bold dodger_blue1]", console=console)
    
    console.print(f"\n[bold bright_green]You entered:[/bold bright_green] [bold bright_yellow]{input_path}[/bold bright_yellow]\n")

    output_file = "uncompressed_output.txt"
    processed_file = "compressed_output.txt"
    urls_list_file = "processed_urls.txt"

    with Progress(
        TextColumn("[bold bright_blue]{task.description}"),
        BarColumn(bar_width=None),
        TimeRemainingColumn(),
        console=console,
    ) as progress:

        task = progress.add_task("[bright_blue]Processing...", total=100)

        try:
            if "github.com" in input_path:
                if "/pull/" in input_path:
                    final_output = process_github_pull_request(input_path)
                elif "/issues/" in input_path:
                    final_output = process_github_issue(input_path)
                else:
                    final_output = process_github_repo(input_path)
            elif urlparse(input_path).scheme in ["http", "https"]:
                if "youtube.com" in input_path or "youtu.be" in input_path:
                    final_output = fetch_youtube_transcript(input_path)
                elif "arxiv.org" in input_path:
                    final_output = process_arxiv_pdf(input_path)
                else:
                    crawl_result = crawl_and_extract_text(input_path, max_depth=2, include_pdfs=True, ignore_epubs=True)
                    final_output = crawl_result['content']
                    with open(urls_list_file, 'w', encoding='utf-8') as urls_file:
                        urls_file.write('\n'.join(crawl_result['processed_urls']))
            elif input_path.startswith("10.") and "/" in input_path or input_path.isdigit():
                final_output = process_doi_or_pmid(input_path)
            else:
                final_output = process_local_folder(input_path)

            progress.update(task, advance=50)

            # Write the uncompressed output
            with open(output_file, "w", encoding="utf-8") as file:
                file.write(final_output)


            # Process the compressed output
            preprocess_text(output_file, processed_file)

            progress.update(task, advance=50)

            compressed_text = safe_file_read(processed_file)
            compressed_token_count = get_token_count(compressed_text)
            console.print(f"\n[bright_green]Compressed Token Count:[/bright_green] [bold bright_cyan]{compressed_token_count}[/bold bright_cyan]")

            uncompressed_text = safe_file_read(output_file)
            uncompressed_token_count = get_token_count(uncompressed_text)
            console.print(f"[bright_green]Uncompressed Token Count:[/bright_green] [bold bright_cyan]{uncompressed_token_count}[/bold bright_cyan]")

            console.print(f"\n[bold bright_yellow]{processed_file}[/bold bright_yellow] and [bold bright_blue]{output_file}[/bold bright_blue] have been created in the working directory.")

            pyperclip.copy(uncompressed_text)
            console.print(f"\n[bright_white]The contents of [bold bright_blue]{output_file}[/bold bright_blue] have been copied to the clipboard.[/bright_white]")

        except Exception as e:
            console.print(f"\n[bold red]An error occurred:[/bold red] {str(e)}")
            console.print("\nPlease check your input and try again.")
            raise  # Re-raise the exception for debugging purposes
        
if __name__ == "__main__":
    main()

</file>
<file name="src/python/rag/create_vector_db.py">
import os
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from dotenv import load_dotenv

load_dotenv()
huggingfacehub_api_token = os.getenv('HUGGINGFACEHUB_API_TOKEN')

# Step 1: Read the file
def read_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        return file.read()

# Step 2: Preprocess the data
def preprocess_text(text):
    # Clean and split the text into chunks
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=300)
    return text_splitter.split_text(text)

# Step 3: Vectorization and Indexing
def create_vector_store(docs):
    # Use SentenceTransformer for embeddings
    # model = SentenceTransformer('all-MiniLM-L6-v2')
    # embeddings = model.encode(docs) #, convert_to_tensor=True)  # Generate embeddings for all documents
    embeddings = HuggingFaceEmbeddings()
    text_embeddings = embeddings.embed_documents(docs)
    text_embedding_pairs = zip(docs, text_embeddings)
    vector_store = FAISS.from_embeddings(text_embedding_pairs, embeddings)

    vector_store.save_local("faiss_AiDoc")  # Save the index locally
    return vector_store

# Main function to run the program
def main():
    file_path = 'uncompressed_output.txt'
    text = read_file(file_path)
    docs = preprocess_text(text)
    vector_store = create_vector_store(docs)


if __name__ == "__main__":
    main()
</file>
<file name="src/python/rag/generic_functions.py">
import requests
import os
import re

from nbconvert import PythonExporter
import nbformat
from nltk.corpus import stopwords
import nltk


def is_allowed_filetype(filename):
    allowed_extensions = ['.py', '.txt', '.js', '.tsx', '.ts', '.md', '.cjs', '.html', '.json', '.ipynb', '.h', '.localhost', '.sh', '.yaml', '.example', '.ps1', '.sql']
    return any(filename.endswith(ext) for ext in allowed_extensions)
    

def download_file(url, target_path, headers):
    response = requests.get(url, headers=headers)
    response.raise_for_status()
    with open(target_path, "wb") as f:
        f.write(response.content)


def escape_xml(text):
    return (
        str(text)
        .replace("&amp;", "&amp;amp;")
        .replace("&lt;", "&amp;lt;")
        .replace("&gt;", "&amp;gt;")
        # Remove the following lines to stop converting apostrophes and quotes
        # .replace("\"", "&amp;quot;")
        # .replace("'", "&amp;apos;")
    )


def process_ipynb_file(temp_file):
    with open(temp_file, "r", encoding='utf-8', errors='ignore') as f:
        notebook_content = f.read()

    exporter = PythonExporter()
    python_code, _ = exporter.from_notebook_node(nbformat.reads(notebook_content, as_version=4))
    return python_code




## NEW: TO CHECK
def is_same_domain(base_url, new_url):
    return urlparse(base_url).netloc == urlparse(new_url).netloc

def is_within_depth(base_url, current_url, max_depth):
    base_parts = urlparse(base_url).path.rstrip('/').split('/')
    current_parts = urlparse(current_url).path.rstrip('/').split('/')

    if current_parts[:len(base_parts)] != base_parts:
        return False

    return len(current_parts) - len(base_parts) &lt;= max_depth


def extract_links(input_file, output_file):
    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')
    
    with open(input_file, 'r', encoding='utf-8') as file:
        content = file.read()
        urls = re.findall(url_pattern, content)
    
    with open(output_file, 'w', encoding='utf-8') as output:
        for url in urls:
            output.write(url + '\n')

def get_stopword_list():
    nltk.download("stopwords", quiet=True)
    stop_words = set(stopwords.words("english"))
    return stop_words
</file>
<file name="src/python/rag/prompt_llm.py">
from langchain_huggingface import HuggingFaceEmbeddings, HuggingFaceEndpoint
from langchain_community.vectorstores import FAISS

# Step 1: Set up the LLM
def load_llm():
    return HuggingFaceEndpoint(repo_id="mistralai/Mixtral-8x7B-Instruct-v0.1")

# Step 2: Query function
def query_llm(llm, vector_store, user_query):
    retriever = vector_store.as_retriever()
    # context = retriever.retrieve(user_query)
    # Retrieve relevant documents
    relevant_docs = retriever.invoke(user_query)

    # Combine the context with the user query
    context = " ".join(doc.page_content for doc in relevant_docs)
    response = llm.generate([context + user_query])
    return response


# Main function to run the program
def main():
    user_query = input("Ask a question: \n")
    llm = load_llm()
    embeddings = HuggingFaceEmbeddings()
    vector_store = FAISS.load_local("faiss_AiDoc", embeddings, allow_dangerous_deserialization=True)
    answer = query_llm(llm, vector_store, user_query)
    print("Answer:", answer)

if __name__ == "__main__":
    main()
</file>
</source>